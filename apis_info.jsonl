{"name": "flask_login.login_user", "type": "callable", "signature": "(user, remember=False, duration=None, force=False, fresh=True)", "docstring": "Logs a user in. You should pass the actual user object to this. If the\nuser's `is_active` property is ``False``, they will not be logged in\nunless `force` is ``True``.\n\nThis will return ``True`` if the log in attempt succeeds, and ``False`` if\nit fails (i.e. because the user is inactive).\n\n:param user: The user object to log in.\n:type user: object\n:param remember: Whether to remember the user after their session expires.\n    Defaults to ``False``.\n:type remember: bool\n:param duration: The amount of time before the remember cookie expires. If\n    ``None`` the value set in the settings is used. Defaults to ``None``.\n:type duration: :class:`datetime.timedelta`\n:param force: If the user is inactive, setting this to ``True`` will log\n    them in regardless. Defaults to ``False``.\n:type force: bool\n:param fresh: setting this to ``False`` will log in the user with a session\n    marked as not \"fresh\". Defaults to ``True``.\n:type fresh: bool", "short_docstring": "Logs a user in. You should pass the actual user object to this. If the\nuser's `is_active` property is ``False``, they will not be logged in\nunless `force` is ``True``."}
{"name": "pandas.DataFrame", "type": "class", "signature": "(data=None, index: 'Axes | None' = None, columns: 'Axes | None' = None, dtype: 'Dtype | None' = None, copy: 'bool | None' = None) -> 'None'", "docstring": "Two-dimensional, size-mutable, potentially heterogeneous tabular data.\n\nData structure also contains labeled axes (rows and columns).\nArithmetic operations align on both row and column labels. Can be\nthought of as a dict-like container for Series objects. The primary\npandas data structure.\n\nParameters\n----------\ndata : ndarray (structured or homogeneous), Iterable, dict, or DataFrame\n    Dict can contain Series, arrays, constants, dataclass or list-like objects. If\n    data is a dict, column order follows insertion-order. If a dict contains Series\n    which have an index defined, it is aligned by its index. This alignment also\n    occurs if data is a Series or a DataFrame itself. Alignment is done on\n    Series/DataFrame inputs.\n\n    If data is a list of dicts, column order follows insertion-order.\n\nindex : Index or array-like\n    Index to use for resulting frame. Will default to RangeIndex if\n    no indexing information part of input data and no index provided.\ncolumns : Index or array-like\n    Column labels to use for resulting frame when data does not have them,\n    defaulting to RangeIndex(0, 1, 2, ..., n). If data contains column labels,\n    will perform column selection instead.\ndtype : dtype, default None\n    Data type to force. Only a single dtype is allowed. If None, infer.\ncopy : bool or None, default None\n    Copy data from inputs.\n    For dict data, the default of None behaves like ``copy=True``.  For DataFrame\n    or 2d ndarray input, the default of None behaves like ``copy=False``.\n    If data is a dict containing one or more Series (possibly of different dtypes),\n    ``copy=False`` will ensure that these inputs are not copied.\n\n    .. versionchanged:: 1.3.0\n\nSee Also\n--------\nDataFrame.from_records : Constructor from tuples, also record arrays.\nDataFrame.from_dict : From dicts of Series, arrays, or dicts.\nread_csv : Read a comma-separated values (csv) file into DataFrame.\nread_table : Read general delimited file into DataFrame.\nread_clipboard : Read text from clipboard into DataFrame.\n\nNotes\n-----\nPlease reference the :ref:`User Guide <basics.dataframe>` for more information.\n\nExamples\n--------\nConstructing DataFrame from a dictionary.\n\n>>> d = {'col1': [1, 2], 'col2': [3, 4]}\n>>> df = pd.DataFrame(data=d)\n>>> df\n   col1  col2\n0     1     3\n1     2     4\n\nNotice that the inferred dtype is int64.\n\n>>> df.dtypes\ncol1    int64\ncol2    int64\ndtype: object\n\nTo enforce a single dtype:\n\n>>> df = pd.DataFrame(data=d, dtype=np.int8)\n>>> df.dtypes\ncol1    int8\ncol2    int8\ndtype: object\n\nConstructing DataFrame from a dictionary including Series:\n\n>>> d = {'col1': [0, 1, 2, 3], 'col2': pd.Series([2, 3], index=[2, 3])}\n>>> pd.DataFrame(data=d, index=[0, 1, 2, 3])\n   col1  col2\n0     0   NaN\n1     1   NaN\n2     2   2.0\n3     3   3.0\n\nConstructing DataFrame from numpy ndarray:\n\n>>> df2 = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),\n...                    columns=['a', 'b', 'c'])\n>>> df2\n   a  b  c\n0  1  2  3\n1  4  5  6\n2  7  8  9\n\nConstructing DataFrame from a numpy ndarray that has labeled columns:\n\n>>> data = np.array([(1, 2, 3), (4, 5, 6), (7, 8, 9)],\n...                 dtype=[(\"a\", \"i4\"), (\"b\", \"i4\"), (\"c\", \"i4\")])\n>>> df3 = pd.DataFrame(data, columns=['c', 'a'])\n...\n>>> df3\n   c  a\n0  3  1\n1  6  4\n2  9  7\n\nConstructing DataFrame from dataclass:\n\n>>> from dataclasses import make_dataclass\n>>> Point = make_dataclass(\"Point\", [(\"x\", int), (\"y\", int)])\n>>> pd.DataFrame([Point(0, 0), Point(0, 3), Point(2, 3)])\n   x  y\n0  0  0\n1  0  3\n2  2  3\n\nConstructing DataFrame from Series/DataFrame:\n\n>>> ser = pd.Series([1, 2, 3], index=[\"a\", \"b\", \"c\"])\n>>> df = pd.DataFrame(data=ser, index=[\"a\", \"c\"])\n>>> df\n   0\na  1\nc  3\n\n>>> df1 = pd.DataFrame([1, 2, 3], index=[\"a\", \"b\", \"c\"], columns=[\"x\"])\n>>> df2 = pd.DataFrame(data=df1, index=[\"a\", \"c\"])\n>>> df2\n   x\na  1\nc  3", "short_docstring": "Two-dimensional, size-mutable, potentially heterogeneous tabular data."}
{"name": "datetime.datetime.datetime", "type": "class", "signature": null, "docstring": "datetime(year, month, day[, hour[, minute[, second[, microsecond[,tzinfo]]]]])\n\nThe year, month and day arguments are required. tzinfo may be None, or an\ninstance of a tzinfo subclass. The remaining arguments may be ints.", "short_docstring": "datetime(year, month, day[, hour[, minute[, second[, microsecond[,tzinfo]]]]])"}
{"name": "unicodedata.normalize", "type": "callable", "signature": "(form, unistr, /)", "docstring": "Return the normal form 'form' for the Unicode string unistr.\n\nValid values for form are 'NFC', 'NFKC', 'NFD', and 'NFKD'.", "short_docstring": "Return the normal form 'form' for the Unicode string unistr."}
{"name": "json.dump", "type": "callable", "signature": "(obj, fp, *, skipkeys=False, ensure_ascii=True, check_circular=True, allow_nan=True, cls=None, indent=None, separators=None, default=None, sort_keys=False, **kw)", "docstring": "Serialize ``obj`` as a JSON formatted stream to ``fp`` (a\n``.write()``-supporting file-like object).\n\nIf ``skipkeys`` is true then ``dict`` keys that are not basic types\n(``str``, ``int``, ``float``, ``bool``, ``None``) will be skipped\ninstead of raising a ``TypeError``.\n\nIf ``ensure_ascii`` is false, then the strings written to ``fp`` can\ncontain non-ASCII characters if they appear in strings contained in\n``obj``. Otherwise, all such characters are escaped in JSON strings.\n\nIf ``check_circular`` is false, then the circular reference check\nfor container types will be skipped and a circular reference will\nresult in an ``OverflowError`` (or worse).\n\nIf ``allow_nan`` is false, then it will be a ``ValueError`` to\nserialize out of range ``float`` values (``nan``, ``inf``, ``-inf``)\nin strict compliance of the JSON specification, instead of using the\nJavaScript equivalents (``NaN``, ``Infinity``, ``-Infinity``).\n\nIf ``indent`` is a non-negative integer, then JSON array elements and\nobject members will be pretty-printed with that indent level. An indent\nlevel of 0 will only insert newlines. ``None`` is the most compact\nrepresentation.\n\nIf specified, ``separators`` should be an ``(item_separator, key_separator)``\ntuple.  The default is ``(', ', ': ')`` if *indent* is ``None`` and\n``(',', ': ')`` otherwise.  To get the most compact JSON representation,\nyou should specify ``(',', ':')`` to eliminate whitespace.\n\n``default(obj)`` is a function that should return a serializable version\nof obj or raise TypeError. The default simply raises TypeError.\n\nIf *sort_keys* is true (default: ``False``), then the output of\ndictionaries will be sorted by key.\n\nTo use a custom ``JSONEncoder`` subclass (e.g. one that overrides the\n``.default()`` method to serialize additional types), specify it with\nthe ``cls`` kwarg; otherwise ``JSONEncoder`` is used.", "short_docstring": "Serialize ``obj`` as a JSON formatted stream to ``fp`` (a\n``.write()``-supporting file-like object)."}
{"name": "numpy.max", "type": "callable", "signature": "(a, axis=None, out=None, keepdims=<no value>, initial=<no value>, where=<no value>)", "docstring": "Return the maximum of an array or maximum along an axis.\n\nParameters\n----------\na : array_like\n    Input data.\naxis : None or int or tuple of ints, optional\n    Axis or axes along which to operate.  By default, flattened input is\n    used.\n\n    .. versionadded:: 1.7.0\n\n    If this is a tuple of ints, the maximum is selected over multiple axes,\n    instead of a single axis or all the axes as before.\nout : ndarray, optional\n    Alternative output array in which to place the result.  Must\n    be of the same shape and buffer length as the expected output.\n    See :ref:`ufuncs-output-type` for more details.\n\nkeepdims : bool, optional\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\n    If the default value is passed, then `keepdims` will not be\n    passed through to the `amax` method of sub-classes of\n    `ndarray`, however any non-default value will be.  If the\n    sub-class' method does not implement `keepdims` any\n    exceptions will be raised.\n\ninitial : scalar, optional\n    The minimum value of an output element. Must be present to allow\n    computation on empty slice. See `~numpy.ufunc.reduce` for details.\n\n    .. versionadded:: 1.15.0\n\nwhere : array_like of bool, optional\n    Elements to compare for the maximum. See `~numpy.ufunc.reduce`\n    for details.\n\n    .. versionadded:: 1.17.0\n\nReturns\n-------\namax : ndarray or scalar\n    Maximum of `a`. If `axis` is None, the result is a scalar value.\n    If `axis` is given, the result is an array of dimension\n    ``a.ndim - 1``.\n\nSee Also\n--------\namin :\n    The minimum value of an array along a given axis, propagating any NaNs.\nnanmax :\n    The maximum value of an array along a given axis, ignoring any NaNs.\nmaximum :\n    Element-wise maximum of two arrays, propagating any NaNs.\nfmax :\n    Element-wise maximum of two arrays, ignoring any NaNs.\nargmax :\n    Return the indices of the maximum values.\n\nnanmin, minimum, fmin\n\nNotes\n-----\nNaN values are propagated, that is if at least one item is NaN, the\ncorresponding max value will be NaN as well. To ignore NaN values\n(MATLAB behavior), please use nanmax.\n\nDon't use `amax` for element-wise comparison of 2 arrays; when\n``a.shape[0]`` is 2, ``maximum(a[0], a[1])`` is faster than\n``amax(a, axis=0)``.\n\nExamples\n--------\n>>> a = np.arange(4).reshape((2,2))\n>>> a\narray([[0, 1],\n       [2, 3]])\n>>> np.amax(a)           # Maximum of the flattened array\n3\n>>> np.amax(a, axis=0)   # Maxima along the first axis\narray([2, 3])\n>>> np.amax(a, axis=1)   # Maxima along the second axis\narray([1, 3])\n>>> np.amax(a, where=[False, True], initial=-1, axis=0)\narray([-1,  3])\n>>> b = np.arange(5, dtype=float)\n>>> b[2] = np.NaN\n>>> np.amax(b)\nnan\n>>> np.amax(b, where=~np.isnan(b), initial=-1)\n4.0\n>>> np.nanmax(b)\n4.0\n\nYou can use an initial value to compute the maximum of an empty slice, or\nto initialize it to a different value:\n\n>>> np.amax([[-50], [10]], axis=-1, initial=0)\narray([ 0, 10])\n\nNotice that the initial value is used as one of the elements for which the\nmaximum is determined, unlike for the default argument Python's max\nfunction, which is only used for empty iterables.\n\n>>> np.amax([5], initial=6)\n6\n>>> max([5], default=6)\n5", "short_docstring": "Return the maximum of an array or maximum along an axis."}
{"name": "os.urandom", "type": "callable", "signature": "(size, /)", "docstring": "Return a bytes object containing random bytes suitable for cryptographic use.", "short_docstring": "Return a bytes object containing random bytes suitable for cryptographic use."}
{"name": "numpy.random.randint", "type": "callable", "signature": null, "docstring": "randint(low, high=None, size=None, dtype=int)\n\nReturn random integers from `low` (inclusive) to `high` (exclusive).\n\nReturn random integers from the \"discrete uniform\" distribution of\nthe specified dtype in the \"half-open\" interval [`low`, `high`). If\n`high` is None (the default), then results are from [0, `low`).\n\n.. note::\n    New code should use the ``integers`` method of a ``default_rng()``\n    instance instead; please see the :ref:`random-quick-start`.\n\nParameters\n----------\nlow : int or array-like of ints\n    Lowest (signed) integers to be drawn from the distribution (unless\n    ``high=None``, in which case this parameter is one above the\n    *highest* such integer).\nhigh : int or array-like of ints, optional\n    If provided, one above the largest (signed) integer to be drawn\n    from the distribution (see above for behavior if ``high=None``).\n    If array-like, must contain integer values\nsize : int or tuple of ints, optional\n    Output shape.  If the given shape is, e.g., ``(m, n, k)``, then\n    ``m * n * k`` samples are drawn.  Default is None, in which case a\n    single value is returned.\ndtype : dtype, optional\n    Desired dtype of the result. Byteorder must be native.\n    The default value is int.\n\n    .. versionadded:: 1.11.0\n\nReturns\n-------\nout : int or ndarray of ints\n    `size`-shaped array of random integers from the appropriate\n    distribution, or a single such random int if `size` not provided.\n\nSee Also\n--------\nrandom_integers : similar to `randint`, only for the closed\n    interval [`low`, `high`], and 1 is the lowest value if `high` is\n    omitted.\nrandom.Generator.integers: which should be used for new code.\n\nExamples\n--------\n>>> np.random.randint(2, size=10)\narray([1, 0, 0, 0, 1, 1, 0, 0, 1, 0]) # random\n>>> np.random.randint(1, size=10)\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n\nGenerate a 2 x 4 array of ints between 0 and 4, inclusive:\n\n>>> np.random.randint(5, size=(2, 4))\narray([[4, 0, 2, 1], # random\n       [3, 2, 2, 0]])\n\nGenerate a 1 x 3 array with 3 different upper bounds\n\n>>> np.random.randint(1, [3, 5, 10])\narray([2, 2, 9]) # random\n\nGenerate a 1 by 3 array with 3 different lower bounds\n\n>>> np.random.randint([1, 5, 7], 10)\narray([9, 8, 7]) # random\n\nGenerate a 2 by 4 array using broadcasting with dtype of uint8\n\n>>> np.random.randint([1, 3, 5, 7], [[10], [20]], dtype=np.uint8)\narray([[ 8,  6,  9,  7], # random\n       [ 1, 16,  9, 12]], dtype=uint8)", "short_docstring": "randint(low, high=None, size=None, dtype=int)"}
{"name": "requests.RequestException", "type": "class", "signature": "(*args, **kwargs)", "docstring": "There was an ambiguous exception that occurred while handling your\nrequest.", "short_docstring": "There was an ambiguous exception that occurred while handling your\nrequest."}
{"name": "cryptography.hazmat.primitives.ciphers.Cipher", "type": "class", "signature": "(algorithm: cryptography.hazmat.primitives._cipheralgorithm.CipherAlgorithm, mode: Optional[cryptography.hazmat.primitives.ciphers.modes.Mode], backend=None)", "docstring": null, "short_docstring": ""}
{"name": "numpy.random.randn", "type": "callable", "signature": null, "docstring": "randn(d0, d1, ..., dn)\n\nReturn a sample (or samples) from the \"standard normal\" distribution.\n\n.. note::\n    This is a convenience function for users porting code from Matlab,\n    and wraps `standard_normal`. That function takes a\n    tuple to specify the size of the output, which is consistent with\n    other NumPy functions like `numpy.zeros` and `numpy.ones`.\n\n.. note::\n    New code should use the ``standard_normal`` method of a ``default_rng()``\n    instance instead; please see the :ref:`random-quick-start`.\n\nIf positive int_like arguments are provided, `randn` generates an array\nof shape ``(d0, d1, ..., dn)``, filled\nwith random floats sampled from a univariate \"normal\" (Gaussian)\ndistribution of mean 0 and variance 1. A single float randomly sampled\nfrom the distribution is returned if no argument is provided.\n\nParameters\n----------\nd0, d1, ..., dn : int, optional\n    The dimensions of the returned array, must be non-negative.\n    If no argument is given a single Python float is returned.\n\nReturns\n-------\nZ : ndarray or float\n    A ``(d0, d1, ..., dn)``-shaped array of floating-point samples from\n    the standard normal distribution, or a single such float if\n    no parameters were supplied.\n\nSee Also\n--------\nstandard_normal : Similar, but takes a tuple as its argument.\nnormal : Also accepts mu and sigma arguments.\nrandom.Generator.standard_normal: which should be used for new code.\n\nNotes\n-----\nFor random samples from :math:`N(\\mu, \\sigma^2)`, use:\n\n``sigma * np.random.randn(...) + mu``\n\nExamples\n--------\n>>> np.random.randn()\n2.1923875335537315  # random\n\nTwo-by-four array of samples from N(3, 6.25):\n\n>>> 3 + 2.5 * np.random.randn(2, 4)\narray([[-4.49401501,  4.00950034, -1.81814867,  7.29718677],   # random\n       [ 0.39924804,  4.68456316,  4.99394529,  4.84057254]])  # random", "short_docstring": "randn(d0, d1, ..., dn)"}
{"name": "numpy.sum", "type": "callable", "signature": "(a, axis=None, dtype=None, out=None, keepdims=<no value>, initial=<no value>, where=<no value>)", "docstring": "Sum of array elements over a given axis.\n\nParameters\n----------\na : array_like\n    Elements to sum.\naxis : None or int or tuple of ints, optional\n    Axis or axes along which a sum is performed.  The default,\n    axis=None, will sum all of the elements of the input array.  If\n    axis is negative it counts from the last to the first axis.\n\n    .. versionadded:: 1.7.0\n\n    If axis is a tuple of ints, a sum is performed on all of the axes\n    specified in the tuple instead of a single axis or all the axes as\n    before.\ndtype : dtype, optional\n    The type of the returned array and of the accumulator in which the\n    elements are summed.  The dtype of `a` is used by default unless `a`\n    has an integer dtype of less precision than the default platform\n    integer.  In that case, if `a` is signed then the platform integer\n    is used while if `a` is unsigned then an unsigned integer of the\n    same precision as the platform integer is used.\nout : ndarray, optional\n    Alternative output array in which to place the result. It must have\n    the same shape as the expected output, but the type of the output\n    values will be cast if necessary.\nkeepdims : bool, optional\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\n    If the default value is passed, then `keepdims` will not be\n    passed through to the `sum` method of sub-classes of\n    `ndarray`, however any non-default value will be.  If the\n    sub-class' method does not implement `keepdims` any\n    exceptions will be raised.\ninitial : scalar, optional\n    Starting value for the sum. See `~numpy.ufunc.reduce` for details.\n\n    .. versionadded:: 1.15.0\n\nwhere : array_like of bool, optional\n    Elements to include in the sum. See `~numpy.ufunc.reduce` for details.\n\n    .. versionadded:: 1.17.0\n\nReturns\n-------\nsum_along_axis : ndarray\n    An array with the same shape as `a`, with the specified\n    axis removed.   If `a` is a 0-d array, or if `axis` is None, a scalar\n    is returned.  If an output array is specified, a reference to\n    `out` is returned.\n\nSee Also\n--------\nndarray.sum : Equivalent method.\n\nadd.reduce : Equivalent functionality of `add`.\n\ncumsum : Cumulative sum of array elements.\n\ntrapz : Integration of array values using the composite trapezoidal rule.\n\nmean, average\n\nNotes\n-----\nArithmetic is modular when using integer types, and no error is\nraised on overflow.\n\nThe sum of an empty array is the neutral element 0:\n\n>>> np.sum([])\n0.0\n\nFor floating point numbers the numerical precision of sum (and\n``np.add.reduce``) is in general limited by directly adding each number\nindividually to the result causing rounding errors in every step.\nHowever, often numpy will use a  numerically better approach (partial\npairwise summation) leading to improved precision in many use-cases.\nThis improved precision is always provided when no ``axis`` is given.\nWhen ``axis`` is given, it will depend on which axis is summed.\nTechnically, to provide the best speed possible, the improved precision\nis only used when the summation is along the fast axis in memory.\nNote that the exact precision may vary depending on other parameters.\nIn contrast to NumPy, Python's ``math.fsum`` function uses a slower but\nmore precise approach to summation.\nEspecially when summing a large number of lower precision floating point\nnumbers, such as ``float32``, numerical errors can become significant.\nIn such cases it can be advisable to use `dtype=\"float64\"` to use a higher\nprecision for the output.\n\nExamples\n--------\n>>> np.sum([0.5, 1.5])\n2.0\n>>> np.sum([0.5, 0.7, 0.2, 1.5], dtype=np.int32)\n1\n>>> np.sum([[0, 1], [0, 5]])\n6\n>>> np.sum([[0, 1], [0, 5]], axis=0)\narray([0, 6])\n>>> np.sum([[0, 1], [0, 5]], axis=1)\narray([1, 5])\n>>> np.sum([[0, 1], [np.nan, 5]], where=[False, True], axis=1)\narray([1., 5.])\n\nIf the accumulator is too small, overflow occurs:\n\n>>> np.ones(128, dtype=np.int8).sum(dtype=np.int8)\n-128\n\nYou can also start the sum with a value other than zero:\n\n>>> np.sum([10], initial=5)\n15", "short_docstring": "Sum of array elements over a given axis."}
{"name": "flask_login.LoginManager", "type": "class", "signature": "(app=None, add_context_processor=True)", "docstring": "This object is used to hold the settings used for logging in. Instances\nof :class:`LoginManager` are *not* bound to specific apps, so you can\ncreate one in the main body of your code and then bind it to your\napp in a factory function.", "short_docstring": "This object is used to hold the settings used for logging in. Instances\nof :class:`LoginManager` are *not* bound to specific apps, so you can\ncreate one in the main body of your code and then bind it to your\napp in a factory function."}
{"name": "numpy.nanmedian", "type": "callable", "signature": "(a, axis=None, out=None, overwrite_input=False, keepdims=<no value>)", "docstring": "Compute the median along the specified axis, while ignoring NaNs.\n\nReturns the median of the array elements.\n\n.. versionadded:: 1.9.0\n\nParameters\n----------\na : array_like\n    Input array or object that can be converted to an array.\naxis : {int, sequence of int, None}, optional\n    Axis or axes along which the medians are computed. The default\n    is to compute the median along a flattened version of the array.\n    A sequence of axes is supported since version 1.9.0.\nout : ndarray, optional\n    Alternative output array in which to place the result. It must\n    have the same shape and buffer length as the expected output,\n    but the type (of the output) will be cast if necessary.\noverwrite_input : bool, optional\n   If True, then allow use of memory of input array `a` for\n   calculations. The input array will be modified by the call to\n   `median`. This will save memory when you do not need to preserve\n   the contents of the input array. Treat the input as undefined,\n   but it will probably be fully or partially sorted. Default is\n   False. If `overwrite_input` is ``True`` and `a` is not already an\n   `ndarray`, an error will be raised.\nkeepdims : bool, optional\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the original `a`.\n\n    If this is anything but the default value it will be passed\n    through (in the special case of an empty array) to the\n    `mean` function of the underlying array.  If the array is\n    a sub-class and `mean` does not have the kwarg `keepdims` this\n    will raise a RuntimeError.\n\nReturns\n-------\nmedian : ndarray\n    A new array holding the result. If the input contains integers\n    or floats smaller than ``float64``, then the output data-type is\n    ``np.float64``.  Otherwise, the data-type of the output is the\n    same as that of the input. If `out` is specified, that array is\n    returned instead.\n\nSee Also\n--------\nmean, median, percentile\n\nNotes\n-----\nGiven a vector ``V`` of length ``N``, the median of ``V`` is the\nmiddle value of a sorted copy of ``V``, ``V_sorted`` - i.e.,\n``V_sorted[(N-1)/2]``, when ``N`` is odd and the average of the two\nmiddle values of ``V_sorted`` when ``N`` is even.\n\nExamples\n--------\n>>> a = np.array([[10.0, 7, 4], [3, 2, 1]])\n>>> a[0, 1] = np.nan\n>>> a\narray([[10., nan,  4.],\n       [ 3.,  2.,  1.]])\n>>> np.median(a)\nnan\n>>> np.nanmedian(a)\n3.0\n>>> np.nanmedian(a, axis=0)\narray([6.5, 2. , 2.5])\n>>> np.median(a, axis=1)\narray([nan,  2.])\n>>> b = a.copy()\n>>> np.nanmedian(b, axis=1, overwrite_input=True)\narray([7.,  2.])\n>>> assert not np.all(a==b)\n>>> b = a.copy()\n>>> np.nanmedian(b, axis=None, overwrite_input=True)\n3.0\n>>> assert not np.all(a==b)", "short_docstring": "Compute the median along the specified axis, while ignoring NaNs."}
{"name": "Levenshtein.ratio", "type": "callable", "signature": null, "docstring": "Compute similarity of two strings.\n\nratio(string1, string2)\n\nThe similarity is a number between 0 and 1, it's usually equal or\nsomewhat higher than difflib.SequenceMatcher.ratio(), because it's\nbased on real minimal edit distance.\n\nExamples:\n\n>>> ratio('Hello world!', 'Holly grail!')  # doctest: +ELLIPSIS\n0.583333...\n>>> ratio('Brian', 'Jesus')\n0.0\n\nReally?  I thought there was some similarity.", "short_docstring": "Compute similarity of two strings."}
{"name": "flask_login.UserMixin", "type": "class", "signature": "()", "docstring": "This provides default implementations for the methods that Flask-Login\nexpects user objects to have.", "short_docstring": "This provides default implementations for the methods that Flask-Login\nexpects user objects to have."}
{"name": "hashlib.md5", "type": "callable", "signature": "(string=b'', *, usedforsecurity=True)", "docstring": "Returns a md5 hash object; optionally initialized with a string", "short_docstring": "Returns a md5 hash object; optionally initialized with a string"}
{"name": "psutil.ZombieProcess", "type": "class", "signature": "(pid, name=None, ppid=None, msg=None)", "docstring": "Exception raised when querying a zombie process. This is\nraised on macOS, BSD and Solaris only, and not always: depending\non the query the OS may be able to succeed anyway.\nOn Linux all zombie processes are querable (hence this is never\nraised). Windows doesn't have zombie processes.", "short_docstring": "Exception raised when querying a zombie process. This is\nraised on macOS, BSD and Solaris only, and not always: depending\non the query the OS may be able to succeed anyway.\nOn Linux all zombie processes are querable (hence this is never\nraised). Windows doesn't have zombie processes."}
{"name": "re.compile", "type": "callable", "signature": "(pattern, flags=0)", "docstring": "Compile a regular expression pattern, returning a Pattern object.", "short_docstring": "Compile a regular expression pattern, returning a Pattern object."}
{"name": "statsmodels.tsa.arima.model.ARIMA", "type": "class", "signature": "(endog, exog=None, order=(0, 0, 0), seasonal_order=(0, 0, 0, 0), trend=None, enforce_stationarity=True, enforce_invertibility=True, concentrate_scale=False, trend_offset=1, dates=None, freq=None, missing='none', validate_specification=True)", "docstring": "Autoregressive Integrated Moving Average (ARIMA) model, and extensions\n\nThis model is the basic interface for ARIMA-type models, including those\nwith exogenous regressors and those with seasonal components. The most\ngeneral form of the model is SARIMAX(p, d, q)x(P, D, Q, s). It also allows\nall specialized cases, including\n\n- autoregressive models: AR(p)\n- moving average models: MA(q)\n- mixed autoregressive moving average models: ARMA(p, q)\n- integration models: ARIMA(p, d, q)\n- seasonal models: SARIMA(P, D, Q, s)\n- regression with errors that follow one of the above ARIMA-type models\n\nParameters\n----------\nendog : array_like, optional\n    The observed time-series process :math:`y`.\nexog : array_like, optional\n    Array of exogenous regressors.\norder : tuple, optional\n    The (p,d,q) order of the model for the autoregressive, differences, and\n    moving average components. d is always an integer, while p and q may\n    either be integers or lists of integers.\nseasonal_order : tuple, optional\n    The (P,D,Q,s) order of the seasonal component of the model for the\n    AR parameters, differences, MA parameters, and periodicity. Default\n    is (0, 0, 0, 0). D and s are always integers, while P and Q\n    may either be integers or lists of positive integers.\ntrend : str{'n','c','t','ct'} or iterable, optional\n    Parameter controlling the deterministic trend. Can be specified as a\n    string where 'c' indicates a constant term, 't' indicates a\n    linear trend in time, and 'ct' includes both. Can also be specified as\n    an iterable defining a polynomial, as in `numpy.poly1d`, where\n    `[1,1,0,1]` would denote :math:`a + bt + ct^3`. Default is 'c' for\n    models without integration, and no trend for models with integration.\n    Note that all trend terms are included in the model as exogenous\n    regressors, which differs from how trends are included in ``SARIMAX``\n    models.  See the Notes section for a precise definition of the\n    treatment of trend terms.\nenforce_stationarity : bool, optional\n    Whether or not to require the autoregressive parameters to correspond\n    to a stationarity process.\nenforce_invertibility : bool, optional\n    Whether or not to require the moving average parameters to correspond\n    to an invertible process.\nconcentrate_scale : bool, optional\n    Whether or not to concentrate the scale (variance of the error term)\n    out of the likelihood. This reduces the number of parameters by one.\n    This is only applicable when considering estimation by numerical\n    maximum likelihood.\ntrend_offset : int, optional\n    The offset at which to start time trend values. Default is 1, so that\n    if `trend='t'` the trend is equal to 1, 2, ..., nobs. Typically is only\n    set when the model created by extending a previous dataset.\ndates : array_like of datetime, optional\n    If no index is given by `endog` or `exog`, an array-like object of\n    datetime objects can be provided.\nfreq : str, optional\n    If no index is given by `endog` or `exog`, the frequency of the\n    time-series may be specified here as a Pandas offset or offset string.\nmissing : str\n    Available options are 'none', 'drop', and 'raise'. If 'none', no nan\n    checking is done. If 'drop', any observations with nans are dropped.\n    If 'raise', an error is raised. Default is 'none'.\n\nNotes\n-----\nThis model incorporates both exogenous regressors and trend components\nthrough \"regression with ARIMA errors\". This differs from the\nspecification estimated using ``SARIMAX`` which treats the trend\ncomponents separately from any included exogenous regressors. The full\nspecification of the model estimated here is:\n\n.. math::\n\n    Y_{t}-\\delta_{0}-\\delta_{1}t-\\ldots-\\delta_{k}t^{k}-X_{t}\\beta\n        & =\\epsilon_{t} \\\\\n    \\left(1-L\\right)^{d}\\left(1-L^{s}\\right)^{D}\\Phi\\left(L\\right)\n    \\Phi_{s}\\left(L\\right)\\epsilon_{t}\n        & =\\Theta\\left(L\\right)\\Theta_{s}\\left(L\\right)\\eta_{t}\n\nwhere :math:`\\eta_t \\sim WN(0,\\sigma^2)` is a white noise process, L\nis the lag operator, and :math:`G(L)` are lag polynomials corresponding\nto the autoregressive (:math:`\\Phi`), seasonal autoregressive\n(:math:`\\Phi_s`), moving average (:math:`\\Theta`), and seasonal moving\naverage components (:math:`\\Theta_s`).\n\n`enforce_stationarity` and `enforce_invertibility` are specified in the\nconstructor because they affect loglikelihood computations, and so should\nnot be changed on the fly. This is why they are not instead included as\narguments to the `fit` method.\n\nSee the notebook `ARMA: Sunspots Data\n<../examples/notebooks/generated/tsa_arma_0.html>`__ and\n`ARMA: Artificial Data <../examples/notebooks/generated/tsa_arma_1.html>`__\nfor an overview.\n\n.. todo:: should concentrate_scale=True by default\n\nExamples\n--------\n>>> mod = sm.tsa.arima.ARIMA(endog, order=(1, 0, 0))\n>>> res = mod.fit()\n>>> print(res.summary())", "short_docstring": "Autoregressive Integrated Moving Average (ARIMA) model, and extensions"}
{"name": "sklearn.cluster.KMeans", "type": "class", "signature": "(n_clusters=8, *, init='k-means++', n_init='auto', max_iter=300, tol=0.0001, verbose=0, random_state=None, copy_x=True, algorithm='lloyd')", "docstring": "K-Means clustering.\n\nRead more in the :ref:`User Guide <k_means>`.\n\nParameters\n----------\n\nn_clusters : int, default=8\n    The number of clusters to form as well as the number of\n    centroids to generate.\n\n    For an example of how to choose an optimal value for `n_clusters` refer to\n    :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_silhouette_analysis.py`.\n\ninit : {'k-means++', 'random'}, callable or array-like of shape             (n_clusters, n_features), default='k-means++'\n    Method for initialization:\n\n    * 'k-means++' : selects initial cluster centroids using sampling             based on an empirical probability distribution of the points'             contribution to the overall inertia. This technique speeds up             convergence. The algorithm implemented is \"greedy k-means++\". It             differs from the vanilla k-means++ by making several trials at             each sampling step and choosing the best centroid among them.\n\n    * 'random': choose `n_clusters` observations (rows) at random from         data for the initial centroids.\n\n    * If an array is passed, it should be of shape (n_clusters, n_features)        and gives the initial centers.\n\n    * If a callable is passed, it should take arguments X, n_clusters and a        random state and return an initialization.\n\n    For an example of how to use the different `init` strategy, see the example\n    entitled :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_digits.py`.\n\nn_init : 'auto' or int, default='auto'\n    Number of times the k-means algorithm is run with different centroid\n    seeds. The final results is the best output of `n_init` consecutive runs\n    in terms of inertia. Several runs are recommended for sparse\n    high-dimensional problems (see :ref:`kmeans_sparse_high_dim`).\n\n    When `n_init='auto'`, the number of runs depends on the value of init:\n    10 if using `init='random'` or `init` is a callable;\n    1 if using `init='k-means++'` or `init` is an array-like.\n\n    .. versionadded:: 1.2\n       Added 'auto' option for `n_init`.\n\n    .. versionchanged:: 1.4\n       Default value for `n_init` changed to `'auto'`.\n\nmax_iter : int, default=300\n    Maximum number of iterations of the k-means algorithm for a\n    single run.\n\ntol : float, default=1e-4\n    Relative tolerance with regards to Frobenius norm of the difference\n    in the cluster centers of two consecutive iterations to declare\n    convergence.\n\nverbose : int, default=0\n    Verbosity mode.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for centroid initialization. Use\n    an int to make the randomness deterministic.\n    See :term:`Glossary <random_state>`.\n\ncopy_x : bool, default=True\n    When pre-computing distances it is more numerically accurate to center\n    the data first. If copy_x is True (default), then the original data is\n    not modified. If False, the original data is modified, and put back\n    before the function returns, but small numerical differences may be\n    introduced by subtracting and then adding the data mean. Note that if\n    the original data is not C-contiguous, a copy will be made even if\n    copy_x is False. If the original data is sparse, but not in CSR format,\n    a copy will be made even if copy_x is False.\n\nalgorithm : {\"lloyd\", \"elkan\"}, default=\"lloyd\"\n    K-means algorithm to use. The classical EM-style algorithm is `\"lloyd\"`.\n    The `\"elkan\"` variation can be more efficient on some datasets with\n    well-defined clusters, by using the triangle inequality. However it's\n    more memory intensive due to the allocation of an extra array of shape\n    `(n_samples, n_clusters)`.\n\n    .. versionchanged:: 0.18\n        Added Elkan algorithm\n\n    .. versionchanged:: 1.1\n        Renamed \"full\" to \"lloyd\", and deprecated \"auto\" and \"full\".\n        Changed \"auto\" to use \"lloyd\" instead of \"elkan\".\n\nAttributes\n----------\ncluster_centers_ : ndarray of shape (n_clusters, n_features)\n    Coordinates of cluster centers. If the algorithm stops before fully\n    converging (see ``tol`` and ``max_iter``), these will not be\n    consistent with ``labels_``.\n\nlabels_ : ndarray of shape (n_samples,)\n    Labels of each point\n\ninertia_ : float\n    Sum of squared distances of samples to their closest cluster center,\n    weighted by the sample weights if provided.\n\nn_iter_ : int\n    Number of iterations run.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nMiniBatchKMeans : Alternative online implementation that does incremental\n    updates of the centers positions using mini-batches.\n    For large scale learning (say n_samples > 10k) MiniBatchKMeans is\n    probably much faster than the default batch implementation.\n\nNotes\n-----\nThe k-means problem is solved using either Lloyd's or Elkan's algorithm.\n\nThe average complexity is given by O(k n T), where n is the number of\nsamples and T is the number of iteration.\n\nThe worst case complexity is given by O(n^(k+2/p)) with\nn = n_samples, p = n_features.\nRefer to :doi:`\"How slow is the k-means method?\" D. Arthur and S. Vassilvitskii -\nSoCG2006.<10.1145/1137856.1137880>` for more details.\n\nIn practice, the k-means algorithm is very fast (one of the fastest\nclustering algorithms available), but it falls in local minima. That's why\nit can be useful to restart it several times.\n\nIf the algorithm stops before fully converging (because of ``tol`` or\n``max_iter``), ``labels_`` and ``cluster_centers_`` will not be consistent,\ni.e. the ``cluster_centers_`` will not be the means of the points in each\ncluster. Also, the estimator will reassign ``labels_`` after the last\niteration to make ``labels_`` consistent with ``predict`` on the training\nset.\n\nExamples\n--------\n\n>>> from sklearn.cluster import KMeans\n>>> import numpy as np\n>>> X = np.array([[1, 2], [1, 4], [1, 0],\n...               [10, 2], [10, 4], [10, 0]])\n>>> kmeans = KMeans(n_clusters=2, random_state=0, n_init=\"auto\").fit(X)\n>>> kmeans.labels_\narray([1, 1, 1, 0, 0, 0], dtype=int32)\n>>> kmeans.predict([[0, 0], [12, 3]])\narray([1, 0], dtype=int32)\n>>> kmeans.cluster_centers_\narray([[10.,  2.],\n       [ 1.,  2.]])\n\nFor a more detailed example of K-Means using the iris dataset see\n:ref:`sphx_glr_auto_examples_cluster_plot_cluster_iris.py`.\n\nFor examples of common problems with K-Means and how to address them see\n:ref:`sphx_glr_auto_examples_cluster_plot_kmeans_assumptions.py`.\n\nFor an example of how to use K-Means to perform color quantization see\n:ref:`sphx_glr_auto_examples_cluster_plot_color_quantization.py`.\n\nFor a demonstration of how K-Means can be used to cluster text documents see\n:ref:`sphx_glr_auto_examples_text_plot_document_clustering.py`.\n\nFor a comparison between K-Means and MiniBatchKMeans refer to example\n:ref:`sphx_glr_auto_examples_cluster_plot_mini_batch_kmeans.py`.", "short_docstring": "K-Means clustering."}
{"name": "tensorflow.keras.Sequential", "error": "Import error: No module named 'tensorflow'"}
{"name": "seaborn.set_theme", "type": "callable", "signature": "(context='notebook', style='darkgrid', palette='deep', font='sans-serif', font_scale=1, color_codes=True, rc=None)", "docstring": "Set aspects of the visual theme for all matplotlib and seaborn plots.\n\nThis function changes the global defaults for all plots using the\nmatplotlib rcParams system. The themeing is decomposed into several distinct\nsets of parameter values.\n\nThe options are illustrated in the :doc:`aesthetics <../tutorial/aesthetics>`\nand :doc:`color palette <../tutorial/color_palettes>` tutorials.\n\nParameters\n----------\ncontext : string or dict\n    Scaling parameters, see :func:`plotting_context`.\nstyle : string or dict\n    Axes style parameters, see :func:`axes_style`.\npalette : string or sequence\n    Color palette, see :func:`color_palette`.\nfont : string\n    Font family, see matplotlib font manager.\nfont_scale : float, optional\n    Separate scaling factor to independently scale the size of the\n    font elements.\ncolor_codes : bool\n    If ``True`` and ``palette`` is a seaborn palette, remap the shorthand\n    color codes (e.g. \"b\", \"g\", \"r\", etc.) to the colors from this palette.\nrc : dict or None\n    Dictionary of rc parameter mappings to override the above.\n\nExamples\n--------\n\n.. include:: ../docstrings/set_theme.rst", "short_docstring": "Set aspects of the visual theme for all matplotlib and seaborn plots."}
{"name": "random.sample", "type": "callable", "signature": "(population, k, *, counts=None)", "docstring": "Chooses k unique random elements from a population sequence or set.\n\nReturns a new list containing elements from the population while\nleaving the original population unchanged.  The resulting list is\nin selection order so that all sub-slices will also be valid random\nsamples.  This allows raffle winners (the sample) to be partitioned\ninto grand prize and second place winners (the subslices).\n\nMembers of the population need not be hashable or unique.  If the\npopulation contains repeats, then each occurrence is a possible\nselection in the sample.\n\nRepeated elements can be specified one at a time or with the optional\ncounts parameter.  For example:\n\n    sample(['red', 'blue'], counts=[4, 2], k=5)\n\nis equivalent to:\n\n    sample(['red', 'red', 'red', 'red', 'blue', 'blue'], k=5)\n\nTo choose a sample from a range of integers, use range() for the\npopulation argument.  This is especially fast and space efficient\nfor sampling from a large population:\n\n    sample(range(10000000), 60)", "short_docstring": "Chooses k unique random elements from a population sequence or set."}
{"name": "difflib.ndiff", "type": "callable", "signature": "(a, b, linejunk=None, charjunk=<function IS_CHARACTER_JUNK at 0x7ffa5f9852d0>)", "docstring": "Compare `a` and `b` (lists of strings); return a `Differ`-style delta.\n\nOptional keyword parameters `linejunk` and `charjunk` are for filter\nfunctions, or can be None:\n\n- linejunk: A function that should accept a single string argument and\n  return true iff the string is junk.  The default is None, and is\n  recommended; the underlying SequenceMatcher class has an adaptive\n  notion of \"noise\" lines.\n\n- charjunk: A function that accepts a character (string of length\n  1), and returns true iff the character is junk. The default is\n  the module-level function IS_CHARACTER_JUNK, which filters out\n  whitespace characters (a blank or tab; note: it's a bad idea to\n  include newline in this!).\n\nTools/scripts/ndiff.py is a command-line front-end to this function.\n\nExample:\n\n>>> diff = ndiff('one\\ntwo\\nthree\\n'.splitlines(keepends=True),\n...              'ore\\ntree\\nemu\\n'.splitlines(keepends=True))\n>>> print(''.join(diff), end=\"\")\n- one\n?  ^\n+ ore\n?  ^\n- two\n- three\n?  -\n+ tree\n+ emu", "short_docstring": "Compare `a` and `b` (lists of strings); return a `Differ`-style delta."}
{"name": "sqlite3.connect", "type": "callable", "signature": null, "docstring": "connect(database[, timeout, detect_types, isolation_level,\n        check_same_thread, factory, cached_statements, uri])\n\nOpens a connection to the SQLite database file *database*. You can use\n\":memory:\" to open a database connection to a database that resides in\nRAM instead of on disk.", "short_docstring": "connect(database[, timeout, detect_types, isolation_level,\n        check_same_thread, factory, cached_statements, uri])"}
{"name": "rsa.newkeys", "type": "callable", "signature": "(nbits: int, accurate: bool = True, poolsize: int = 1, exponent: int = 65537) -> Tuple[rsa.key.PublicKey, rsa.key.PrivateKey]", "docstring": "Generates public and private keys, and returns them as (pub, priv).\n\nThe public key is also known as the 'encryption key', and is a\n:py:class:`rsa.PublicKey` object. The private key is also known as the\n'decryption key' and is a :py:class:`rsa.PrivateKey` object.\n\n:param nbits: the number of bits required to store ``n = p*q``.\n:param accurate: when True, ``n`` will have exactly the number of bits you\n    asked for. However, this makes key generation much slower. When False,\n    `n`` may have slightly less bits.\n:param poolsize: the number of processes to use to generate the prime\n    numbers. If set to a number > 1, a parallel algorithm will be used.\n    This requires Python 2.6 or newer.\n:param exponent: the exponent for the key; only change this if you know\n    what you're doing, as the exponent influences how difficult your\n    private key can be cracked. A very common choice for e is 65537.\n:type exponent: int\n\n:returns: a tuple (:py:class:`rsa.PublicKey`, :py:class:`rsa.PrivateKey`)\n\nThe ``poolsize`` parameter was added in *Python-RSA 3.1* and requires\nPython 2.6 or newer.", "short_docstring": "Generates public and private keys, and returns them as (pub, priv)."}
{"name": "numpy.stack", "type": "callable", "signature": "(arrays, axis=0, out=None)", "docstring": "Join a sequence of arrays along a new axis.\n\nThe ``axis`` parameter specifies the index of the new axis in the\ndimensions of the result. For example, if ``axis=0`` it will be the first\ndimension and if ``axis=-1`` it will be the last dimension.\n\n.. versionadded:: 1.10.0\n\nParameters\n----------\narrays : sequence of array_like\n    Each array must have the same shape.\n\naxis : int, optional\n    The axis in the result array along which the input arrays are stacked.\n\nout : ndarray, optional\n    If provided, the destination to place the result. The shape must be\n    correct, matching that of what stack would have returned if no\n    out argument were specified.\n\nReturns\n-------\nstacked : ndarray\n    The stacked array has one more dimension than the input arrays.\n\nSee Also\n--------\nconcatenate : Join a sequence of arrays along an existing axis.\nblock : Assemble an nd-array from nested lists of blocks.\nsplit : Split array into a list of multiple sub-arrays of equal size.\n\nExamples\n--------\n>>> arrays = [np.random.randn(3, 4) for _ in range(10)]\n>>> np.stack(arrays, axis=0).shape\n(10, 3, 4)\n\n>>> np.stack(arrays, axis=1).shape\n(3, 10, 4)\n\n>>> np.stack(arrays, axis=2).shape\n(3, 4, 10)\n\n>>> a = np.array([1, 2, 3])\n>>> b = np.array([4, 5, 6])\n>>> np.stack((a, b))\narray([[1, 2, 3],\n       [4, 5, 6]])\n\n>>> np.stack((a, b), axis=-1)\narray([[1, 4],\n       [2, 5],\n       [3, 6]])", "short_docstring": "Join a sequence of arrays along a new axis."}
{"name": "pandas.DataFrame", "type": "class", "signature": "(data=None, index: 'Axes | None' = None, columns: 'Axes | None' = None, dtype: 'Dtype | None' = None, copy: 'bool | None' = None) -> 'None'", "docstring": "Two-dimensional, size-mutable, potentially heterogeneous tabular data.\n\nData structure also contains labeled axes (rows and columns).\nArithmetic operations align on both row and column labels. Can be\nthought of as a dict-like container for Series objects. The primary\npandas data structure.\n\nParameters\n----------\ndata : ndarray (structured or homogeneous), Iterable, dict, or DataFrame\n    Dict can contain Series, arrays, constants, dataclass or list-like objects. If\n    data is a dict, column order follows insertion-order. If a dict contains Series\n    which have an index defined, it is aligned by its index. This alignment also\n    occurs if data is a Series or a DataFrame itself. Alignment is done on\n    Series/DataFrame inputs.\n\n    If data is a list of dicts, column order follows insertion-order.\n\nindex : Index or array-like\n    Index to use for resulting frame. Will default to RangeIndex if\n    no indexing information part of input data and no index provided.\ncolumns : Index or array-like\n    Column labels to use for resulting frame when data does not have them,\n    defaulting to RangeIndex(0, 1, 2, ..., n). If data contains column labels,\n    will perform column selection instead.\ndtype : dtype, default None\n    Data type to force. Only a single dtype is allowed. If None, infer.\ncopy : bool or None, default None\n    Copy data from inputs.\n    For dict data, the default of None behaves like ``copy=True``.  For DataFrame\n    or 2d ndarray input, the default of None behaves like ``copy=False``.\n    If data is a dict containing one or more Series (possibly of different dtypes),\n    ``copy=False`` will ensure that these inputs are not copied.\n\n    .. versionchanged:: 1.3.0\n\nSee Also\n--------\nDataFrame.from_records : Constructor from tuples, also record arrays.\nDataFrame.from_dict : From dicts of Series, arrays, or dicts.\nread_csv : Read a comma-separated values (csv) file into DataFrame.\nread_table : Read general delimited file into DataFrame.\nread_clipboard : Read text from clipboard into DataFrame.\n\nNotes\n-----\nPlease reference the :ref:`User Guide <basics.dataframe>` for more information.\n\nExamples\n--------\nConstructing DataFrame from a dictionary.\n\n>>> d = {'col1': [1, 2], 'col2': [3, 4]}\n>>> df = pd.DataFrame(data=d)\n>>> df\n   col1  col2\n0     1     3\n1     2     4\n\nNotice that the inferred dtype is int64.\n\n>>> df.dtypes\ncol1    int64\ncol2    int64\ndtype: object\n\nTo enforce a single dtype:\n\n>>> df = pd.DataFrame(data=d, dtype=np.int8)\n>>> df.dtypes\ncol1    int8\ncol2    int8\ndtype: object\n\nConstructing DataFrame from a dictionary including Series:\n\n>>> d = {'col1': [0, 1, 2, 3], 'col2': pd.Series([2, 3], index=[2, 3])}\n>>> pd.DataFrame(data=d, index=[0, 1, 2, 3])\n   col1  col2\n0     0   NaN\n1     1   NaN\n2     2   2.0\n3     3   3.0\n\nConstructing DataFrame from numpy ndarray:\n\n>>> df2 = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),\n...                    columns=['a', 'b', 'c'])\n>>> df2\n   a  b  c\n0  1  2  3\n1  4  5  6\n2  7  8  9\n\nConstructing DataFrame from a numpy ndarray that has labeled columns:\n\n>>> data = np.array([(1, 2, 3), (4, 5, 6), (7, 8, 9)],\n...                 dtype=[(\"a\", \"i4\"), (\"b\", \"i4\"), (\"c\", \"i4\")])\n>>> df3 = pd.DataFrame(data, columns=['c', 'a'])\n...\n>>> df3\n   c  a\n0  3  1\n1  6  4\n2  9  7\n\nConstructing DataFrame from dataclass:\n\n>>> from dataclasses import make_dataclass\n>>> Point = make_dataclass(\"Point\", [(\"x\", int), (\"y\", int)])\n>>> pd.DataFrame([Point(0, 0), Point(0, 3), Point(2, 3)])\n   x  y\n0  0  0\n1  0  3\n2  2  3\n\nConstructing DataFrame from Series/DataFrame:\n\n>>> ser = pd.Series([1, 2, 3], index=[\"a\", \"b\", \"c\"])\n>>> df = pd.DataFrame(data=ser, index=[\"a\", \"c\"])\n>>> df\n   0\na  1\nc  3\n\n>>> df1 = pd.DataFrame([1, 2, 3], index=[\"a\", \"b\", \"c\"], columns=[\"x\"])\n>>> df2 = pd.DataFrame(data=df1, index=[\"a\", \"c\"])\n>>> df2\n   x\na  1\nc  3", "short_docstring": "Two-dimensional, size-mutable, potentially heterogeneous tabular data."}
{"name": "matplotlib.pyplot.subplots", "type": "callable", "signature": "(nrows: 'int' = 1, ncols: 'int' = 1, *, sharex: \"bool | Literal['none', 'all', 'row', 'col']\" = False, sharey: \"bool | Literal['none', 'all', 'row', 'col']\" = False, squeeze: 'bool' = True, width_ratios: 'Sequence[float] | None' = None, height_ratios: 'Sequence[float] | None' = None, subplot_kw: 'dict[str, Any] | None' = None, gridspec_kw: 'dict[str, Any] | None' = None, **fig_kw) -> 'tuple[Figure, Any]'", "docstring": "Create a figure and a set of subplots.\n\nThis utility wrapper makes it convenient to create common layouts of\nsubplots, including the enclosing figure object, in a single call.\n\nParameters\n----------\nnrows, ncols : int, default: 1\n    Number of rows/columns of the subplot grid.\n\nsharex, sharey : bool or {'none', 'all', 'row', 'col'}, default: False\n    Controls sharing of properties among x (*sharex*) or y (*sharey*)\n    axes:\n\n    - True or 'all': x- or y-axis will be shared among all subplots.\n    - False or 'none': each subplot x- or y-axis will be independent.\n    - 'row': each subplot row will share an x- or y-axis.\n    - 'col': each subplot column will share an x- or y-axis.\n\n    When subplots have a shared x-axis along a column, only the x tick\n    labels of the bottom subplot are created. Similarly, when subplots\n    have a shared y-axis along a row, only the y tick labels of the first\n    column subplot are created. To later turn other subplots' ticklabels\n    on, use `~matplotlib.axes.Axes.tick_params`.\n\n    When subplots have a shared axis that has units, calling\n    `~matplotlib.axis.Axis.set_units` will update each axis with the\n    new units.\n\nsqueeze : bool, default: True\n    - If True, extra dimensions are squeezed out from the returned\n      array of `~matplotlib.axes.Axes`:\n\n      - if only one subplot is constructed (nrows=ncols=1), the\n        resulting single Axes object is returned as a scalar.\n      - for Nx1 or 1xM subplots, the returned object is a 1D numpy\n        object array of Axes objects.\n      - for NxM, subplots with N>1 and M>1 are returned as a 2D array.\n\n    - If False, no squeezing at all is done: the returned Axes object is\n      always a 2D array containing Axes instances, even if it ends up\n      being 1x1.\n\nwidth_ratios : array-like of length *ncols*, optional\n    Defines the relative widths of the columns. Each column gets a\n    relative width of ``width_ratios[i] / sum(width_ratios)``.\n    If not given, all columns will have the same width.  Equivalent\n    to ``gridspec_kw={'width_ratios': [...]}``.\n\nheight_ratios : array-like of length *nrows*, optional\n    Defines the relative heights of the rows. Each row gets a\n    relative height of ``height_ratios[i] / sum(height_ratios)``.\n    If not given, all rows will have the same height. Convenience\n    for ``gridspec_kw={'height_ratios': [...]}``.\n\nsubplot_kw : dict, optional\n    Dict with keywords passed to the\n    `~matplotlib.figure.Figure.add_subplot` call used to create each\n    subplot.\n\ngridspec_kw : dict, optional\n    Dict with keywords passed to the `~matplotlib.gridspec.GridSpec`\n    constructor used to create the grid the subplots are placed on.\n\n**fig_kw\n    All additional keyword arguments are passed to the\n    `.pyplot.figure` call.\n\nReturns\n-------\nfig : `.Figure`\n\nax : `~matplotlib.axes.Axes` or array of Axes\n    *ax* can be either a single `~.axes.Axes` object, or an array of Axes\n    objects if more than one subplot was created.  The dimensions of the\n    resulting array can be controlled with the squeeze keyword, see above.\n\n    Typical idioms for handling the return value are::\n\n        # using the variable ax for single a Axes\n        fig, ax = plt.subplots()\n\n        # using the variable axs for multiple Axes\n        fig, axs = plt.subplots(2, 2)\n\n        # using tuple unpacking for multiple Axes\n        fig, (ax1, ax2) = plt.subplots(1, 2)\n        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n\n    The names ``ax`` and pluralized ``axs`` are preferred over ``axes``\n    because for the latter it's not clear if it refers to a single\n    `~.axes.Axes` instance or a collection of these.\n\nSee Also\n--------\n.pyplot.figure\n.pyplot.subplot\n.pyplot.axes\n.Figure.subplots\n.Figure.add_subplot\n\nExamples\n--------\n::\n\n    # First create some toy data:\n    x = np.linspace(0, 2*np.pi, 400)\n    y = np.sin(x**2)\n\n    # Create just a figure and only one subplot\n    fig, ax = plt.subplots()\n    ax.plot(x, y)\n    ax.set_title('Simple plot')\n\n    # Create two subplots and unpack the output array immediately\n    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n    ax1.plot(x, y)\n    ax1.set_title('Sharing Y axis')\n    ax2.scatter(x, y)\n\n    # Create four polar axes and access them through the returned array\n    fig, axs = plt.subplots(2, 2, subplot_kw=dict(projection=\"polar\"))\n    axs[0, 0].plot(x, y)\n    axs[1, 1].scatter(x, y)\n\n    # Share a X axis with each column of subplots\n    plt.subplots(2, 2, sharex='col')\n\n    # Share a Y axis with each row of subplots\n    plt.subplots(2, 2, sharey='row')\n\n    # Share both X and Y axes with all subplots\n    plt.subplots(2, 2, sharex='all', sharey='all')\n\n    # Note that this is the same as\n    plt.subplots(2, 2, sharex=True, sharey=True)\n\n    # Create figure number 10 with a single subplot\n    # and clears it if it already exists.\n    fig, ax = plt.subplots(num=10, clear=True)", "short_docstring": "Create a figure and a set of subplots."}
{"name": "flask.render_template", "type": "callable", "signature": "(template_name_or_list: 'str | Template | list[str | Template]', **context: 't.Any') -> 'str'", "docstring": "Render a template by name with the given context.\n\n:param template_name_or_list: The name of the template to render. If\n    a list is given, the first name to exist will be rendered.\n:param context: The variables to make available in the template.", "short_docstring": "Render a template by name with the given context."}
{"name": "regex.sub", "type": "callable", "signature": "(pattern, repl, string, count=0, flags=0, pos=None, endpos=None, concurrent=None, timeout=None, ignore_unused=False, **kwargs)", "docstring": "Return the string obtained by replacing the leftmost (or rightmost with a\nreverse pattern) non-overlapping occurrences of the pattern in string by the\nreplacement repl. repl can be either a string or a callable; if a string,\nbackslash escapes in it are processed; if a callable, it's passed the match\nobject and must return a replacement string to be used.", "short_docstring": "Return the string obtained by replacing the leftmost (or rightmost with a\nreverse pattern) non-overlapping occurrences of the pattern in string by the\nreplacement repl. repl can be either a string or a callable; if a string,\nbackslash escapes in it are processed; if a callable, it's passed the match\nobject and must return a replacement string to be used."}
{"name": "cv2.imwrite", "type": "callable", "signature": null, "docstring": "imwrite(filename, img[, params]) -> retval\n.   @brief Saves an image to a specified file.\n.   \n.   The function imwrite saves the image to the specified file. The image format is chosen based on the\n.   filename extension (see cv::imread for the list of extensions). In general, only 8-bit unsigned (CV_8U)\n.   single-channel or 3-channel (with 'BGR' channel order) images\n.   can be saved using this function, with these exceptions:\n.   \n.   - With OpenEXR encoder, only 32-bit float (CV_32F) images can be saved.\n.     - 8-bit unsigned (CV_8U) images are not supported.\n.   - With Radiance HDR encoder, non 64-bit float (CV_64F) images can be saved.\n.     - All images will be converted to 32-bit float (CV_32F).\n.   - With JPEG 2000 encoder, 8-bit unsigned (CV_8U) and 16-bit unsigned (CV_16U) images can be saved.\n.   - With PAM encoder, 8-bit unsigned (CV_8U) and 16-bit unsigned (CV_16U) images can be saved.\n.   - With PNG encoder, 8-bit unsigned (CV_8U) and 16-bit unsigned (CV_16U) images can be saved.\n.     - PNG images with an alpha channel can be saved using this function. To do this, create\n.       8-bit (or 16-bit) 4-channel image BGRA, where the alpha channel goes last. Fully transparent pixels\n.       should have alpha set to 0, fully opaque pixels should have alpha set to 255/65535 (see the code sample below).\n.   - With PGM/PPM encoder, 8-bit unsigned (CV_8U) and 16-bit unsigned (CV_16U) images can be saved.\n.   - With TIFF encoder, 8-bit unsigned (CV_8U), 16-bit unsigned (CV_16U),\n.                        32-bit float (CV_32F) and 64-bit float (CV_64F) images can be saved.\n.     - Multiple images (vector of Mat) can be saved in TIFF format (see the code sample below).\n.     - 32-bit float 3-channel (CV_32FC3) TIFF images will be saved\n.       using the LogLuv high dynamic range encoding (4 bytes per pixel)\n.   \n.   If the image format is not supported, the image will be converted to 8-bit unsigned (CV_8U) and saved that way.\n.   \n.   If the format, depth or channel order is different, use\n.   Mat::convertTo and cv::cvtColor to convert it before saving. Or, use the universal FileStorage I/O\n.   functions to save the image to XML or YAML format.\n.   \n.   The sample below shows how to create a BGRA image, how to set custom compression parameters and save it to a PNG file.\n.   It also demonstrates how to save multiple images in a TIFF file:\n.   @include snippets/imgcodecs_imwrite.cpp\n.   @param filename Name of the file.\n.   @param img (Mat or vector of Mat) Image or Images to be saved.\n.   @param params Format-specific parameters encoded as pairs (paramId_1, paramValue_1, paramId_2, paramValue_2, ... .) see cv::ImwriteFlags", "short_docstring": "imwrite(filename, img[, params]) -> retval\n.   @brief Saves an image to a specified file.\n.   \n.   The function imwrite saves the image to the specified file. The image format is chosen based on the\n.   filename extension (see cv::imread for the list of extensions). In general, only 8-bit unsigned (CV_8U)\n.   single-channel or 3-channel (with 'BGR' channel order) images\n.   can be saved using this function, with these exceptions:\n.   \n.   - With OpenEXR encoder, only 32-bit float (CV_32F) images can be saved.\n.     - 8-bit unsigned (CV_8U) images are not supported.\n.   - With Radiance HDR encoder, non 64-bit float (CV_64F) images can be saved.\n.     - All images will be converted to 32-bit float (CV_32F).\n.   - With JPEG 2000 encoder, 8-bit unsigned (CV_8U) and 16-bit unsigned (CV_16U) images can be saved.\n.   - With PAM encoder, 8-bit unsigned (CV_8U) and 16-bit unsigned (CV_16U) images can be saved.\n.   - With PNG encoder, 8-bit unsigned (CV_8U) and 16-bit unsigned (CV_16U) images can be saved.\n.     - PNG images with an alpha channel can be saved using this function. To do this, create\n.       8-bit (or 16-bit) 4-channel image BGRA, where the alpha channel goes last. Fully transparent pixels\n.       should have alpha set to 0, fully opaque pixels should have alpha set to 255/65535 (see the code sample below).\n.   - With PGM/PPM encoder, 8-bit unsigned (CV_8U) and 16-bit unsigned (CV_16U) images can be saved.\n.   - With TIFF encoder, 8-bit unsigned (CV_8U), 16-bit unsigned (CV_16U),\n.                        32-bit float (CV_32F) and 64-bit float (CV_64F) images can be saved.\n.     - Multiple images (vector of Mat) can be saved in TIFF format (see the code sample below).\n.     - 32-bit float 3-channel (CV_32FC3) TIFF images will be saved\n.       using the LogLuv high dynamic range encoding (4 bytes per pixel)\n.   \n.   If the image format is not supported, the image will be converted to 8-bit unsigned (CV_8U) and saved that way.\n.   \n.   If the format, depth or channel order is different, use\n.   Mat::convertTo and cv::cvtColor to convert it before saving. Or, use the universal FileStorage I/O\n.   functions to save the image to XML or YAML format.\n.   \n.   The sample below shows how to create a BGRA image, how to set custom compression parameters and save it to a PNG file.\n.   It also demonstrates how to save multiple images in a TIFF file:\n.   @include snippets/imgcodecs_imwrite.cpp\n.   @param filename Name of the file.\n.   @param img (Mat or vector of Mat) Image or Images to be saved.\n.   @param params Format-specific parameters encoded as pairs (paramId_1, paramValue_1, paramId_2, paramValue_2, ... .) see cv::ImwriteFlags"}
{"name": "requests.ConnectionError", "type": "class", "signature": "(*args, **kwargs)", "docstring": "A Connection error occurred.", "short_docstring": "A Connection error occurred."}
{"name": "smtplib.SMTPAuthenticationError", "type": "class", "signature": "(code, msg)", "docstring": "Authentication error.\n\nMost probably the server didn't accept the username/password\ncombination provided.", "short_docstring": "Authentication error."}
{"name": "numpy.random.choice", "type": "callable", "signature": null, "docstring": "choice(a, size=None, replace=True, p=None)\n\nGenerates a random sample from a given 1-D array\n\n.. versionadded:: 1.7.0\n\n.. note::\n    New code should use the ``choice`` method of a ``default_rng()``\n    instance instead; please see the :ref:`random-quick-start`.\n\nParameters\n----------\na : 1-D array-like or int\n    If an ndarray, a random sample is generated from its elements.\n    If an int, the random sample is generated as if it were ``np.arange(a)``\nsize : int or tuple of ints, optional\n    Output shape.  If the given shape is, e.g., ``(m, n, k)``, then\n    ``m * n * k`` samples are drawn.  Default is None, in which case a\n    single value is returned.\nreplace : boolean, optional\n    Whether the sample is with or without replacement. Default is True,\n    meaning that a value of ``a`` can be selected multiple times.\np : 1-D array-like, optional\n    The probabilities associated with each entry in a.\n    If not given, the sample assumes a uniform distribution over all\n    entries in ``a``.\n\nReturns\n-------\nsamples : single item or ndarray\n    The generated random samples\n\nRaises\n------\nValueError\n    If a is an int and less than zero, if a or p are not 1-dimensional,\n    if a is an array-like of size 0, if p is not a vector of\n    probabilities, if a and p have different lengths, or if\n    replace=False and the sample size is greater than the population\n    size\n\nSee Also\n--------\nrandint, shuffle, permutation\nrandom.Generator.choice: which should be used in new code\n\nNotes\n-----\nSetting user-specified probabilities through ``p`` uses a more general but less\nefficient sampler than the default. The general sampler produces a different sample\nthan the optimized sampler even if each element of ``p`` is 1 / len(a).\n\nSampling random rows from a 2-D array is not possible with this function,\nbut is possible with `Generator.choice` through its ``axis`` keyword.\n\nExamples\n--------\nGenerate a uniform random sample from np.arange(5) of size 3:\n\n>>> np.random.choice(5, 3)\narray([0, 3, 4]) # random\n>>> #This is equivalent to np.random.randint(0,5,3)\n\nGenerate a non-uniform random sample from np.arange(5) of size 3:\n\n>>> np.random.choice(5, 3, p=[0.1, 0, 0.3, 0.6, 0])\narray([3, 3, 0]) # random\n\nGenerate a uniform random sample from np.arange(5) of size 3 without\nreplacement:\n\n>>> np.random.choice(5, 3, replace=False)\narray([3,1,0]) # random\n>>> #This is equivalent to np.random.permutation(np.arange(5))[:3]\n\nGenerate a non-uniform random sample from np.arange(5) of size\n3 without replacement:\n\n>>> np.random.choice(5, 3, replace=False, p=[0.1, 0, 0.3, 0.6, 0])\narray([2, 3, 0]) # random\n\nAny of the above can be repeated with an arbitrary array-like\ninstead of just integers. For instance:\n\n>>> aa_milne_arr = ['pooh', 'rabbit', 'piglet', 'Christopher']\n>>> np.random.choice(aa_milne_arr, 5, p=[0.5, 0.1, 0.1, 0.3])\narray(['pooh', 'pooh', 'pooh', 'Christopher', 'piglet'], # random\n      dtype='<U11')", "short_docstring": "choice(a, size=None, replace=True, p=None)"}
{"name": "pandas.to_datetime", "type": "callable", "signature": "(arg: 'DatetimeScalarOrArrayConvertible | DictConvertible', errors: 'DateTimeErrorChoices' = 'raise', dayfirst: 'bool' = False, yearfirst: 'bool' = False, utc: 'bool' = False, format: 'str | None' = None, exact: 'bool | lib.NoDefault' = <no_default>, unit: 'str | None' = None, infer_datetime_format: 'lib.NoDefault | bool' = <no_default>, origin: 'str' = 'unix', cache: 'bool' = True) -> 'DatetimeIndex | Series | DatetimeScalar | NaTType | None'", "docstring": "Convert argument to datetime.\n\nThis function converts a scalar, array-like, :class:`Series` or\n:class:`DataFrame`/dict-like to a pandas datetime object.\n\nParameters\n----------\narg : int, float, str, datetime, list, tuple, 1-d array, Series, DataFrame/dict-like\n    The object to convert to a datetime. If a :class:`DataFrame` is provided, the\n    method expects minimally the following columns: :const:`\"year\"`,\n    :const:`\"month\"`, :const:`\"day\"`. The column \"year\"\n    must be specified in 4-digit format.\nerrors : {'ignore', 'raise', 'coerce'}, default 'raise'\n    - If :const:`'raise'`, then invalid parsing will raise an exception.\n    - If :const:`'coerce'`, then invalid parsing will be set as :const:`NaT`.\n    - If :const:`'ignore'`, then invalid parsing will return the input.\ndayfirst : bool, default False\n    Specify a date parse order if `arg` is str or is list-like.\n    If :const:`True`, parses dates with the day first, e.g. :const:`\"10/11/12\"`\n    is parsed as :const:`2012-11-10`.\n\n    .. warning::\n\n        ``dayfirst=True`` is not strict, but will prefer to parse\n        with day first.\n\nyearfirst : bool, default False\n    Specify a date parse order if `arg` is str or is list-like.\n\n    - If :const:`True` parses dates with the year first, e.g.\n      :const:`\"10/11/12\"` is parsed as :const:`2010-11-12`.\n    - If both `dayfirst` and `yearfirst` are :const:`True`, `yearfirst` is\n      preceded (same as :mod:`dateutil`).\n\n    .. warning::\n\n        ``yearfirst=True`` is not strict, but will prefer to parse\n        with year first.\n\nutc : bool, default False\n    Control timezone-related parsing, localization and conversion.\n\n    - If :const:`True`, the function *always* returns a timezone-aware\n      UTC-localized :class:`Timestamp`, :class:`Series` or\n      :class:`DatetimeIndex`. To do this, timezone-naive inputs are\n      *localized* as UTC, while timezone-aware inputs are *converted* to UTC.\n\n    - If :const:`False` (default), inputs will not be coerced to UTC.\n      Timezone-naive inputs will remain naive, while timezone-aware ones\n      will keep their time offsets. Limitations exist for mixed\n      offsets (typically, daylight savings), see :ref:`Examples\n      <to_datetime_tz_examples>` section for details.\n\n    .. warning::\n\n        In a future version of pandas, parsing datetimes with mixed time\n        zones will raise an error unless `utc=True`.\n        Please specify `utc=True` to opt in to the new behaviour\n        and silence this warning. To create a `Series` with mixed offsets and\n        `object` dtype, please use `apply` and `datetime.datetime.strptime`.\n\n    See also: pandas general documentation about `timezone conversion and\n    localization\n    <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html\n    #time-zone-handling>`_.\n\nformat : str, default None\n    The strftime to parse time, e.g. :const:`\"%d/%m/%Y\"`. See\n    `strftime documentation\n    <https://docs.python.org/3/library/datetime.html\n    #strftime-and-strptime-behavior>`_ for more information on choices, though\n    note that :const:`\"%f\"` will parse all the way up to nanoseconds.\n    You can also pass:\n\n    - \"ISO8601\", to parse any `ISO8601 <https://en.wikipedia.org/wiki/ISO_8601>`_\n      time string (not necessarily in exactly the same format);\n    - \"mixed\", to infer the format for each element individually. This is risky,\n      and you should probably use it along with `dayfirst`.\n\n    .. note::\n\n        If a :class:`DataFrame` is passed, then `format` has no effect.\n\nexact : bool, default True\n    Control how `format` is used:\n\n    - If :const:`True`, require an exact `format` match.\n    - If :const:`False`, allow the `format` to match anywhere in the target\n      string.\n\n    Cannot be used alongside ``format='ISO8601'`` or ``format='mixed'``.\nunit : str, default 'ns'\n    The unit of the arg (D,s,ms,us,ns) denote the unit, which is an\n    integer or float number. This will be based off the origin.\n    Example, with ``unit='ms'`` and ``origin='unix'``, this would calculate\n    the number of milliseconds to the unix epoch start.\ninfer_datetime_format : bool, default False\n    If :const:`True` and no `format` is given, attempt to infer the format\n    of the datetime strings based on the first non-NaN element,\n    and if it can be inferred, switch to a faster method of parsing them.\n    In some cases this can increase the parsing speed by ~5-10x.\n\n    .. deprecated:: 2.0.0\n        A strict version of this argument is now the default, passing it has\n        no effect.\n\norigin : scalar, default 'unix'\n    Define the reference date. The numeric values would be parsed as number\n    of units (defined by `unit`) since this reference date.\n\n    - If :const:`'unix'` (or POSIX) time; origin is set to 1970-01-01.\n    - If :const:`'julian'`, unit must be :const:`'D'`, and origin is set to\n      beginning of Julian Calendar. Julian day number :const:`0` is assigned\n      to the day starting at noon on January 1, 4713 BC.\n    - If Timestamp convertible (Timestamp, dt.datetime, np.datetimt64 or date\n      string), origin is set to Timestamp identified by origin.\n    - If a float or integer, origin is the difference\n      (in units determined by the ``unit`` argument) relative to 1970-01-01.\ncache : bool, default True\n    If :const:`True`, use a cache of unique, converted dates to apply the\n    datetime conversion. May produce significant speed-up when parsing\n    duplicate date strings, especially ones with timezone offsets. The cache\n    is only used when there are at least 50 values. The presence of\n    out-of-bounds values will render the cache unusable and may slow down\n    parsing.\n\nReturns\n-------\ndatetime\n    If parsing succeeded.\n    Return type depends on input (types in parenthesis correspond to\n    fallback in case of unsuccessful timezone or out-of-range timestamp\n    parsing):\n\n    - scalar: :class:`Timestamp` (or :class:`datetime.datetime`)\n    - array-like: :class:`DatetimeIndex` (or :class:`Series` with\n      :class:`object` dtype containing :class:`datetime.datetime`)\n    - Series: :class:`Series` of :class:`datetime64` dtype (or\n      :class:`Series` of :class:`object` dtype containing\n      :class:`datetime.datetime`)\n    - DataFrame: :class:`Series` of :class:`datetime64` dtype (or\n      :class:`Series` of :class:`object` dtype containing\n      :class:`datetime.datetime`)\n\nRaises\n------\nParserError\n    When parsing a date from string fails.\nValueError\n    When another datetime conversion error happens. For example when one\n    of 'year', 'month', day' columns is missing in a :class:`DataFrame`, or\n    when a Timezone-aware :class:`datetime.datetime` is found in an array-like\n    of mixed time offsets, and ``utc=False``.\n\nSee Also\n--------\nDataFrame.astype : Cast argument to a specified dtype.\nto_timedelta : Convert argument to timedelta.\nconvert_dtypes : Convert dtypes.\n\nNotes\n-----\n\nMany input types are supported, and lead to different output types:\n\n- **scalars** can be int, float, str, datetime object (from stdlib :mod:`datetime`\n  module or :mod:`numpy`). They are converted to :class:`Timestamp` when\n  possible, otherwise they are converted to :class:`datetime.datetime`.\n  None/NaN/null scalars are converted to :const:`NaT`.\n\n- **array-like** can contain int, float, str, datetime objects. They are\n  converted to :class:`DatetimeIndex` when possible, otherwise they are\n  converted to :class:`Index` with :class:`object` dtype, containing\n  :class:`datetime.datetime`. None/NaN/null entries are converted to\n  :const:`NaT` in both cases.\n\n- **Series** are converted to :class:`Series` with :class:`datetime64`\n  dtype when possible, otherwise they are converted to :class:`Series` with\n  :class:`object` dtype, containing :class:`datetime.datetime`. None/NaN/null\n  entries are converted to :const:`NaT` in both cases.\n\n- **DataFrame/dict-like** are converted to :class:`Series` with\n  :class:`datetime64` dtype. For each row a datetime is created from assembling\n  the various dataframe columns. Column keys can be common abbreviations\n  like ['year', 'month', 'day', 'minute', 'second', 'ms', 'us', 'ns']) or\n  plurals of the same.\n\nThe following causes are responsible for :class:`datetime.datetime` objects\nbeing returned (possibly inside an :class:`Index` or a :class:`Series` with\n:class:`object` dtype) instead of a proper pandas designated type\n(:class:`Timestamp`, :class:`DatetimeIndex` or :class:`Series`\nwith :class:`datetime64` dtype):\n\n- when any input element is before :const:`Timestamp.min` or after\n  :const:`Timestamp.max`, see `timestamp limitations\n  <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html\n  #timeseries-timestamp-limits>`_.\n\n- when ``utc=False`` (default) and the input is an array-like or\n  :class:`Series` containing mixed naive/aware datetime, or aware with mixed\n  time offsets. Note that this happens in the (quite frequent) situation when\n  the timezone has a daylight savings policy. In that case you may wish to\n  use ``utc=True``.\n\nExamples\n--------\n\n**Handling various input formats**\n\nAssembling a datetime from multiple columns of a :class:`DataFrame`. The keys\ncan be common abbreviations like ['year', 'month', 'day', 'minute', 'second',\n'ms', 'us', 'ns']) or plurals of the same\n\n>>> df = pd.DataFrame({'year': [2015, 2016],\n...                    'month': [2, 3],\n...                    'day': [4, 5]})\n>>> pd.to_datetime(df)\n0   2015-02-04\n1   2016-03-05\ndtype: datetime64[ns]\n\nUsing a unix epoch time\n\n>>> pd.to_datetime(1490195805, unit='s')\nTimestamp('2017-03-22 15:16:45')\n>>> pd.to_datetime(1490195805433502912, unit='ns')\nTimestamp('2017-03-22 15:16:45.433502912')\n\n.. warning:: For float arg, precision rounding might happen. To prevent\n    unexpected behavior use a fixed-width exact type.\n\nUsing a non-unix epoch origin\n\n>>> pd.to_datetime([1, 2, 3], unit='D',\n...                origin=pd.Timestamp('1960-01-01'))\nDatetimeIndex(['1960-01-02', '1960-01-03', '1960-01-04'],\n              dtype='datetime64[ns]', freq=None)\n\n**Differences with strptime behavior**\n\n:const:`\"%f\"` will parse all the way up to nanoseconds.\n\n>>> pd.to_datetime('2018-10-26 12:00:00.0000000011',\n...                format='%Y-%m-%d %H:%M:%S.%f')\nTimestamp('2018-10-26 12:00:00.000000001')\n\n**Non-convertible date/times**\n\nPassing ``errors='coerce'`` will force an out-of-bounds date to :const:`NaT`,\nin addition to forcing non-dates (or non-parseable dates) to :const:`NaT`.\n\n>>> pd.to_datetime('13000101', format='%Y%m%d', errors='coerce')\nNaT\n\n.. _to_datetime_tz_examples:\n\n**Timezones and time offsets**\n\nThe default behaviour (``utc=False``) is as follows:\n\n- Timezone-naive inputs are converted to timezone-naive :class:`DatetimeIndex`:\n\n>>> pd.to_datetime(['2018-10-26 12:00:00', '2018-10-26 13:00:15'])\nDatetimeIndex(['2018-10-26 12:00:00', '2018-10-26 13:00:15'],\n              dtype='datetime64[ns]', freq=None)\n\n- Timezone-aware inputs *with constant time offset* are converted to\n  timezone-aware :class:`DatetimeIndex`:\n\n>>> pd.to_datetime(['2018-10-26 12:00 -0500', '2018-10-26 13:00 -0500'])\nDatetimeIndex(['2018-10-26 12:00:00-05:00', '2018-10-26 13:00:00-05:00'],\n              dtype='datetime64[ns, UTC-05:00]', freq=None)\n\n- However, timezone-aware inputs *with mixed time offsets* (for example\n  issued from a timezone with daylight savings, such as Europe/Paris)\n  are **not successfully converted** to a :class:`DatetimeIndex`.\n  Parsing datetimes with mixed time zones will show a warning unless\n  `utc=True`. If you specify `utc=False` the warning below will be shown\n  and a simple :class:`Index` containing :class:`datetime.datetime`\n  objects will be returned:\n\n>>> pd.to_datetime(['2020-10-25 02:00 +0200',\n...                 '2020-10-25 04:00 +0100'])  # doctest: +SKIP\nFutureWarning: In a future version of pandas, parsing datetimes with mixed\ntime zones will raise an error unless `utc=True`. Please specify `utc=True`\nto opt in to the new behaviour and silence this warning. To create a `Series`\nwith mixed offsets and `object` dtype, please use `apply` and\n`datetime.datetime.strptime`.\nIndex([2020-10-25 02:00:00+02:00, 2020-10-25 04:00:00+01:00],\n      dtype='object')\n\n- A mix of timezone-aware and timezone-naive inputs is also converted to\n  a simple :class:`Index` containing :class:`datetime.datetime` objects:\n\n>>> from datetime import datetime\n>>> pd.to_datetime([\"2020-01-01 01:00:00-01:00\",\n...                 datetime(2020, 1, 1, 3, 0)])  # doctest: +SKIP\nFutureWarning: In a future version of pandas, parsing datetimes with mixed\ntime zones will raise an error unless `utc=True`. Please specify `utc=True`\nto opt in to the new behaviour and silence this warning. To create a `Series`\nwith mixed offsets and `object` dtype, please use `apply` and\n`datetime.datetime.strptime`.\nIndex([2020-01-01 01:00:00-01:00, 2020-01-01 03:00:00], dtype='object')\n\n|\n\nSetting ``utc=True`` solves most of the above issues:\n\n- Timezone-naive inputs are *localized* as UTC\n\n>>> pd.to_datetime(['2018-10-26 12:00', '2018-10-26 13:00'], utc=True)\nDatetimeIndex(['2018-10-26 12:00:00+00:00', '2018-10-26 13:00:00+00:00'],\n              dtype='datetime64[ns, UTC]', freq=None)\n\n- Timezone-aware inputs are *converted* to UTC (the output represents the\n  exact same datetime, but viewed from the UTC time offset `+00:00`).\n\n>>> pd.to_datetime(['2018-10-26 12:00 -0530', '2018-10-26 12:00 -0500'],\n...                utc=True)\nDatetimeIndex(['2018-10-26 17:30:00+00:00', '2018-10-26 17:00:00+00:00'],\n              dtype='datetime64[ns, UTC]', freq=None)\n\n- Inputs can contain both string or datetime, the above\n  rules still apply\n\n>>> pd.to_datetime(['2018-10-26 12:00', datetime(2020, 1, 1, 18)], utc=True)\nDatetimeIndex(['2018-10-26 12:00:00+00:00', '2020-01-01 18:00:00+00:00'],\n              dtype='datetime64[ns, UTC]', freq=None)", "short_docstring": "Convert argument to datetime."}
{"name": "sklearn.linear_model.LinearRegression", "type": "class", "signature": "(*, fit_intercept=True, copy_X=True, n_jobs=None, positive=False)", "docstring": "Ordinary least squares Linear Regression.\n\nLinearRegression fits a linear model with coefficients w = (w1, ..., wp)\nto minimize the residual sum of squares between the observed targets in\nthe dataset, and the targets predicted by the linear approximation.\n\nParameters\n----------\nfit_intercept : bool, default=True\n    Whether to calculate the intercept for this model. If set\n    to False, no intercept will be used in calculations\n    (i.e. data is expected to be centered).\n\ncopy_X : bool, default=True\n    If True, X will be copied; else, it may be overwritten.\n\nn_jobs : int, default=None\n    The number of jobs to use for the computation. This will only provide\n    speedup in case of sufficiently large problems, that is if firstly\n    `n_targets > 1` and secondly `X` is sparse or if `positive` is set\n    to `True`. ``None`` means 1 unless in a\n    :obj:`joblib.parallel_backend` context. ``-1`` means using all\n    processors. See :term:`Glossary <n_jobs>` for more details.\n\npositive : bool, default=False\n    When set to ``True``, forces the coefficients to be positive. This\n    option is only supported for dense arrays.\n\n    .. versionadded:: 0.24\n\nAttributes\n----------\ncoef_ : array of shape (n_features, ) or (n_targets, n_features)\n    Estimated coefficients for the linear regression problem.\n    If multiple targets are passed during the fit (y 2D), this\n    is a 2D array of shape (n_targets, n_features), while if only\n    one target is passed, this is a 1D array of length n_features.\n\nrank_ : int\n    Rank of matrix `X`. Only available when `X` is dense.\n\nsingular_ : array of shape (min(X, y),)\n    Singular values of `X`. Only available when `X` is dense.\n\nintercept_ : float or array of shape (n_targets,)\n    Independent term in the linear model. Set to 0.0 if\n    `fit_intercept = False`.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nRidge : Ridge regression addresses some of the\n    problems of Ordinary Least Squares by imposing a penalty on the\n    size of the coefficients with l2 regularization.\nLasso : The Lasso is a linear model that estimates\n    sparse coefficients with l1 regularization.\nElasticNet : Elastic-Net is a linear regression\n    model trained with both l1 and l2 -norm regularization of the\n    coefficients.\n\nNotes\n-----\nFrom the implementation point of view, this is just plain Ordinary\nLeast Squares (scipy.linalg.lstsq) or Non Negative Least Squares\n(scipy.optimize.nnls) wrapped as a predictor object.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.linear_model import LinearRegression\n>>> X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n>>> # y = 1 * x_0 + 2 * x_1 + 3\n>>> y = np.dot(X, np.array([1, 2])) + 3\n>>> reg = LinearRegression().fit(X, y)\n>>> reg.score(X, y)\n1.0\n>>> reg.coef_\narray([1., 2.])\n>>> reg.intercept_\n3.0...\n>>> reg.predict(np.array([[3, 5]]))\narray([16.])", "short_docstring": "Ordinary least squares Linear Regression."}
{"name": "matplotlib.pyplot.xlabel", "type": "callable", "signature": "(xlabel: 'str', fontdict: 'dict[str, Any] | None' = None, labelpad: 'float | None' = None, *, loc: \"Literal['left', 'center', 'right'] | None\" = None, **kwargs) -> 'Text'", "docstring": "Set the label for the x-axis.\n\nParameters\n----------\nxlabel : str\n    The label text.\n\nlabelpad : float, default: :rc:`axes.labelpad`\n    Spacing in points from the Axes bounding box including ticks\n    and tick labels.  If None, the previous value is left as is.\n\nloc : {'left', 'center', 'right'}, default: :rc:`xaxis.labellocation`\n    The label position. This is a high-level alternative for passing\n    parameters *x* and *horizontalalignment*.\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    `.Text` properties control the appearance of the label.\n\nSee Also\n--------\ntext : Documents the properties supported by `.Text`.", "short_docstring": "Set the label for the x-axis."}
{"name": "numpy.sqrt", "type": "callable", "signature": null, "docstring": "sqrt(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n\nReturn the non-negative square-root of an array, element-wise.\n\nParameters\n----------\nx : array_like\n    The values whose square-roots are required.\nout : ndarray, None, or tuple of ndarray and None, optional\n    A location into which the result is stored. If provided, it must have\n    a shape that the inputs broadcast to. If not provided or None,\n    a freshly-allocated array is returned. A tuple (possible only as a\n    keyword argument) must have length equal to the number of outputs.\nwhere : array_like, optional\n    This condition is broadcast over the input. At locations where the\n    condition is True, the `out` array will be set to the ufunc result.\n    Elsewhere, the `out` array will retain its original value.\n    Note that if an uninitialized `out` array is created via the default\n    ``out=None``, locations within it where the condition is False will\n    remain uninitialized.\n**kwargs\n    For other keyword-only arguments, see the\n    :ref:`ufunc docs <ufuncs.kwargs>`.\n\nReturns\n-------\ny : ndarray\n    An array of the same shape as `x`, containing the positive\n    square-root of each element in `x`.  If any element in `x` is\n    complex, a complex array is returned (and the square-roots of\n    negative reals are calculated).  If all of the elements in `x`\n    are real, so is `y`, with negative elements returning ``nan``.\n    If `out` was provided, `y` is a reference to it.\n    This is a scalar if `x` is a scalar.\n\nSee Also\n--------\nemath.sqrt\n    A version which returns complex numbers when given negative reals.\n    Note: 0.0 and -0.0 are handled differently for complex inputs.\n\nNotes\n-----\n*sqrt* has--consistent with common convention--as its branch cut the\nreal \"interval\" [`-inf`, 0), and is continuous from above on it.\nA branch cut is a curve in the complex plane across which a given\ncomplex function fails to be continuous.\n\nExamples\n--------\n>>> np.sqrt([1,4,9])\narray([ 1.,  2.,  3.])\n\n>>> np.sqrt([4, -1, -3+4J])\narray([ 2.+0.j,  0.+1.j,  1.+2.j])\n\n>>> np.sqrt([4, -1, np.inf])\narray([ 2., nan, inf])", "short_docstring": "sqrt(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])"}
{"name": "matplotlib.pyplot.ylabel", "type": "callable", "signature": "(ylabel: 'str', fontdict: 'dict[str, Any] | None' = None, labelpad: 'float | None' = None, *, loc: \"Literal['bottom', 'center', 'top'] | None\" = None, **kwargs) -> 'Text'", "docstring": "Set the label for the y-axis.\n\nParameters\n----------\nylabel : str\n    The label text.\n\nlabelpad : float, default: :rc:`axes.labelpad`\n    Spacing in points from the Axes bounding box including ticks\n    and tick labels.  If None, the previous value is left as is.\n\nloc : {'bottom', 'center', 'top'}, default: :rc:`yaxis.labellocation`\n    The label position. This is a high-level alternative for passing\n    parameters *y* and *horizontalalignment*.\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    `.Text` properties control the appearance of the label.\n\nSee Also\n--------\ntext : Documents the properties supported by `.Text`.", "short_docstring": "Set the label for the y-axis."}
{"name": "matplotlib.pyplot.imshow", "type": "callable", "signature": "(X: 'ArrayLike | PIL.Image.Image', cmap: 'str | Colormap | None' = None, norm: 'str | Normalize | None' = None, *, aspect: \"Literal['equal', 'auto'] | float | None\" = None, interpolation: 'str | None' = None, alpha: 'float | ArrayLike | None' = None, vmin: 'float | None' = None, vmax: 'float | None' = None, origin: \"Literal['upper', 'lower'] | None\" = None, extent: 'tuple[float, float, float, float] | None' = None, interpolation_stage: \"Literal['data', 'rgba'] | None\" = None, filternorm: 'bool' = True, filterrad: 'float' = 4.0, resample: 'bool | None' = None, url: 'str | None' = None, data=None, **kwargs) -> 'AxesImage'", "docstring": "Display data as an image, i.e., on a 2D regular raster.\n\nThe input may either be actual RGB(A) data, or 2D scalar data, which\nwill be rendered as a pseudocolor image. For displaying a grayscale\nimage, set up the colormapping using the parameters\n``cmap='gray', vmin=0, vmax=255``.\n\nThe number of pixels used to render an image is set by the Axes size\nand the figure *dpi*. This can lead to aliasing artifacts when\nthe image is resampled, because the displayed image size will usually\nnot match the size of *X* (see\n:doc:`/gallery/images_contours_and_fields/image_antialiasing`).\nThe resampling can be controlled via the *interpolation* parameter\nand/or :rc:`image.interpolation`.\n\nParameters\n----------\nX : array-like or PIL image\n    The image data. Supported array shapes are:\n\n    - (M, N): an image with scalar data. The values are mapped to\n      colors using normalization and a colormap. See parameters *norm*,\n      *cmap*, *vmin*, *vmax*.\n    - (M, N, 3): an image with RGB values (0-1 float or 0-255 int).\n    - (M, N, 4): an image with RGBA values (0-1 float or 0-255 int),\n      i.e. including transparency.\n\n    The first two dimensions (M, N) define the rows and columns of\n    the image.\n\n    Out-of-range RGB(A) values are clipped.\n\ncmap : str or `~matplotlib.colors.Colormap`, default: :rc:`image.cmap`\n    The Colormap instance or registered colormap name used to map scalar data\n    to colors.\n\n    This parameter is ignored if *X* is RGB(A).\n\nnorm : str or `~matplotlib.colors.Normalize`, optional\n    The normalization method used to scale scalar data to the [0, 1] range\n    before mapping to colors using *cmap*. By default, a linear scaling is\n    used, mapping the lowest value to 0 and the highest to 1.\n\n    If given, this can be one of the following:\n\n    - An instance of `.Normalize` or one of its subclasses\n      (see :ref:`colormapnorms`).\n    - A scale name, i.e. one of \"linear\", \"log\", \"symlog\", \"logit\", etc.  For a\n      list of available scales, call `matplotlib.scale.get_scale_names()`.\n      In that case, a suitable `.Normalize` subclass is dynamically generated\n      and instantiated.\n\n    This parameter is ignored if *X* is RGB(A).\n\nvmin, vmax : float, optional\n    When using scalar data and no explicit *norm*, *vmin* and *vmax* define\n    the data range that the colormap covers. By default, the colormap covers\n    the complete value range of the supplied data. It is an error to use\n    *vmin*/*vmax* when a *norm* instance is given (but using a `str` *norm*\n    name together with *vmin*/*vmax* is acceptable).\n\n    This parameter is ignored if *X* is RGB(A).\n\naspect : {'equal', 'auto'} or float or None, default: None\n    The aspect ratio of the Axes.  This parameter is particularly\n    relevant for images since it determines whether data pixels are\n    square.\n\n    This parameter is a shortcut for explicitly calling\n    `.Axes.set_aspect`. See there for further details.\n\n    - 'equal': Ensures an aspect ratio of 1. Pixels will be square\n      (unless pixel sizes are explicitly made non-square in data\n      coordinates using *extent*).\n    - 'auto': The Axes is kept fixed and the aspect is adjusted so\n      that the data fit in the Axes. In general, this will result in\n      non-square pixels.\n\n    Normally, None (the default) means to use :rc:`image.aspect`.  However, if\n    the image uses a transform that does not contain the axes data transform,\n    then None means to not modify the axes aspect at all (in that case, directly\n    call `.Axes.set_aspect` if desired).\n\ninterpolation : str, default: :rc:`image.interpolation`\n    The interpolation method used.\n\n    Supported values are 'none', 'antialiased', 'nearest', 'bilinear',\n    'bicubic', 'spline16', 'spline36', 'hanning', 'hamming', 'hermite',\n    'kaiser', 'quadric', 'catrom', 'gaussian', 'bessel', 'mitchell',\n    'sinc', 'lanczos', 'blackman'.\n\n    The data *X* is resampled to the pixel size of the image on the\n    figure canvas, using the interpolation method to either up- or\n    downsample the data.\n\n    If *interpolation* is 'none', then for the ps, pdf, and svg\n    backends no down- or upsampling occurs, and the image data is\n    passed to the backend as a native image.  Note that different ps,\n    pdf, and svg viewers may display these raw pixels differently. On\n    other backends, 'none' is the same as 'nearest'.\n\n    If *interpolation* is the default 'antialiased', then 'nearest'\n    interpolation is used if the image is upsampled by more than a\n    factor of three (i.e. the number of display pixels is at least\n    three times the size of the data array).  If the upsampling rate is\n    smaller than 3, or the image is downsampled, then 'hanning'\n    interpolation is used to act as an anti-aliasing filter, unless the\n    image happens to be upsampled by exactly a factor of two or one.\n\n    See\n    :doc:`/gallery/images_contours_and_fields/interpolation_methods`\n    for an overview of the supported interpolation methods, and\n    :doc:`/gallery/images_contours_and_fields/image_antialiasing` for\n    a discussion of image antialiasing.\n\n    Some interpolation methods require an additional radius parameter,\n    which can be set by *filterrad*. Additionally, the antigrain image\n    resize filter is controlled by the parameter *filternorm*.\n\ninterpolation_stage : {'data', 'rgba'}, default: 'data'\n    If 'data', interpolation\n    is carried out on the data provided by the user.  If 'rgba', the\n    interpolation is carried out after the colormapping has been\n    applied (visual interpolation).\n\nalpha : float or array-like, optional\n    The alpha blending value, between 0 (transparent) and 1 (opaque).\n    If *alpha* is an array, the alpha blending values are applied pixel\n    by pixel, and *alpha* must have the same shape as *X*.\n\norigin : {'upper', 'lower'}, default: :rc:`image.origin`\n    Place the [0, 0] index of the array in the upper left or lower\n    left corner of the Axes. The convention (the default) 'upper' is\n    typically used for matrices and images.\n\n    Note that the vertical axis points upward for 'lower'\n    but downward for 'upper'.\n\n    See the :ref:`imshow_extent` tutorial for\n    examples and a more detailed description.\n\nextent : floats (left, right, bottom, top), optional\n    The bounding box in data coordinates that the image will fill.\n    These values may be unitful and match the units of the Axes.\n    The image is stretched individually along x and y to fill the box.\n\n    The default extent is determined by the following conditions.\n    Pixels have unit size in data coordinates. Their centers are on\n    integer coordinates, and their center coordinates range from 0 to\n    columns-1 horizontally and from 0 to rows-1 vertically.\n\n    Note that the direction of the vertical axis and thus the default\n    values for top and bottom depend on *origin*:\n\n    - For ``origin == 'upper'`` the default is\n      ``(-0.5, numcols-0.5, numrows-0.5, -0.5)``.\n    - For ``origin == 'lower'`` the default is\n      ``(-0.5, numcols-0.5, -0.5, numrows-0.5)``.\n\n    See the :ref:`imshow_extent` tutorial for\n    examples and a more detailed description.\n\nfilternorm : bool, default: True\n    A parameter for the antigrain image resize filter (see the\n    antigrain documentation).  If *filternorm* is set, the filter\n    normalizes integer values and corrects the rounding errors. It\n    doesn't do anything with the source floating point values, it\n    corrects only integers according to the rule of 1.0 which means\n    that any sum of pixel weights must be equal to 1.0.  So, the\n    filter function must produce a graph of the proper shape.\n\nfilterrad : float > 0, default: 4.0\n    The filter radius for filters that have a radius parameter, i.e.\n    when interpolation is one of: 'sinc', 'lanczos' or 'blackman'.\n\nresample : bool, default: :rc:`image.resample`\n    When *True*, use a full resampling method.  When *False*, only\n    resample when the output image is larger than the input image.\n\nurl : str, optional\n    Set the url of the created `.AxesImage`. See `.Artist.set_url`.\n\nReturns\n-------\n`~matplotlib.image.AxesImage`\n\nOther Parameters\n----------------\ndata : indexable object, optional\n    If given, all parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` (unless this raises an exception).\n\n**kwargs : `~matplotlib.artist.Artist` properties\n    These parameters are passed on to the constructor of the\n    `.AxesImage` artist.\n\nSee Also\n--------\nmatshow : Plot a matrix or an array as an image.\n\nNotes\n-----\nUnless *extent* is used, pixel centers will be located at integer\ncoordinates. In other words: the origin will coincide with the center\nof pixel (0, 0).\n\nThere are two common representations for RGB images with an alpha\nchannel:\n\n-   Straight (unassociated) alpha: R, G, and B channels represent the\n    color of the pixel, disregarding its opacity.\n-   Premultiplied (associated) alpha: R, G, and B channels represent\n    the color of the pixel, adjusted for its opacity by multiplication.\n\n`~matplotlib.pyplot.imshow` expects RGB images adopting the straight\n(unassociated) alpha representation.", "short_docstring": "Display data as an image, i.e., on a 2D regular raster."}
{"name": "seaborn.boxplot", "type": "callable", "signature": "(data=None, *, x=None, y=None, hue=None, order=None, hue_order=None, orient=None, color=None, palette=None, saturation=0.75, fill=True, dodge='auto', width=0.8, gap=0, whis=1.5, linecolor='auto', linewidth=None, fliersize=None, hue_norm=None, native_scale=False, log_scale=None, formatter=None, legend='auto', ax=None, **kwargs)", "docstring": "Draw a box plot to show distributions with respect to categories.\n\nA box plot (or box-and-whisker plot) shows the distribution of quantitative\ndata in a way that facilitates comparisons between variables or across\nlevels of a categorical variable. The box shows the quartiles of the\ndataset while the whiskers extend to show the rest of the distribution,\nexcept for points that are determined to be \"outliers\" using a method\nthat is a function of the inter-quartile range.\n\nSee the :ref:`tutorial <categorical_tutorial>` for more information.\n\n.. note::\n    By default, this function treats one of the variables as categorical\n    and draws data at ordinal positions (0, 1, ... n) on the relevant axis.\n    As of version 0.13.0, this can be disabled by setting `native_scale=True`.\n\n\nParameters\n----------\ndata : DataFrame, Series, dict, array, or list of arrays\n    Dataset for plotting. If `x` and `y` are absent, this is\n    interpreted as wide-form. Otherwise it is expected to be long-form.    \nx, y, hue : names of variables in `data` or vector data\n    Inputs for plotting long-form data. See examples for interpretation.    \norder, hue_order : lists of strings\n    Order to plot the categorical levels in; otherwise the levels are\n    inferred from the data objects.    \norient : \"v\" | \"h\" | \"x\" | \"y\"\n    Orientation of the plot (vertical or horizontal). This is usually\n    inferred based on the type of the input variables, but it can be used\n    to resolve ambiguity when both `x` and `y` are numeric or when\n    plotting wide-form data.\n\n    .. versionchanged:: v0.13.0\n        Added 'x'/'y' as options, equivalent to 'v'/'h'.    \ncolor : matplotlib color\n    Single color for the elements in the plot.    \npalette : palette name, list, or dict\n    Colors to use for the different levels of the ``hue`` variable. Should\n    be something that can be interpreted by :func:`color_palette`, or a\n    dictionary mapping hue levels to matplotlib colors.    \nsaturation : float\n    Proportion of the original saturation to draw fill colors in. Large\n    patches often look better with desaturated colors, but set this to\n    `1` if you want the colors to perfectly match the input values.    \nfill : bool\n    If True, use a solid patch. Otherwise, draw as line art.\n\n    .. versionadded:: v0.13.0    \ndodge : \"auto\" or bool\n    When hue mapping is used, whether elements should be narrowed and shifted along\n    the orient axis to eliminate overlap. If `\"auto\"`, set to `True` when the\n    orient variable is crossed with the categorical variable or `False` otherwise.\n\n    .. versionchanged:: 0.13.0\n\n        Added `\"auto\"` mode as a new default.    \nwidth : float\n    Width allotted to each element on the orient axis. When `native_scale=True`,\n    it is relative to the minimum distance between two values in the native scale.    \ngap : float\n    Shrink on the orient axis by this factor to add a gap between dodged elements.\n\n    .. versionadded:: 0.13.0    \nwhis : float or pair of floats\n    Paramater that controls whisker length. If scalar, whiskers are drawn\n    to the farthest datapoint within *whis * IQR* from the nearest hinge.\n    If a tuple, it is interpreted as percentiles that whiskers represent.\nlinecolor : color\n    Color to use for line elements, when `fill` is True.\n\n    .. versionadded:: v0.13.0    \nlinewidth : float\n    Width of the lines that frame the plot elements.    \nfliersize : float\n    Size of the markers used to indicate outlier observations.\nhue_norm : tuple or :class:`matplotlib.colors.Normalize` object\n    Normalization in data units for colormap applied to the `hue`\n    variable when it is numeric. Not relevant if `hue` is categorical.\n\n    .. versionadded:: v0.12.0    \nlog_scale : bool or number, or pair of bools or numbers\n    Set axis scale(s) to log. A single value sets the data axis for any numeric\n    axes in the plot. A pair of values sets each axis independently.\n    Numeric values are interpreted as the desired base (default 10).\n    When `None` or `False`, seaborn defers to the existing Axes scale.\n\n    .. versionadded:: v0.13.0    \nnative_scale : bool\n    When True, numeric or datetime values on the categorical axis will maintain\n    their original scaling rather than being converted to fixed indices.\n\n    .. versionadded:: v0.13.0    \nformatter : callable\n    Function for converting categorical data into strings. Affects both grouping\n    and tick labels.\n\n    .. versionadded:: v0.13.0    \nlegend : \"auto\", \"brief\", \"full\", or False\n    How to draw the legend. If \"brief\", numeric `hue` and `size`\n    variables will be represented with a sample of evenly spaced values.\n    If \"full\", every group will get an entry in the legend. If \"auto\",\n    choose between brief or full representation based on number of levels.\n    If `False`, no legend data is added and no legend is drawn.\n\n    .. versionadded:: v0.13.0    \nax : matplotlib Axes\n    Axes object to draw the plot onto, otherwise uses the current Axes.    \nkwargs : key, value mappings\n    Other keyword arguments are passed through to\n    :meth:`matplotlib.axes.Axes.boxplot`.\n\nReturns\n-------\nax : matplotlib Axes\n    Returns the Axes object with the plot drawn onto it.    \n\nSee Also\n--------\nviolinplot : A combination of boxplot and kernel density estimation.    \nstripplot : A scatterplot where one variable is categorical. Can be used\n            in conjunction with other plots to show each observation.    \nswarmplot : A categorical scatterplot where the points do not overlap. Can\n            be used with other plots to show each observation.    \ncatplot : Combine a categorical plot with a :class:`FacetGrid`.    \n\nExamples\n--------\n.. include:: ../docstrings/boxplot.rst", "short_docstring": "Draw a box plot to show distributions with respect to categories."}
{"name": "librosa.amplitude_to_db", "type": "callable", "signature": "(S: '_ScalarOrSequence[_ComplexLike_co]', *, ref: 'Union[float, Callable]' = 1.0, amin: 'float' = 1e-05, top_db: 'Optional[float]' = 80.0) -> 'Union[np.floating[Any], np.ndarray]'", "docstring": "Convert an amplitude spectrogram to dB-scaled spectrogram.\n\nThis is equivalent to ``power_to_db(S**2, ref=ref**2, amin=amin**2, top_db=top_db)``,\nbut is provided for convenience.\n\nParameters\n----------\nS : np.ndarray\n    input amplitude\n\nref : scalar or callable\n    If scalar, the amplitude ``abs(S)`` is scaled relative to ``ref``:\n    ``20 * log10(S / ref)``.\n    Zeros in the output correspond to positions where ``S == ref``.\n\n    If callable, the reference value is computed as ``ref(S)``.\n\namin : float > 0 [scalar]\n    minimum threshold for ``S`` and ``ref``\n\ntop_db : float >= 0 [scalar]\n    threshold the output at ``top_db`` below the peak:\n    ``max(20 * log10(S/ref)) - top_db``\n\nReturns\n-------\nS_db : np.ndarray\n    ``S`` measured in dB\n\nSee Also\n--------\npower_to_db, db_to_amplitude\n\nNotes\n-----\nThis function caches at level 30.", "short_docstring": "Convert an amplitude spectrogram to dB-scaled spectrogram."}
{"name": "random.randint", "type": "callable", "signature": "(a, b)", "docstring": "Return random integer in range [a, b], including both end points.\n        ", "short_docstring": "Return random integer in range [a, b], including both end points.\n        "}
{"name": "nltk.download", "type": "callable", "signature": "(info_or_id=None, download_dir=None, quiet=False, force=False, prefix='[nltk_data] ', halt_on_error=True, raise_on_error=False, print_error_to=<_io.TextIOWrapper name='<stderr>' mode='w' encoding='utf-8'>)", "docstring": null, "short_docstring": ""}
{"name": "scipy.fftpack.fft", "type": "callable", "signature": "(x, n=None, axis=-1, overwrite_x=False)", "docstring": "Return discrete Fourier transform of real or complex sequence.\n\nThe returned complex array contains ``y(0), y(1),..., y(n-1)``, where\n\n``y(j) = (x * exp(-2*pi*sqrt(-1)*j*np.arange(n)/n)).sum()``.\n\nParameters\n----------\nx : array_like\n    Array to Fourier transform.\nn : int, optional\n    Length of the Fourier transform. If ``n < x.shape[axis]``, `x` is\n    truncated. If ``n > x.shape[axis]``, `x` is zero-padded. The\n    default results in ``n = x.shape[axis]``.\naxis : int, optional\n    Axis along which the fft's are computed; the default is over the\n    last axis (i.e., ``axis=-1``).\noverwrite_x : bool, optional\n    If True, the contents of `x` can be destroyed; the default is False.\n\nReturns\n-------\nz : complex ndarray\n    with the elements::\n\n        [y(0),y(1),..,y(n/2),y(1-n/2),...,y(-1)]        if n is even\n        [y(0),y(1),..,y((n-1)/2),y(-(n-1)/2),...,y(-1)]  if n is odd\n\n    where::\n\n        y(j) = sum[k=0..n-1] x[k] * exp(-sqrt(-1)*j*k* 2*pi/n), j = 0..n-1\n\nSee Also\n--------\nifft : Inverse FFT\nrfft : FFT of a real sequence\n\nNotes\n-----\nThe packing of the result is \"standard\": If ``A = fft(a, n)``, then\n``A[0]`` contains the zero-frequency term, ``A[1:n/2]`` contains the\npositive-frequency terms, and ``A[n/2:]`` contains the negative-frequency\nterms, in order of decreasingly negative frequency. So ,for an 8-point\ntransform, the frequencies of the result are [0, 1, 2, 3, -4, -3, -2, -1].\nTo rearrange the fft output so that the zero-frequency component is\ncentered, like [-4, -3, -2, -1,  0,  1,  2,  3], use `fftshift`.\n\nBoth single and double precision routines are implemented. Half precision\ninputs will be converted to single precision. Non-floating-point inputs\nwill be converted to double precision. Long-double precision inputs are\nnot supported.\n\nThis function is most efficient when `n` is a power of two, and least\nefficient when `n` is prime.\n\nNote that if ``x`` is real-valued, then ``A[j] == A[n-j].conjugate()``.\nIf ``x`` is real-valued and ``n`` is even, then ``A[n/2]`` is real.\n\nIf the data type of `x` is real, a \"real FFT\" algorithm is automatically\nused, which roughly halves the computation time. To increase efficiency\na little further, use `rfft`, which does the same calculation, but only\noutputs half of the symmetrical spectrum. If the data is both real and\nsymmetrical, the `dct` can again double the efficiency by generating\nhalf of the spectrum from half of the signal.\n\nExamples\n--------\n>>> import numpy as np\n>>> from scipy.fftpack import fft, ifft\n>>> x = np.arange(5)\n>>> np.allclose(fft(ifft(x)), x, atol=1e-15)  # within numerical accuracy.\nTrue", "short_docstring": "Return discrete Fourier transform of real or complex sequence."}
{"name": "json.dumps", "type": "callable", "signature": "(obj, *, skipkeys=False, ensure_ascii=True, check_circular=True, allow_nan=True, cls=None, indent=None, separators=None, default=None, sort_keys=False, **kw)", "docstring": "Serialize ``obj`` to a JSON formatted ``str``.\n\nIf ``skipkeys`` is true then ``dict`` keys that are not basic types\n(``str``, ``int``, ``float``, ``bool``, ``None``) will be skipped\ninstead of raising a ``TypeError``.\n\nIf ``ensure_ascii`` is false, then the return value can contain non-ASCII\ncharacters if they appear in strings contained in ``obj``. Otherwise, all\nsuch characters are escaped in JSON strings.\n\nIf ``check_circular`` is false, then the circular reference check\nfor container types will be skipped and a circular reference will\nresult in an ``OverflowError`` (or worse).\n\nIf ``allow_nan`` is false, then it will be a ``ValueError`` to\nserialize out of range ``float`` values (``nan``, ``inf``, ``-inf``) in\nstrict compliance of the JSON specification, instead of using the\nJavaScript equivalents (``NaN``, ``Infinity``, ``-Infinity``).\n\nIf ``indent`` is a non-negative integer, then JSON array elements and\nobject members will be pretty-printed with that indent level. An indent\nlevel of 0 will only insert newlines. ``None`` is the most compact\nrepresentation.\n\nIf specified, ``separators`` should be an ``(item_separator, key_separator)``\ntuple.  The default is ``(', ', ': ')`` if *indent* is ``None`` and\n``(',', ': ')`` otherwise.  To get the most compact JSON representation,\nyou should specify ``(',', ':')`` to eliminate whitespace.\n\n``default(obj)`` is a function that should return a serializable version\nof obj or raise TypeError. The default simply raises TypeError.\n\nIf *sort_keys* is true (default: ``False``), then the output of\ndictionaries will be sorted by key.\n\nTo use a custom ``JSONEncoder`` subclass (e.g. one that overrides the\n``.default()`` method to serialize additional types), specify it with\nthe ``cls`` kwarg; otherwise ``JSONEncoder`` is used.", "short_docstring": "Serialize ``obj`` to a JSON formatted ``str``."}
{"name": "ssl.SSLContext", "type": "class", "signature": "(protocol=None, *args, **kwargs)", "docstring": "An SSLContext holds various SSL-related configuration options and\ndata, such as certificates and possibly a private key.", "short_docstring": "An SSLContext holds various SSL-related configuration options and\ndata, such as certificates and possibly a private key."}
{"name": "csv.writer", "type": "callable", "signature": null, "docstring": "csv_writer = csv.writer(fileobj [, dialect='excel']\n                            [optional keyword args])\n    for row in sequence:\n        csv_writer.writerow(row)\n\n    [or]\n\n    csv_writer = csv.writer(fileobj [, dialect='excel']\n                            [optional keyword args])\n    csv_writer.writerows(rows)\n\nThe \"fileobj\" argument can be any object that supports the file API.", "short_docstring": "csv_writer = csv.writer(fileobj [, dialect='excel']\n                            [optional keyword args])\n    for row in sequence:\n        csv_writer.writerow(row)"}
{"name": "datetime.timezone.utc", "type": "constant", "value": "UTC", "signature": null, "docstring": "Fixed offset from UTC implementation of tzinfo.", "short_docstring": "Fixed offset from UTC implementation of tzinfo."}
{"name": "lxml.etree.XML", "type": "callable", "signature": "(text, parser=None, *, base_url=None)", "docstring": "XML(text, parser=None, base_url=None)\n\nParses an XML document or fragment from a string constant.\nReturns the root node (or the result returned by a parser target).\nThis function can be used to embed \"XML literals\" in Python code,\nlike in\n\n   >>> root = XML(\"<root><test/></root>\")\n   >>> print(root.tag)\n   root\n\nTo override the parser with a different ``XMLParser`` you can pass it to\nthe ``parser`` keyword argument.\n\nThe ``base_url`` keyword argument allows to set the original base URL of\nthe document to support relative Paths when looking up external entities\n(DTD, XInclude, ...).", "short_docstring": "XML(text, parser=None, base_url=None)"}
{"name": "datetime.datetime", "type": "class", "signature": null, "docstring": "datetime(year, month, day[, hour[, minute[, second[, microsecond[,tzinfo]]]]])\n\nThe year, month and day arguments are required. tzinfo may be None, or an\ninstance of a tzinfo subclass. The remaining arguments may be ints.", "short_docstring": "datetime(year, month, day[, hour[, minute[, second[, microsecond[,tzinfo]]]]])"}
{"name": "subprocess.run", "type": "callable", "signature": "(*popenargs, input=None, capture_output=False, timeout=None, check=False, **kwargs)", "docstring": "Run command with arguments and return a CompletedProcess instance.\n\nThe returned instance will have attributes args, returncode, stdout and\nstderr. By default, stdout and stderr are not captured, and those attributes\nwill be None. Pass stdout=PIPE and/or stderr=PIPE in order to capture them.\n\nIf check is True and the exit code was non-zero, it raises a\nCalledProcessError. The CalledProcessError object will have the return code\nin the returncode attribute, and output & stderr attributes if those streams\nwere captured.\n\nIf timeout is given, and the process takes too long, a TimeoutExpired\nexception will be raised.\n\nThere is an optional argument \"input\", allowing you to\npass bytes or a string to the subprocess's stdin.  If you use this argument\nyou may not also use the Popen constructor's \"stdin\" argument, as\nit will be used internally.\n\nBy default, all communication is in bytes, and therefore any \"input\" should\nbe bytes, and the stdout and stderr will be bytes. If in text mode, any\n\"input\" should be a string, and stdout and stderr will be strings decoded\naccording to locale encoding, or by \"encoding\" if set. Text mode is\ntriggered by setting any of text, encoding, errors or universal_newlines.\n\nThe other arguments are the same as for the Popen constructor.", "short_docstring": "Run command with arguments and return a CompletedProcess instance."}
{"name": "pandas.concat", "type": "callable", "signature": "(objs: 'Iterable[Series | DataFrame] | Mapping[HashableT, Series | DataFrame]', *, axis: 'Axis' = 0, join: 'str' = 'outer', ignore_index: 'bool' = False, keys: 'Iterable[Hashable] | None' = None, levels=None, names: 'list[HashableT] | None' = None, verify_integrity: 'bool' = False, sort: 'bool' = False, copy: 'bool | None' = None) -> 'DataFrame | Series'", "docstring": "Concatenate pandas objects along a particular axis.\n\nAllows optional set logic along the other axes.\n\nCan also add a layer of hierarchical indexing on the concatenation axis,\nwhich may be useful if the labels are the same (or overlapping) on\nthe passed axis number.\n\nParameters\n----------\nobjs : a sequence or mapping of Series or DataFrame objects\n    If a mapping is passed, the sorted keys will be used as the `keys`\n    argument, unless it is passed, in which case the values will be\n    selected (see below). Any None objects will be dropped silently unless\n    they are all None in which case a ValueError will be raised.\naxis : {0/'index', 1/'columns'}, default 0\n    The axis to concatenate along.\njoin : {'inner', 'outer'}, default 'outer'\n    How to handle indexes on other axis (or axes).\nignore_index : bool, default False\n    If True, do not use the index values along the concatenation axis. The\n    resulting axis will be labeled 0, ..., n - 1. This is useful if you are\n    concatenating objects where the concatenation axis does not have\n    meaningful indexing information. Note the index values on the other\n    axes are still respected in the join.\nkeys : sequence, default None\n    If multiple levels passed, should contain tuples. Construct\n    hierarchical index using the passed keys as the outermost level.\nlevels : list of sequences, default None\n    Specific levels (unique values) to use for constructing a\n    MultiIndex. Otherwise they will be inferred from the keys.\nnames : list, default None\n    Names for the levels in the resulting hierarchical index.\nverify_integrity : bool, default False\n    Check whether the new concatenated axis contains duplicates. This can\n    be very expensive relative to the actual data concatenation.\nsort : bool, default False\n    Sort non-concatenation axis if it is not already aligned. One exception to\n    this is when the non-concatentation axis is a DatetimeIndex and join='outer'\n    and the axis is not already aligned. In that case, the non-concatenation\n    axis is always sorted lexicographically.\ncopy : bool, default True\n    If False, do not copy data unnecessarily.\n\nReturns\n-------\nobject, type of objs\n    When concatenating all ``Series`` along the index (axis=0), a\n    ``Series`` is returned. When ``objs`` contains at least one\n    ``DataFrame``, a ``DataFrame`` is returned. When concatenating along\n    the columns (axis=1), a ``DataFrame`` is returned.\n\nSee Also\n--------\nDataFrame.join : Join DataFrames using indexes.\nDataFrame.merge : Merge DataFrames by indexes or columns.\n\nNotes\n-----\nThe keys, levels, and names arguments are all optional.\n\nA walkthrough of how this method fits in with other tools for combining\npandas objects can be found `here\n<https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html>`__.\n\nIt is not recommended to build DataFrames by adding single rows in a\nfor loop. Build a list of rows and make a DataFrame in a single concat.\n\nExamples\n--------\nCombine two ``Series``.\n\n>>> s1 = pd.Series(['a', 'b'])\n>>> s2 = pd.Series(['c', 'd'])\n>>> pd.concat([s1, s2])\n0    a\n1    b\n0    c\n1    d\ndtype: object\n\nClear the existing index and reset it in the result\nby setting the ``ignore_index`` option to ``True``.\n\n>>> pd.concat([s1, s2], ignore_index=True)\n0    a\n1    b\n2    c\n3    d\ndtype: object\n\nAdd a hierarchical index at the outermost level of\nthe data with the ``keys`` option.\n\n>>> pd.concat([s1, s2], keys=['s1', 's2'])\ns1  0    a\n    1    b\ns2  0    c\n    1    d\ndtype: object\n\nLabel the index keys you create with the ``names`` option.\n\n>>> pd.concat([s1, s2], keys=['s1', 's2'],\n...           names=['Series name', 'Row ID'])\nSeries name  Row ID\ns1           0         a\n             1         b\ns2           0         c\n             1         d\ndtype: object\n\nCombine two ``DataFrame`` objects with identical columns.\n\n>>> df1 = pd.DataFrame([['a', 1], ['b', 2]],\n...                    columns=['letter', 'number'])\n>>> df1\n  letter  number\n0      a       1\n1      b       2\n>>> df2 = pd.DataFrame([['c', 3], ['d', 4]],\n...                    columns=['letter', 'number'])\n>>> df2\n  letter  number\n0      c       3\n1      d       4\n>>> pd.concat([df1, df2])\n  letter  number\n0      a       1\n1      b       2\n0      c       3\n1      d       4\n\nCombine ``DataFrame`` objects with overlapping columns\nand return everything. Columns outside the intersection will\nbe filled with ``NaN`` values.\n\n>>> df3 = pd.DataFrame([['c', 3, 'cat'], ['d', 4, 'dog']],\n...                    columns=['letter', 'number', 'animal'])\n>>> df3\n  letter  number animal\n0      c       3    cat\n1      d       4    dog\n>>> pd.concat([df1, df3], sort=False)\n  letter  number animal\n0      a       1    NaN\n1      b       2    NaN\n0      c       3    cat\n1      d       4    dog\n\nCombine ``DataFrame`` objects with overlapping columns\nand return only those that are shared by passing ``inner`` to\nthe ``join`` keyword argument.\n\n>>> pd.concat([df1, df3], join=\"inner\")\n  letter  number\n0      a       1\n1      b       2\n0      c       3\n1      d       4\n\nCombine ``DataFrame`` objects horizontally along the x axis by\npassing in ``axis=1``.\n\n>>> df4 = pd.DataFrame([['bird', 'polly'], ['monkey', 'george']],\n...                    columns=['animal', 'name'])\n>>> pd.concat([df1, df4], axis=1)\n  letter  number  animal    name\n0      a       1    bird   polly\n1      b       2  monkey  george\n\nPrevent the result from including duplicate index values with the\n``verify_integrity`` option.\n\n>>> df5 = pd.DataFrame([1], index=['a'])\n>>> df5\n   0\na  1\n>>> df6 = pd.DataFrame([2], index=['a'])\n>>> df6\n   0\na  2\n>>> pd.concat([df5, df6], verify_integrity=True)\nTraceback (most recent call last):\n    ...\nValueError: Indexes have overlapping values: ['a']\n\nAppend a single row to the end of a ``DataFrame`` object.\n\n>>> df7 = pd.DataFrame({'a': 1, 'b': 2}, index=[0])\n>>> df7\n    a   b\n0   1   2\n>>> new_row = pd.Series({'a': 3, 'b': 4})\n>>> new_row\na    3\nb    4\ndtype: int64\n>>> pd.concat([df7, new_row.to_frame().T], ignore_index=True)\n    a   b\n0   1   2\n1   3   4", "short_docstring": "Concatenate pandas objects along a particular axis."}
{"name": "numpy.exp", "type": "callable", "signature": null, "docstring": "exp(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n\nCalculate the exponential of all elements in the input array.\n\nParameters\n----------\nx : array_like\n    Input values.\nout : ndarray, None, or tuple of ndarray and None, optional\n    A location into which the result is stored. If provided, it must have\n    a shape that the inputs broadcast to. If not provided or None,\n    a freshly-allocated array is returned. A tuple (possible only as a\n    keyword argument) must have length equal to the number of outputs.\nwhere : array_like, optional\n    This condition is broadcast over the input. At locations where the\n    condition is True, the `out` array will be set to the ufunc result.\n    Elsewhere, the `out` array will retain its original value.\n    Note that if an uninitialized `out` array is created via the default\n    ``out=None``, locations within it where the condition is False will\n    remain uninitialized.\n**kwargs\n    For other keyword-only arguments, see the\n    :ref:`ufunc docs <ufuncs.kwargs>`.\n\nReturns\n-------\nout : ndarray or scalar\n    Output array, element-wise exponential of `x`.\n    This is a scalar if `x` is a scalar.\n\nSee Also\n--------\nexpm1 : Calculate ``exp(x) - 1`` for all elements in the array.\nexp2  : Calculate ``2**x`` for all elements in the array.\n\nNotes\n-----\nThe irrational number ``e`` is also known as Euler's number.  It is\napproximately 2.718281, and is the base of the natural logarithm,\n``ln`` (this means that, if :math:`x = \\ln y = \\log_e y`,\nthen :math:`e^x = y`. For real input, ``exp(x)`` is always positive.\n\nFor complex arguments, ``x = a + ib``, we can write\n:math:`e^x = e^a e^{ib}`.  The first term, :math:`e^a`, is already\nknown (it is the real argument, described above).  The second term,\n:math:`e^{ib}`, is :math:`\\cos b + i \\sin b`, a function with\nmagnitude 1 and a periodic phase.\n\nReferences\n----------\n.. [1] Wikipedia, \"Exponential function\",\n       https://en.wikipedia.org/wiki/Exponential_function\n.. [2] M. Abramovitz and I. A. Stegun, \"Handbook of Mathematical Functions\n       with Formulas, Graphs, and Mathematical Tables,\" Dover, 1964, p. 69,\n       https://personal.math.ubc.ca/~cbm/aands/page_69.htm\n\nExamples\n--------\nPlot the magnitude and phase of ``exp(x)`` in the complex plane:\n\n>>> import matplotlib.pyplot as plt\n\n>>> x = np.linspace(-2*np.pi, 2*np.pi, 100)\n>>> xx = x + 1j * x[:, np.newaxis] # a + ib over complex plane\n>>> out = np.exp(xx)\n\n>>> plt.subplot(121)\n>>> plt.imshow(np.abs(out),\n...            extent=[-2*np.pi, 2*np.pi, -2*np.pi, 2*np.pi], cmap='gray')\n>>> plt.title('Magnitude of exp(x)')\n\n>>> plt.subplot(122)\n>>> plt.imshow(np.angle(out),\n...            extent=[-2*np.pi, 2*np.pi, -2*np.pi, 2*np.pi], cmap='hsv')\n>>> plt.title('Phase (angle) of exp(x)')\n>>> plt.show()", "short_docstring": "exp(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])"}
{"name": "matplotlib.pyplot.figure", "type": "callable", "signature": "(num: 'int | str | Figure | SubFigure | None' = None, figsize: 'tuple[float, float] | None' = None, dpi: 'float | None' = None, *, facecolor: 'ColorType | None' = None, edgecolor: 'ColorType | None' = None, frameon: 'bool' = True, FigureClass: 'type[Figure]' = <class 'matplotlib.figure.Figure'>, clear: 'bool' = False, **kwargs) -> 'Figure'", "docstring": "Create a new figure, or activate an existing figure.\n\nParameters\n----------\nnum : int or str or `.Figure` or `.SubFigure`, optional\n    A unique identifier for the figure.\n\n    If a figure with that identifier already exists, this figure is made\n    active and returned. An integer refers to the ``Figure.number``\n    attribute, a string refers to the figure label.\n\n    If there is no figure with the identifier or *num* is not given, a new\n    figure is created, made active and returned.  If *num* is an int, it\n    will be used for the ``Figure.number`` attribute, otherwise, an\n    auto-generated integer value is used (starting at 1 and incremented\n    for each new figure). If *num* is a string, the figure label and the\n    window title is set to this value.  If num is a ``SubFigure``, its\n    parent ``Figure`` is activated.\n\nfigsize : (float, float), default: :rc:`figure.figsize`\n    Width, height in inches.\n\ndpi : float, default: :rc:`figure.dpi`\n    The resolution of the figure in dots-per-inch.\n\nfacecolor : color, default: :rc:`figure.facecolor`\n    The background color.\n\nedgecolor : color, default: :rc:`figure.edgecolor`\n    The border color.\n\nframeon : bool, default: True\n    If False, suppress drawing the figure frame.\n\nFigureClass : subclass of `~matplotlib.figure.Figure`\n    If set, an instance of this subclass will be created, rather than a\n    plain `.Figure`.\n\nclear : bool, default: False\n    If True and the figure already exists, then it is cleared.\n\nlayout : {'constrained', 'compressed', 'tight', 'none', `.LayoutEngine`, None}, default: None\n    The layout mechanism for positioning of plot elements to avoid\n    overlapping Axes decorations (labels, ticks, etc). Note that layout\n    managers can measurably slow down figure display.\n\n    - 'constrained': The constrained layout solver adjusts axes sizes\n      to avoid overlapping axes decorations.  Can handle complex plot\n      layouts and colorbars, and is thus recommended.\n\n      See :ref:`constrainedlayout_guide`\n      for examples.\n\n    - 'compressed': uses the same algorithm as 'constrained', but\n      removes extra space between fixed-aspect-ratio Axes.  Best for\n      simple grids of axes.\n\n    - 'tight': Use the tight layout mechanism. This is a relatively\n      simple algorithm that adjusts the subplot parameters so that\n      decorations do not overlap. See `.Figure.set_tight_layout` for\n      further details.\n\n    - 'none': Do not use a layout engine.\n\n    - A `.LayoutEngine` instance. Builtin layout classes are\n      `.ConstrainedLayoutEngine` and `.TightLayoutEngine`, more easily\n      accessible by 'constrained' and 'tight'.  Passing an instance\n      allows third parties to provide their own layout engine.\n\n    If not given, fall back to using the parameters *tight_layout* and\n    *constrained_layout*, including their config defaults\n    :rc:`figure.autolayout` and :rc:`figure.constrained_layout.use`.\n\n**kwargs\n    Additional keyword arguments are passed to the `.Figure` constructor.\n\nReturns\n-------\n`~matplotlib.figure.Figure`\n\nNotes\n-----\nA newly created figure is passed to the `~.FigureCanvasBase.new_manager`\nmethod or the `new_figure_manager` function provided by the current\nbackend, which install a canvas and a manager on the figure.\n\nOnce this is done, :rc:`figure.hooks` are called, one at a time, on the\nfigure; these hooks allow arbitrary customization of the figure (e.g.,\nattaching callbacks) or of associated elements (e.g., modifying the\ntoolbar).  See :doc:`/gallery/user_interfaces/mplcvd` for an example of\ntoolbar customization.\n\nIf you are creating many figures, make sure you explicitly call\n`.pyplot.close` on the figures you are not using, because this will\nenable pyplot to properly clean up the memory.\n\n`~matplotlib.rcParams` defines the default values, which can be modified\nin the matplotlibrc file.", "short_docstring": "Create a new figure, or activate an existing figure."}
{"name": "itertools.zip_longest", "type": "class", "signature": null, "docstring": "zip_longest(iter1 [,iter2 [...]], [fillvalue=None]) --> zip_longest object\n\nReturn a zip_longest object whose .__next__() method returns a tuple where\nthe i-th element comes from the i-th iterable argument.  The .__next__()\nmethod continues until the longest iterable in the argument sequence\nis exhausted and then it raises StopIteration.  When the shorter iterables\nare exhausted, the fillvalue is substituted in their place.  The fillvalue\ndefaults to None or can be specified by a keyword argument.", "short_docstring": "zip_longest(iter1 [,iter2 [...]], [fillvalue=None]) --> zip_longest object"}
{"name": "matplotlib.pyplot.rc", "type": "callable", "signature": "(group: 'str', **kwargs) -> 'None'", "docstring": "Set the current `.rcParams`.  *group* is the grouping for the rc, e.g.,\nfor ``lines.linewidth`` the group is ``lines``, for\n``axes.facecolor``, the group is ``axes``, and so on.  Group may\nalso be a list or tuple of group names, e.g., (*xtick*, *ytick*).\n*kwargs* is a dictionary attribute name/value pairs, e.g.,::\n\n  rc('lines', linewidth=2, color='r')\n\nsets the current `.rcParams` and is equivalent to::\n\n  rcParams['lines.linewidth'] = 2\n  rcParams['lines.color'] = 'r'\n\nThe following aliases are available to save typing for interactive users:\n\n=====   =================\nAlias   Property\n=====   =================\n'lw'    'linewidth'\n'ls'    'linestyle'\n'c'     'color'\n'fc'    'facecolor'\n'ec'    'edgecolor'\n'mew'   'markeredgewidth'\n'aa'    'antialiased'\n=====   =================\n\nThus you could abbreviate the above call as::\n\n      rc('lines', lw=2, c='r')\n\nNote you can use python's kwargs dictionary facility to store\ndictionaries of default parameters.  e.g., you can customize the\nfont rc as follows::\n\n  font = {'family' : 'monospace',\n          'weight' : 'bold',\n          'size'   : 'larger'}\n  rc('font', **font)  # pass in the font dict as kwargs\n\nThis enables you to easily switch between several configurations.  Use\n``matplotlib.style.use('default')`` or :func:`~matplotlib.rcdefaults` to\nrestore the default `.rcParams` after changes.\n\nNotes\n-----\nSimilar functionality is available by using the normal dict interface, i.e.\n``rcParams.update({\"lines.linewidth\": 2, ...})`` (but ``rcParams.update``\ndoes not support abbreviations or grouping).", "short_docstring": "Set the current `.rcParams`.  *group* is the grouping for the rc, e.g.,\nfor ``lines.linewidth`` the group is ``lines``, for\n``axes.facecolor``, the group is ``axes``, and so on.  Group may\nalso be a list or tuple of group names, e.g., (*xtick*, *ytick*).\n*kwargs* is a dictionary attribute name/value pairs, e.g.,::"}
{"name": "flask_wtf.FlaskForm", "type": "class", "signature": "(*args, **kwargs)", "docstring": "Flask-specific subclass of WTForms :class:`~wtforms.form.Form`.\n\nIf ``formdata`` is not specified, this will use :attr:`flask.request.form`\nand :attr:`flask.request.files`.  Explicitly pass ``formdata=None`` to\nprevent this.", "short_docstring": "Flask-specific subclass of WTForms :class:`~wtforms.form.Form`."}
{"name": "psutil.process_iter", "type": "callable", "signature": "(attrs=None, ad_value=None)", "docstring": "Return a generator yielding a Process instance for all\nrunning processes.\n\nEvery new Process instance is only created once and then cached\ninto an internal table which is updated every time this is used.\n\nCached Process instances are checked for identity so that you're\nsafe in case a PID has been reused by another process, in which\ncase the cached instance is updated.\n\nThe sorting order in which processes are yielded is based on\ntheir PIDs.\n\n*attrs* and *ad_value* have the same meaning as in\nProcess.as_dict(). If *attrs* is specified as_dict() is called\nand the resulting dict is stored as a 'info' attribute attached\nto returned Process instance.\nIf *attrs* is an empty list it will retrieve all process info\n(slow).", "short_docstring": "Return a generator yielding a Process instance for all\nrunning processes."}
{"name": "pandas.Timedelta", "type": "class", "signature": "(value=<object object at 0x7ffa4f0a8040>, unit=None, **kwargs)", "docstring": "Represents a duration, the difference between two dates or times.\n\nTimedelta is the pandas equivalent of python's ``datetime.timedelta``\nand is interchangeable with it in most cases.\n\nParameters\n----------\nvalue : Timedelta, timedelta, np.timedelta64, str, or int\nunit : str, default 'ns'\n    Denote the unit of the input, if input is an integer.\n\n    Possible values:\n\n    * 'W', or 'D'\n    * 'days', or 'day'\n    * 'hours', 'hour', 'hr', or 'h'\n    * 'minutes', 'minute', 'min', or 'm'\n    * 'seconds', 'second', 'sec', or 's'\n    * 'milliseconds', 'millisecond', 'millis', 'milli', or 'ms'\n    * 'microseconds', 'microsecond', 'micros', 'micro', or 'us'\n    * 'nanoseconds', 'nanosecond', 'nanos', 'nano', or 'ns'.\n\n    .. deprecated:: 2.2.0\n\n        Values `H`, `T`, `S`, `L`, `U`, and `N` are deprecated in favour\n        of the values `h`, `min`, `s`, `ms`, `us`, and `ns`.\n\n**kwargs\n    Available kwargs: {days, seconds, microseconds,\n    milliseconds, minutes, hours, weeks}.\n    Values for construction in compat with datetime.timedelta.\n    Numpy ints and floats will be coerced to python ints and floats.\n\nNotes\n-----\nThe constructor may take in either both values of value and unit or\nkwargs as above. Either one of them must be used during initialization\n\nThe ``.value`` attribute is always in ns.\n\nIf the precision is higher than nanoseconds, the precision of the duration is\ntruncated to nanoseconds.\n\nExamples\n--------\nHere we initialize Timedelta object with both value and unit\n\n>>> td = pd.Timedelta(1, \"d\")\n>>> td\nTimedelta('1 days 00:00:00')\n\nHere we initialize the Timedelta object with kwargs\n\n>>> td2 = pd.Timedelta(days=1)\n>>> td2\nTimedelta('1 days 00:00:00')\n\nWe see that either way we get the same result", "short_docstring": "Represents a duration, the difference between two dates or times."}
{"name": "itertools.product", "type": "class", "signature": null, "docstring": "product(*iterables, repeat=1) --> product object\n\nCartesian product of input iterables.  Equivalent to nested for-loops.\n\nFor example, product(A, B) returns the same as:  ((x,y) for x in A for y in B).\nThe leftmost iterators are in the outermost for-loop, so the output tuples\ncycle in a manner similar to an odometer (with the rightmost element changing\non every iteration).\n\nTo compute the product of an iterable with itself, specify the number\nof repetitions with the optional repeat keyword argument. For example,\nproduct(A, repeat=4) means the same as product(A, A, A, A).\n\nproduct('ab', range(3)) --> ('a',0) ('a',1) ('a',2) ('b',0) ('b',1) ('b',2)\nproduct((0,1), (0,1), (0,1)) --> (0,0,0) (0,0,1) (0,1,0) (0,1,1) (1,0,0) ...", "short_docstring": "product(*iterables, repeat=1) --> product object"}
{"name": "keras.optimizers.SGD", "error": "Import error: No module named 'tensorflow'"}
{"name": "ssl.PROTOCOL_TLS_SERVER", "type": "constant", "value": "_SSLMethod.PROTOCOL_TLS_SERVER", "signature": null, "docstring": "An enumeration.", "short_docstring": "An enumeration."}
{"name": "geopandas.GeoDataFrame", "type": "class", "signature": "(data=None, *args, geometry=None, crs=None, **kwargs)", "docstring": "A GeoDataFrame object is a pandas.DataFrame that has a column\nwith geometry. In addition to the standard DataFrame constructor arguments,\nGeoDataFrame also accepts the following keyword arguments:\n\nParameters\n----------\ncrs : value (optional)\n    Coordinate Reference System of the geometry objects. Can be anything accepted by\n    :meth:`pyproj.CRS.from_user_input() <pyproj.crs.CRS.from_user_input>`,\n    such as an authority string (eg \"EPSG:4326\") or a WKT string.\ngeometry : str or array (optional)\n    If str, column to use as geometry. If array, will be set as 'geometry'\n    column on GeoDataFrame.\n\nExamples\n--------\nConstructing GeoDataFrame from a dictionary.\n\n>>> from shapely.geometry import Point\n>>> d = {'col1': ['name1', 'name2'], 'geometry': [Point(1, 2), Point(2, 1)]}\n>>> gdf = geopandas.GeoDataFrame(d, crs=\"EPSG:4326\")\n>>> gdf\n    col1                 geometry\n0  name1  POINT (1.00000 2.00000)\n1  name2  POINT (2.00000 1.00000)\n\nNotice that the inferred dtype of 'geometry' columns is geometry.\n\n>>> gdf.dtypes\ncol1          object\ngeometry    geometry\ndtype: object\n\nConstructing GeoDataFrame from a pandas DataFrame with a column of WKT geometries:\n\n>>> import pandas as pd\n>>> d = {'col1': ['name1', 'name2'], 'wkt': ['POINT (1 2)', 'POINT (2 1)']}\n>>> df = pd.DataFrame(d)\n>>> gs = geopandas.GeoSeries.from_wkt(df['wkt'])\n>>> gdf = geopandas.GeoDataFrame(df, geometry=gs, crs=\"EPSG:4326\")\n>>> gdf\n    col1          wkt                 geometry\n0  name1  POINT (1 2)  POINT (1.00000 2.00000)\n1  name2  POINT (2 1)  POINT (2.00000 1.00000)\n\nSee also\n--------\nGeoSeries : Series object designed to store shapely geometry objects", "short_docstring": "A GeoDataFrame object is a pandas.DataFrame that has a column\nwith geometry. In addition to the standard DataFrame constructor arguments,\nGeoDataFrame also accepts the following keyword arguments:"}
{"name": "matplotlib.pyplot.xticks", "type": "callable", "signature": "(ticks: 'ArrayLike | None' = None, labels: 'Sequence[str] | None' = None, *, minor: 'bool' = False, **kwargs) -> 'tuple[list[Tick] | np.ndarray, list[Text]]'", "docstring": "Get or set the current tick locations and labels of the x-axis.\n\nPass no arguments to return the current values without modifying them.\n\nParameters\n----------\nticks : array-like, optional\n    The list of xtick locations.  Passing an empty list removes all xticks.\nlabels : array-like, optional\n    The labels to place at the given *ticks* locations.  This argument can\n    only be passed if *ticks* is passed as well.\nminor : bool, default: False\n    If ``False``, get/set the major ticks/labels; if ``True``, the minor\n    ticks/labels.\n**kwargs\n    `.Text` properties can be used to control the appearance of the labels.\n\nReturns\n-------\nlocs\n    The list of xtick locations.\nlabels\n    The list of xlabel `.Text` objects.\n\nNotes\n-----\nCalling this function with no arguments (e.g. ``xticks()``) is the pyplot\nequivalent of calling `~.Axes.get_xticks` and `~.Axes.get_xticklabels` on\nthe current axes.\nCalling this function with arguments is the pyplot equivalent of calling\n`~.Axes.set_xticks` and `~.Axes.set_xticklabels` on the current axes.\n\nExamples\n--------\n>>> locs, labels = xticks()  # Get the current locations and labels.\n>>> xticks(np.arange(0, 1, step=0.2))  # Set label locations.\n>>> xticks(np.arange(3), ['Tom', 'Dick', 'Sue'])  # Set text labels.\n>>> xticks([0, 1, 2], ['January', 'February', 'March'],\n...        rotation=20)  # Set text labels and properties.\n>>> xticks([])  # Disable xticks.", "short_docstring": "Get or set the current tick locations and labels of the x-axis."}
{"name": "flask_login.login_required", "type": "callable", "signature": "(func)", "docstring": "If you decorate a view with this, it will ensure that the current user is\nlogged in and authenticated before calling the actual view. (If they are\nnot, it calls the :attr:`LoginManager.unauthorized` callback.) For\nexample::\n\n    @app.route('/post')\n    @login_required\n    def post():\n        pass\n\nIf there are only certain times you need to require that your user is\nlogged in, you can do so with::\n\n    if not current_user.is_authenticated:\n        return current_app.login_manager.unauthorized()\n\n...which is essentially the code that this function adds to your views.\n\nIt can be convenient to globally turn off authentication when unit testing.\nTo enable this, if the application configuration variable `LOGIN_DISABLED`\nis set to `True`, this decorator will be ignored.\n\n.. Note ::\n\n    Per `W3 guidelines for CORS preflight requests\n    <http://www.w3.org/TR/cors/#cross-origin-request-with-preflight-0>`_,\n    HTTP ``OPTIONS`` requests are exempt from login checks.\n\n:param func: The view function to decorate.\n:type func: function", "short_docstring": "If you decorate a view with this, it will ensure that the current user is\nlogged in and authenticated before calling the actual view. (If they are\nnot, it calls the :attr:`LoginManager.unauthorized` callback.) For\nexample::"}
{"name": "flask_login.current_user.id", "error": "Import error: No module named 'flask_login.current_user'"}
{"name": "sys.executable", "type": "constant", "value": "/home/aiops/zhuoyt/miniconda3/bin/python", "signature": null, "docstring": "str(object='') -> str\nstr(bytes_or_buffer[, encoding[, errors]]) -> str\n\nCreate a new string object from the given object. If encoding or\nerrors is specified, then the object must expose a data buffer\nthat will be decoded using the given encoding and error handler.\nOtherwise, returns the result of object.__str__() (if defined)\nor repr(object).\nencoding defaults to sys.getdefaultencoding().\nerrors defaults to 'strict'.", "short_docstring": "str(object='') -> str\nstr(bytes_or_buffer[, encoding[, errors]]) -> str"}
{"name": "numpy.delete", "type": "callable", "signature": "(arr, obj, axis=None)", "docstring": "Return a new array with sub-arrays along an axis deleted. For a one\ndimensional array, this returns those entries not returned by\n`arr[obj]`.\n\nParameters\n----------\narr : array_like\n    Input array.\nobj : slice, int or array of ints\n    Indicate indices of sub-arrays to remove along the specified axis.\n\n    .. versionchanged:: 1.19.0\n        Boolean indices are now treated as a mask of elements to remove,\n        rather than being cast to the integers 0 and 1.\n\naxis : int, optional\n    The axis along which to delete the subarray defined by `obj`.\n    If `axis` is None, `obj` is applied to the flattened array.\n\nReturns\n-------\nout : ndarray\n    A copy of `arr` with the elements specified by `obj` removed. Note\n    that `delete` does not occur in-place. If `axis` is None, `out` is\n    a flattened array.\n\nSee Also\n--------\ninsert : Insert elements into an array.\nappend : Append elements at the end of an array.\n\nNotes\n-----\nOften it is preferable to use a boolean mask. For example:\n\n>>> arr = np.arange(12) + 1\n>>> mask = np.ones(len(arr), dtype=bool)\n>>> mask[[0,2,4]] = False\n>>> result = arr[mask,...]\n\nIs equivalent to ``np.delete(arr, [0,2,4], axis=0)``, but allows further\nuse of `mask`.\n\nExamples\n--------\n>>> arr = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\n>>> arr\narray([[ 1,  2,  3,  4],\n       [ 5,  6,  7,  8],\n       [ 9, 10, 11, 12]])\n>>> np.delete(arr, 1, 0)\narray([[ 1,  2,  3,  4],\n       [ 9, 10, 11, 12]])\n\n>>> np.delete(arr, np.s_[::2], 1)\narray([[ 2,  4],\n       [ 6,  8],\n       [10, 12]])\n>>> np.delete(arr, [1,3,5], None)\narray([ 1,  3,  5,  7,  8,  9, 10, 11, 12])", "short_docstring": "Return a new array with sub-arrays along an axis deleted. For a one\ndimensional array, this returns those entries not returned by\n`arr[obj]`."}
{"name": "numpy.random.shuffle", "type": "callable", "signature": null, "docstring": "shuffle(x)\n\nModify a sequence in-place by shuffling its contents.\n\nThis function only shuffles the array along the first axis of a\nmulti-dimensional array. The order of sub-arrays is changed but\ntheir contents remains the same.\n\n.. note::\n    New code should use the ``shuffle`` method of a ``default_rng()``\n    instance instead; please see the :ref:`random-quick-start`.\n\nParameters\n----------\nx : ndarray or MutableSequence\n    The array, list or mutable sequence to be shuffled.\n\nReturns\n-------\nNone\n\nSee Also\n--------\nrandom.Generator.shuffle: which should be used for new code.\n\nExamples\n--------\n>>> arr = np.arange(10)\n>>> np.random.shuffle(arr)\n>>> arr\n[1 7 5 2 9 4 3 6 0 8] # random\n\nMulti-dimensional arrays are only shuffled along the first axis:\n\n>>> arr = np.arange(9).reshape((3, 3))\n>>> np.random.shuffle(arr)\n>>> arr\narray([[3, 4, 5], # random\n       [6, 7, 8],\n       [0, 1, 2]])", "short_docstring": "shuffle(x)"}
{"name": "requests.exceptions.HTTPError", "type": "class", "signature": "(*args, **kwargs)", "docstring": "An HTTP error occurred.", "short_docstring": "An HTTP error occurred."}
{"name": "pytesseract.image_to_string", "type": "callable", "signature": "(image, lang=None, config='', nice=0, output_type='string', timeout=0)", "docstring": "Returns the result of a Tesseract OCR run on the provided image to string", "short_docstring": "Returns the result of a Tesseract OCR run on the provided image to string"}
{"name": "socket.AF_INET", "type": "constant", "value": "AddressFamily.AF_INET", "signature": null, "docstring": "An enumeration.", "short_docstring": "An enumeration."}
{"name": "math.pi", "type": "constant", "value": "3.141592653589793", "signature": null, "docstring": "Convert a string or number to a floating point number, if possible.", "short_docstring": "Convert a string or number to a floating point number, if possible."}
{"name": "nltk.corpus.stopwords.words", "type": "callable", "signature": "(fileids=None, ignore_lines_startswith='\\n')", "docstring": null, "short_docstring": ""}
{"name": "numpy.nanmean", "type": "callable", "signature": "(a, axis=None, dtype=None, out=None, keepdims=<no value>, *, where=<no value>)", "docstring": "Compute the arithmetic mean along the specified axis, ignoring NaNs.\n\nReturns the average of the array elements.  The average is taken over\nthe flattened array by default, otherwise over the specified axis.\n`float64` intermediate and return values are used for integer inputs.\n\nFor all-NaN slices, NaN is returned and a `RuntimeWarning` is raised.\n\n.. versionadded:: 1.8.0\n\nParameters\n----------\na : array_like\n    Array containing numbers whose mean is desired. If `a` is not an\n    array, a conversion is attempted.\naxis : {int, tuple of int, None}, optional\n    Axis or axes along which the means are computed. The default is to compute\n    the mean of the flattened array.\ndtype : data-type, optional\n    Type to use in computing the mean.  For integer inputs, the default\n    is `float64`; for inexact inputs, it is the same as the input\n    dtype.\nout : ndarray, optional\n    Alternate output array in which to place the result.  The default\n    is ``None``; if provided, it must have the same shape as the\n    expected output, but the type will be cast if necessary. See\n    :ref:`ufuncs-output-type` for more details.\nkeepdims : bool, optional\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the original `a`.\n\n    If the value is anything but the default, then\n    `keepdims` will be passed through to the `mean` or `sum` methods\n    of sub-classes of `ndarray`.  If the sub-classes methods\n    does not implement `keepdims` any exceptions will be raised.\nwhere : array_like of bool, optional\n    Elements to include in the mean. See `~numpy.ufunc.reduce` for details.\n\n    .. versionadded:: 1.22.0\n\nReturns\n-------\nm : ndarray, see dtype parameter above\n    If `out=None`, returns a new array containing the mean values,\n    otherwise a reference to the output array is returned. Nan is\n    returned for slices that contain only NaNs.\n\nSee Also\n--------\naverage : Weighted average\nmean : Arithmetic mean taken while not ignoring NaNs\nvar, nanvar\n\nNotes\n-----\nThe arithmetic mean is the sum of the non-NaN elements along the axis\ndivided by the number of non-NaN elements.\n\nNote that for floating-point input, the mean is computed using the same\nprecision the input has.  Depending on the input data, this can cause\nthe results to be inaccurate, especially for `float32`.  Specifying a\nhigher-precision accumulator using the `dtype` keyword can alleviate\nthis issue.\n\nExamples\n--------\n>>> a = np.array([[1, np.nan], [3, 4]])\n>>> np.nanmean(a)\n2.6666666666666665\n>>> np.nanmean(a, axis=0)\narray([2.,  4.])\n>>> np.nanmean(a, axis=1)\narray([1.,  3.5]) # may vary", "short_docstring": "Compute the arithmetic mean along the specified axis, ignoring NaNs."}
{"name": "seaborn.stripplot", "type": "callable", "signature": "(data=None, *, x=None, y=None, hue=None, order=None, hue_order=None, jitter=True, dodge=False, orient=None, color=None, palette=None, size=5, edgecolor=<default>, linewidth=0, hue_norm=None, log_scale=None, native_scale=False, formatter=None, legend='auto', ax=None, **kwargs)", "docstring": "Draw a categorical scatterplot using jitter to reduce overplotting.\n\nA strip plot can be drawn on its own, but it is also a good complement\nto a box or violin plot in cases where you want to show all observations\nalong with some representation of the underlying distribution.\n\nSee the :ref:`tutorial <categorical_tutorial>` for more information.\n\n.. note::\n    By default, this function treats one of the variables as categorical\n    and draws data at ordinal positions (0, 1, ... n) on the relevant axis.\n    As of version 0.13.0, this can be disabled by setting `native_scale=True`.\n\n\nParameters\n----------\ndata : DataFrame, Series, dict, array, or list of arrays\n    Dataset for plotting. If `x` and `y` are absent, this is\n    interpreted as wide-form. Otherwise it is expected to be long-form.    \nx, y, hue : names of variables in `data` or vector data\n    Inputs for plotting long-form data. See examples for interpretation.    \norder, hue_order : lists of strings\n    Order to plot the categorical levels in; otherwise the levels are\n    inferred from the data objects.    \njitter : float, `True`/`1` is special-cased\n    Amount of jitter (only along the categorical axis) to apply. This\n    can be useful when you have many points and they overlap, so that\n    it is easier to see the distribution. You can specify the amount\n    of jitter (half the width of the uniform random variable support),\n    or use `True` for a good default.\ndodge : bool\n    When a `hue` variable is assigned, setting this to `True` will\n    separate the strips for different hue levels along the categorical\n    axis and narrow the amount of space allotedto each strip. Otherwise,\n    the points for each level will be plotted in the same strip.\norient : \"v\" | \"h\" | \"x\" | \"y\"\n    Orientation of the plot (vertical or horizontal). This is usually\n    inferred based on the type of the input variables, but it can be used\n    to resolve ambiguity when both `x` and `y` are numeric or when\n    plotting wide-form data.\n\n    .. versionchanged:: v0.13.0\n        Added 'x'/'y' as options, equivalent to 'v'/'h'.    \ncolor : matplotlib color\n    Single color for the elements in the plot.    \npalette : palette name, list, or dict\n    Colors to use for the different levels of the ``hue`` variable. Should\n    be something that can be interpreted by :func:`color_palette`, or a\n    dictionary mapping hue levels to matplotlib colors.    \nsize : float\n    Radius of the markers, in points.\nedgecolor : matplotlib color, \"gray\" is special-cased\n    Color of the lines around each point. If you pass `\"gray\"`, the\n    brightness is determined by the color palette used for the body\n    of the points. Note that `stripplot` has `linewidth=0` by default,\n    so edge colors are only visible with nonzero line width.\nlinewidth : float\n    Width of the lines that frame the plot elements.    \nhue_norm : tuple or :class:`matplotlib.colors.Normalize` object\n    Normalization in data units for colormap applied to the `hue`\n    variable when it is numeric. Not relevant if `hue` is categorical.\n\n    .. versionadded:: v0.12.0    \nlog_scale : bool or number, or pair of bools or numbers\n    Set axis scale(s) to log. A single value sets the data axis for any numeric\n    axes in the plot. A pair of values sets each axis independently.\n    Numeric values are interpreted as the desired base (default 10).\n    When `None` or `False`, seaborn defers to the existing Axes scale.\n\n    .. versionadded:: v0.13.0    \nnative_scale : bool\n    When True, numeric or datetime values on the categorical axis will maintain\n    their original scaling rather than being converted to fixed indices.\n\n    .. versionadded:: v0.13.0    \nformatter : callable\n    Function for converting categorical data into strings. Affects both grouping\n    and tick labels.\n\n    .. versionadded:: v0.13.0    \nlegend : \"auto\", \"brief\", \"full\", or False\n    How to draw the legend. If \"brief\", numeric `hue` and `size`\n    variables will be represented with a sample of evenly spaced values.\n    If \"full\", every group will get an entry in the legend. If \"auto\",\n    choose between brief or full representation based on number of levels.\n    If `False`, no legend data is added and no legend is drawn.\n\n    .. versionadded:: v0.13.0    \nax : matplotlib Axes\n    Axes object to draw the plot onto, otherwise uses the current Axes.    \nkwargs : key, value mappings\n    Other keyword arguments are passed through to\n    :meth:`matplotlib.axes.Axes.scatter`.\n\nReturns\n-------\nax : matplotlib Axes\n    Returns the Axes object with the plot drawn onto it.    \n\nSee Also\n--------\nswarmplot : A categorical scatterplot where the points do not overlap. Can\n            be used with other plots to show each observation.    \nboxplot : A traditional box-and-whisker plot with a similar API.    \nviolinplot : A combination of boxplot and kernel density estimation.    \ncatplot : Combine a categorical plot with a :class:`FacetGrid`.    \n\nExamples\n--------\n.. include:: ../docstrings/stripplot.rst", "short_docstring": "Draw a categorical scatterplot using jitter to reduce overplotting."}
{"name": "numpy.uint8", "type": "class", "signature": null, "docstring": "Unsigned integer type, compatible with C ``unsigned char``.\n\n:Character code: ``'B'``\n:Canonical name: `numpy.ubyte`\n:Alias on this platform (Linux x86_64): `numpy.uint8`: 8-bit unsigned integer (``0`` to ``255``).", "short_docstring": "Unsigned integer type, compatible with C ``unsigned char``."}
{"name": "pandas.read_html", "type": "callable", "signature": "(io: 'FilePath | ReadBuffer[str]', *, match: 'str | Pattern' = '.+', flavor: 'HTMLFlavors | Sequence[HTMLFlavors] | None' = None, header: 'int | Sequence[int] | None' = None, index_col: 'int | Sequence[int] | None' = None, skiprows: 'int | Sequence[int] | slice | None' = None, attrs: 'dict[str, str] | None' = None, parse_dates: 'bool' = False, thousands: 'str | None' = ',', encoding: 'str | None' = None, decimal: 'str' = '.', converters: 'dict | None' = None, na_values: 'Iterable[object] | None' = None, keep_default_na: 'bool' = True, displayed_only: 'bool' = True, extract_links: \"Literal[None, 'header', 'footer', 'body', 'all']\" = None, dtype_backend: 'DtypeBackend | lib.NoDefault' = <no_default>, storage_options: 'StorageOptions' = None) -> 'list[DataFrame]'", "docstring": "Read HTML tables into a ``list`` of ``DataFrame`` objects.\n\nParameters\n----------\nio : str, path object, or file-like object\n    String, path object (implementing ``os.PathLike[str]``), or file-like\n    object implementing a string ``read()`` function.\n    The string can represent a URL or the HTML itself. Note that\n    lxml only accepts the http, ftp and file url protocols. If you have a\n    URL that starts with ``'https'`` you might try removing the ``'s'``.\n\n    .. deprecated:: 2.1.0\n        Passing html literal strings is deprecated.\n        Wrap literal string/bytes input in ``io.StringIO``/``io.BytesIO`` instead.\n\nmatch : str or compiled regular expression, optional\n    The set of tables containing text matching this regex or string will be\n    returned. Unless the HTML is extremely simple you will probably need to\n    pass a non-empty string here. Defaults to '.+' (match any non-empty\n    string). The default value will return all tables contained on a page.\n    This value is converted to a regular expression so that there is\n    consistent behavior between Beautiful Soup and lxml.\n\nflavor : {\"lxml\", \"html5lib\", \"bs4\"} or list-like, optional\n    The parsing engine (or list of parsing engines) to use. 'bs4' and\n    'html5lib' are synonymous with each other, they are both there for\n    backwards compatibility. The default of ``None`` tries to use ``lxml``\n    to parse and if that fails it falls back on ``bs4`` + ``html5lib``.\n\nheader : int or list-like, optional\n    The row (or list of rows for a :class:`~pandas.MultiIndex`) to use to\n    make the columns headers.\n\nindex_col : int or list-like, optional\n    The column (or list of columns) to use to create the index.\n\nskiprows : int, list-like or slice, optional\n    Number of rows to skip after parsing the column integer. 0-based. If a\n    sequence of integers or a slice is given, will skip the rows indexed by\n    that sequence.  Note that a single element sequence means 'skip the nth\n    row' whereas an integer means 'skip n rows'.\n\nattrs : dict, optional\n    This is a dictionary of attributes that you can pass to use to identify\n    the table in the HTML. These are not checked for validity before being\n    passed to lxml or Beautiful Soup. However, these attributes must be\n    valid HTML table attributes to work correctly. For example, ::\n\n        attrs = {'id': 'table'}\n\n    is a valid attribute dictionary because the 'id' HTML tag attribute is\n    a valid HTML attribute for *any* HTML tag as per `this document\n    <https://html.spec.whatwg.org/multipage/dom.html#global-attributes>`__. ::\n\n        attrs = {'asdf': 'table'}\n\n    is *not* a valid attribute dictionary because 'asdf' is not a valid\n    HTML attribute even if it is a valid XML attribute.  Valid HTML 4.01\n    table attributes can be found `here\n    <http://www.w3.org/TR/REC-html40/struct/tables.html#h-11.2>`__. A\n    working draft of the HTML 5 spec can be found `here\n    <https://html.spec.whatwg.org/multipage/tables.html>`__. It contains the\n    latest information on table attributes for the modern web.\n\nparse_dates : bool, optional\n    See :func:`~read_csv` for more details.\n\nthousands : str, optional\n    Separator to use to parse thousands. Defaults to ``','``.\n\nencoding : str, optional\n    The encoding used to decode the web page. Defaults to ``None``.``None``\n    preserves the previous encoding behavior, which depends on the\n    underlying parser library (e.g., the parser library will try to use\n    the encoding provided by the document).\n\ndecimal : str, default '.'\n    Character to recognize as decimal point (e.g. use ',' for European\n    data).\n\nconverters : dict, default None\n    Dict of functions for converting values in certain columns. Keys can\n    either be integers or column labels, values are functions that take one\n    input argument, the cell (not column) content, and return the\n    transformed content.\n\nna_values : iterable, default None\n    Custom NA values.\n\nkeep_default_na : bool, default True\n    If na_values are specified and keep_default_na is False the default NaN\n    values are overridden, otherwise they're appended to.\n\ndisplayed_only : bool, default True\n    Whether elements with \"display: none\" should be parsed.\n\nextract_links : {None, \"all\", \"header\", \"body\", \"footer\"}\n    Table elements in the specified section(s) with <a> tags will have their\n    href extracted.\n\n    .. versionadded:: 1.5.0\n\ndtype_backend : {'numpy_nullable', 'pyarrow'}, default 'numpy_nullable'\n    Back-end data type applied to the resultant :class:`DataFrame`\n    (still experimental). Behaviour is as follows:\n\n    * ``\"numpy_nullable\"``: returns nullable-dtype-backed :class:`DataFrame`\n      (default).\n    * ``\"pyarrow\"``: returns pyarrow-backed nullable :class:`ArrowDtype`\n      DataFrame.\n\n    .. versionadded:: 2.0\n\nstorage_options : dict, optional\n    Extra options that make sense for a particular storage connection, e.g.\n    host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n    are forwarded to ``urllib.request.Request`` as header options. For other\n    URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n    forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n    details, and for more examples on storage options refer `here\n    <https://pandas.pydata.org/docs/user_guide/io.html?\n    highlight=storage_options#reading-writing-remote-files>`_.\n\n    .. versionadded:: 2.1.0\n\nReturns\n-------\ndfs\n    A list of DataFrames.\n\nSee Also\n--------\nread_csv : Read a comma-separated values (csv) file into DataFrame.\n\nNotes\n-----\nBefore using this function you should read the :ref:`gotchas about the\nHTML parsing libraries <io.html.gotchas>`.\n\nExpect to do some cleanup after you call this function. For example, you\nmight need to manually assign column names if the column names are\nconverted to NaN when you pass the `header=0` argument. We try to assume as\nlittle as possible about the structure of the table and push the\nidiosyncrasies of the HTML contained in the table to the user.\n\nThis function searches for ``<table>`` elements and only for ``<tr>``\nand ``<th>`` rows and ``<td>`` elements within each ``<tr>`` or ``<th>``\nelement in the table. ``<td>`` stands for \"table data\". This function\nattempts to properly handle ``colspan`` and ``rowspan`` attributes.\nIf the function has a ``<thead>`` argument, it is used to construct\nthe header, otherwise the function attempts to find the header within\nthe body (by putting rows with only ``<th>`` elements into the header).\n\nSimilar to :func:`~read_csv` the `header` argument is applied\n**after** `skiprows` is applied.\n\nThis function will *always* return a list of :class:`DataFrame` *or*\nit will fail, e.g., it will *not* return an empty list.\n\nExamples\n--------\nSee the :ref:`read_html documentation in the IO section of the docs\n<io.read_html>` for some examples of reading in HTML tables.", "short_docstring": "Read HTML tables into a ``list`` of ``DataFrame`` objects."}
{"name": "scipy.stats.norm.fit", "type": "callable", "signature": "(data, **kwds)", "docstring": "Return estimates of shape (if applicable), location, and scale\nparameters from data. The default estimation method is Maximum\nLikelihood Estimation (MLE), but Method of Moments (MM)\nis also available.\n\nStarting estimates for the fit are given by input arguments;\nfor any arguments not provided with starting estimates,\n``self._fitstart(data)`` is called to generate such.\n\nOne can hold some parameters fixed to specific values by passing in\nkeyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)\nand ``floc`` and ``fscale`` (for location and scale parameters,\nrespectively).\n\nParameters\n----------\ndata : array_like or `CensoredData` instance\n    Data to use in estimating the distribution parameters.\narg1, arg2, arg3,... : floats, optional\n    Starting value(s) for any shape-characterizing arguments (those not\n    provided will be determined by a call to ``_fitstart(data)``).\n    No default value.\n**kwds : floats, optional\n    - `loc`: initial guess of the distribution's location parameter.\n    - `scale`: initial guess of the distribution's scale parameter.\n\n    Special keyword arguments are recognized as holding certain\n    parameters fixed:\n\n    - f0...fn : hold respective shape parameters fixed.\n      Alternatively, shape parameters to fix can be specified by name.\n      For example, if ``self.shapes == \"a, b\"``, ``fa`` and ``fix_a``\n      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are\n      equivalent to ``f1``.\n\n    - floc : hold location parameter fixed to specified value.\n\n    - fscale : hold scale parameter fixed to specified value.\n\n    - optimizer : The optimizer to use.  The optimizer must take\n      ``func`` and starting position as the first two arguments,\n      plus ``args`` (for extra arguments to pass to the\n      function to be optimized) and ``disp``. \n      The ``fit`` method calls the optimizer with ``disp=0`` to suppress output.\n      The optimizer must return the estimated parameters.\n\n    - method : The method to use. The default is \"MLE\" (Maximum\n      Likelihood Estimate); \"MM\" (Method of Moments)\n      is also available.\n\nRaises\n------\nTypeError, ValueError\n    If an input is invalid\n`~scipy.stats.FitError`\n    If fitting fails or the fit produced would be invalid\n\nReturns\n-------\nparameter_tuple : tuple of floats\n    Estimates for any shape parameters (if applicable), followed by\n    those for location and scale. For most random variables, shape\n    statistics will be returned, but there are exceptions (e.g.\n    ``norm``).\n\nNotes\n-----\nFor the normal distribution, method of moments and maximum likelihood\nestimation give identical fits, and explicit formulas for the estimates\nare available.\nThis function uses these explicit formulas for the maximum likelihood\nestimation of the normal distribution parameters, so the\n`optimizer` and `method` arguments are ignored.\n\nExamples\n--------\n\nGenerate some data to fit: draw random variates from the `beta`\ndistribution\n\n>>> import numpy as np\n>>> from scipy.stats import beta\n>>> a, b = 1., 2.\n>>> rng = np.random.default_rng(172786373191770012695001057628748821561)\n>>> x = beta.rvs(a, b, size=1000, random_state=rng)\n\nNow we can fit all four parameters (``a``, ``b``, ``loc`` and\n``scale``):\n\n>>> a1, b1, loc1, scale1 = beta.fit(x)\n>>> a1, b1, loc1, scale1\n(1.0198945204435628, 1.9484708982737828, 4.372241314917588e-05, 0.9979078845964814)\n\nThe fit can be done also using a custom optimizer:\n\n>>> from scipy.optimize import minimize\n>>> def custom_optimizer(func, x0, args=(), disp=0):\n...     res = minimize(func, x0, args, method=\"slsqp\", options={\"disp\": disp})\n...     if res.success:\n...         return res.x\n...     raise RuntimeError('optimization routine failed')\n>>> a1, b1, loc1, scale1 = beta.fit(x, method=\"MLE\", optimizer=custom_optimizer)\n>>> a1, b1, loc1, scale1\n(1.0198821087258905, 1.948484145914738, 4.3705304486881485e-05, 0.9979104663953395)\n\nWe can also use some prior knowledge about the dataset: let's keep\n``loc`` and ``scale`` fixed:\n\n>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)\n>>> loc1, scale1\n(0, 1)\n\nWe can also keep shape parameters fixed by using ``f``-keywords. To\nkeep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,\nequivalently, ``fa=1``:\n\n>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)\n>>> a1\n1\n\nNot all distributions return estimates for the shape parameters.\n``norm`` for example just returns estimates for location and scale:\n\n>>> from scipy.stats import norm\n>>> x = norm.rvs(a, b, size=1000, random_state=123)\n>>> loc1, scale1 = norm.fit(x)\n>>> loc1, scale1\n(0.92087172783841631, 2.0015750750324668)", "short_docstring": "Return estimates of shape (if applicable), location, and scale\nparameters from data. The default estimation method is Maximum\nLikelihood Estimation (MLE), but Method of Moments (MM)\nis also available."}
{"name": "shutil.copyfile", "type": "callable", "signature": "(src, dst, *, follow_symlinks=True)", "docstring": "Copy data from src to dst in the most efficient way possible.\n\nIf follow_symlinks is not set and src is a symbolic link, a new\nsymlink will be created instead of copying the file it points to.", "short_docstring": "Copy data from src to dst in the most efficient way possible."}
{"name": "collections.defaultdict", "type": "class", "signature": null, "docstring": "defaultdict(default_factory=None, /, [...]) --> dict with default factory\n\nThe default factory is called without arguments to produce\na new value when a key is not present, in __getitem__ only.\nA defaultdict compares equal to a dict with the same items.\nAll remaining arguments are treated the same as if they were\npassed to the dict constructor, including keyword arguments.", "short_docstring": "defaultdict(default_factory=None, /, [...]) --> dict with default factory"}
{"name": "pandas.Series", "type": "class", "signature": "(data=None, index=None, dtype: 'Dtype | None' = None, name=None, copy: 'bool | None' = None, fastpath: 'bool | lib.NoDefault' = <no_default>) -> 'None'", "docstring": "One-dimensional ndarray with axis labels (including time series).\n\nLabels need not be unique but must be a hashable type. The object\nsupports both integer- and label-based indexing and provides a host of\nmethods for performing operations involving the index. Statistical\nmethods from ndarray have been overridden to automatically exclude\nmissing data (currently represented as NaN).\n\nOperations between Series (+, -, /, \\*, \\*\\*) align values based on their\nassociated index values-- they need not be the same length. The result\nindex will be the sorted union of the two indexes.\n\nParameters\n----------\ndata : array-like, Iterable, dict, or scalar value\n    Contains data stored in Series. If data is a dict, argument order is\n    maintained.\nindex : array-like or Index (1d)\n    Values must be hashable and have the same length as `data`.\n    Non-unique index values are allowed. Will default to\n    RangeIndex (0, 1, 2, ..., n) if not provided. If data is dict-like\n    and index is None, then the keys in the data are used as the index. If the\n    index is not None, the resulting Series is reindexed with the index values.\ndtype : str, numpy.dtype, or ExtensionDtype, optional\n    Data type for the output Series. If not specified, this will be\n    inferred from `data`.\n    See the :ref:`user guide <basics.dtypes>` for more usages.\nname : Hashable, default None\n    The name to give to the Series.\ncopy : bool, default False\n    Copy input data. Only affects Series or 1d ndarray input. See examples.\n\nNotes\n-----\nPlease reference the :ref:`User Guide <basics.series>` for more information.\n\nExamples\n--------\nConstructing Series from a dictionary with an Index specified\n\n>>> d = {'a': 1, 'b': 2, 'c': 3}\n>>> ser = pd.Series(data=d, index=['a', 'b', 'c'])\n>>> ser\na   1\nb   2\nc   3\ndtype: int64\n\nThe keys of the dictionary match with the Index values, hence the Index\nvalues have no effect.\n\n>>> d = {'a': 1, 'b': 2, 'c': 3}\n>>> ser = pd.Series(data=d, index=['x', 'y', 'z'])\n>>> ser\nx   NaN\ny   NaN\nz   NaN\ndtype: float64\n\nNote that the Index is first build with the keys from the dictionary.\nAfter this the Series is reindexed with the given Index values, hence we\nget all NaN as a result.\n\nConstructing Series from a list with `copy=False`.\n\n>>> r = [1, 2]\n>>> ser = pd.Series(r, copy=False)\n>>> ser.iloc[0] = 999\n>>> r\n[1, 2]\n>>> ser\n0    999\n1      2\ndtype: int64\n\nDue to input data type the Series has a `copy` of\nthe original data even though `copy=False`, so\nthe data is unchanged.\n\nConstructing Series from a 1d ndarray with `copy=False`.\n\n>>> r = np.array([1, 2])\n>>> ser = pd.Series(r, copy=False)\n>>> ser.iloc[0] = 999\n>>> r\narray([999,   2])\n>>> ser\n0    999\n1      2\ndtype: int64\n\nDue to input data type the Series has a `view` on\nthe original data, so\nthe data is changed as well.", "short_docstring": "One-dimensional ndarray with axis labels (including time series)."}
{"name": "re.IGNORECASE", "type": "constant", "value": "re.IGNORECASE", "signature": null, "docstring": "An enumeration.", "short_docstring": "An enumeration."}
{"name": "email.message.EmailMessage", "type": "class", "signature": "(policy=None)", "docstring": "Basic message object.\n\nA message object is defined as something that has a bunch of RFC 2822\nheaders and a payload.  It may optionally have an envelope header\n(a.k.a. Unix-From or From_ header).  If the message is a container (i.e. a\nmultipart or a message/rfc822), then the payload is a list of Message\nobjects, otherwise it is a string.\n\nMessage objects implement part of the `mapping' interface, which assumes\nthere is exactly one occurrence of the header per message.  Some headers\ndo in fact appear multiple times (e.g. Received) and for those headers,\nyou must use the explicit API to set or get all the headers.  Not all of\nthe mapping methods are implemented.", "short_docstring": "Basic message object."}
{"name": "os.path.splitext", "type": "callable", "signature": "(p)", "docstring": "Split the extension from a pathname.\n\nExtension is everything from the last dot to the end, ignoring\nleading dots.  Returns \"(root, ext)\"; ext may be empty.", "short_docstring": "Split the extension from a pathname."}
{"name": "datetime.timedelta", "type": "class", "signature": null, "docstring": "Difference between two datetime values.\n\ntimedelta(days=0, seconds=0, microseconds=0, milliseconds=0, minutes=0, hours=0, weeks=0)\n\nAll arguments are optional and default to 0.\nArguments may be integers or floats, and may be positive or negative.", "short_docstring": "Difference between two datetime values."}
{"name": "pandas.errors.EmptyDataError", "type": "class", "signature": null, "docstring": "Exception raised in ``pd.read_csv`` when empty data or header is encountered.\n\nExamples\n--------\n>>> from io import StringIO\n>>> empty = StringIO()\n>>> pd.read_csv(empty)\nTraceback (most recent call last):\nEmptyDataError: No columns to parse from file", "short_docstring": "Exception raised in ``pd.read_csv`` when empty data or header is encountered."}
{"name": "socket.error", "type": "class", "signature": null, "docstring": "Base class for I/O related errors.", "short_docstring": "Base class for I/O related errors."}
{"name": "Crypto.Random.get_random_bytes", "type": "callable", "signature": "(size, /)", "docstring": "Return a bytes object containing random bytes suitable for cryptographic use.", "short_docstring": "Return a bytes object containing random bytes suitable for cryptographic use."}
{"name": "wtforms.PasswordField", "type": "class", "signature": "(*args, **kwargs)", "docstring": "A StringField, except renders an ``<input type=\"password\">``.\n\nAlso, whatever value is accepted by this field is not rendered back\nto the browser like normal fields.", "short_docstring": "A StringField, except renders an ``<input type=\"password\">``."}
{"name": "os.path.abspath", "type": "callable", "signature": "(path)", "docstring": "Return an absolute path.", "short_docstring": "Return an absolute path."}
{"name": "PIL.Image.open", "type": "callable", "signature": "(fp, mode='r', formats=None) -> 'Image'", "docstring": "Opens and identifies the given image file.\n\nThis is a lazy operation; this function identifies the file, but\nthe file remains open and the actual image data is not read from\nthe file until you try to process the data (or call the\n:py:meth:`~PIL.Image.Image.load` method).  See\n:py:func:`~PIL.Image.new`. See :ref:`file-handling`.\n\n:param fp: A filename (string), os.PathLike object or a file object.\n   The file object must implement ``file.read``,\n   ``file.seek``, and ``file.tell`` methods,\n   and be opened in binary mode. The file object will also seek to zero\n   before reading.\n:param mode: The mode.  If given, this argument must be \"r\".\n:param formats: A list or tuple of formats to attempt to load the file in.\n   This can be used to restrict the set of formats checked.\n   Pass ``None`` to try all supported formats. You can print the set of\n   available formats by running ``python3 -m PIL`` or using\n   the :py:func:`PIL.features.pilinfo` function.\n:returns: An :py:class:`~PIL.Image.Image` object.\n:exception FileNotFoundError: If the file cannot be found.\n:exception PIL.UnidentifiedImageError: If the image cannot be opened and\n   identified.\n:exception ValueError: If the ``mode`` is not \"r\", or if a ``StringIO``\n   instance is used for ``fp``.\n:exception TypeError: If ``formats`` is not ``None``, a list or a tuple.", "short_docstring": "Opens and identifies the given image file."}
{"name": "numpy.random.normal", "type": "callable", "signature": null, "docstring": "normal(loc=0.0, scale=1.0, size=None)\n\nDraw random samples from a normal (Gaussian) distribution.\n\nThe probability density function of the normal distribution, first\nderived by De Moivre and 200 years later by both Gauss and Laplace\nindependently [2]_, is often called the bell curve because of\nits characteristic shape (see the example below).\n\nThe normal distributions occurs often in nature.  For example, it\ndescribes the commonly occurring distribution of samples influenced\nby a large number of tiny, random disturbances, each with its own\nunique distribution [2]_.\n\n.. note::\n    New code should use the ``normal`` method of a ``default_rng()``\n    instance instead; please see the :ref:`random-quick-start`.\n\nParameters\n----------\nloc : float or array_like of floats\n    Mean (\"centre\") of the distribution.\nscale : float or array_like of floats\n    Standard deviation (spread or \"width\") of the distribution. Must be\n    non-negative.\nsize : int or tuple of ints, optional\n    Output shape.  If the given shape is, e.g., ``(m, n, k)``, then\n    ``m * n * k`` samples are drawn.  If size is ``None`` (default),\n    a single value is returned if ``loc`` and ``scale`` are both scalars.\n    Otherwise, ``np.broadcast(loc, scale).size`` samples are drawn.\n\nReturns\n-------\nout : ndarray or scalar\n    Drawn samples from the parameterized normal distribution.\n\nSee Also\n--------\nscipy.stats.norm : probability density function, distribution or\n    cumulative density function, etc.\nrandom.Generator.normal: which should be used for new code.\n\nNotes\n-----\nThe probability density for the Gaussian distribution is\n\n.. math:: p(x) = \\frac{1}{\\sqrt{ 2 \\pi \\sigma^2 }}\n                 e^{ - \\frac{ (x - \\mu)^2 } {2 \\sigma^2} },\n\nwhere :math:`\\mu` is the mean and :math:`\\sigma` the standard\ndeviation. The square of the standard deviation, :math:`\\sigma^2`,\nis called the variance.\n\nThe function has its peak at the mean, and its \"spread\" increases with\nthe standard deviation (the function reaches 0.607 times its maximum at\n:math:`x + \\sigma` and :math:`x - \\sigma` [2]_).  This implies that\nnormal is more likely to return samples lying close to the mean, rather\nthan those far away.\n\nReferences\n----------\n.. [1] Wikipedia, \"Normal distribution\",\n       https://en.wikipedia.org/wiki/Normal_distribution\n.. [2] P. R. Peebles Jr., \"Central Limit Theorem\" in \"Probability,\n       Random Variables and Random Signal Principles\", 4th ed., 2001,\n       pp. 51, 51, 125.\n\nExamples\n--------\nDraw samples from the distribution:\n\n>>> mu, sigma = 0, 0.1 # mean and standard deviation\n>>> s = np.random.normal(mu, sigma, 1000)\n\nVerify the mean and the variance:\n\n>>> abs(mu - np.mean(s))\n0.0  # may vary\n\n>>> abs(sigma - np.std(s, ddof=1))\n0.1  # may vary\n\nDisplay the histogram of the samples, along with\nthe probability density function:\n\n>>> import matplotlib.pyplot as plt\n>>> count, bins, ignored = plt.hist(s, 30, density=True)\n>>> plt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) *\n...                np.exp( - (bins - mu)**2 / (2 * sigma**2) ),\n...          linewidth=2, color='r')\n>>> plt.show()\n\nTwo-by-four array of samples from N(3, 6.25):\n\n>>> np.random.normal(3, 2.5, size=(2, 4))\narray([[-4.49401501,  4.00950034, -1.81814867,  7.29718677],   # random\n       [ 0.39924804,  4.68456316,  4.99394529,  4.84057254]])  # random", "short_docstring": "normal(loc=0.0, scale=1.0, size=None)"}
{"name": "numpy.pi", "type": "constant", "value": "3.141592653589793", "signature": null, "docstring": "Convert a string or number to a floating point number, if possible.", "short_docstring": "Convert a string or number to a floating point number, if possible."}
{"name": "wtforms.validators.Length", "type": "class", "signature": "(min=-1, max=-1, message=None)", "docstring": "Validates the length of a string.\n\n:param min:\n    The minimum required length of the string. If not provided, minimum\n    length will not be checked.\n:param max:\n    The maximum length of the string. If not provided, maximum length\n    will not be checked.\n:param message:\n    Error message to raise in case of a validation error. Can be\n    interpolated using `%(min)d` and `%(max)d` if desired. Useful defaults\n    are provided depending on the existence of min and max.\n\nWhen supported, sets the `minlength` and `maxlength` attributes on widgets.", "short_docstring": "Validates the length of a string."}
{"name": "socket.socket", "type": "class", "signature": "(family=-1, type=-1, proto=-1, fileno=None)", "docstring": "A subclass of _socket.socket adding the makefile() method.", "short_docstring": "A subclass of _socket.socket adding the makefile() method."}
{"name": "os.walk", "type": "callable", "signature": "(top, topdown=True, onerror=None, followlinks=False)", "docstring": "Directory tree generator.\n\nFor each directory in the directory tree rooted at top (including top\nitself, but excluding '.' and '..'), yields a 3-tuple\n\n    dirpath, dirnames, filenames\n\ndirpath is a string, the path to the directory.  dirnames is a list of\nthe names of the subdirectories in dirpath (excluding '.' and '..').\nfilenames is a list of the names of the non-directory files in dirpath.\nNote that the names in the lists are just names, with no path components.\nTo get a full path (which begins with top) to a file or directory in\ndirpath, do os.path.join(dirpath, name).\n\nIf optional arg 'topdown' is true or not specified, the triple for a\ndirectory is generated before the triples for any of its subdirectories\n(directories are generated top down).  If topdown is false, the triple\nfor a directory is generated after the triples for all of its\nsubdirectories (directories are generated bottom up).\n\nWhen topdown is true, the caller can modify the dirnames list in-place\n(e.g., via del or slice assignment), and walk will only recurse into the\nsubdirectories whose names remain in dirnames; this can be used to prune the\nsearch, or to impose a specific order of visiting.  Modifying dirnames when\ntopdown is false has no effect on the behavior of os.walk(), since the\ndirectories in dirnames have already been generated by the time dirnames\nitself is generated. No matter the value of topdown, the list of\nsubdirectories is retrieved before the tuples for the directory and its\nsubdirectories are generated.\n\nBy default errors from the os.scandir() call are ignored.  If\noptional arg 'onerror' is specified, it should be a function; it\nwill be called with one argument, an OSError instance.  It can\nreport the error to continue with the walk, or raise the exception\nto abort the walk.  Note that the filename is available as the\nfilename attribute of the exception object.\n\nBy default, os.walk does not follow symbolic links to subdirectories on\nsystems that support them.  In order to get this functionality, set the\noptional argument 'followlinks' to true.\n\nCaution:  if you pass a relative pathname for top, don't change the\ncurrent working directory between resumptions of walk.  walk never\nchanges the current directory, and assumes that the client doesn't\neither.\n\nExample:\n\nimport os\nfrom os.path import join, getsize\nfor root, dirs, files in os.walk('python/Lib/email'):\n    print(root, \"consumes\", end=\"\")\n    print(sum(getsize(join(root, name)) for name in files), end=\"\")\n    print(\"bytes in\", len(files), \"non-directory files\")\n    if 'CVS' in dirs:\n        dirs.remove('CVS')  # don't visit CVS directories", "short_docstring": "Directory tree generator."}
{"name": "cryptography.hazmat.primitives.padding.PKCS7", "type": "class", "signature": "(block_size: int)", "docstring": null, "short_docstring": ""}
{"name": "numpy.array", "type": "callable", "signature": null, "docstring": "array(object, dtype=None, *, copy=True, order='K', subok=False, ndmin=0,\n      like=None)\n\nCreate an array.\n\nParameters\n----------\nobject : array_like\n    An array, any object exposing the array interface, an object whose\n    __array__ method returns an array, or any (nested) sequence.\n    If object is a scalar, a 0-dimensional array containing object is\n    returned.\ndtype : data-type, optional\n    The desired data-type for the array.  If not given, then the type will\n    be determined as the minimum type required to hold the objects in the\n    sequence.\ncopy : bool, optional\n    If true (default), then the object is copied.  Otherwise, a copy will\n    only be made if __array__ returns a copy, if obj is a nested sequence,\n    or if a copy is needed to satisfy any of the other requirements\n    (`dtype`, `order`, etc.).\norder : {'K', 'A', 'C', 'F'}, optional\n    Specify the memory layout of the array. If object is not an array, the\n    newly created array will be in C order (row major) unless 'F' is\n    specified, in which case it will be in Fortran order (column major).\n    If object is an array the following holds.\n\n    ===== ========= ===================================================\n    order  no copy                     copy=True\n    ===== ========= ===================================================\n    'K'   unchanged F & C order preserved, otherwise most similar order\n    'A'   unchanged F order if input is F and not C, otherwise C order\n    'C'   C order   C order\n    'F'   F order   F order\n    ===== ========= ===================================================\n\n    When ``copy=False`` and a copy is made for other reasons, the result is\n    the same as if ``copy=True``, with some exceptions for 'A', see the\n    Notes section. The default order is 'K'.\nsubok : bool, optional\n    If True, then sub-classes will be passed-through, otherwise\n    the returned array will be forced to be a base-class array (default).\nndmin : int, optional\n    Specifies the minimum number of dimensions that the resulting\n    array should have.  Ones will be prepended to the shape as\n    needed to meet this requirement.\nlike : array_like, optional\n    Reference object to allow the creation of arrays which are not\n    NumPy arrays. If an array-like passed in as ``like`` supports\n    the ``__array_function__`` protocol, the result will be defined\n    by it. In this case, it ensures the creation of an array object\n    compatible with that passed in via this argument.\n\n    .. versionadded:: 1.20.0\n\nReturns\n-------\nout : ndarray\n    An array object satisfying the specified requirements.\n\nSee Also\n--------\nempty_like : Return an empty array with shape and type of input.\nones_like : Return an array of ones with shape and type of input.\nzeros_like : Return an array of zeros with shape and type of input.\nfull_like : Return a new array with shape of input filled with value.\nempty : Return a new uninitialized array.\nones : Return a new array setting values to one.\nzeros : Return a new array setting values to zero.\nfull : Return a new array of given shape filled with value.\n\n\nNotes\n-----\nWhen order is 'A' and `object` is an array in neither 'C' nor 'F' order,\nand a copy is forced by a change in dtype, then the order of the result is\nnot necessarily 'C' as expected. This is likely a bug.\n\nExamples\n--------\n>>> np.array([1, 2, 3])\narray([1, 2, 3])\n\nUpcasting:\n\n>>> np.array([1, 2, 3.0])\narray([ 1.,  2.,  3.])\n\nMore than one dimension:\n\n>>> np.array([[1, 2], [3, 4]])\narray([[1, 2],\n       [3, 4]])\n\nMinimum dimensions 2:\n\n>>> np.array([1, 2, 3], ndmin=2)\narray([[1, 2, 3]])\n\nType provided:\n\n>>> np.array([1, 2, 3], dtype=complex)\narray([ 1.+0.j,  2.+0.j,  3.+0.j])\n\nData-type consisting of more than one element:\n\n>>> x = np.array([(1,2),(3,4)],dtype=[('a','<i4'),('b','<i4')])\n>>> x['a']\narray([1, 3])\n\nCreating an array from sub-classes:\n\n>>> np.array(np.mat('1 2; 3 4'))\narray([[1, 2],\n       [3, 4]])\n\n>>> np.array(np.mat('1 2; 3 4'), subok=True)\nmatrix([[1, 2],\n        [3, 4]])", "short_docstring": "array(object, dtype=None, *, copy=True, order='K', subok=False, ndmin=0,\n      like=None)"}
{"name": "psutil.NoSuchProcess", "type": "class", "signature": "(pid, name=None, msg=None)", "docstring": "Exception raised when a process with a certain PID doesn't\nor no longer exists.", "short_docstring": "Exception raised when a process with a certain PID doesn't\nor no longer exists."}
{"name": "keras.models.Sequential", "error": "Import error: No module named 'tensorflow'"}
{"name": "matplotlib.pyplot.tight_layout", "type": "callable", "signature": "(*, pad: 'float' = 1.08, h_pad: 'float | None' = None, w_pad: 'float | None' = None, rect: 'tuple[float, float, float, float] | None' = None) -> 'None'", "docstring": "Adjust the padding between and around subplots.\n\nTo exclude an artist on the Axes from the bounding box calculation\nthat determines the subplot parameters (i.e. legend, or annotation),\nset ``a.set_in_layout(False)`` for that artist.\n\nParameters\n----------\npad : float, default: 1.08\n    Padding between the figure edge and the edges of subplots,\n    as a fraction of the font size.\nh_pad, w_pad : float, default: *pad*\n    Padding (height/width) between edges of adjacent subplots,\n    as a fraction of the font size.\nrect : tuple (left, bottom, right, top), default: (0, 0, 1, 1)\n    A rectangle in normalized figure coordinates into which the whole\n    subplots area (including labels) will fit.\n\nSee Also\n--------\n.Figure.set_layout_engine\n.pyplot.tight_layout", "short_docstring": "Adjust the padding between and around subplots."}
{"name": "random.uniform", "type": "callable", "signature": "(a, b)", "docstring": "Get a random number in the range [a, b) or [a, b] depending on rounding.", "short_docstring": "Get a random number in the range [a, b) or [a, b] depending on rounding."}
{"name": "os.scandir", "type": "callable", "signature": "(path=None)", "docstring": "Return an iterator of DirEntry objects for given path.\n\npath can be specified as either str, bytes, or a path-like object.  If path\nis bytes, the names of yielded DirEntry objects will also be bytes; in\nall other circumstances they will be str.\n\nIf path is None, uses the path='.'.", "short_docstring": "Return an iterator of DirEntry objects for given path."}
{"name": "matplotlib.pyplot.savefig", "type": "callable", "signature": "(*args, **kwargs) -> 'None'", "docstring": "Save the current figure.\n\nCall signature::\n\n  savefig(fname, *, transparent=None, dpi='figure', format=None,\n          metadata=None, bbox_inches=None, pad_inches=0.1,\n          facecolor='auto', edgecolor='auto', backend=None,\n          **kwargs\n         )\n\nThe available output formats depend on the backend being used.\n\nParameters\n----------\nfname : str or path-like or binary file-like\n    A path, or a Python file-like object, or\n    possibly some backend-dependent object such as\n    `matplotlib.backends.backend_pdf.PdfPages`.\n\n    If *format* is set, it determines the output format, and the file\n    is saved as *fname*.  Note that *fname* is used verbatim, and there\n    is no attempt to make the extension, if any, of *fname* match\n    *format*, and no extension is appended.\n\n    If *format* is not set, then the format is inferred from the\n    extension of *fname*, if there is one.  If *format* is not\n    set and *fname* has no extension, then the file is saved with\n    :rc:`savefig.format` and the appropriate extension is appended to\n    *fname*.\n\nOther Parameters\n----------------\ntransparent : bool, default: :rc:`savefig.transparent`\n    If *True*, the Axes patches will all be transparent; the\n    Figure patch will also be transparent unless *facecolor*\n    and/or *edgecolor* are specified via kwargs.\n\n    If *False* has no effect and the color of the Axes and\n    Figure patches are unchanged (unless the Figure patch\n    is specified via the *facecolor* and/or *edgecolor* keyword\n    arguments in which case those colors are used).\n\n    The transparency of these patches will be restored to their\n    original values upon exit of this function.\n\n    This is useful, for example, for displaying\n    a plot on top of a colored background on a web page.\n\ndpi : float or 'figure', default: :rc:`savefig.dpi`\n    The resolution in dots per inch.  If 'figure', use the figure's\n    dpi value.\n\nformat : str\n    The file format, e.g. 'png', 'pdf', 'svg', ... The behavior when\n    this is unset is documented under *fname*.\n\nmetadata : dict, optional\n    Key/value pairs to store in the image metadata. The supported keys\n    and defaults depend on the image format and backend:\n\n    - 'png' with Agg backend: See the parameter ``metadata`` of\n      `~.FigureCanvasAgg.print_png`.\n    - 'pdf' with pdf backend: See the parameter ``metadata`` of\n      `~.backend_pdf.PdfPages`.\n    - 'svg' with svg backend: See the parameter ``metadata`` of\n      `~.FigureCanvasSVG.print_svg`.\n    - 'eps' and 'ps' with PS backend: Only 'Creator' is supported.\n\n    Not supported for 'pgf', 'raw', and 'rgba' as those formats do not support\n    embedding metadata.\n    Does not currently support 'jpg', 'tiff', or 'webp', but may include\n    embedding EXIF metadata in the future.\n\nbbox_inches : str or `.Bbox`, default: :rc:`savefig.bbox`\n    Bounding box in inches: only the given portion of the figure is\n    saved.  If 'tight', try to figure out the tight bbox of the figure.\n\npad_inches : float or 'layout', default: :rc:`savefig.pad_inches`\n    Amount of padding in inches around the figure when bbox_inches is\n    'tight'. If 'layout' use the padding from the constrained or\n    compressed layout engine; ignored if one of those engines is not in\n    use.\n\nfacecolor : color or 'auto', default: :rc:`savefig.facecolor`\n    The facecolor of the figure.  If 'auto', use the current figure\n    facecolor.\n\nedgecolor : color or 'auto', default: :rc:`savefig.edgecolor`\n    The edgecolor of the figure.  If 'auto', use the current figure\n    edgecolor.\n\nbackend : str, optional\n    Use a non-default backend to render the file, e.g. to render a\n    png file with the \"cairo\" backend rather than the default \"agg\",\n    or a pdf file with the \"pgf\" backend rather than the default\n    \"pdf\".  Note that the default backend is normally sufficient.  See\n    :ref:`the-builtin-backends` for a list of valid backends for each\n    file format.  Custom backends can be referenced as \"module://...\".\n\norientation : {'landscape', 'portrait'}\n    Currently only supported by the postscript backend.\n\npapertype : str\n    One of 'letter', 'legal', 'executive', 'ledger', 'a0' through\n    'a10', 'b0' through 'b10'. Only supported for postscript\n    output.\n\nbbox_extra_artists : list of `~matplotlib.artist.Artist`, optional\n    A list of extra artists that will be considered when the\n    tight bbox is calculated.\n\npil_kwargs : dict, optional\n    Additional keyword arguments that are passed to\n    `PIL.Image.Image.save` when saving the figure.", "short_docstring": "Save the current figure."}
{"name": "matplotlib.pyplot.legend", "type": "callable", "signature": "(*args, **kwargs) -> 'Legend'", "docstring": "Place a legend on the Axes.\n\nCall signatures::\n\n    legend()\n    legend(handles, labels)\n    legend(handles=handles)\n    legend(labels)\n\nThe call signatures correspond to the following different ways to use\nthis method:\n\n**1. Automatic detection of elements to be shown in the legend**\n\nThe elements to be added to the legend are automatically determined,\nwhen you do not pass in any extra arguments.\n\nIn this case, the labels are taken from the artist. You can specify\nthem either at artist creation or by calling the\n:meth:`~.Artist.set_label` method on the artist::\n\n    ax.plot([1, 2, 3], label='Inline label')\n    ax.legend()\n\nor::\n\n    line, = ax.plot([1, 2, 3])\n    line.set_label('Label via method')\n    ax.legend()\n\n.. note::\n    Specific artists can be excluded from the automatic legend element\n    selection by using a label starting with an underscore, \"_\".\n    A string starting with an underscore is the default label for all\n    artists, so calling `.Axes.legend` without any arguments and\n    without setting the labels manually will result in no legend being\n    drawn.\n\n\n**2. Explicitly listing the artists and labels in the legend**\n\nFor full control of which artists have a legend entry, it is possible\nto pass an iterable of legend artists followed by an iterable of\nlegend labels respectively::\n\n    ax.legend([line1, line2, line3], ['label1', 'label2', 'label3'])\n\n\n**3. Explicitly listing the artists in the legend**\n\nThis is similar to 2, but the labels are taken from the artists'\nlabel properties. Example::\n\n    line1, = ax.plot([1, 2, 3], label='label1')\n    line2, = ax.plot([1, 2, 3], label='label2')\n    ax.legend(handles=[line1, line2])\n\n\n**4. Labeling existing plot elements**\n\n.. admonition:: Discouraged\n\n    This call signature is discouraged, because the relation between\n    plot elements and labels is only implicit by their order and can\n    easily be mixed up.\n\nTo make a legend for all artists on an Axes, call this function with\nan iterable of strings, one for each legend item. For example::\n\n    ax.plot([1, 2, 3])\n    ax.plot([5, 6, 7])\n    ax.legend(['First line', 'Second line'])\n\n\nParameters\n----------\nhandles : list of (`.Artist` or tuple of `.Artist`), optional\n    A list of Artists (lines, patches) to be added to the legend.\n    Use this together with *labels*, if you need full control on what\n    is shown in the legend and the automatic mechanism described above\n    is not sufficient.\n\n    The length of handles and labels should be the same in this\n    case. If they are not, they are truncated to the smaller length.\n\n    If an entry contains a tuple, then the legend handler for all Artists in the\n    tuple will be placed alongside a single label.\n\nlabels : list of str, optional\n    A list of labels to show next to the artists.\n    Use this together with *handles*, if you need full control on what\n    is shown in the legend and the automatic mechanism described above\n    is not sufficient.\n\nReturns\n-------\n`~matplotlib.legend.Legend`\n\nOther Parameters\n----------------\n\nloc : str or pair of floats, default: :rc:`legend.loc`\n    The location of the legend.\n\n    The strings ``'upper left'``, ``'upper right'``, ``'lower left'``,\n    ``'lower right'`` place the legend at the corresponding corner of the\n    axes.\n\n    The strings ``'upper center'``, ``'lower center'``, ``'center left'``,\n    ``'center right'`` place the legend at the center of the corresponding edge\n    of the axes.\n\n    The string ``'center'`` places the legend at the center of the axes.\n\n    The string ``'best'`` places the legend at the location, among the nine\n    locations defined so far, with the minimum overlap with other drawn\n    artists.  This option can be quite slow for plots with large amounts of\n    data; your plotting speed may benefit from providing a specific location.\n\n    The location can also be a 2-tuple giving the coordinates of the lower-left\n    corner of the legend in axes coordinates (in which case *bbox_to_anchor*\n    will be ignored).\n\n    For back-compatibility, ``'center right'`` (but no other location) can also\n    be spelled ``'right'``, and each \"string\" location can also be given as a\n    numeric value:\n\n    ==================   =============\n    Location String      Location Code\n    ==================   =============\n    'best' (Axes only)   0\n    'upper right'        1\n    'upper left'         2\n    'lower left'         3\n    'lower right'        4\n    'right'              5\n    'center left'        6\n    'center right'       7\n    'lower center'       8\n    'upper center'       9\n    'center'             10\n    ==================   =============\n    \nbbox_to_anchor : `.BboxBase`, 2-tuple, or 4-tuple of floats\n    Box that is used to position the legend in conjunction with *loc*.\n    Defaults to `axes.bbox` (if called as a method to `.Axes.legend`) or\n    `figure.bbox` (if `.Figure.legend`).  This argument allows arbitrary\n    placement of the legend.\n\n    Bbox coordinates are interpreted in the coordinate system given by\n    *bbox_transform*, with the default transform\n    Axes or Figure coordinates, depending on which ``legend`` is called.\n\n    If a 4-tuple or `.BboxBase` is given, then it specifies the bbox\n    ``(x, y, width, height)`` that the legend is placed in.\n    To put the legend in the best location in the bottom right\n    quadrant of the axes (or figure)::\n\n        loc='best', bbox_to_anchor=(0.5, 0., 0.5, 0.5)\n\n    A 2-tuple ``(x, y)`` places the corner of the legend specified by *loc* at\n    x, y.  For example, to put the legend's upper right-hand corner in the\n    center of the axes (or figure) the following keywords can be used::\n\n        loc='upper right', bbox_to_anchor=(0.5, 0.5)\n\nncols : int, default: 1\n    The number of columns that the legend has.\n\n    For backward compatibility, the spelling *ncol* is also supported\n    but it is discouraged. If both are given, *ncols* takes precedence.\n\nprop : None or `~matplotlib.font_manager.FontProperties` or dict\n    The font properties of the legend. If None (default), the current\n    :data:`matplotlib.rcParams` will be used.\n\nfontsize : int or {'xx-small', 'x-small', 'small', 'medium', 'large', 'x-large', 'xx-large'}\n    The font size of the legend. If the value is numeric the size will be the\n    absolute font size in points. String values are relative to the current\n    default font size. This argument is only used if *prop* is not specified.\n\nlabelcolor : str or list, default: :rc:`legend.labelcolor`\n    The color of the text in the legend. Either a valid color string\n    (for example, 'red'), or a list of color strings. The labelcolor can\n    also be made to match the color of the line or marker using 'linecolor',\n    'markerfacecolor' (or 'mfc'), or 'markeredgecolor' (or 'mec').\n\n    Labelcolor can be set globally using :rc:`legend.labelcolor`. If None,\n    use :rc:`text.color`.\n\nnumpoints : int, default: :rc:`legend.numpoints`\n    The number of marker points in the legend when creating a legend\n    entry for a `.Line2D` (line).\n\nscatterpoints : int, default: :rc:`legend.scatterpoints`\n    The number of marker points in the legend when creating\n    a legend entry for a `.PathCollection` (scatter plot).\n\nscatteryoffsets : iterable of floats, default: ``[0.375, 0.5, 0.3125]``\n    The vertical offset (relative to the font size) for the markers\n    created for a scatter plot legend entry. 0.0 is at the base the\n    legend text, and 1.0 is at the top. To draw all markers at the\n    same height, set to ``[0.5]``.\n\nmarkerscale : float, default: :rc:`legend.markerscale`\n    The relative size of legend markers compared to the originally drawn ones.\n\nmarkerfirst : bool, default: True\n    If *True*, legend marker is placed to the left of the legend label.\n    If *False*, legend marker is placed to the right of the legend label.\n\nreverse : bool, default: False\n    If *True*, the legend labels are displayed in reverse order from the input.\n    If *False*, the legend labels are displayed in the same order as the input.\n\n    .. versionadded:: 3.7\n\nframeon : bool, default: :rc:`legend.frameon`\n    Whether the legend should be drawn on a patch (frame).\n\nfancybox : bool, default: :rc:`legend.fancybox`\n    Whether round edges should be enabled around the `.FancyBboxPatch` which\n    makes up the legend's background.\n\nshadow : None, bool or dict, default: :rc:`legend.shadow`\n    Whether to draw a shadow behind the legend.\n    The shadow can be configured using `.Patch` keywords.\n    Customization via :rc:`legend.shadow` is currently not supported.\n\nframealpha : float, default: :rc:`legend.framealpha`\n    The alpha transparency of the legend's background.\n    If *shadow* is activated and *framealpha* is ``None``, the default value is\n    ignored.\n\nfacecolor : \"inherit\" or color, default: :rc:`legend.facecolor`\n    The legend's background color.\n    If ``\"inherit\"``, use :rc:`axes.facecolor`.\n\nedgecolor : \"inherit\" or color, default: :rc:`legend.edgecolor`\n    The legend's background patch edge color.\n    If ``\"inherit\"``, use take :rc:`axes.edgecolor`.\n\nmode : {\"expand\", None}\n    If *mode* is set to ``\"expand\"`` the legend will be horizontally\n    expanded to fill the axes area (or *bbox_to_anchor* if defines\n    the legend's size).\n\nbbox_transform : None or `~matplotlib.transforms.Transform`\n    The transform for the bounding box (*bbox_to_anchor*). For a value\n    of ``None`` (default) the Axes'\n    :data:`~matplotlib.axes.Axes.transAxes` transform will be used.\n\ntitle : str or None\n    The legend's title. Default is no title (``None``).\n\ntitle_fontproperties : None or `~matplotlib.font_manager.FontProperties` or dict\n    The font properties of the legend's title. If None (default), the\n    *title_fontsize* argument will be used if present; if *title_fontsize* is\n    also None, the current :rc:`legend.title_fontsize` will be used.\n\ntitle_fontsize : int or {'xx-small', 'x-small', 'small', 'medium', 'large', 'x-large', 'xx-large'}, default: :rc:`legend.title_fontsize`\n    The font size of the legend's title.\n    Note: This cannot be combined with *title_fontproperties*. If you want\n    to set the fontsize alongside other font properties, use the *size*\n    parameter in *title_fontproperties*.\n\nalignment : {'center', 'left', 'right'}, default: 'center'\n    The alignment of the legend title and the box of entries. The entries\n    are aligned as a single block, so that markers always lined up.\n\nborderpad : float, default: :rc:`legend.borderpad`\n    The fractional whitespace inside the legend border, in font-size units.\n\nlabelspacing : float, default: :rc:`legend.labelspacing`\n    The vertical space between the legend entries, in font-size units.\n\nhandlelength : float, default: :rc:`legend.handlelength`\n    The length of the legend handles, in font-size units.\n\nhandleheight : float, default: :rc:`legend.handleheight`\n    The height of the legend handles, in font-size units.\n\nhandletextpad : float, default: :rc:`legend.handletextpad`\n    The pad between the legend handle and text, in font-size units.\n\nborderaxespad : float, default: :rc:`legend.borderaxespad`\n    The pad between the axes and legend border, in font-size units.\n\ncolumnspacing : float, default: :rc:`legend.columnspacing`\n    The spacing between columns, in font-size units.\n\nhandler_map : dict or None\n    The custom dictionary mapping instances or types to a legend\n    handler. This *handler_map* updates the default handler map\n    found at `matplotlib.legend.Legend.get_legend_handler_map`.\n\ndraggable : bool, default: False\n    Whether the legend can be dragged with the mouse.\n\n\nSee Also\n--------\n.Figure.legend\n\nNotes\n-----\nSome artists are not supported by this function.  See\n:ref:`legend_guide` for details.\n\nExamples\n--------\n.. plot:: gallery/text_labels_and_annotations/legend.py", "short_docstring": "Place a legend on the Axes."}
{"name": "re.match", "type": "callable", "signature": "(pattern, string, flags=0)", "docstring": "Try to apply the pattern at the start of the string, returning\na Match object, or None if no match was found.", "short_docstring": "Try to apply the pattern at the start of the string, returning\na Match object, or None if no match was found."}
{"name": "werkzeug.security.check_password_hash", "type": "callable", "signature": "(pwhash: 'str', password: 'str') -> 'bool'", "docstring": "Securely check that the given stored password hash, previously generated using\n:func:`generate_password_hash`, matches the given password.\n\nMethods may be deprecated and removed if they are no longer considered secure. To\nmigrate old hashes, you may generate a new hash when checking an old hash, or you\nmay contact users with a link to reset their password.\n\n:param pwhash: The hashed password.\n:param password: The plaintext password.\n\n.. versionchanged:: 2.3\n    All plain hashes are deprecated and will not be supported in Werkzeug 3.0.", "short_docstring": "Securely check that the given stored password hash, previously generated using\n:func:`generate_password_hash`, matches the given password."}
{"name": "numpy.bincount", "type": "callable", "signature": null, "docstring": "bincount(x, /, weights=None, minlength=0)\n\nCount number of occurrences of each value in array of non-negative ints.\n\nThe number of bins (of size 1) is one larger than the largest value in\n`x`. If `minlength` is specified, there will be at least this number\nof bins in the output array (though it will be longer if necessary,\ndepending on the contents of `x`).\nEach bin gives the number of occurrences of its index value in `x`.\nIf `weights` is specified the input array is weighted by it, i.e. if a\nvalue ``n`` is found at position ``i``, ``out[n] += weight[i]`` instead\nof ``out[n] += 1``.\n\nParameters\n----------\nx : array_like, 1 dimension, nonnegative ints\n    Input array.\nweights : array_like, optional\n    Weights, array of the same shape as `x`.\nminlength : int, optional\n    A minimum number of bins for the output array.\n\n    .. versionadded:: 1.6.0\n\nReturns\n-------\nout : ndarray of ints\n    The result of binning the input array.\n    The length of `out` is equal to ``np.amax(x)+1``.\n\nRaises\n------\nValueError\n    If the input is not 1-dimensional, or contains elements with negative\n    values, or if `minlength` is negative.\nTypeError\n    If the type of the input is float or complex.\n\nSee Also\n--------\nhistogram, digitize, unique\n\nExamples\n--------\n>>> np.bincount(np.arange(5))\narray([1, 1, 1, 1, 1])\n>>> np.bincount(np.array([0, 1, 1, 3, 2, 1, 7]))\narray([1, 3, 1, 1, 0, 0, 0, 1])\n\n>>> x = np.array([0, 1, 1, 3, 2, 1, 7, 23])\n>>> np.bincount(x).size == np.amax(x)+1\nTrue\n\nThe input array needs to be of integer dtype, otherwise a\nTypeError is raised:\n\n>>> np.bincount(np.arange(5, dtype=float))\nTraceback (most recent call last):\n  ...\nTypeError: Cannot cast array data from dtype('float64') to dtype('int64')\naccording to the rule 'safe'\n\nA possible use of ``bincount`` is to perform sums over\nvariable-size chunks of an array, using the ``weights`` keyword.\n\n>>> w = np.array([0.3, 0.5, 0.2, 0.7, 1., -0.6]) # weights\n>>> x = np.array([0, 1, 1, 2, 2, 2])\n>>> np.bincount(x,  weights=w)\narray([ 0.3,  0.7,  1.1])", "short_docstring": "bincount(x, /, weights=None, minlength=0)"}
{"name": "matplotlib.pyplot.plot", "type": "callable", "signature": "(*args: 'float | ArrayLike | str', scalex: 'bool' = True, scaley: 'bool' = True, data=None, **kwargs) -> 'list[Line2D]'", "docstring": "Plot y versus x as lines and/or markers.\n\nCall signatures::\n\n    plot([x], y, [fmt], *, data=None, **kwargs)\n    plot([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n\nThe coordinates of the points or line nodes are given by *x*, *y*.\n\nThe optional parameter *fmt* is a convenient way for defining basic\nformatting like color, marker and linestyle. It's a shortcut string\nnotation described in the *Notes* section below.\n\n>>> plot(x, y)        # plot x and y using default line style and color\n>>> plot(x, y, 'bo')  # plot x and y using blue circle markers\n>>> plot(y)           # plot y using x as index array 0..N-1\n>>> plot(y, 'r+')     # ditto, but with red plusses\n\nYou can use `.Line2D` properties as keyword arguments for more\ncontrol on the appearance. Line properties and *fmt* can be mixed.\nThe following two calls yield identical results:\n\n>>> plot(x, y, 'go--', linewidth=2, markersize=12)\n>>> plot(x, y, color='green', marker='o', linestyle='dashed',\n...      linewidth=2, markersize=12)\n\nWhen conflicting with *fmt*, keyword arguments take precedence.\n\n\n**Plotting labelled data**\n\nThere's a convenient way for plotting objects with labelled data (i.e.\ndata that can be accessed by index ``obj['y']``). Instead of giving\nthe data in *x* and *y*, you can provide the object in the *data*\nparameter and just give the labels for *x* and *y*::\n\n>>> plot('xlabel', 'ylabel', data=obj)\n\nAll indexable objects are supported. This could e.g. be a `dict`, a\n`pandas.DataFrame` or a structured numpy array.\n\n\n**Plotting multiple sets of data**\n\nThere are various ways to plot multiple sets of data.\n\n- The most straight forward way is just to call `plot` multiple times.\n  Example:\n\n  >>> plot(x1, y1, 'bo')\n  >>> plot(x2, y2, 'go')\n\n- If *x* and/or *y* are 2D arrays a separate data set will be drawn\n  for every column. If both *x* and *y* are 2D, they must have the\n  same shape. If only one of them is 2D with shape (N, m) the other\n  must have length N and will be used for every data set m.\n\n  Example:\n\n  >>> x = [1, 2, 3]\n  >>> y = np.array([[1, 2], [3, 4], [5, 6]])\n  >>> plot(x, y)\n\n  is equivalent to:\n\n  >>> for col in range(y.shape[1]):\n  ...     plot(x, y[:, col])\n\n- The third way is to specify multiple sets of *[x]*, *y*, *[fmt]*\n  groups::\n\n  >>> plot(x1, y1, 'g^', x2, y2, 'g-')\n\n  In this case, any additional keyword argument applies to all\n  datasets. Also, this syntax cannot be combined with the *data*\n  parameter.\n\nBy default, each line is assigned a different style specified by a\n'style cycle'. The *fmt* and line property parameters are only\nnecessary if you want explicit deviations from these defaults.\nAlternatively, you can also change the style cycle using\n:rc:`axes.prop_cycle`.\n\n\nParameters\n----------\nx, y : array-like or scalar\n    The horizontal / vertical coordinates of the data points.\n    *x* values are optional and default to ``range(len(y))``.\n\n    Commonly, these parameters are 1D arrays.\n\n    They can also be scalars, or two-dimensional (in that case, the\n    columns represent separate data sets).\n\n    These arguments cannot be passed as keywords.\n\nfmt : str, optional\n    A format string, e.g. 'ro' for red circles. See the *Notes*\n    section for a full description of the format strings.\n\n    Format strings are just an abbreviation for quickly setting\n    basic line properties. All of these and more can also be\n    controlled by keyword arguments.\n\n    This argument cannot be passed as keyword.\n\ndata : indexable object, optional\n    An object with labelled data. If given, provide the label names to\n    plot in *x* and *y*.\n\n    .. note::\n        Technically there's a slight ambiguity in calls where the\n        second label is a valid *fmt*. ``plot('n', 'o', data=obj)``\n        could be ``plt(x, y)`` or ``plt(y, fmt)``. In such cases,\n        the former interpretation is chosen, but a warning is issued.\n        You may suppress the warning by adding an empty format string\n        ``plot('n', 'o', '', data=obj)``.\n\nReturns\n-------\nlist of `.Line2D`\n    A list of lines representing the plotted data.\n\nOther Parameters\n----------------\nscalex, scaley : bool, default: True\n    These parameters determine if the view limits are adapted to the\n    data limits. The values are passed on to\n    `~.axes.Axes.autoscale_view`.\n\n**kwargs : `~matplotlib.lines.Line2D` properties, optional\n    *kwargs* are used to specify properties like a line label (for\n    auto legends), linewidth, antialiasing, marker face color.\n    Example::\n\n    >>> plot([1, 2, 3], [1, 2, 3], 'go-', label='line 1', linewidth=2)\n    >>> plot([1, 2, 3], [1, 4, 9], 'rs', label='line 2')\n\n    If you specify multiple lines with one plot call, the kwargs apply\n    to all those lines. In case the label object is iterable, each\n    element is used as labels for each set of data.\n\n    Here is a list of available `.Line2D` properties:\n\n    Properties:\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: scalar or None\n    animated: bool\n    antialiased or aa: bool\n    clip_box: `~matplotlib.transforms.BboxBase` or None\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    color or c: color\n    dash_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    dash_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    dashes: sequence of floats (on/off ink in points) or (None, None)\n    data: (2, N) array or two 1D arrays\n    drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n    figure: `~matplotlib.figure.Figure`\n    fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n    gapcolor: color or None\n    gid: str\n    in_layout: bool\n    label: object\n    linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n    linewidth or lw: float\n    marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n    markeredgecolor or mec: color\n    markeredgewidth or mew: float\n    markerfacecolor or mfc: color\n    markerfacecoloralt or mfcalt: color\n    markersize or ms: float\n    markevery: None or int or (int, int) or slice or list[int] or float or (float, float) or list[bool]\n    mouseover: bool\n    path_effects: list of `.AbstractPathEffect`\n    picker: float or callable[[Artist, Event], tuple[bool, dict]]\n    pickradius: float\n    rasterized: bool\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    solid_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    solid_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    transform: unknown\n    url: str\n    visible: bool\n    xdata: 1D array\n    ydata: 1D array\n    zorder: float\n\nSee Also\n--------\nscatter : XY scatter plot with markers of varying size and/or color (\n    sometimes also called bubble chart).\n\nNotes\n-----\n**Format Strings**\n\nA format string consists of a part for color, marker and line::\n\n    fmt = '[marker][line][color]'\n\nEach of them is optional. If not provided, the value from the style\ncycle is used. Exception: If ``line`` is given, but no ``marker``,\nthe data will be a line without markers.\n\nOther combinations such as ``[color][marker][line]`` are also\nsupported, but note that their parsing may be ambiguous.\n\n**Markers**\n\n=============   ===============================\ncharacter       description\n=============   ===============================\n``'.'``         point marker\n``','``         pixel marker\n``'o'``         circle marker\n``'v'``         triangle_down marker\n``'^'``         triangle_up marker\n``'<'``         triangle_left marker\n``'>'``         triangle_right marker\n``'1'``         tri_down marker\n``'2'``         tri_up marker\n``'3'``         tri_left marker\n``'4'``         tri_right marker\n``'8'``         octagon marker\n``'s'``         square marker\n``'p'``         pentagon marker\n``'P'``         plus (filled) marker\n``'*'``         star marker\n``'h'``         hexagon1 marker\n``'H'``         hexagon2 marker\n``'+'``         plus marker\n``'x'``         x marker\n``'X'``         x (filled) marker\n``'D'``         diamond marker\n``'d'``         thin_diamond marker\n``'|'``         vline marker\n``'_'``         hline marker\n=============   ===============================\n\n**Line Styles**\n\n=============    ===============================\ncharacter        description\n=============    ===============================\n``'-'``          solid line style\n``'--'``         dashed line style\n``'-.'``         dash-dot line style\n``':'``          dotted line style\n=============    ===============================\n\nExample format strings::\n\n    'b'    # blue markers with default shape\n    'or'   # red circles\n    '-g'   # green solid line\n    '--'   # dashed line with default color\n    '^k:'  # black triangle_up markers connected by a dotted line\n\n**Colors**\n\nThe supported color abbreviations are the single letter codes\n\n=============    ===============================\ncharacter        color\n=============    ===============================\n``'b'``          blue\n``'g'``          green\n``'r'``          red\n``'c'``          cyan\n``'m'``          magenta\n``'y'``          yellow\n``'k'``          black\n``'w'``          white\n=============    ===============================\n\nand the ``'CN'`` colors that index into the default property cycle.\n\nIf the color is the only part of the format string, you can\nadditionally use any  `matplotlib.colors` spec, e.g. full names\n(``'green'``) or hex strings (``'#008000'``).", "short_docstring": "Plot y versus x as lines and/or markers."}
{"name": "statsmodels.tsa.seasonal.seasonal_decompose", "type": "callable", "signature": "(x, model='additive', filt=None, period=None, two_sided=True, extrapolate_trend=0)", "docstring": "Seasonal decomposition using moving averages.\n\nParameters\n----------\nx : array_like\n    Time series. If 2d, individual series are in columns. x must contain 2\n    complete cycles.\nmodel : {\"additive\", \"multiplicative\"}, optional\n    Type of seasonal component. Abbreviations are accepted.\nfilt : array_like, optional\n    The filter coefficients for filtering out the seasonal component.\n    The concrete moving average method used in filtering is determined by\n    two_sided.\nperiod : int, optional\n    Period of the series. Must be used if x is not a pandas object or if\n    the index of x does not have  a frequency. Overrides default\n    periodicity of x if x is a pandas object with a timeseries index.\ntwo_sided : bool, optional\n    The moving average method used in filtering.\n    If True (default), a centered moving average is computed using the\n    filt. If False, the filter coefficients are for past values only.\nextrapolate_trend : int or 'freq', optional\n    If set to > 0, the trend resulting from the convolution is\n    linear least-squares extrapolated on both ends (or the single one\n    if two_sided is False) considering this many (+1) closest points.\n    If set to 'freq', use `freq` closest points. Setting this parameter\n    results in no NaN values in trend or resid components.\n\nReturns\n-------\nDecomposeResult\n    A object with seasonal, trend, and resid attributes.\n\nSee Also\n--------\nstatsmodels.tsa.filters.bk_filter.bkfilter\n    Baxter-King filter.\nstatsmodels.tsa.filters.cf_filter.cffilter\n    Christiano-Fitzgerald asymmetric, random walk filter.\nstatsmodels.tsa.filters.hp_filter.hpfilter\n    Hodrick-Prescott filter.\nstatsmodels.tsa.filters.convolution_filter\n    Linear filtering via convolution.\nstatsmodels.tsa.seasonal.STL\n    Season-Trend decomposition using LOESS.\n\nNotes\n-----\nThis is a naive decomposition. More sophisticated methods should\nbe preferred.\n\nThe additive model is Y[t] = T[t] + S[t] + e[t]\n\nThe multiplicative model is Y[t] = T[t] * S[t] * e[t]\n\nThe results are obtained by first estimating the trend by applying\na convolution filter to the data. The trend is then removed from the\nseries and the average of this de-trended series for each period is\nthe returned seasonal component.", "short_docstring": "Seasonal decomposition using moving averages."}
{"name": "math.floor", "type": "callable", "signature": "(x, /)", "docstring": "Return the floor of x as an Integral.\n\nThis is the largest integer <= x.", "short_docstring": "Return the floor of x as an Integral."}
{"name": "sqlite3.DatabaseError", "type": "class", "signature": null, "docstring": "Common base class for all non-exit exceptions.", "short_docstring": "Common base class for all non-exit exceptions."}
{"name": "matplotlib.pyplot.subplot", "type": "callable", "signature": "(*args, **kwargs) -> 'Axes'", "docstring": "Add an Axes to the current figure or retrieve an existing Axes.\n\nThis is a wrapper of `.Figure.add_subplot` which provides additional\nbehavior when working with the implicit API (see the notes section).\n\nCall signatures::\n\n   subplot(nrows, ncols, index, **kwargs)\n   subplot(pos, **kwargs)\n   subplot(**kwargs)\n   subplot(ax)\n\nParameters\n----------\n*args : int, (int, int, *index*), or `.SubplotSpec`, default: (1, 1, 1)\n    The position of the subplot described by one of\n\n    - Three integers (*nrows*, *ncols*, *index*). The subplot will take the\n      *index* position on a grid with *nrows* rows and *ncols* columns.\n      *index* starts at 1 in the upper left corner and increases to the\n      right. *index* can also be a two-tuple specifying the (*first*,\n      *last*) indices (1-based, and including *last*) of the subplot, e.g.,\n      ``fig.add_subplot(3, 1, (1, 2))`` makes a subplot that spans the\n      upper 2/3 of the figure.\n    - A 3-digit integer. The digits are interpreted as if given separately\n      as three single-digit integers, i.e. ``fig.add_subplot(235)`` is the\n      same as ``fig.add_subplot(2, 3, 5)``. Note that this can only be used\n      if there are no more than 9 subplots.\n    - A `.SubplotSpec`.\n\nprojection : {None, 'aitoff', 'hammer', 'lambert', 'mollweide', 'polar', 'rectilinear', str}, optional\n    The projection type of the subplot (`~.axes.Axes`). *str* is the name\n    of a custom projection, see `~matplotlib.projections`. The default\n    None results in a 'rectilinear' projection.\n\npolar : bool, default: False\n    If True, equivalent to projection='polar'.\n\nsharex, sharey : `~matplotlib.axes.Axes`, optional\n    Share the x or y `~matplotlib.axis` with sharex and/or sharey. The\n    axis will have the same limits, ticks, and scale as the axis of the\n    shared axes.\n\nlabel : str\n    A label for the returned axes.\n\nReturns\n-------\n`~.axes.Axes`\n\n    The Axes of the subplot. The returned Axes can actually be an instance\n    of a subclass, such as `.projections.polar.PolarAxes` for polar\n    projections.\n\nOther Parameters\n----------------\n**kwargs\n    This method also takes the keyword arguments for the returned axes\n    base class; except for the *figure* argument. The keyword arguments\n    for the rectilinear base class `~.axes.Axes` can be found in\n    the following table but there might also be other keyword\n    arguments if another projection is used.\n\n    Properties:\n    adjustable: {'box', 'datalim'}\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: scalar or None\n    anchor: (float, float) or {'C', 'SW', 'S', 'SE', 'E', 'NE', ...}\n    animated: bool\n    aspect: {'auto', 'equal'} or float\n    autoscale_on: bool\n    autoscalex_on: unknown\n    autoscaley_on: unknown\n    axes_locator: Callable[[Axes, Renderer], Bbox]\n    axisbelow: bool or 'line'\n    box_aspect: float or None\n    clip_box: `~matplotlib.transforms.BboxBase` or None\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    facecolor or fc: color\n    figure: `~matplotlib.figure.Figure`\n    frame_on: bool\n    gid: str\n    in_layout: bool\n    label: object\n    mouseover: bool\n    navigate: bool\n    navigate_mode: unknown\n    path_effects: list of `.AbstractPathEffect`\n    picker: None or bool or float or callable\n    position: [left, bottom, width, height] or `~matplotlib.transforms.Bbox`\n    prop_cycle: `~cycler.Cycler`\n    rasterization_zorder: float or None\n    rasterized: bool\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    subplotspec: unknown\n    title: str\n    transform: `~matplotlib.transforms.Transform`\n    url: str\n    visible: bool\n    xbound: (lower: float, upper: float)\n    xlabel: str\n    xlim: (left: float, right: float)\n    xmargin: float greater than -0.5\n    xscale: unknown\n    xticklabels: unknown\n    xticks: unknown\n    ybound: (lower: float, upper: float)\n    ylabel: str\n    ylim: (bottom: float, top: float)\n    ymargin: float greater than -0.5\n    yscale: unknown\n    yticklabels: unknown\n    yticks: unknown\n    zorder: float\n\nNotes\n-----\nCreating a new Axes will delete any preexisting Axes that\noverlaps with it beyond sharing a boundary::\n\n    import matplotlib.pyplot as plt\n    # plot a line, implicitly creating a subplot(111)\n    plt.plot([1, 2, 3])\n    # now create a subplot which represents the top plot of a grid\n    # with 2 rows and 1 column. Since this subplot will overlap the\n    # first, the plot (and its axes) previously created, will be removed\n    plt.subplot(211)\n\nIf you do not want this behavior, use the `.Figure.add_subplot` method\nor the `.pyplot.axes` function instead.\n\nIf no *kwargs* are passed and there exists an Axes in the location\nspecified by *args* then that Axes will be returned rather than a new\nAxes being created.\n\nIf *kwargs* are passed and there exists an Axes in the location\nspecified by *args*, the projection type is the same, and the\n*kwargs* match with the existing Axes, then the existing Axes is\nreturned.  Otherwise a new Axes is created with the specified\nparameters.  We save a reference to the *kwargs* which we use\nfor this comparison.  If any of the values in *kwargs* are\nmutable we will not detect the case where they are mutated.\nIn these cases we suggest using `.Figure.add_subplot` and the\nexplicit Axes API rather than the implicit pyplot API.\n\nSee Also\n--------\n.Figure.add_subplot\n.pyplot.subplots\n.pyplot.axes\n.Figure.subplots\n\nExamples\n--------\n::\n\n    plt.subplot(221)\n\n    # equivalent but more general\n    ax1 = plt.subplot(2, 2, 1)\n\n    # add a subplot with no frame\n    ax2 = plt.subplot(222, frameon=False)\n\n    # add a polar subplot\n    plt.subplot(223, projection='polar')\n\n    # add a red subplot that shares the x-axis with ax1\n    plt.subplot(224, sharex=ax1, facecolor='red')\n\n    # delete ax2 from the figure\n    plt.delaxes(ax2)\n\n    # add ax2 to the figure again\n    plt.subplot(ax2)\n\n    # make the first axes \"current\" again\n    plt.subplot(221)", "short_docstring": "Add an Axes to the current figure or retrieve an existing Axes."}
{"name": "Crypto.Cipher.AES.MODE_EAX", "type": "constant", "value": "9", "signature": null, "docstring": "int([x]) -> integer\nint(x, base=10) -> integer\n\nConvert a number or string to an integer, or return 0 if no arguments\nare given.  If x is a number, return x.__int__().  For floating point\nnumbers, this truncates towards zero.\n\nIf x is not a number or if base is given, then x must be a string,\nbytes, or bytearray instance representing an integer literal in the\ngiven base.  The literal can be preceded by '+' or '-' and be surrounded\nby whitespace.  The base defaults to 10.  Valid bases are 0 and 2-36.\nBase 0 means to interpret the base from the string as an integer literal.\n>>> int('0b100', base=0)\n4", "short_docstring": "int([x]) -> integer\nint(x, base=10) -> integer"}
{"name": "numpy.std", "type": "callable", "signature": "(a, axis=None, dtype=None, out=None, ddof=0, keepdims=<no value>, *, where=<no value>)", "docstring": "Compute the standard deviation along the specified axis.\n\nReturns the standard deviation, a measure of the spread of a distribution,\nof the array elements. The standard deviation is computed for the\nflattened array by default, otherwise over the specified axis.\n\nParameters\n----------\na : array_like\n    Calculate the standard deviation of these values.\naxis : None or int or tuple of ints, optional\n    Axis or axes along which the standard deviation is computed. The\n    default is to compute the standard deviation of the flattened array.\n\n    .. versionadded:: 1.7.0\n\n    If this is a tuple of ints, a standard deviation is performed over\n    multiple axes, instead of a single axis or all the axes as before.\ndtype : dtype, optional\n    Type to use in computing the standard deviation. For arrays of\n    integer type the default is float64, for arrays of float types it is\n    the same as the array type.\nout : ndarray, optional\n    Alternative output array in which to place the result. It must have\n    the same shape as the expected output but the type (of the calculated\n    values) will be cast if necessary.\nddof : int, optional\n    Means Delta Degrees of Freedom.  The divisor used in calculations\n    is ``N - ddof``, where ``N`` represents the number of elements.\n    By default `ddof` is zero.\nkeepdims : bool, optional\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\n    If the default value is passed, then `keepdims` will not be\n    passed through to the `std` method of sub-classes of\n    `ndarray`, however any non-default value will be.  If the\n    sub-class' method does not implement `keepdims` any\n    exceptions will be raised.\n\nwhere : array_like of bool, optional\n    Elements to include in the standard deviation.\n    See `~numpy.ufunc.reduce` for details.\n\n    .. versionadded:: 1.20.0\n\nReturns\n-------\nstandard_deviation : ndarray, see dtype parameter above.\n    If `out` is None, return a new array containing the standard deviation,\n    otherwise return a reference to the output array.\n\nSee Also\n--------\nvar, mean, nanmean, nanstd, nanvar\n:ref:`ufuncs-output-type`\n\nNotes\n-----\nThe standard deviation is the square root of the average of the squared\ndeviations from the mean, i.e., ``std = sqrt(mean(x))``, where\n``x = abs(a - a.mean())**2``.\n\nThe average squared deviation is typically calculated as ``x.sum() / N``,\nwhere ``N = len(x)``. If, however, `ddof` is specified, the divisor\n``N - ddof`` is used instead. In standard statistical practice, ``ddof=1``\nprovides an unbiased estimator of the variance of the infinite population.\n``ddof=0`` provides a maximum likelihood estimate of the variance for\nnormally distributed variables. The standard deviation computed in this\nfunction is the square root of the estimated variance, so even with\n``ddof=1``, it will not be an unbiased estimate of the standard deviation\nper se.\n\nNote that, for complex numbers, `std` takes the absolute\nvalue before squaring, so that the result is always real and nonnegative.\n\nFor floating-point input, the *std* is computed using the same\nprecision the input has. Depending on the input data, this can cause\nthe results to be inaccurate, especially for float32 (see example below).\nSpecifying a higher-accuracy accumulator using the `dtype` keyword can\nalleviate this issue.\n\nExamples\n--------\n>>> a = np.array([[1, 2], [3, 4]])\n>>> np.std(a)\n1.1180339887498949 # may vary\n>>> np.std(a, axis=0)\narray([1.,  1.])\n>>> np.std(a, axis=1)\narray([0.5,  0.5])\n\nIn single precision, std() can be inaccurate:\n\n>>> a = np.zeros((2, 512*512), dtype=np.float32)\n>>> a[0, :] = 1.0\n>>> a[1, :] = 0.1\n>>> np.std(a)\n0.45000005\n\nComputing the standard deviation in float64 is more accurate:\n\n>>> np.std(a, dtype=np.float64)\n0.44999999925494177 # may vary\n\nSpecifying a where argument:\n\n>>> a = np.array([[14, 8, 11, 10], [7, 9, 10, 11], [10, 15, 5, 10]])\n>>> np.std(a)\n2.614064523559687 # may vary\n>>> np.std(a, where=[[True], [True], [False]])\n2.0", "short_docstring": "Compute the standard deviation along the specified axis."}
{"name": "base64.b64encode", "type": "callable", "signature": "(s, altchars=None)", "docstring": "Encode the bytes-like object s using Base64 and return a bytes object.\n\nOptional altchars should be a byte string of length 2 which specifies an\nalternative alphabet for the '+' and '/' characters.  This allows an\napplication to e.g. generate url or filesystem safe Base64 strings.", "short_docstring": "Encode the bytes-like object s using Base64 and return a bytes object."}
{"name": "datetime.datetime.fromtimestamp", "type": "callable", "signature": null, "docstring": "timestamp[, tz] -> tz's local time from POSIX timestamp.", "short_docstring": "timestamp[, tz] -> tz's local time from POSIX timestamp."}
{"name": "soundfile.read", "type": "callable", "signature": "(file, frames=-1, start=0, stop=None, dtype='float64', always_2d=False, fill_value=None, out=None, samplerate=None, channels=None, format=None, subtype=None, endian=None, closefd=True)", "docstring": "Provide audio data from a sound file as NumPy array.\n\nBy default, the whole file is read from the beginning, but the\nposition to start reading can be specified with *start* and the\nnumber of frames to read can be specified with *frames*.\nAlternatively, a range can be specified with *start* and *stop*.\n\nIf there is less data left in the file than requested, the rest of\nthe frames are filled with *fill_value*.\nIf no *fill_value* is specified, a smaller array is returned.\n\nParameters\n----------\nfile : str or int or file-like object\n    The file to read from.  See `SoundFile` for details.\nframes : int, optional\n    The number of frames to read. If *frames* is negative, the whole\n    rest of the file is read.  Not allowed if *stop* is given.\nstart : int, optional\n    Where to start reading.  A negative value counts from the end.\nstop : int, optional\n    The index after the last frame to be read.  A negative value\n    counts from the end.  Not allowed if *frames* is given.\ndtype : {'float64', 'float32', 'int32', 'int16'}, optional\n    Data type of the returned array, by default ``'float64'``.\n    Floating point audio data is typically in the range from\n    ``-1.0`` to ``1.0``.  Integer data is in the range from\n    ``-2**15`` to ``2**15-1`` for ``'int16'`` and from ``-2**31`` to\n    ``2**31-1`` for ``'int32'``.\n\n    .. note:: Reading int values from a float file will *not*\n        scale the data to [-1.0, 1.0). If the file contains\n        ``np.array([42.6], dtype='float32')``, you will read\n        ``np.array([43], dtype='int32')`` for ``dtype='int32'``.\n\nReturns\n-------\naudiodata : `numpy.ndarray` or type(out)\n    A two-dimensional (frames x channels) NumPy array is returned.\n    If the sound file has only one channel, a one-dimensional array\n    is returned.  Use ``always_2d=True`` to return a two-dimensional\n    array anyway.\n\n    If *out* was specified, it is returned.  If *out* has more\n    frames than available in the file (or if *frames* is smaller\n    than the length of *out*) and no *fill_value* is given, then\n    only a part of *out* is overwritten and a view containing all\n    valid frames is returned.\nsamplerate : int\n    The sample rate of the audio file.\n\nOther Parameters\n----------------\nalways_2d : bool, optional\n    By default, reading a mono sound file will return a\n    one-dimensional array.  With ``always_2d=True``, audio data is\n    always returned as a two-dimensional array, even if the audio\n    file has only one channel.\nfill_value : float, optional\n    If more frames are requested than available in the file, the\n    rest of the output is be filled with *fill_value*.  If\n    *fill_value* is not specified, a smaller array is returned.\nout : `numpy.ndarray` or subclass, optional\n    If *out* is specified, the data is written into the given array\n    instead of creating a new array.  In this case, the arguments\n    *dtype* and *always_2d* are silently ignored!  If *frames* is\n    not given, it is obtained from the length of *out*.\nsamplerate, channels, format, subtype, endian, closefd\n    See `SoundFile`.\n\nExamples\n--------\n>>> import soundfile as sf\n>>> data, samplerate = sf.read('stereo_file.wav')\n>>> data\narray([[ 0.71329652,  0.06294799],\n       [-0.26450912, -0.38874483],\n       ...\n       [ 0.67398441, -0.11516333]])\n>>> samplerate\n44100", "short_docstring": "Provide audio data from a sound file as NumPy array."}
{"name": "json.loads", "type": "callable", "signature": "(s, *, cls=None, object_hook=None, parse_float=None, parse_int=None, parse_constant=None, object_pairs_hook=None, **kw)", "docstring": "Deserialize ``s`` (a ``str``, ``bytes`` or ``bytearray`` instance\ncontaining a JSON document) to a Python object.\n\n``object_hook`` is an optional function that will be called with the\nresult of any object literal decode (a ``dict``). The return value of\n``object_hook`` will be used instead of the ``dict``. This feature\ncan be used to implement custom decoders (e.g. JSON-RPC class hinting).\n\n``object_pairs_hook`` is an optional function that will be called with the\nresult of any object literal decoded with an ordered list of pairs.  The\nreturn value of ``object_pairs_hook`` will be used instead of the ``dict``.\nThis feature can be used to implement custom decoders.  If ``object_hook``\nis also defined, the ``object_pairs_hook`` takes priority.\n\n``parse_float``, if specified, will be called with the string\nof every JSON float to be decoded. By default this is equivalent to\nfloat(num_str). This can be used to use another datatype or parser\nfor JSON floats (e.g. decimal.Decimal).\n\n``parse_int``, if specified, will be called with the string\nof every JSON int to be decoded. By default this is equivalent to\nint(num_str). This can be used to use another datatype or parser\nfor JSON integers (e.g. float).\n\n``parse_constant``, if specified, will be called with one of the\nfollowing strings: -Infinity, Infinity, NaN.\nThis can be used to raise an exception if invalid JSON numbers\nare encountered.\n\nTo use a custom ``JSONDecoder`` subclass, specify it with the ``cls``\nkwarg; otherwise ``JSONDecoder`` is used.", "short_docstring": "Deserialize ``s`` (a ``str``, ``bytes`` or ``bytearray`` instance\ncontaining a JSON document) to a Python object."}
{"name": "matplotlib.axes.Axes", "type": "class", "signature": "(fig, *args, facecolor=None, frameon=True, sharex=None, sharey=None, label='', xscale=None, yscale=None, box_aspect=None, **kwargs)", "docstring": "An Axes object encapsulates all the elements of an individual (sub-)plot in\na figure.\n\nIt contains most of the (sub-)plot elements: `~.axis.Axis`,\n`~.axis.Tick`, `~.lines.Line2D`, `~.text.Text`, `~.patches.Polygon`, etc.,\nand sets the coordinate system.\n\nLike all visible elements in a figure, Axes is an `.Artist` subclass.\n\nThe `Axes` instance supports callbacks through a callbacks attribute which\nis a `~.cbook.CallbackRegistry` instance.  The events you can connect to\nare 'xlim_changed' and 'ylim_changed' and the callback will be called with\nfunc(*ax*) where *ax* is the `Axes` instance.\n\n.. note::\n\n    As a user, you do not instantiate Axes directly, but use Axes creation\n    methods instead; e.g. from `.pyplot` or `.Figure`:\n    `~.pyplot.subplots`, `~.pyplot.subplot_mosaic` or `.Figure.add_axes`.\n\nAttributes\n----------\ndataLim : `.Bbox`\n    The bounding box enclosing all data displayed in the Axes.\nviewLim : `.Bbox`\n    The view limits in data coordinates.", "short_docstring": "An Axes object encapsulates all the elements of an individual (sub-)plot in\na figure."}
{"name": "requests.exceptions.RequestException", "type": "class", "signature": "(*args, **kwargs)", "docstring": "There was an ambiguous exception that occurred while handling your\nrequest.", "short_docstring": "There was an ambiguous exception that occurred while handling your\nrequest."}
{"name": "sklearn.metrics.auc", "type": "callable", "signature": "(x, y)", "docstring": "Compute Area Under the Curve (AUC) using the trapezoidal rule.\n\nThis is a general function, given points on a curve.  For computing the\narea under the ROC-curve, see :func:`roc_auc_score`.  For an alternative\nway to summarize a precision-recall curve, see\n:func:`average_precision_score`.\n\nParameters\n----------\nx : array-like of shape (n,)\n    X coordinates. These must be either monotonic increasing or monotonic\n    decreasing.\ny : array-like of shape (n,)\n    Y coordinates.\n\nReturns\n-------\nauc : float\n    Area Under the Curve.\n\nSee Also\n--------\nroc_auc_score : Compute the area under the ROC curve.\naverage_precision_score : Compute average precision from prediction scores.\nprecision_recall_curve : Compute precision-recall pairs for different\n    probability thresholds.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn import metrics\n>>> y = np.array([1, 1, 2, 2])\n>>> pred = np.array([0.1, 0.4, 0.35, 0.8])\n>>> fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=2)\n>>> metrics.auc(fpr, tpr)\n0.75", "short_docstring": "Compute Area Under the Curve (AUC) using the trapezoidal rule."}
{"name": "email.mime.text.MIMEText", "type": "class", "signature": "(_text, _subtype='plain', _charset=None, *, policy=None)", "docstring": "Class for generating text/* type MIME documents.", "short_docstring": "Class for generating text/* type MIME documents."}
{"name": "re.sub", "type": "callable", "signature": "(pattern, repl, string, count=0, flags=0)", "docstring": "Return the string obtained by replacing the leftmost\nnon-overlapping occurrences of the pattern in string by the\nreplacement repl.  repl can be either a string or a callable;\nif a string, backslash escapes in it are processed.  If it is\na callable, it's passed the Match object and must return\na replacement string to be used.", "short_docstring": "Return the string obtained by replacing the leftmost\nnon-overlapping occurrences of the pattern in string by the\nreplacement repl.  repl can be either a string or a callable;\nif a string, backslash escapes in it are processed.  If it is\na callable, it's passed the Match object and must return\na replacement string to be used."}
{"name": "pytz.timezone", "type": "callable", "signature": "(zone)", "docstring": "Return a datetime.tzinfo implementation for the given timezone\n\n>>> from datetime import datetime, timedelta\n>>> utc = timezone('UTC')\n>>> eastern = timezone('US/Eastern')\n>>> eastern.zone\n'US/Eastern'\n>>> timezone(unicode('US/Eastern')) is eastern\nTrue\n>>> utc_dt = datetime(2002, 10, 27, 6, 0, 0, tzinfo=utc)\n>>> loc_dt = utc_dt.astimezone(eastern)\n>>> fmt = '%Y-%m-%d %H:%M:%S %Z (%z)'\n>>> loc_dt.strftime(fmt)\n'2002-10-27 01:00:00 EST (-0500)'\n>>> (loc_dt - timedelta(minutes=10)).strftime(fmt)\n'2002-10-27 00:50:00 EST (-0500)'\n>>> eastern.normalize(loc_dt - timedelta(minutes=10)).strftime(fmt)\n'2002-10-27 01:50:00 EDT (-0400)'\n>>> (loc_dt + timedelta(minutes=10)).strftime(fmt)\n'2002-10-27 01:10:00 EST (-0500)'\n\nRaises UnknownTimeZoneError if passed an unknown zone.\n\n>>> try:\n...     timezone('Asia/Shangri-La')\n... except UnknownTimeZoneError:\n...     print('Unknown')\nUnknown\n\n>>> try:\n...     timezone(unicode('\\N{TRADE MARK SIGN}'))\n... except UnknownTimeZoneError:\n...     print('Unknown')\nUnknown", "short_docstring": "Return a datetime.tzinfo implementation for the given timezone"}
{"name": "glob.glob", "type": "callable", "signature": "(pathname, *, root_dir=None, dir_fd=None, recursive=False)", "docstring": "Return a list of paths matching a pathname pattern.\n\nThe pattern may contain simple shell-style wildcards a la\nfnmatch. However, unlike fnmatch, filenames starting with a\ndot are special cases that are not matched by '*' and '?'\npatterns.\n\nIf recursive is true, the pattern '**' will match any files and\nzero or more directories and subdirectories.", "short_docstring": "Return a list of paths matching a pathname pattern."}
{"name": "re.search", "type": "callable", "signature": "(pattern, string, flags=0)", "docstring": "Scan through string looking for a match to the pattern, returning\na Match object, or None if no match was found.", "short_docstring": "Scan through string looking for a match to the pattern, returning\na Match object, or None if no match was found."}
{"name": "sklearn.decomposition.PCA", "type": "class", "signature": "(n_components=None, *, copy=True, whiten=False, svd_solver='auto', tol=0.0, iterated_power='auto', n_oversamples=10, power_iteration_normalizer='auto', random_state=None)", "docstring": "Principal component analysis (PCA).\n\nLinear dimensionality reduction using Singular Value Decomposition of the\ndata to project it to a lower dimensional space. The input data is centered\nbut not scaled for each feature before applying the SVD.\n\nIt uses the LAPACK implementation of the full SVD or a randomized truncated\nSVD by the method of Halko et al. 2009, depending on the shape of the input\ndata and the number of components to extract.\n\nIt can also use the scipy.sparse.linalg ARPACK implementation of the\ntruncated SVD.\n\nNotice that this class does not support sparse input. See\n:class:`TruncatedSVD` for an alternative with sparse data.\n\nFor a usage example, see\n:ref:`sphx_glr_auto_examples_decomposition_plot_pca_iris.py`\n\nRead more in the :ref:`User Guide <PCA>`.\n\nParameters\n----------\nn_components : int, float or 'mle', default=None\n    Number of components to keep.\n    if n_components is not set all components are kept::\n\n        n_components == min(n_samples, n_features)\n\n    If ``n_components == 'mle'`` and ``svd_solver == 'full'``, Minka's\n    MLE is used to guess the dimension. Use of ``n_components == 'mle'``\n    will interpret ``svd_solver == 'auto'`` as ``svd_solver == 'full'``.\n\n    If ``0 < n_components < 1`` and ``svd_solver == 'full'``, select the\n    number of components such that the amount of variance that needs to be\n    explained is greater than the percentage specified by n_components.\n\n    If ``svd_solver == 'arpack'``, the number of components must be\n    strictly less than the minimum of n_features and n_samples.\n\n    Hence, the None case results in::\n\n        n_components == min(n_samples, n_features) - 1\n\ncopy : bool, default=True\n    If False, data passed to fit are overwritten and running\n    fit(X).transform(X) will not yield the expected results,\n    use fit_transform(X) instead.\n\nwhiten : bool, default=False\n    When True (False by default) the `components_` vectors are multiplied\n    by the square root of n_samples and then divided by the singular values\n    to ensure uncorrelated outputs with unit component-wise variances.\n\n    Whitening will remove some information from the transformed signal\n    (the relative variance scales of the components) but can sometime\n    improve the predictive accuracy of the downstream estimators by\n    making their data respect some hard-wired assumptions.\n\nsvd_solver : {'auto', 'full', 'arpack', 'randomized'}, default='auto'\n    If auto :\n        The solver is selected by a default policy based on `X.shape` and\n        `n_components`: if the input data is larger than 500x500 and the\n        number of components to extract is lower than 80% of the smallest\n        dimension of the data, then the more efficient 'randomized'\n        method is enabled. Otherwise the exact full SVD is computed and\n        optionally truncated afterwards.\n    If full :\n        run exact full SVD calling the standard LAPACK solver via\n        `scipy.linalg.svd` and select the components by postprocessing\n    If arpack :\n        run SVD truncated to n_components calling ARPACK solver via\n        `scipy.sparse.linalg.svds`. It requires strictly\n        0 < n_components < min(X.shape)\n    If randomized :\n        run randomized SVD by the method of Halko et al.\n\n    .. versionadded:: 0.18.0\n\ntol : float, default=0.0\n    Tolerance for singular values computed by svd_solver == 'arpack'.\n    Must be of range [0.0, infinity).\n\n    .. versionadded:: 0.18.0\n\niterated_power : int or 'auto', default='auto'\n    Number of iterations for the power method computed by\n    svd_solver == 'randomized'.\n    Must be of range [0, infinity).\n\n    .. versionadded:: 0.18.0\n\nn_oversamples : int, default=10\n    This parameter is only relevant when `svd_solver=\"randomized\"`.\n    It corresponds to the additional number of random vectors to sample the\n    range of `X` so as to ensure proper conditioning. See\n    :func:`~sklearn.utils.extmath.randomized_svd` for more details.\n\n    .. versionadded:: 1.1\n\npower_iteration_normalizer : {'auto', 'QR', 'LU', 'none'}, default='auto'\n    Power iteration normalizer for randomized SVD solver.\n    Not used by ARPACK. See :func:`~sklearn.utils.extmath.randomized_svd`\n    for more details.\n\n    .. versionadded:: 1.1\n\nrandom_state : int, RandomState instance or None, default=None\n    Used when the 'arpack' or 'randomized' solvers are used. Pass an int\n    for reproducible results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\n    .. versionadded:: 0.18.0\n\nAttributes\n----------\ncomponents_ : ndarray of shape (n_components, n_features)\n    Principal axes in feature space, representing the directions of\n    maximum variance in the data. Equivalently, the right singular\n    vectors of the centered input data, parallel to its eigenvectors.\n    The components are sorted by decreasing ``explained_variance_``.\n\nexplained_variance_ : ndarray of shape (n_components,)\n    The amount of variance explained by each of the selected components.\n    The variance estimation uses `n_samples - 1` degrees of freedom.\n\n    Equal to n_components largest eigenvalues\n    of the covariance matrix of X.\n\n    .. versionadded:: 0.18\n\nexplained_variance_ratio_ : ndarray of shape (n_components,)\n    Percentage of variance explained by each of the selected components.\n\n    If ``n_components`` is not set then all components are stored and the\n    sum of the ratios is equal to 1.0.\n\nsingular_values_ : ndarray of shape (n_components,)\n    The singular values corresponding to each of the selected components.\n    The singular values are equal to the 2-norms of the ``n_components``\n    variables in the lower-dimensional space.\n\n    .. versionadded:: 0.19\n\nmean_ : ndarray of shape (n_features,)\n    Per-feature empirical mean, estimated from the training set.\n\n    Equal to `X.mean(axis=0)`.\n\nn_components_ : int\n    The estimated number of components. When n_components is set\n    to 'mle' or a number between 0 and 1 (with svd_solver == 'full') this\n    number is estimated from input data. Otherwise it equals the parameter\n    n_components, or the lesser value of n_features and n_samples\n    if n_components is None.\n\nn_samples_ : int\n    Number of samples in the training data.\n\nnoise_variance_ : float\n    The estimated noise covariance following the Probabilistic PCA model\n    from Tipping and Bishop 1999. See \"Pattern Recognition and\n    Machine Learning\" by C. Bishop, 12.2.1 p. 574 or\n    http://www.miketipping.com/papers/met-mppca.pdf. It is required to\n    compute the estimated data covariance and score samples.\n\n    Equal to the average of (min(n_features, n_samples) - n_components)\n    smallest eigenvalues of the covariance matrix of X.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nKernelPCA : Kernel Principal Component Analysis.\nSparsePCA : Sparse Principal Component Analysis.\nTruncatedSVD : Dimensionality reduction using truncated SVD.\nIncrementalPCA : Incremental Principal Component Analysis.\n\nReferences\n----------\nFor n_components == 'mle', this class uses the method from:\n`Minka, T. P.. \"Automatic choice of dimensionality for PCA\".\nIn NIPS, pp. 598-604 <https://tminka.github.io/papers/pca/minka-pca.pdf>`_\n\nImplements the probabilistic PCA model from:\n`Tipping, M. E., and Bishop, C. M. (1999). \"Probabilistic principal\ncomponent analysis\". Journal of the Royal Statistical Society:\nSeries B (Statistical Methodology), 61(3), 611-622.\n<http://www.miketipping.com/papers/met-mppca.pdf>`_\nvia the score and score_samples methods.\n\nFor svd_solver == 'arpack', refer to `scipy.sparse.linalg.svds`.\n\nFor svd_solver == 'randomized', see:\n:doi:`Halko, N., Martinsson, P. G., and Tropp, J. A. (2011).\n\"Finding structure with randomness: Probabilistic algorithms for\nconstructing approximate matrix decompositions\".\nSIAM review, 53(2), 217-288.\n<10.1137/090771806>`\nand also\n:doi:`Martinsson, P. G., Rokhlin, V., and Tygert, M. (2011).\n\"A randomized algorithm for the decomposition of matrices\".\nApplied and Computational Harmonic Analysis, 30(1), 47-68.\n<10.1016/j.acha.2010.02.003>`\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.decomposition import PCA\n>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n>>> pca = PCA(n_components=2)\n>>> pca.fit(X)\nPCA(n_components=2)\n>>> print(pca.explained_variance_ratio_)\n[0.9924... 0.0075...]\n>>> print(pca.singular_values_)\n[6.30061... 0.54980...]\n\n>>> pca = PCA(n_components=2, svd_solver='full')\n>>> pca.fit(X)\nPCA(n_components=2, svd_solver='full')\n>>> print(pca.explained_variance_ratio_)\n[0.9924... 0.00755...]\n>>> print(pca.singular_values_)\n[6.30061... 0.54980...]\n\n>>> pca = PCA(n_components=1, svd_solver='arpack')\n>>> pca.fit(X)\nPCA(n_components=1, svd_solver='arpack')\n>>> print(pca.explained_variance_ratio_)\n[0.99244...]\n>>> print(pca.singular_values_)\n[6.30061...]", "short_docstring": "Principal component analysis (PCA)."}
{"name": "numpy.mean", "type": "callable", "signature": "(a, axis=None, dtype=None, out=None, keepdims=<no value>, *, where=<no value>)", "docstring": "Compute the arithmetic mean along the specified axis.\n\nReturns the average of the array elements.  The average is taken over\nthe flattened array by default, otherwise over the specified axis.\n`float64` intermediate and return values are used for integer inputs.\n\nParameters\n----------\na : array_like\n    Array containing numbers whose mean is desired. If `a` is not an\n    array, a conversion is attempted.\naxis : None or int or tuple of ints, optional\n    Axis or axes along which the means are computed. The default is to\n    compute the mean of the flattened array.\n\n    .. versionadded:: 1.7.0\n\n    If this is a tuple of ints, a mean is performed over multiple axes,\n    instead of a single axis or all the axes as before.\ndtype : data-type, optional\n    Type to use in computing the mean.  For integer inputs, the default\n    is `float64`; for floating point inputs, it is the same as the\n    input dtype.\nout : ndarray, optional\n    Alternate output array in which to place the result.  The default\n    is ``None``; if provided, it must have the same shape as the\n    expected output, but the type will be cast if necessary.\n    See :ref:`ufuncs-output-type` for more details.\n\nkeepdims : bool, optional\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\n    If the default value is passed, then `keepdims` will not be\n    passed through to the `mean` method of sub-classes of\n    `ndarray`, however any non-default value will be.  If the\n    sub-class' method does not implement `keepdims` any\n    exceptions will be raised.\n\nwhere : array_like of bool, optional\n    Elements to include in the mean. See `~numpy.ufunc.reduce` for details.\n\n    .. versionadded:: 1.20.0\n\nReturns\n-------\nm : ndarray, see dtype parameter above\n    If `out=None`, returns a new array containing the mean values,\n    otherwise a reference to the output array is returned.\n\nSee Also\n--------\naverage : Weighted average\nstd, var, nanmean, nanstd, nanvar\n\nNotes\n-----\nThe arithmetic mean is the sum of the elements along the axis divided\nby the number of elements.\n\nNote that for floating-point input, the mean is computed using the\nsame precision the input has.  Depending on the input data, this can\ncause the results to be inaccurate, especially for `float32` (see\nexample below).  Specifying a higher-precision accumulator using the\n`dtype` keyword can alleviate this issue.\n\nBy default, `float16` results are computed using `float32` intermediates\nfor extra precision.\n\nExamples\n--------\n>>> a = np.array([[1, 2], [3, 4]])\n>>> np.mean(a)\n2.5\n>>> np.mean(a, axis=0)\narray([2., 3.])\n>>> np.mean(a, axis=1)\narray([1.5, 3.5])\n\nIn single precision, `mean` can be inaccurate:\n\n>>> a = np.zeros((2, 512*512), dtype=np.float32)\n>>> a[0, :] = 1.0\n>>> a[1, :] = 0.1\n>>> np.mean(a)\n0.54999924\n\nComputing the mean in float64 is more accurate:\n\n>>> np.mean(a, dtype=np.float64)\n0.55000000074505806 # may vary\n\nSpecifying a where argument:\n\n>>> a = np.array([[5, 9, 13], [14, 10, 12], [11, 15, 19]])\n>>> np.mean(a)\n12.0\n>>> np.mean(a, where=[[True], [False], [False]])\n9.0", "short_docstring": "Compute the arithmetic mean along the specified axis."}
{"name": "codecs.encode", "type": "callable", "signature": "(obj, encoding='utf-8', errors='strict')", "docstring": "Encodes obj using the codec registered for encoding.\n\nThe default encoding is 'utf-8'.  errors may be given to set a\ndifferent error handling scheme.  Default is 'strict' meaning that encoding\nerrors raise a ValueError.  Other possible values are 'ignore', 'replace'\nand 'backslashreplace' as well as any other name registered with\ncodecs.register_error that can handle ValueErrors.", "short_docstring": "Encodes obj using the codec registered for encoding."}
{"name": "lxml.etree.XMLSyntaxError", "type": "class", "signature": "(message, code, line, column, filename=None)", "docstring": "Syntax error while parsing an XML document.\n    ", "short_docstring": "Syntax error while parsing an XML document.\n    "}
{"name": "os.path.basename", "type": "callable", "signature": "(p)", "docstring": "Returns the final component of a pathname", "short_docstring": "Returns the final component of a pathname"}
{"name": "random.seed", "type": "callable", "signature": "(a=None, version=2)", "docstring": "Initialize internal state from a seed.\n\nThe only supported seed types are None, int, float,\nstr, bytes, and bytearray.\n\nNone or no argument seeds from current time or from an operating\nsystem specific randomness source if available.\n\nIf *a* is an int, all bits are used.\n\nFor version 2 (the default), all of the bits are used if *a* is a str,\nbytes, or bytearray.  For version 1 (provided for reproducing random\nsequences from older versions of Python), the algorithm for str and\nbytes generates a narrower range of seeds.", "short_docstring": "Initialize internal state from a seed."}
{"name": "scipy.stats.norm.pdf", "type": "callable", "signature": "(x, *args, **kwds)", "docstring": "Probability density function at x of the given RV.\n\nParameters\n----------\nx : array_like\n    quantiles\narg1, arg2, arg3,... : array_like\n    The shape parameter(s) for the distribution (see docstring of the\n    instance object for more information)\nloc : array_like, optional\n    location parameter (default=0)\nscale : array_like, optional\n    scale parameter (default=1)\n\nReturns\n-------\npdf : ndarray\n    Probability density function evaluated at x", "short_docstring": "Probability density function at x of the given RV."}
{"name": "os.getcwd", "type": "callable", "signature": "()", "docstring": "Return a unicode string representing the current working directory.", "short_docstring": "Return a unicode string representing the current working directory."}
{"name": "threading.Thread", "type": "class", "signature": "(group=None, target=None, name=None, args=(), kwargs=None, *, daemon=None)", "docstring": "A class that represents a thread of control.\n\nThis class can be safely subclassed in a limited fashion. There are two ways\nto specify the activity: by passing a callable object to the constructor, or\nby overriding the run() method in a subclass.", "short_docstring": "A class that represents a thread of control."}
{"name": "numpy.random.seed", "type": "callable", "signature": null, "docstring": "seed(self, seed=None)\n\nReseed a legacy MT19937 BitGenerator\n\nNotes\n-----\nThis is a convenience, legacy function.\n\nThe best practice is to **not** reseed a BitGenerator, rather to\nrecreate a new one. This method is here for legacy reasons.\nThis example demonstrates best practice.\n\n>>> from numpy.random import MT19937\n>>> from numpy.random import RandomState, SeedSequence\n>>> rs = RandomState(MT19937(SeedSequence(123456789)))\n# Later, you want to restart the stream\n>>> rs = RandomState(MT19937(SeedSequence(987654321)))", "short_docstring": "seed(self, seed=None)"}
{"name": "typing.List", "type": "callable", "signature": "(*args, **kwargs)", "docstring": "A generic version of list.", "short_docstring": "A generic version of list."}
{"name": "flask.Flask", "type": "class", "signature": "(import_name: 'str', static_url_path: 'str | None' = None, static_folder: 'str | os.PathLike[str] | None' = 'static', static_host: 'str | None' = None, host_matching: 'bool' = False, subdomain_matching: 'bool' = False, template_folder: 'str | os.PathLike[str] | None' = 'templates', instance_path: 'str | None' = None, instance_relative_config: 'bool' = False, root_path: 'str | None' = None)", "docstring": "The flask object implements a WSGI application and acts as the central\nobject.  It is passed the name of the module or package of the\napplication.  Once it is created it will act as a central registry for\nthe view functions, the URL rules, template configuration and much more.\n\nThe name of the package is used to resolve resources from inside the\npackage or the folder the module is contained in depending on if the\npackage parameter resolves to an actual python package (a folder with\nan :file:`__init__.py` file inside) or a standard module (just a ``.py`` file).\n\nFor more information about resource loading, see :func:`open_resource`.\n\nUsually you create a :class:`Flask` instance in your main module or\nin the :file:`__init__.py` file of your package like this::\n\n    from flask import Flask\n    app = Flask(__name__)\n\n.. admonition:: About the First Parameter\n\n    The idea of the first parameter is to give Flask an idea of what\n    belongs to your application.  This name is used to find resources\n    on the filesystem, can be used by extensions to improve debugging\n    information and a lot more.\n\n    So it's important what you provide there.  If you are using a single\n    module, `__name__` is always the correct value.  If you however are\n    using a package, it's usually recommended to hardcode the name of\n    your package there.\n\n    For example if your application is defined in :file:`yourapplication/app.py`\n    you should create it with one of the two versions below::\n\n        app = Flask('yourapplication')\n        app = Flask(__name__.split('.')[0])\n\n    Why is that?  The application will work even with `__name__`, thanks\n    to how resources are looked up.  However it will make debugging more\n    painful.  Certain extensions can make assumptions based on the\n    import name of your application.  For example the Flask-SQLAlchemy\n    extension will look for the code in your application that triggered\n    an SQL query in debug mode.  If the import name is not properly set\n    up, that debugging information is lost.  (For example it would only\n    pick up SQL queries in `yourapplication.app` and not\n    `yourapplication.views.frontend`)\n\n.. versionadded:: 0.7\n   The `static_url_path`, `static_folder`, and `template_folder`\n   parameters were added.\n\n.. versionadded:: 0.8\n   The `instance_path` and `instance_relative_config` parameters were\n   added.\n\n.. versionadded:: 0.11\n   The `root_path` parameter was added.\n\n.. versionadded:: 1.0\n   The ``host_matching`` and ``static_host`` parameters were added.\n\n.. versionadded:: 1.0\n   The ``subdomain_matching`` parameter was added. Subdomain\n   matching needs to be enabled manually now. Setting\n   :data:`SERVER_NAME` does not implicitly enable it.\n\n:param import_name: the name of the application package\n:param static_url_path: can be used to specify a different path for the\n                        static files on the web.  Defaults to the name\n                        of the `static_folder` folder.\n:param static_folder: The folder with static files that is served at\n    ``static_url_path``. Relative to the application ``root_path``\n    or an absolute path. Defaults to ``'static'``.\n:param static_host: the host to use when adding the static route.\n    Defaults to None. Required when using ``host_matching=True``\n    with a ``static_folder`` configured.\n:param host_matching: set ``url_map.host_matching`` attribute.\n    Defaults to False.\n:param subdomain_matching: consider the subdomain relative to\n    :data:`SERVER_NAME` when matching routes. Defaults to False.\n:param template_folder: the folder that contains the templates that should\n                        be used by the application.  Defaults to\n                        ``'templates'`` folder in the root path of the\n                        application.\n:param instance_path: An alternative instance path for the application.\n                      By default the folder ``'instance'`` next to the\n                      package or module is assumed to be the instance\n                      path.\n:param instance_relative_config: if set to ``True`` relative filenames\n                                 for loading the config are assumed to\n                                 be relative to the instance path instead\n                                 of the application root.\n:param root_path: The path to the root of the application files.\n    This should only be set manually when it can't be detected\n    automatically, such as for namespace packages.", "short_docstring": "The flask object implements a WSGI application and acts as the central\nobject.  It is passed the name of the module or package of the\napplication.  Once it is created it will act as a central registry for\nthe view functions, the URL rules, template configuration and much more."}
{"name": "subprocess.CalledProcessError", "type": "class", "signature": "(returncode, cmd, output=None, stderr=None)", "docstring": "Raised when run() is called with check=True and the process\nreturns a non-zero exit status.\n\nAttributes:\n  cmd, returncode, stdout, stderr, output", "short_docstring": "Raised when run() is called with check=True and the process\nreturns a non-zero exit status."}
{"name": "os.path.join", "type": "callable", "signature": "(a, *p)", "docstring": "Join two or more pathname components, inserting '/' as needed.\nIf any component is an absolute path, all previous path components\nwill be discarded.  An empty last part will result in a path that\nends with a separator.", "short_docstring": "Join two or more pathname components, inserting '/' as needed.\nIf any component is an absolute path, all previous path components\nwill be discarded.  An empty last part will result in a path that\nends with a separator."}
{"name": "numpy.median", "type": "callable", "signature": "(a, axis=None, out=None, overwrite_input=False, keepdims=False)", "docstring": "Compute the median along the specified axis.\n\nReturns the median of the array elements.\n\nParameters\n----------\na : array_like\n    Input array or object that can be converted to an array.\naxis : {int, sequence of int, None}, optional\n    Axis or axes along which the medians are computed. The default\n    is to compute the median along a flattened version of the array.\n    A sequence of axes is supported since version 1.9.0.\nout : ndarray, optional\n    Alternative output array in which to place the result. It must\n    have the same shape and buffer length as the expected output,\n    but the type (of the output) will be cast if necessary.\noverwrite_input : bool, optional\n   If True, then allow use of memory of input array `a` for\n   calculations. The input array will be modified by the call to\n   `median`. This will save memory when you do not need to preserve\n   the contents of the input array. Treat the input as undefined,\n   but it will probably be fully or partially sorted. Default is\n   False. If `overwrite_input` is ``True`` and `a` is not already an\n   `ndarray`, an error will be raised.\nkeepdims : bool, optional\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the original `arr`.\n\n    .. versionadded:: 1.9.0\n\nReturns\n-------\nmedian : ndarray\n    A new array holding the result. If the input contains integers\n    or floats smaller than ``float64``, then the output data-type is\n    ``np.float64``.  Otherwise, the data-type of the output is the\n    same as that of the input. If `out` is specified, that array is\n    returned instead.\n\nSee Also\n--------\nmean, percentile\n\nNotes\n-----\nGiven a vector ``V`` of length ``N``, the median of ``V`` is the\nmiddle value of a sorted copy of ``V``, ``V_sorted`` - i\ne., ``V_sorted[(N-1)/2]``, when ``N`` is odd, and the average of the\ntwo middle values of ``V_sorted`` when ``N`` is even.\n\nExamples\n--------\n>>> a = np.array([[10, 7, 4], [3, 2, 1]])\n>>> a\narray([[10,  7,  4],\n       [ 3,  2,  1]])\n>>> np.median(a)\n3.5\n>>> np.median(a, axis=0)\narray([6.5, 4.5, 2.5])\n>>> np.median(a, axis=1)\narray([7.,  2.])\n>>> m = np.median(a, axis=0)\n>>> out = np.zeros_like(m)\n>>> np.median(a, axis=0, out=m)\narray([6.5,  4.5,  2.5])\n>>> m\narray([6.5,  4.5,  2.5])\n>>> b = a.copy()\n>>> np.median(b, axis=1, overwrite_input=True)\narray([7.,  2.])\n>>> assert not np.all(a==b)\n>>> b = a.copy()\n>>> np.median(b, axis=None, overwrite_input=True)\n3.5\n>>> assert not np.all(a==b)", "short_docstring": "Compute the median along the specified axis."}
{"name": "sklearn.model_selection.train_test_split", "type": "callable", "signature": "(*arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)", "docstring": "Split arrays or matrices into random train and test subsets.\n\nQuick utility that wraps input validation,\n``next(ShuffleSplit().split(X, y))``, and application to input data\ninto a single call for splitting (and optionally subsampling) data into a\none-liner.\n\nRead more in the :ref:`User Guide <cross_validation>`.\n\nParameters\n----------\n*arrays : sequence of indexables with same length / shape[0]\n    Allowed inputs are lists, numpy arrays, scipy-sparse\n    matrices or pandas dataframes.\n\ntest_size : float or int, default=None\n    If float, should be between 0.0 and 1.0 and represent the proportion\n    of the dataset to include in the test split. If int, represents the\n    absolute number of test samples. If None, the value is set to the\n    complement of the train size. If ``train_size`` is also None, it will\n    be set to 0.25.\n\ntrain_size : float or int, default=None\n    If float, should be between 0.0 and 1.0 and represent the\n    proportion of the dataset to include in the train split. If\n    int, represents the absolute number of train samples. If None,\n    the value is automatically set to the complement of the test size.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the shuffling applied to the data before applying the split.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nshuffle : bool, default=True\n    Whether or not to shuffle the data before splitting. If shuffle=False\n    then stratify must be None.\n\nstratify : array-like, default=None\n    If not None, data is split in a stratified fashion, using this as\n    the class labels.\n    Read more in the :ref:`User Guide <stratification>`.\n\nReturns\n-------\nsplitting : list, length=2 * len(arrays)\n    List containing train-test split of inputs.\n\n    .. versionadded:: 0.16\n        If the input is sparse, the output will be a\n        ``scipy.sparse.csr_matrix``. Else, output type is the same as the\n        input type.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.model_selection import train_test_split\n>>> X, y = np.arange(10).reshape((5, 2)), range(5)\n>>> X\narray([[0, 1],\n       [2, 3],\n       [4, 5],\n       [6, 7],\n       [8, 9]])\n>>> list(y)\n[0, 1, 2, 3, 4]\n\n>>> X_train, X_test, y_train, y_test = train_test_split(\n...     X, y, test_size=0.33, random_state=42)\n...\n>>> X_train\narray([[4, 5],\n       [0, 1],\n       [6, 7]])\n>>> y_train\n[2, 0, 3]\n>>> X_test\narray([[2, 3],\n       [8, 9]])\n>>> y_test\n[1, 4]\n\n>>> train_test_split(y, shuffle=False)\n[[0, 1, 2], [3, 4]]", "short_docstring": "Split arrays or matrices into random train and test subsets."}
{"name": "functools.reduce", "type": "callable", "signature": null, "docstring": "reduce(function, iterable[, initial]) -> value\n\nApply a function of two arguments cumulatively to the items of a sequence\nor iterable, from left to right, so as to reduce the iterable to a single\nvalue.  For example, reduce(lambda x, y: x+y, [1, 2, 3, 4, 5]) calculates\n((((1+2)+3)+4)+5).  If initial is present, it is placed before the items\nof the iterable in the calculation, and serves as a default when the\niterable is empty.", "short_docstring": "reduce(function, iterable[, initial]) -> value"}
{"name": "string.ascii_lowercase", "type": "constant", "value": "abcdefghijklmnopqrstuvwxyz", "signature": null, "docstring": "str(object='') -> str\nstr(bytes_or_buffer[, encoding[, errors]]) -> str\n\nCreate a new string object from the given object. If encoding or\nerrors is specified, then the object must expose a data buffer\nthat will be decoded using the given encoding and error handler.\nOtherwise, returns the result of object.__str__() (if defined)\nor repr(object).\nencoding defaults to sys.getdefaultencoding().\nerrors defaults to 'strict'.", "short_docstring": "str(object='') -> str\nstr(bytes_or_buffer[, encoding[, errors]]) -> str"}
{"name": "seaborn.heatmap", "type": "callable", "signature": "(data, *, vmin=None, vmax=None, cmap=None, center=None, robust=False, annot=None, fmt='.2g', annot_kws=None, linewidths=0, linecolor='white', cbar=True, cbar_kws=None, cbar_ax=None, square=False, xticklabels='auto', yticklabels='auto', mask=None, ax=None, **kwargs)", "docstring": "Plot rectangular data as a color-encoded matrix.\n\nThis is an Axes-level function and will draw the heatmap into the\ncurrently-active Axes if none is provided to the ``ax`` argument.  Part of\nthis Axes space will be taken and used to plot a colormap, unless ``cbar``\nis False or a separate Axes is provided to ``cbar_ax``.\n\nParameters\n----------\ndata : rectangular dataset\n    2D dataset that can be coerced into an ndarray. If a Pandas DataFrame\n    is provided, the index/column information will be used to label the\n    columns and rows.\nvmin, vmax : floats, optional\n    Values to anchor the colormap, otherwise they are inferred from the\n    data and other keyword arguments.\ncmap : matplotlib colormap name or object, or list of colors, optional\n    The mapping from data values to color space. If not provided, the\n    default will depend on whether ``center`` is set.\ncenter : float, optional\n    The value at which to center the colormap when plotting divergent data.\n    Using this parameter will change the default ``cmap`` if none is\n    specified.\nrobust : bool, optional\n    If True and ``vmin`` or ``vmax`` are absent, the colormap range is\n    computed with robust quantiles instead of the extreme values.\nannot : bool or rectangular dataset, optional\n    If True, write the data value in each cell. If an array-like with the\n    same shape as ``data``, then use this to annotate the heatmap instead\n    of the data. Note that DataFrames will match on position, not index.\nfmt : str, optional\n    String formatting code to use when adding annotations.\nannot_kws : dict of key, value mappings, optional\n    Keyword arguments for :meth:`matplotlib.axes.Axes.text` when ``annot``\n    is True.\nlinewidths : float, optional\n    Width of the lines that will divide each cell.\nlinecolor : color, optional\n    Color of the lines that will divide each cell.\ncbar : bool, optional\n    Whether to draw a colorbar.\ncbar_kws : dict of key, value mappings, optional\n    Keyword arguments for :meth:`matplotlib.figure.Figure.colorbar`.\ncbar_ax : matplotlib Axes, optional\n    Axes in which to draw the colorbar, otherwise take space from the\n    main Axes.\nsquare : bool, optional\n    If True, set the Axes aspect to \"equal\" so each cell will be\n    square-shaped.\nxticklabels, yticklabels : \"auto\", bool, list-like, or int, optional\n    If True, plot the column names of the dataframe. If False, don't plot\n    the column names. If list-like, plot these alternate labels as the\n    xticklabels. If an integer, use the column names but plot only every\n    n label. If \"auto\", try to densely plot non-overlapping labels.\nmask : bool array or DataFrame, optional\n    If passed, data will not be shown in cells where ``mask`` is True.\n    Cells with missing values are automatically masked.\nax : matplotlib Axes, optional\n    Axes in which to draw the plot, otherwise use the currently-active\n    Axes.\nkwargs : other keyword arguments\n    All other keyword arguments are passed to\n    :meth:`matplotlib.axes.Axes.pcolormesh`.\n\nReturns\n-------\nax : matplotlib Axes\n    Axes object with the heatmap.\n\nSee Also\n--------\nclustermap : Plot a matrix using hierarchical clustering to arrange the\n             rows and columns.\n\nExamples\n--------\n\n.. include:: ../docstrings/heatmap.rst", "short_docstring": "Plot rectangular data as a color-encoded matrix."}
{"name": "matplotlib.pyplot.axis", "type": "callable", "signature": "(arg: 'tuple[float, float, float, float] | bool | str | None' = None, /, *, emit: 'bool' = True, **kwargs) -> 'tuple[float, float, float, float]'", "docstring": "Convenience method to get or set some axis properties.\n\nCall signatures::\n\n  xmin, xmax, ymin, ymax = axis()\n  xmin, xmax, ymin, ymax = axis([xmin, xmax, ymin, ymax])\n  xmin, xmax, ymin, ymax = axis(option)\n  xmin, xmax, ymin, ymax = axis(**kwargs)\n\nParameters\n----------\nxmin, xmax, ymin, ymax : float, optional\n    The axis limits to be set.  This can also be achieved using ::\n\n        ax.set(xlim=(xmin, xmax), ylim=(ymin, ymax))\n\noption : bool or str\n    If a bool, turns axis lines and labels on or off. If a string,\n    possible values are:\n\n    ================ ===========================================================\n    Value            Description\n    ================ ===========================================================\n    'off' or `False` Hide all axis decorations, i.e. axis labels, spines,\n                     tick marks, tick labels, and grid lines.\n                     This is the same as `~.Axes.set_axis_off()`.\n    'on' or `True`   Do not hide all axis decorations, i.e. axis labels, spines,\n                     tick marks, tick labels, and grid lines.\n                     This is the same as `~.Axes.set_axis_on()`.\n    'equal'          Set equal scaling (i.e., make circles circular) by\n                     changing the axis limits. This is the same as\n                     ``ax.set_aspect('equal', adjustable='datalim')``.\n                     Explicit data limits may not be respected in this case.\n    'scaled'         Set equal scaling (i.e., make circles circular) by\n                     changing dimensions of the plot box. This is the same as\n                     ``ax.set_aspect('equal', adjustable='box', anchor='C')``.\n                     Additionally, further autoscaling will be disabled.\n    'tight'          Set limits just large enough to show all data, then\n                     disable further autoscaling.\n    'auto'           Automatic scaling (fill plot box with data).\n    'image'          'scaled' with axis limits equal to data limits.\n    'square'         Square plot; similar to 'scaled', but initially forcing\n                     ``xmax-xmin == ymax-ymin``.\n    ================ ===========================================================\n\nemit : bool, default: True\n    Whether observers are notified of the axis limit change.\n    This option is passed on to `~.Axes.set_xlim` and\n    `~.Axes.set_ylim`.\n\nReturns\n-------\nxmin, xmax, ymin, ymax : float\n    The axis limits.\n\nSee Also\n--------\nmatplotlib.axes.Axes.set_xlim\nmatplotlib.axes.Axes.set_ylim\n\nNotes\n-----\nFor 3D axes, this method additionally takes *zmin*, *zmax* as\nparameters and likewise returns them.", "short_docstring": "Convenience method to get or set some axis properties."}
{"name": "os.getenv", "type": "callable", "signature": "(key, default=None)", "docstring": "Get an environment variable, return None if it doesn't exist.\nThe optional second argument can specify an alternate default.\nkey, default and the result are str.", "short_docstring": "Get an environment variable, return None if it doesn't exist.\nThe optional second argument can specify an alternate default.\nkey, default and the result are str."}
{"name": "hashlib.sha256", "type": "callable", "signature": "(string=b'', *, usedforsecurity=True)", "docstring": "Returns a sha256 hash object; optionally initialized with a string", "short_docstring": "Returns a sha256 hash object; optionally initialized with a string"}
{"name": "docx.Document", "type": "callable", "signature": "(docx: 'str | IO[bytes] | None' = None)", "docstring": "Return a |Document| object loaded from `docx`, where `docx` can be either a path\nto a ``.docx`` file (a string) or a file-like object.\n\nIf `docx` is missing or ``None``, the built-in default document \"template\" is\nloaded.", "short_docstring": "Return a |Document| object loaded from `docx`, where `docx` can be either a path\nto a ``.docx`` file (a string) or a file-like object."}
{"name": "scipy.spatial.voronoi_plot_2d", "type": "callable", "signature": "(vor, ax=None, **kw)", "docstring": "Plot the given Voronoi diagram in 2-D\n\nParameters\n----------\nvor : scipy.spatial.Voronoi instance\n    Diagram to plot\nax : matplotlib.axes.Axes instance, optional\n    Axes to plot on\nshow_points : bool, optional\n    Add the Voronoi points to the plot.\nshow_vertices : bool, optional\n    Add the Voronoi vertices to the plot.\nline_colors : string, optional\n    Specifies the line color for polygon boundaries\nline_width : float, optional\n    Specifies the line width for polygon boundaries\nline_alpha : float, optional\n    Specifies the line alpha for polygon boundaries\npoint_size : float, optional\n    Specifies the size of points\n\nReturns\n-------\nfig : matplotlib.figure.Figure instance\n    Figure for the plot\n\nSee Also\n--------\nVoronoi\n\nNotes\n-----\nRequires Matplotlib.\n\nExamples\n--------\n>>> import numpy as np\n>>> import matplotlib.pyplot as plt\n>>> from scipy.spatial import Voronoi, voronoi_plot_2d\n\nCreate a set of points for the example:\n\n>>> rng = np.random.default_rng()\n>>> points = rng.random((10,2))\n\nGenerate the Voronoi diagram for the points:\n\n>>> vor = Voronoi(points)\n\nUse `voronoi_plot_2d` to plot the diagram:\n\n>>> fig = voronoi_plot_2d(vor)\n\nUse `voronoi_plot_2d` to plot the diagram again, with some settings\ncustomized:\n\n>>> fig = voronoi_plot_2d(vor, show_vertices=False, line_colors='orange',\n...                       line_width=2, line_alpha=0.6, point_size=2)\n>>> plt.show()", "short_docstring": "Plot the given Voronoi diagram in 2-D"}
{"name": "numpy.min", "type": "callable", "signature": "(a, axis=None, out=None, keepdims=<no value>, initial=<no value>, where=<no value>)", "docstring": "Return the minimum of an array or minimum along an axis.\n\nParameters\n----------\na : array_like\n    Input data.\naxis : None or int or tuple of ints, optional\n    Axis or axes along which to operate.  By default, flattened input is\n    used.\n\n    .. versionadded:: 1.7.0\n\n    If this is a tuple of ints, the minimum is selected over multiple axes,\n    instead of a single axis or all the axes as before.\nout : ndarray, optional\n    Alternative output array in which to place the result.  Must\n    be of the same shape and buffer length as the expected output.\n    See :ref:`ufuncs-output-type` for more details.\n\nkeepdims : bool, optional\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\n    If the default value is passed, then `keepdims` will not be\n    passed through to the `amin` method of sub-classes of\n    `ndarray`, however any non-default value will be.  If the\n    sub-class' method does not implement `keepdims` any\n    exceptions will be raised.\n\ninitial : scalar, optional\n    The maximum value of an output element. Must be present to allow\n    computation on empty slice. See `~numpy.ufunc.reduce` for details.\n\n    .. versionadded:: 1.15.0\n\nwhere : array_like of bool, optional\n    Elements to compare for the minimum. See `~numpy.ufunc.reduce`\n    for details.\n\n    .. versionadded:: 1.17.0\n\nReturns\n-------\namin : ndarray or scalar\n    Minimum of `a`. If `axis` is None, the result is a scalar value.\n    If `axis` is given, the result is an array of dimension\n    ``a.ndim - 1``.\n\nSee Also\n--------\namax :\n    The maximum value of an array along a given axis, propagating any NaNs.\nnanmin :\n    The minimum value of an array along a given axis, ignoring any NaNs.\nminimum :\n    Element-wise minimum of two arrays, propagating any NaNs.\nfmin :\n    Element-wise minimum of two arrays, ignoring any NaNs.\nargmin :\n    Return the indices of the minimum values.\n\nnanmax, maximum, fmax\n\nNotes\n-----\nNaN values are propagated, that is if at least one item is NaN, the\ncorresponding min value will be NaN as well. To ignore NaN values\n(MATLAB behavior), please use nanmin.\n\nDon't use `amin` for element-wise comparison of 2 arrays; when\n``a.shape[0]`` is 2, ``minimum(a[0], a[1])`` is faster than\n``amin(a, axis=0)``.\n\nExamples\n--------\n>>> a = np.arange(4).reshape((2,2))\n>>> a\narray([[0, 1],\n       [2, 3]])\n>>> np.amin(a)           # Minimum of the flattened array\n0\n>>> np.amin(a, axis=0)   # Minima along the first axis\narray([0, 1])\n>>> np.amin(a, axis=1)   # Minima along the second axis\narray([0, 2])\n>>> np.amin(a, where=[False, True], initial=10, axis=0)\narray([10,  1])\n\n>>> b = np.arange(5, dtype=float)\n>>> b[2] = np.NaN\n>>> np.amin(b)\nnan\n>>> np.amin(b, where=~np.isnan(b), initial=10)\n0.0\n>>> np.nanmin(b)\n0.0\n\n>>> np.amin([[-50], [10]], axis=-1, initial=0)\narray([-50,   0])\n\nNotice that the initial value is used as one of the elements for which the\nminimum is determined, unlike for the default argument Python's max\nfunction, which is only used for empty iterables.\n\nNotice that this isn't the same as Python's ``default`` argument.\n\n>>> np.amin([6], initial=5)\n5\n>>> min([6], default=5)\n6", "short_docstring": "Return the minimum of an array or minimum along an axis."}
{"name": "pyquery.PyQuery", "type": "class", "signature": "(*args, **kwargs)", "docstring": "The main class\n    ", "short_docstring": "The main class\n    "}
{"name": "lxml.html.fromstring", "type": "callable", "signature": "(html, base_url=None, parser=None, **kw)", "docstring": "Parse the html, returning a single element/document.\n\nThis tries to minimally parse the chunk of text, without knowing if it\nis a fragment or a document.\n\nbase_url will set the document's base_url attribute (and the tree's docinfo.URL)", "short_docstring": "Parse the html, returning a single element/document."}
{"name": "re.findall", "type": "callable", "signature": "(pattern, string, flags=0)", "docstring": "Return a list of all non-overlapping matches in the string.\n\nIf one or more capturing groups are present in the pattern, return\na list of groups; this will be a list of tuples if the pattern\nhas more than one group.\n\nEmpty matches are included in the result.", "short_docstring": "Return a list of all non-overlapping matches in the string."}
{"name": "sys.stderr", "type": "constant", "value": "<_io.TextIOWrapper name='<stderr>' mode='w' encoding='utf-8'>", "signature": null, "docstring": "Character and line based layer over a BufferedIOBase object, buffer.\n\nencoding gives the name of the encoding that the stream will be\ndecoded or encoded with. It defaults to locale.getpreferredencoding(False).\n\nerrors determines the strictness of encoding and decoding (see\nhelp(codecs.Codec) or the documentation for codecs.register) and\ndefaults to \"strict\".\n\nnewline controls how line endings are handled. It can be None, '',\n'\\n', '\\r', and '\\r\\n'.  It works as follows:\n\n* On input, if newline is None, universal newlines mode is\n  enabled. Lines in the input can end in '\\n', '\\r', or '\\r\\n', and\n  these are translated into '\\n' before being returned to the\n  caller. If it is '', universal newline mode is enabled, but line\n  endings are returned to the caller untranslated. If it has any of\n  the other legal values, input lines are only terminated by the given\n  string, and the line ending is returned to the caller untranslated.\n\n* On output, if newline is None, any '\\n' characters written are\n  translated to the system default line separator, os.linesep. If\n  newline is '' or '\\n', no translation takes place. If newline is any\n  of the other legal values, any '\\n' characters written are translated\n  to the given string.\n\nIf line_buffering is True, a call to flush is implied when a call to\nwrite contains a newline character.", "short_docstring": "Character and line based layer over a BufferedIOBase object, buffer."}
{"name": "pandas.Timestamp.timestamp", "type": "callable", "signature": "(self)", "docstring": "Return POSIX timestamp as float.\n\nExamples\n--------\n>>> ts = pd.Timestamp('2020-03-14T15:32:52.192548')\n>>> ts.timestamp()\n1584199972.192548", "short_docstring": "Return POSIX timestamp as float."}
{"name": "numpy.nan", "type": "constant", "value": "nan", "signature": null, "docstring": "Convert a string or number to a floating point number, if possible.", "short_docstring": "Convert a string or number to a floating point number, if possible."}
{"name": "sklearn.preprocessing.StandardScaler", "type": "class", "signature": "(*, copy=True, with_mean=True, with_std=True)", "docstring": "Standardize features by removing the mean and scaling to unit variance.\n\nThe standard score of a sample `x` is calculated as:\n\n    z = (x - u) / s\n\nwhere `u` is the mean of the training samples or zero if `with_mean=False`,\nand `s` is the standard deviation of the training samples or one if\n`with_std=False`.\n\nCentering and scaling happen independently on each feature by computing\nthe relevant statistics on the samples in the training set. Mean and\nstandard deviation are then stored to be used on later data using\n:meth:`transform`.\n\nStandardization of a dataset is a common requirement for many\nmachine learning estimators: they might behave badly if the\nindividual features do not more or less look like standard normally\ndistributed data (e.g. Gaussian with 0 mean and unit variance).\n\nFor instance many elements used in the objective function of\na learning algorithm (such as the RBF kernel of Support Vector\nMachines or the L1 and L2 regularizers of linear models) assume that\nall features are centered around 0 and have variance in the same\norder. If a feature has a variance that is orders of magnitude larger\nthan others, it might dominate the objective function and make the\nestimator unable to learn from other features correctly as expected.\n\n`StandardScaler` is sensitive to outliers, and the features may scale\ndifferently from each other in the presence of outliers. For an example\nvisualization, refer to :ref:`Compare StandardScaler with other scalers\n<plot_all_scaling_standard_scaler_section>`.\n\nThis scaler can also be applied to sparse CSR or CSC matrices by passing\n`with_mean=False` to avoid breaking the sparsity structure of the data.\n\nRead more in the :ref:`User Guide <preprocessing_scaler>`.\n\nParameters\n----------\ncopy : bool, default=True\n    If False, try to avoid a copy and do inplace scaling instead.\n    This is not guaranteed to always work inplace; e.g. if the data is\n    not a NumPy array or scipy.sparse CSR matrix, a copy may still be\n    returned.\n\nwith_mean : bool, default=True\n    If True, center the data before scaling.\n    This does not work (and will raise an exception) when attempted on\n    sparse matrices, because centering them entails building a dense\n    matrix which in common use cases is likely to be too large to fit in\n    memory.\n\nwith_std : bool, default=True\n    If True, scale the data to unit variance (or equivalently,\n    unit standard deviation).\n\nAttributes\n----------\nscale_ : ndarray of shape (n_features,) or None\n    Per feature relative scaling of the data to achieve zero mean and unit\n    variance. Generally this is calculated using `np.sqrt(var_)`. If a\n    variance is zero, we can't achieve unit variance, and the data is left\n    as-is, giving a scaling factor of 1. `scale_` is equal to `None`\n    when `with_std=False`.\n\n    .. versionadded:: 0.17\n       *scale_*\n\nmean_ : ndarray of shape (n_features,) or None\n    The mean value for each feature in the training set.\n    Equal to ``None`` when ``with_mean=False`` and ``with_std=False``.\n\nvar_ : ndarray of shape (n_features,) or None\n    The variance for each feature in the training set. Used to compute\n    `scale_`. Equal to ``None`` when ``with_mean=False`` and\n    ``with_std=False``.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nn_samples_seen_ : int or ndarray of shape (n_features,)\n    The number of samples processed by the estimator for each feature.\n    If there are no missing samples, the ``n_samples_seen`` will be an\n    integer, otherwise it will be an array of dtype int. If\n    `sample_weights` are used it will be a float (if no missing data)\n    or an array of dtype float that sums the weights seen so far.\n    Will be reset on new calls to fit, but increments across\n    ``partial_fit`` calls.\n\nSee Also\n--------\nscale : Equivalent function without the estimator API.\n\n:class:`~sklearn.decomposition.PCA` : Further removes the linear\n    correlation across features with 'whiten=True'.\n\nNotes\n-----\nNaNs are treated as missing values: disregarded in fit, and maintained in\ntransform.\n\nWe use a biased estimator for the standard deviation, equivalent to\n`numpy.std(x, ddof=0)`. Note that the choice of `ddof` is unlikely to\naffect model performance.\n\nExamples\n--------\n>>> from sklearn.preprocessing import StandardScaler\n>>> data = [[0, 0], [0, 0], [1, 1], [1, 1]]\n>>> scaler = StandardScaler()\n>>> print(scaler.fit(data))\nStandardScaler()\n>>> print(scaler.mean_)\n[0.5 0.5]\n>>> print(scaler.transform(data))\n[[-1. -1.]\n [-1. -1.]\n [ 1.  1.]\n [ 1.  1.]]\n>>> print(scaler.transform([[2, 2]]))\n[[3. 3.]]", "short_docstring": "Standardize features by removing the mean and scaling to unit variance."}
{"name": "urllib.parse.urljoin", "type": "callable", "signature": "(base, url, allow_fragments=True)", "docstring": "Join a base URL and a possibly relative URL to form an absolute\ninterpretation of the latter.", "short_docstring": "Join a base URL and a possibly relative URL to form an absolute\ninterpretation of the latter."}
{"name": "matplotlib.pyplot.xlim", "type": "callable", "signature": "(*args, **kwargs) -> 'tuple[float, float]'", "docstring": "Get or set the x limits of the current axes.\n\nCall signatures::\n\n    left, right = xlim()  # return the current xlim\n    xlim((left, right))   # set the xlim to left, right\n    xlim(left, right)     # set the xlim to left, right\n\nIf you do not specify args, you can pass *left* or *right* as kwargs,\ni.e.::\n\n    xlim(right=3)  # adjust the right leaving left unchanged\n    xlim(left=1)  # adjust the left leaving right unchanged\n\nSetting limits turns autoscaling off for the x-axis.\n\nReturns\n-------\nleft, right\n    A tuple of the new x-axis limits.\n\nNotes\n-----\nCalling this function with no arguments (e.g. ``xlim()``) is the pyplot\nequivalent of calling `~.Axes.get_xlim` on the current axes.\nCalling this function with arguments is the pyplot equivalent of calling\n`~.Axes.set_xlim` on the current axes. All arguments are passed though.", "short_docstring": "Get or set the x limits of the current axes."}
{"name": "cryptography.hazmat.primitives.ciphers.algorithms.AES", "type": "class", "signature": "(key: bytes)", "docstring": null, "short_docstring": ""}
{"name": "numpy.number", "type": "class", "signature": "()", "docstring": "Abstract base class of all numeric scalar types.", "short_docstring": "Abstract base class of all numeric scalar types."}
{"name": "random.choice", "type": "callable", "signature": "(seq)", "docstring": "Choose a random element from a non-empty sequence.", "short_docstring": "Choose a random element from a non-empty sequence."}
{"name": "seaborn.histplot", "type": "callable", "signature": "(data=None, *, x=None, y=None, hue=None, weights=None, stat='count', bins='auto', binwidth=None, binrange=None, discrete=None, cumulative=False, common_bins=True, common_norm=True, multiple='layer', element='bars', fill=True, shrink=1, kde=False, kde_kws=None, line_kws=None, thresh=0, pthresh=None, pmax=None, cbar=False, cbar_ax=None, cbar_kws=None, palette=None, hue_order=None, hue_norm=None, color=None, log_scale=None, legend=True, ax=None, **kwargs)", "docstring": "Plot univariate or bivariate histograms to show distributions of datasets.\n\nA histogram is a classic visualization tool that represents the distribution\nof one or more variables by counting the number of observations that fall within\ndiscrete bins.\n\nThis function can normalize the statistic computed within each bin to estimate\nfrequency, density or probability mass, and it can add a smooth curve obtained\nusing a kernel density estimate, similar to :func:`kdeplot`.\n\nMore information is provided in the :ref:`user guide <tutorial_hist>`.\n\nParameters\n----------\ndata : :class:`pandas.DataFrame`, :class:`numpy.ndarray`, mapping, or sequence\n    Input data structure. Either a long-form collection of vectors that can be\n    assigned to named variables or a wide-form dataset that will be internally\n    reshaped.\nx, y : vectors or keys in ``data``\n    Variables that specify positions on the x and y axes.\nhue : vector or key in ``data``\n    Semantic variable that is mapped to determine the color of plot elements.\nweights : vector or key in ``data``\n    If provided, weight the contribution of the corresponding data points\n    towards the count in each bin by these factors.\nstat : str\n    Aggregate statistic to compute in each bin.\n    \n    - `count`: show the number of observations in each bin\n    - `frequency`: show the number of observations divided by the bin width\n    - `probability` or `proportion`: normalize such that bar heights sum to 1\n    - `percent`: normalize such that bar heights sum to 100\n    - `density`: normalize such that the total area of the histogram equals 1\nbins : str, number, vector, or a pair of such values\n    Generic bin parameter that can be the name of a reference rule,\n    the number of bins, or the breaks of the bins.\n    Passed to :func:`numpy.histogram_bin_edges`.\nbinwidth : number or pair of numbers\n    Width of each bin, overrides ``bins`` but can be used with\n    ``binrange``.\nbinrange : pair of numbers or a pair of pairs\n    Lowest and highest value for bin edges; can be used either\n    with ``bins`` or ``binwidth``. Defaults to data extremes.\ndiscrete : bool\n    If True, default to ``binwidth=1`` and draw the bars so that they are\n    centered on their corresponding data points. This avoids \"gaps\" that may\n    otherwise appear when using discrete (integer) data.\ncumulative : bool\n    If True, plot the cumulative counts as bins increase.\ncommon_bins : bool\n    If True, use the same bins when semantic variables produce multiple\n    plots. If using a reference rule to determine the bins, it will be computed\n    with the full dataset.\ncommon_norm : bool\n    If True and using a normalized statistic, the normalization will apply over\n    the full dataset. Otherwise, normalize each histogram independently.\nmultiple : {\"layer\", \"dodge\", \"stack\", \"fill\"}\n    Approach to resolving multiple elements when semantic mapping creates subsets.\n    Only relevant with univariate data.\nelement : {\"bars\", \"step\", \"poly\"}\n    Visual representation of the histogram statistic.\n    Only relevant with univariate data.\nfill : bool\n    If True, fill in the space under the histogram.\n    Only relevant with univariate data.\nshrink : number\n    Scale the width of each bar relative to the binwidth by this factor.\n    Only relevant with univariate data.\nkde : bool\n    If True, compute a kernel density estimate to smooth the distribution\n    and show on the plot as (one or more) line(s).\n    Only relevant with univariate data.\nkde_kws : dict\n    Parameters that control the KDE computation, as in :func:`kdeplot`.\nline_kws : dict\n    Parameters that control the KDE visualization, passed to\n    :meth:`matplotlib.axes.Axes.plot`.\nthresh : number or None\n    Cells with a statistic less than or equal to this value will be transparent.\n    Only relevant with bivariate data.\npthresh : number or None\n    Like ``thresh``, but a value in [0, 1] such that cells with aggregate counts\n    (or other statistics, when used) up to this proportion of the total will be\n    transparent.\npmax : number or None\n    A value in [0, 1] that sets that saturation point for the colormap at a value\n    such that cells below constitute this proportion of the total count (or\n    other statistic, when used).\ncbar : bool\n    If True, add a colorbar to annotate the color mapping in a bivariate plot.\n    Note: Does not currently support plots with a ``hue`` variable well.\ncbar_ax : :class:`matplotlib.axes.Axes`\n    Pre-existing axes for the colorbar.\ncbar_kws : dict\n    Additional parameters passed to :meth:`matplotlib.figure.Figure.colorbar`.\npalette : string, list, dict, or :class:`matplotlib.colors.Colormap`\n    Method for choosing the colors to use when mapping the ``hue`` semantic.\n    String values are passed to :func:`color_palette`. List or dict values\n    imply categorical mapping, while a colormap object implies numeric mapping.\nhue_order : vector of strings\n    Specify the order of processing and plotting for categorical levels of the\n    ``hue`` semantic.\nhue_norm : tuple or :class:`matplotlib.colors.Normalize`\n    Either a pair of values that set the normalization range in data units\n    or an object that will map from data units into a [0, 1] interval. Usage\n    implies numeric mapping.\ncolor : :mod:`matplotlib color <matplotlib.colors>`\n    Single color specification for when hue mapping is not used. Otherwise, the\n    plot will try to hook into the matplotlib property cycle.\nlog_scale : bool or number, or pair of bools or numbers\n    Set axis scale(s) to log. A single value sets the data axis for any numeric\n    axes in the plot. A pair of values sets each axis independently.\n    Numeric values are interpreted as the desired base (default 10).\n    When `None` or `False`, seaborn defers to the existing Axes scale.\nlegend : bool\n    If False, suppress the legend for semantic variables.\nax : :class:`matplotlib.axes.Axes`\n    Pre-existing axes for the plot. Otherwise, call :func:`matplotlib.pyplot.gca`\n    internally.\nkwargs\n    Other keyword arguments are passed to one of the following matplotlib\n    functions:\n\n    - :meth:`matplotlib.axes.Axes.bar` (univariate, element=\"bars\")\n    - :meth:`matplotlib.axes.Axes.fill_between` (univariate, other element, fill=True)\n    - :meth:`matplotlib.axes.Axes.plot` (univariate, other element, fill=False)\n    - :meth:`matplotlib.axes.Axes.pcolormesh` (bivariate)\n\nReturns\n-------\n:class:`matplotlib.axes.Axes`\n    The matplotlib axes containing the plot.\n\nSee Also\n--------\ndisplot : Figure-level interface to distribution plot functions.\nkdeplot : Plot univariate or bivariate distributions using kernel density estimation.\nrugplot : Plot a tick at each observation value along the x and/or y axes.\necdfplot : Plot empirical cumulative distribution functions.\njointplot : Draw a bivariate plot with univariate marginal distributions.\n\nNotes\n-----\n\nThe choice of bins for computing and plotting a histogram can exert\nsubstantial influence on the insights that one is able to draw from the\nvisualization. If the bins are too large, they may erase important features.\nOn the other hand, bins that are too small may be dominated by random\nvariability, obscuring the shape of the true underlying distribution. The\ndefault bin size is determined using a reference rule that depends on the\nsample size and variance. This works well in many cases, (i.e., with\n\"well-behaved\" data) but it fails in others. It is always a good to try\ndifferent bin sizes to be sure that you are not missing something important.\nThis function allows you to specify bins in several different ways, such as\nby setting the total number of bins to use, the width of each bin, or the\nspecific locations where the bins should break.\n\nExamples\n--------\n\n.. include:: ../docstrings/histplot.rst", "short_docstring": "Plot univariate or bivariate histograms to show distributions of datasets."}
{"name": "shapely.geometry.Point", "type": "class", "signature": "(*args)", "docstring": "A geometry type that represents a single coordinate with\nx,y and possibly z values.\n\nA point is a zero-dimensional feature and has zero length and zero area.\n\nParameters\n----------\nargs : float, or sequence of floats\n    The coordinates can either be passed as a single parameter, or as\n    individual float values using multiple parameters:\n\n    1) 1 parameter: a sequence or array-like of with 2 or 3 values.\n    2) 2 or 3 parameters (float): x, y, and possibly z.\n\nAttributes\n----------\nx, y, z : float\n    Coordinate values\n\nExamples\n--------\nConstructing the Point using separate parameters for x and y:\n\n>>> p = Point(1.0, -1.0)\n\nConstructing the Point using a list of x, y coordinates:\n\n>>> p = Point([1.0, -1.0])\n>>> print(p)\nPOINT (1 -1)\n>>> p.y\n-1.0\n>>> p.x\n1.0", "short_docstring": "A geometry type that represents a single coordinate with\nx,y and possibly z values."}
{"name": "tensorflow.keras.optimizers.SGD", "error": "Import error: No module named 'tensorflow'"}
{"name": "matplotlib.pyplot.gcf", "type": "callable", "signature": "() -> 'Figure'", "docstring": "Get the current figure.\n\nIf there is currently no figure on the pyplot figure stack, a new one is\ncreated using `~.pyplot.figure()`.  (To test whether there is currently a\nfigure on the pyplot figure stack, check whether `~.pyplot.get_fignums()`\nis empty.)", "short_docstring": "Get the current figure."}
{"name": "matplotlib.pyplot.show", "type": "callable", "signature": "(*args, **kwargs) -> 'None'", "docstring": "Display all open figures.\n\nParameters\n----------\nblock : bool, optional\n    Whether to wait for all figures to be closed before returning.\n\n    If `True` block and run the GUI main loop until all figure windows\n    are closed.\n\n    If `False` ensure that all figure windows are displayed and return\n    immediately.  In this case, you are responsible for ensuring\n    that the event loop is running to have responsive figures.\n\n    Defaults to True in non-interactive mode and to False in interactive\n    mode (see `.pyplot.isinteractive`).\n\nSee Also\n--------\nion : Enable interactive mode, which shows / updates the figure after\n      every plotting command, so that calling ``show()`` is not necessary.\nioff : Disable interactive mode.\nsavefig : Save the figure to an image file instead of showing it on screen.\n\nNotes\n-----\n**Saving figures to file and showing a window at the same time**\n\nIf you want an image file as well as a user interface window, use\n`.pyplot.savefig` before `.pyplot.show`. At the end of (a blocking)\n``show()`` the figure is closed and thus unregistered from pyplot. Calling\n`.pyplot.savefig` afterwards would save a new and thus empty figure. This\nlimitation of command order does not apply if the show is non-blocking or\nif you keep a reference to the figure and use `.Figure.savefig`.\n\n**Auto-show in jupyter notebooks**\n\nThe jupyter backends (activated via ``%matplotlib inline``,\n``%matplotlib notebook``, or ``%matplotlib widget``), call ``show()`` at\nthe end of every cell by default. Thus, you usually don't have to call it\nexplicitly there.", "short_docstring": "Display all open figures."}
{"name": "warnings.warn", "type": "callable", "signature": "(message, category=None, stacklevel=1, source=None)", "docstring": "Issue a warning, or maybe ignore it or raise an exception.", "short_docstring": "Issue a warning, or maybe ignore it or raise an exception."}
{"name": "collections.Counter", "type": "class", "signature": "(iterable=None, /, **kwds)", "docstring": "Dict subclass for counting hashable items.  Sometimes called a bag\nor multiset.  Elements are stored as dictionary keys and their counts\nare stored as dictionary values.\n\n>>> c = Counter('abcdeabcdabcaba')  # count elements from a string\n\n>>> c.most_common(3)                # three most common elements\n[('a', 5), ('b', 4), ('c', 3)]\n>>> sorted(c)                       # list all unique elements\n['a', 'b', 'c', 'd', 'e']\n>>> ''.join(sorted(c.elements()))   # list elements with repetitions\n'aaaaabbbbcccdde'\n>>> sum(c.values())                 # total of all counts\n15\n\n>>> c['a']                          # count of letter 'a'\n5\n>>> for elem in 'shazam':           # update counts from an iterable\n...     c[elem] += 1                # by adding 1 to each element's count\n>>> c['a']                          # now there are seven 'a'\n7\n>>> del c['b']                      # remove all 'b'\n>>> c['b']                          # now there are zero 'b'\n0\n\n>>> d = Counter('simsalabim')       # make another counter\n>>> c.update(d)                     # add in the second counter\n>>> c['a']                          # now there are nine 'a'\n9\n\n>>> c.clear()                       # empty the counter\n>>> c\nCounter()\n\nNote:  If a count is set to zero or reduced to zero, it will remain\nin the counter until the entry is deleted or the counter is cleared:\n\n>>> c = Counter('aaabbc')\n>>> c['b'] -= 2                     # reduce the count of 'b' by two\n>>> c.most_common()                 # 'b' is still in, but its count is zero\n[('a', 3), ('c', 1), ('b', 0)]", "short_docstring": "Dict subclass for counting hashable items.  Sometimes called a bag\nor multiset.  Elements are stored as dictionary keys and their counts\nare stored as dictionary values."}
{"name": "seaborn.lineplot", "type": "callable", "signature": "(data=None, *, x=None, y=None, hue=None, size=None, style=None, units=None, weights=None, palette=None, hue_order=None, hue_norm=None, sizes=None, size_order=None, size_norm=None, dashes=True, markers=None, style_order=None, estimator='mean', errorbar=('ci', 95), n_boot=1000, seed=None, orient='x', sort=True, err_style='band', err_kws=None, legend='auto', ci='deprecated', ax=None, **kwargs)", "docstring": "Draw a line plot with possibility of several semantic groupings.\n\nThe relationship between `x` and `y` can be shown for different subsets\nof the data using the `hue`, `size`, and `style` parameters. These\nparameters control what visual semantics are used to identify the different\nsubsets. It is possible to show up to three dimensions independently by\nusing all three semantic types, but this style of plot can be hard to\ninterpret and is often ineffective. Using redundant semantics (i.e. both\n`hue` and `style` for the same variable) can be helpful for making\ngraphics more accessible.\n\nSee the :ref:`tutorial <relational_tutorial>` for more information.\n\nThe default treatment of the `hue` (and to a lesser extent, `size`)\nsemantic, if present, depends on whether the variable is inferred to\nrepresent \"numeric\" or \"categorical\" data. In particular, numeric variables\nare represented with a sequential colormap by default, and the legend\nentries show regular \"ticks\" with values that may or may not exist in the\ndata. This behavior can be controlled through various parameters, as\ndescribed and illustrated below.\n\nBy default, the plot aggregates over multiple `y` values at each value of\n`x` and shows an estimate of the central tendency and a confidence\ninterval for that estimate.\n\nParameters\n----------\ndata : :class:`pandas.DataFrame`, :class:`numpy.ndarray`, mapping, or sequence\n    Input data structure. Either a long-form collection of vectors that can be\n    assigned to named variables or a wide-form dataset that will be internally\n    reshaped.\nx, y : vectors or keys in ``data``\n    Variables that specify positions on the x and y axes.\nhue : vector or key in `data`\n    Grouping variable that will produce lines with different colors.\n    Can be either categorical or numeric, although color mapping will\n    behave differently in latter case.\nsize : vector or key in `data`\n    Grouping variable that will produce lines with different widths.\n    Can be either categorical or numeric, although size mapping will\n    behave differently in latter case.\nstyle : vector or key in `data`\n    Grouping variable that will produce lines with different dashes\n    and/or markers. Can have a numeric dtype but will always be treated\n    as categorical.\nunits : vector or key in `data`\n    Grouping variable identifying sampling units. When used, a separate\n    line will be drawn for each unit with appropriate semantics, but no\n    legend entry will be added. Useful for showing distribution of\n    experimental replicates when exact identities are not needed.\nweights : vector or key in `data`\n    Data values or column used to compute weighted estimation.\n    Note that use of weights currently limits the choice of statistics\n    to a 'mean' estimator and 'ci' errorbar.\npalette : string, list, dict, or :class:`matplotlib.colors.Colormap`\n    Method for choosing the colors to use when mapping the ``hue`` semantic.\n    String values are passed to :func:`color_palette`. List or dict values\n    imply categorical mapping, while a colormap object implies numeric mapping.\nhue_order : vector of strings\n    Specify the order of processing and plotting for categorical levels of the\n    ``hue`` semantic.\nhue_norm : tuple or :class:`matplotlib.colors.Normalize`\n    Either a pair of values that set the normalization range in data units\n    or an object that will map from data units into a [0, 1] interval. Usage\n    implies numeric mapping.\nsizes : list, dict, or tuple\n    An object that determines how sizes are chosen when `size` is used.\n    List or dict arguments should provide a size for each unique data value,\n    which forces a categorical interpretation. The argument may also be a\n    min, max tuple.\nsize_order : list\n    Specified order for appearance of the `size` variable levels,\n    otherwise they are determined from the data. Not relevant when the\n    `size` variable is numeric.\nsize_norm : tuple or Normalize object\n    Normalization in data units for scaling plot objects when the\n    `size` variable is numeric.\ndashes : boolean, list, or dictionary\n    Object determining how to draw the lines for different levels of the\n    `style` variable. Setting to `True` will use default dash codes, or\n    you can pass a list of dash codes or a dictionary mapping levels of the\n    `style` variable to dash codes. Setting to `False` will use solid\n    lines for all subsets. Dashes are specified as in matplotlib: a tuple\n    of `(segment, gap)` lengths, or an empty string to draw a solid line.\nmarkers : boolean, list, or dictionary\n    Object determining how to draw the markers for different levels of the\n    `style` variable. Setting to `True` will use default markers, or\n    you can pass a list of markers or a dictionary mapping levels of the\n    `style` variable to markers. Setting to `False` will draw\n    marker-less lines.  Markers are specified as in matplotlib.\nstyle_order : list\n    Specified order for appearance of the `style` variable levels\n    otherwise they are determined from the data. Not relevant when the\n    `style` variable is numeric.\nestimator : name of pandas method or callable or None\n    Method for aggregating across multiple observations of the `y`\n    variable at the same `x` level. If `None`, all observations will\n    be drawn.\nerrorbar : string, (string, number) tuple, or callable\n    Name of errorbar method (either \"ci\", \"pi\", \"se\", or \"sd\"), or a tuple\n    with a method name and a level parameter, or a function that maps from a\n    vector to a (min, max) interval, or None to hide errorbar. See the\n    :doc:`errorbar tutorial </tutorial/error_bars>` for more information.\nn_boot : int\n    Number of bootstraps to use for computing the confidence interval.\nseed : int, numpy.random.Generator, or numpy.random.RandomState\n    Seed or random number generator for reproducible bootstrapping.\norient : \"x\" or \"y\"\n    Dimension along which the data are sorted / aggregated. Equivalently,\n    the \"independent variable\" of the resulting function.\nsort : boolean\n    If True, the data will be sorted by the x and y variables, otherwise\n    lines will connect points in the order they appear in the dataset.\nerr_style : \"band\" or \"bars\"\n    Whether to draw the confidence intervals with translucent error bands\n    or discrete error bars.\nerr_kws : dict of keyword arguments\n    Additional parameters to control the aesthetics of the error bars. The\n    kwargs are passed either to :meth:`matplotlib.axes.Axes.fill_between`\n    or :meth:`matplotlib.axes.Axes.errorbar`, depending on `err_style`.\nlegend : \"auto\", \"brief\", \"full\", or False\n    How to draw the legend. If \"brief\", numeric `hue` and `size`\n    variables will be represented with a sample of evenly spaced values.\n    If \"full\", every group will get an entry in the legend. If \"auto\",\n    choose between brief or full representation based on number of levels.\n    If `False`, no legend data is added and no legend is drawn.\nci : int or \"sd\" or None\n    Size of the confidence interval to draw when aggregating.\n\n    .. deprecated:: 0.12.0\n        Use the new `errorbar` parameter for more flexibility.\n\nax : :class:`matplotlib.axes.Axes`\n    Pre-existing axes for the plot. Otherwise, call :func:`matplotlib.pyplot.gca`\n    internally.\nkwargs : key, value mappings\n    Other keyword arguments are passed down to\n    :meth:`matplotlib.axes.Axes.plot`.\n\nReturns\n-------\n:class:`matplotlib.axes.Axes`\n    The matplotlib axes containing the plot.\n\nSee Also\n--------\nscatterplot : Plot data using points.\npointplot : Plot point estimates and CIs using markers and lines.\n\nExamples\n--------\n\n.. include:: ../docstrings/lineplot.rst", "short_docstring": "Draw a line plot with possibility of several semantic groupings."}
{"name": "flask.url_for", "type": "callable", "signature": "(endpoint: 'str', *, _anchor: 'str | None' = None, _method: 'str | None' = None, _scheme: 'str | None' = None, _external: 'bool | None' = None, **values: 't.Any') -> 'str'", "docstring": "Generate a URL to the given endpoint with the given values.\n\nThis requires an active request or application context, and calls\n:meth:`current_app.url_for() <flask.Flask.url_for>`. See that method\nfor full documentation.\n\n:param endpoint: The endpoint name associated with the URL to\n    generate. If this starts with a ``.``, the current blueprint\n    name (if any) will be used.\n:param _anchor: If given, append this as ``#anchor`` to the URL.\n:param _method: If given, generate the URL associated with this\n    method for the endpoint.\n:param _scheme: If given, the URL will have this scheme if it is\n    external.\n:param _external: If given, prefer the URL to be internal (False) or\n    require it to be external (True). External URLs include the\n    scheme and domain. When not in an active request, URLs are\n    external by default.\n:param values: Values to use for the variable parts of the URL rule.\n    Unknown keys are appended as query string arguments, like\n    ``?a=b&c=d``.\n\n.. versionchanged:: 2.2\n    Calls ``current_app.url_for``, allowing an app to override the\n    behavior.\n\n.. versionchanged:: 0.10\n   The ``_scheme`` parameter was added.\n\n.. versionchanged:: 0.9\n   The ``_anchor`` and ``_method`` parameters were added.\n\n.. versionchanged:: 0.9\n   Calls ``app.handle_url_build_error`` on build errors.", "short_docstring": "Generate a URL to the given endpoint with the given values."}
{"name": "matplotlib.pyplot.colorbar", "type": "callable", "signature": "(mappable: 'ScalarMappable | None' = None, cax: 'matplotlib.axes.Axes | None' = None, ax: 'matplotlib.axes.Axes | Iterable[matplotlib.axes.Axes] | None' = None, **kwargs) -> 'Colorbar'", "docstring": "Add a colorbar to a plot.\n\nParameters\n----------\nmappable\n    The `matplotlib.cm.ScalarMappable` (i.e., `.AxesImage`,\n    `.ContourSet`, etc.) described by this colorbar.  This argument is\n    mandatory for the `.Figure.colorbar` method but optional for the\n    `.pyplot.colorbar` function, which sets the default to the current\n    image.\n\n    Note that one can create a `.ScalarMappable` \"on-the-fly\" to\n    generate colorbars not attached to a previously drawn artist, e.g.\n    ::\n\n        fig.colorbar(cm.ScalarMappable(norm=norm, cmap=cmap), ax=ax)\n\ncax : `~matplotlib.axes.Axes`, optional\n    Axes into which the colorbar will be drawn.  If `None`, then a new\n    Axes is created and the space for it will be stolen from the Axes(s)\n    specified in *ax*.\n\nax : `~matplotlib.axes.Axes` or iterable or `numpy.ndarray` of Axes, optional\n    The one or more parent Axes from which space for a new colorbar Axes\n    will be stolen. This parameter is only used if *cax* is not set.\n\n    Defaults to the Axes that contains the mappable used to create the\n    colorbar.\n\nuse_gridspec : bool, optional\n    If *cax* is ``None``, a new *cax* is created as an instance of\n    Axes.  If *ax* is positioned with a subplotspec and *use_gridspec*\n    is ``True``, then *cax* is also positioned with a subplotspec.\n\nReturns\n-------\ncolorbar : `~matplotlib.colorbar.Colorbar`\n\nOther Parameters\n----------------\n\nlocation : None or {'left', 'right', 'top', 'bottom'}\n    The location, relative to the parent axes, where the colorbar axes\n    is created.  It also determines the *orientation* of the colorbar\n    (colorbars on the left and right are vertical, colorbars at the top\n    and bottom are horizontal).  If None, the location will come from the\n    *orientation* if it is set (vertical colorbars on the right, horizontal\n    ones at the bottom), or default to 'right' if *orientation* is unset.\n\norientation : None or {'vertical', 'horizontal'}\n    The orientation of the colorbar.  It is preferable to set the *location*\n    of the colorbar, as that also determines the *orientation*; passing\n    incompatible values for *location* and *orientation* raises an exception.\n\nfraction : float, default: 0.15\n    Fraction of original axes to use for colorbar.\n\nshrink : float, default: 1.0\n    Fraction by which to multiply the size of the colorbar.\n\naspect : float, default: 20\n    Ratio of long to short dimensions.\n\npad : float, default: 0.05 if vertical, 0.15 if horizontal\n    Fraction of original axes between colorbar and new image axes.\n\nanchor : (float, float), optional\n    The anchor point of the colorbar axes.\n    Defaults to (0.0, 0.5) if vertical; (0.5, 1.0) if horizontal.\n\npanchor : (float, float), or *False*, optional\n    The anchor point of the colorbar parent axes. If *False*, the parent\n    axes' anchor will be unchanged.\n    Defaults to (1.0, 0.5) if vertical; (0.5, 0.0) if horizontal.\n\nextend : {'neither', 'both', 'min', 'max'}\n    Make pointed end(s) for out-of-range values (unless 'neither').  These are\n    set for a given colormap using the colormap set_under and set_over methods.\n\nextendfrac : {*None*, 'auto', length, lengths}\n    If set to *None*, both the minimum and maximum triangular colorbar\n    extensions will have a length of 5% of the interior colorbar length (this\n    is the default setting).\n\n    If set to 'auto', makes the triangular colorbar extensions the same lengths\n    as the interior boxes (when *spacing* is set to 'uniform') or the same\n    lengths as the respective adjacent interior boxes (when *spacing* is set to\n    'proportional').\n\n    If a scalar, indicates the length of both the minimum and maximum\n    triangular colorbar extensions as a fraction of the interior colorbar\n    length.  A two-element sequence of fractions may also be given, indicating\n    the lengths of the minimum and maximum colorbar extensions respectively as\n    a fraction of the interior colorbar length.\n\nextendrect : bool\n    If *False* the minimum and maximum colorbar extensions will be triangular\n    (the default).  If *True* the extensions will be rectangular.\n\nspacing : {'uniform', 'proportional'}\n    For discrete colorbars (`.BoundaryNorm` or contours), 'uniform' gives each\n    color the same space; 'proportional' makes the space proportional to the\n    data interval.\n\nticks : None or list of ticks or Locator\n    If None, ticks are determined automatically from the input.\n\nformat : None or str or Formatter\n    If None, `~.ticker.ScalarFormatter` is used.\n    Format strings, e.g., ``\"%4.2e\"`` or ``\"{x:.2e}\"``, are supported.\n    An alternative `~.ticker.Formatter` may be given instead.\n\ndrawedges : bool\n    Whether to draw lines at color boundaries.\n\nlabel : str\n    The label on the colorbar's long axis.\n\nboundaries, values : None or a sequence\n    If unset, the colormap will be displayed on a 0-1 scale.\n    If sequences, *values* must have a length 1 less than *boundaries*.  For\n    each region delimited by adjacent entries in *boundaries*, the color mapped\n    to the corresponding value in values will be used.\n    Normally only useful for indexed colors (i.e. ``norm=NoNorm()``) or other\n    unusual circumstances.\n\nNotes\n-----\nIf *mappable* is a `~.contour.ContourSet`, its *extend* kwarg is\nincluded automatically.\n\nThe *shrink* kwarg provides a simple way to scale the colorbar with\nrespect to the axes. Note that if *cax* is specified, it determines the\nsize of the colorbar, and *shrink* and *aspect* are ignored.\n\nFor more precise control, you can manually specify the positions of the\naxes objects in which the mappable and the colorbar are drawn.  In this\ncase, do not use any of the axes properties kwargs.\n\nIt is known that some vector graphics viewers (svg and pdf) render\nwhite gaps between segments of the colorbar.  This is due to bugs in\nthe viewers, not Matplotlib.  As a workaround, the colorbar can be\nrendered with overlapping segments::\n\n    cbar = colorbar()\n    cbar.solids.set_edgecolor(\"face\")\n    draw()\n\nHowever, this has negative consequences in other circumstances, e.g.\nwith semi-transparent images (alpha < 1) and colorbar extensions;\ntherefore, this workaround is not used by default (see issue #1188).", "short_docstring": "Add a colorbar to a plot."}
{"name": "matplotlib.pyplot.title", "type": "callable", "signature": "(label: 'str', fontdict: 'dict[str, Any] | None' = None, loc: \"Literal['left', 'center', 'right'] | None\" = None, pad: 'float | None' = None, *, y: 'float | None' = None, **kwargs) -> 'Text'", "docstring": "Set a title for the Axes.\n\nSet one of the three available Axes titles. The available titles\nare positioned above the Axes in the center, flush with the left\nedge, and flush with the right edge.\n\nParameters\n----------\nlabel : str\n    Text to use for the title\n\nfontdict : dict\n\n    .. admonition:: Discouraged\n\n       The use of *fontdict* is discouraged. Parameters should be passed as\n       individual keyword arguments or using dictionary-unpacking\n       ``set_title(..., **fontdict)``.\n\n    A dictionary controlling the appearance of the title text,\n    the default *fontdict* is::\n\n       {'fontsize': rcParams['axes.titlesize'],\n        'fontweight': rcParams['axes.titleweight'],\n        'color': rcParams['axes.titlecolor'],\n        'verticalalignment': 'baseline',\n        'horizontalalignment': loc}\n\nloc : {'center', 'left', 'right'}, default: :rc:`axes.titlelocation`\n    Which title to set.\n\ny : float, default: :rc:`axes.titley`\n    Vertical Axes location for the title (1.0 is the top).  If\n    None (the default) and :rc:`axes.titley` is also None, y is\n    determined automatically to avoid decorators on the Axes.\n\npad : float, default: :rc:`axes.titlepad`\n    The offset of the title from the top of the Axes, in points.\n\nReturns\n-------\n`.Text`\n    The matplotlib text instance representing the title\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    Other keyword arguments are text properties, see `.Text` for a list\n    of valid text properties.", "short_docstring": "Set a title for the Axes."}
{"name": "scipy.stats.chi2_contingency", "type": "callable", "signature": "(observed, correction=True, lambda_=None)", "docstring": "Chi-square test of independence of variables in a contingency table.\n\nThis function computes the chi-square statistic and p-value for the\nhypothesis test of independence of the observed frequencies in the\ncontingency table [1]_ `observed`.  The expected frequencies are computed\nbased on the marginal sums under the assumption of independence; see\n`scipy.stats.contingency.expected_freq`.  The number of degrees of\nfreedom is (expressed using numpy functions and attributes)::\n\n    dof = observed.size - sum(observed.shape) + observed.ndim - 1\n\n\nParameters\n----------\nobserved : array_like\n    The contingency table. The table contains the observed frequencies\n    (i.e. number of occurrences) in each category.  In the two-dimensional\n    case, the table is often described as an \"R x C table\".\ncorrection : bool, optional\n    If True, *and* the degrees of freedom is 1, apply Yates' correction\n    for continuity.  The effect of the correction is to adjust each\n    observed value by 0.5 towards the corresponding expected value.\nlambda_ : float or str, optional\n    By default, the statistic computed in this test is Pearson's\n    chi-squared statistic [2]_.  `lambda_` allows a statistic from the\n    Cressie-Read power divergence family [3]_ to be used instead.  See\n    `scipy.stats.power_divergence` for details.\n\nReturns\n-------\nres : Chi2ContingencyResult\n    An object containing attributes:\n\n    statistic : float\n        The test statistic.\n    pvalue : float\n        The p-value of the test.\n    dof : int\n        The degrees of freedom.\n    expected_freq : ndarray, same shape as `observed`\n        The expected frequencies, based on the marginal sums of the table.\n\nSee Also\n--------\nscipy.stats.contingency.expected_freq\nscipy.stats.fisher_exact\nscipy.stats.chisquare\nscipy.stats.power_divergence\nscipy.stats.barnard_exact\nscipy.stats.boschloo_exact\n\nNotes\n-----\nAn often quoted guideline for the validity of this calculation is that\nthe test should be used only if the observed and expected frequencies\nin each cell are at least 5.\n\nThis is a test for the independence of different categories of a\npopulation. The test is only meaningful when the dimension of\n`observed` is two or more.  Applying the test to a one-dimensional\ntable will always result in `expected` equal to `observed` and a\nchi-square statistic equal to 0.\n\nThis function does not handle masked arrays, because the calculation\ndoes not make sense with missing values.\n\nLike `scipy.stats.chisquare`, this function computes a chi-square\nstatistic; the convenience this function provides is to figure out the\nexpected frequencies and degrees of freedom from the given contingency\ntable. If these were already known, and if the Yates' correction was not\nrequired, one could use `scipy.stats.chisquare`.  That is, if one calls::\n\n    res = chi2_contingency(obs, correction=False)\n\nthen the following is true::\n\n    (res.statistic, res.pvalue) == stats.chisquare(obs.ravel(),\n                                                   f_exp=ex.ravel(),\n                                                   ddof=obs.size - 1 - dof)\n\nThe `lambda_` argument was added in version 0.13.0 of scipy.\n\nReferences\n----------\n.. [1] \"Contingency table\",\n       https://en.wikipedia.org/wiki/Contingency_table\n.. [2] \"Pearson's chi-squared test\",\n       https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test\n.. [3] Cressie, N. and Read, T. R. C., \"Multinomial Goodness-of-Fit\n       Tests\", J. Royal Stat. Soc. Series B, Vol. 46, No. 3 (1984),\n       pp. 440-464.\n.. [4] Berger, Jeffrey S. et al. \"Aspirin for the Primary Prevention of\n       Cardiovascular Events in Women and Men: A Sex-Specific\n       Meta-analysis of Randomized Controlled Trials.\"\n       JAMA, 295(3):306-313, :doi:`10.1001/jama.295.3.306`, 2006.\n\nExamples\n--------\nIn [4]_, the use of aspirin to prevent cardiovascular events in women\nand men was investigated. The study notably concluded:\n\n    ...aspirin therapy reduced the risk of a composite of\n    cardiovascular events due to its effect on reducing the risk of\n    ischemic stroke in women [...]\n\nThe article lists studies of various cardiovascular events. Let's\nfocus on the ischemic stoke in women.\n\nThe following table summarizes the results of the experiment in which\nparticipants took aspirin or a placebo on a regular basis for several\nyears. Cases of ischemic stroke were recorded::\n\n                      Aspirin   Control/Placebo\n    Ischemic stroke     176           230\n    No stroke         21035         21018\n\nIs there evidence that the aspirin reduces the risk of ischemic stroke?\nWe begin by formulating a null hypothesis :math:`H_0`:\n\n    The effect of aspirin is equivalent to that of placebo.\n\nLet's assess the plausibility of this hypothesis with\na chi-square test.\n\n>>> import numpy as np\n>>> from scipy.stats import chi2_contingency\n>>> table = np.array([[176, 230], [21035, 21018]])\n>>> res = chi2_contingency(table)\n>>> res.statistic\n6.892569132546561\n>>> res.pvalue\n0.008655478161175739\n\nUsing a significance level of 5%, we would reject the null hypothesis in\nfavor of the alternative hypothesis: \"the effect of aspirin\nis not equivalent to the effect of placebo\".\nBecause `scipy.stats.contingency.chi2_contingency` performs a two-sided\ntest, the alternative hypothesis does not indicate the direction of the\neffect. We can use `stats.contingency.odds_ratio` to support the\nconclusion that aspirin *reduces* the risk of ischemic stroke.\n\nBelow are further examples showing how larger contingency tables can be\ntested.\n\nA two-way example (2 x 3):\n\n>>> obs = np.array([[10, 10, 20], [20, 20, 20]])\n>>> res = chi2_contingency(obs)\n>>> res.statistic\n2.7777777777777777\n>>> res.pvalue\n0.24935220877729619\n>>> res.dof\n2\n>>> res.expected_freq\narray([[ 12.,  12.,  16.],\n       [ 18.,  18.,  24.]])\n\nPerform the test using the log-likelihood ratio (i.e. the \"G-test\")\ninstead of Pearson's chi-squared statistic.\n\n>>> res = chi2_contingency(obs, lambda_=\"log-likelihood\")\n>>> res.statistic\n2.7688587616781319\n>>> res.pvalue\n0.25046668010954165\n\nA four-way example (2 x 2 x 2 x 2):\n\n>>> obs = np.array(\n...     [[[[12, 17],\n...        [11, 16]],\n...       [[11, 12],\n...        [15, 16]]],\n...      [[[23, 15],\n...        [30, 22]],\n...       [[14, 17],\n...        [15, 16]]]])\n>>> res = chi2_contingency(obs)\n>>> res.statistic\n8.7584514426741897\n>>> res.pvalue\n0.64417725029295503", "short_docstring": "Chi-square test of independence of variables in a contingency table."}
{"name": "csv.reader", "type": "callable", "signature": null, "docstring": "csv_reader = reader(iterable [, dialect='excel']\n                        [optional keyword args])\n    for row in csv_reader:\n        process(row)\n\nThe \"iterable\" argument can be any object that returns a line\nof input for each iteration, such as a file object or a list.  The\noptional \"dialect\" parameter is discussed below.  The function\nalso accepts optional keyword arguments which override settings\nprovided by the dialect.\n\nThe returned object is an iterator.  Each iteration returns a row\nof the CSV file (which can span multiple input lines).", "short_docstring": "csv_reader = reader(iterable [, dialect='excel']\n                        [optional keyword args])\n    for row in csv_reader:\n        process(row)"}
{"name": "cv2.COLOR_BGR2RGB", "type": "constant", "value": "4", "signature": null, "docstring": "int([x]) -> integer\nint(x, base=10) -> integer\n\nConvert a number or string to an integer, or return 0 if no arguments\nare given.  If x is a number, return x.__int__().  For floating point\nnumbers, this truncates towards zero.\n\nIf x is not a number or if base is given, then x must be a string,\nbytes, or bytearray instance representing an integer literal in the\ngiven base.  The literal can be preceded by '+' or '-' and be surrounded\nby whitespace.  The base defaults to 10.  Valid bases are 0 and 2-36.\nBase 0 means to interpret the base from the string as an integer literal.\n>>> int('0b100', base=0)\n4", "short_docstring": "int([x]) -> integer\nint(x, base=10) -> integer"}
{"name": "math.factorial", "type": "callable", "signature": "(x, /)", "docstring": "Find x!.\n\nRaise a ValueError if x is negative or non-integral.", "short_docstring": "Find x!."}
{"name": "queue.Empty", "type": "class", "signature": null, "docstring": "Exception raised by Queue.get(block=0)/get_nowait().", "short_docstring": "Exception raised by Queue.get(block=0)/get_nowait()."}
{"name": "numpy.floor", "type": "callable", "signature": null, "docstring": "floor(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n\nReturn the floor of the input, element-wise.\n\nThe floor of the scalar `x` is the largest integer `i`, such that\n`i <= x`.  It is often denoted as :math:`\\lfloor x \\rfloor`.\n\nParameters\n----------\nx : array_like\n    Input data.\nout : ndarray, None, or tuple of ndarray and None, optional\n    A location into which the result is stored. If provided, it must have\n    a shape that the inputs broadcast to. If not provided or None,\n    a freshly-allocated array is returned. A tuple (possible only as a\n    keyword argument) must have length equal to the number of outputs.\nwhere : array_like, optional\n    This condition is broadcast over the input. At locations where the\n    condition is True, the `out` array will be set to the ufunc result.\n    Elsewhere, the `out` array will retain its original value.\n    Note that if an uninitialized `out` array is created via the default\n    ``out=None``, locations within it where the condition is False will\n    remain uninitialized.\n**kwargs\n    For other keyword-only arguments, see the\n    :ref:`ufunc docs <ufuncs.kwargs>`.\n\nReturns\n-------\ny : ndarray or scalar\n    The floor of each element in `x`.\n    This is a scalar if `x` is a scalar.\n\nSee Also\n--------\nceil, trunc, rint, fix\n\nNotes\n-----\nSome spreadsheet programs calculate the \"floor-towards-zero\", where\n``floor(-2.5) == -2``.  NumPy instead uses the definition of\n`floor` where `floor(-2.5) == -3`. The \"floor-towards-zero\"\nfunction is called ``fix`` in NumPy.\n\nExamples\n--------\n>>> a = np.array([-1.7, -1.5, -0.2, 0.2, 1.5, 1.7, 2.0])\n>>> np.floor(a)\narray([-2., -2., -1.,  0.,  1.,  1.,  2.])", "short_docstring": "floor(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])"}
{"name": "zipfile.BadZipFile", "type": "class", "signature": null, "docstring": "Common base class for all non-exit exceptions.", "short_docstring": "Common base class for all non-exit exceptions."}
{"name": "psutil.Process", "type": "class", "signature": "(pid=None)", "docstring": "Represents an OS process with the given PID.\nIf PID is omitted current process PID (os.getpid()) is used.\nRaise NoSuchProcess if PID does not exist.\n\nNote that most of the methods of this class do not make sure\nthe PID of the process being queried has been reused over time.\nThat means you might end up retrieving an information referring\nto another process in case the original one this instance\nrefers to is gone in the meantime.\n\nThe only exceptions for which process identity is pre-emptively\nchecked and guaranteed are:\n\n - parent()\n - children()\n - nice() (set)\n - ionice() (set)\n - rlimit() (set)\n - cpu_affinity (set)\n - suspend()\n - resume()\n - send_signal()\n - terminate()\n - kill()\n\nTo prevent this problem for all other methods you can:\n - use is_running() before querying the process\n - if you're continuously iterating over a set of Process\n   instances use process_iter() which pre-emptively checks\n   process identity for every yielded instance", "short_docstring": "Represents an OS process with the given PID.\nIf PID is omitted current process PID (os.getpid()) is used.\nRaise NoSuchProcess if PID does not exist."}
{"name": "numpy.copy", "type": "callable", "signature": "(a, order='K', subok=False)", "docstring": "Return an array copy of the given object.\n\nParameters\n----------\na : array_like\n    Input data.\norder : {'C', 'F', 'A', 'K'}, optional\n    Controls the memory layout of the copy. 'C' means C-order,\n    'F' means F-order, 'A' means 'F' if `a` is Fortran contiguous,\n    'C' otherwise. 'K' means match the layout of `a` as closely\n    as possible. (Note that this function and :meth:`ndarray.copy` are very\n    similar, but have different default values for their order=\n    arguments.)\nsubok : bool, optional\n    If True, then sub-classes will be passed-through, otherwise the\n    returned array will be forced to be a base-class array (defaults to False).\n\n    .. versionadded:: 1.19.0\n\nReturns\n-------\narr : ndarray\n    Array interpretation of `a`.\n\nSee Also\n--------\nndarray.copy : Preferred method for creating an array copy\n\nNotes\n-----\nThis is equivalent to:\n\n>>> np.array(a, copy=True)  #doctest: +SKIP\n\nExamples\n--------\nCreate an array x, with a reference y and a copy z:\n\n>>> x = np.array([1, 2, 3])\n>>> y = x\n>>> z = np.copy(x)\n\nNote that, when we modify x, y changes, but not z:\n\n>>> x[0] = 10\n>>> x[0] == y[0]\nTrue\n>>> x[0] == z[0]\nFalse\n\nNote that, np.copy clears previously set WRITEABLE=False flag.\n\n>>> a = np.array([1, 2, 3])\n>>> a.flags[\"WRITEABLE\"] = False\n>>> b = np.copy(a)\n>>> b.flags[\"WRITEABLE\"]\nTrue\n>>> b[0] = 3\n>>> b\narray([3, 2, 3])\n\nNote that np.copy is a shallow copy and will not copy object\nelements within arrays. This is mainly important for arrays\ncontaining Python objects. The new array will contain the\nsame object which may lead to surprises if that object can\nbe modified (is mutable):\n\n>>> a = np.array([1, 'm', [2, 3, 4]], dtype=object)\n>>> b = np.copy(a)\n>>> b[2][0] = 10\n>>> a\narray([1, 'm', list([10, 3, 4])], dtype=object)\n\nTo ensure all elements within an ``object`` array are copied,\nuse `copy.deepcopy`:\n\n>>> import copy\n>>> a = np.array([1, 'm', [2, 3, 4]], dtype=object)\n>>> c = copy.deepcopy(a)\n>>> c[2][0] = 10\n>>> c\narray([1, 'm', list([10, 3, 4])], dtype=object)\n>>> a\narray([1, 'm', list([2, 3, 4])], dtype=object)", "short_docstring": "Return an array copy of the given object."}
{"name": "statistics.mean", "type": "callable", "signature": "(data)", "docstring": "Return the sample arithmetic mean of data.\n\n>>> mean([1, 2, 3, 4, 4])\n2.8\n\n>>> from fractions import Fraction as F\n>>> mean([F(3, 7), F(1, 21), F(5, 3), F(1, 3)])\nFraction(13, 21)\n\n>>> from decimal import Decimal as D\n>>> mean([D(\"0.5\"), D(\"0.75\"), D(\"0.625\"), D(\"0.375\")])\nDecimal('0.5625')\n\nIf ``data`` is empty, StatisticsError will be raised.", "short_docstring": "Return the sample arithmetic mean of data."}
{"name": "matplotlib.pyplot.gca", "type": "callable", "signature": "() -> 'Axes'", "docstring": "Get the current Axes.\n\nIf there is currently no Axes on this Figure, a new one is created\nusing `.Figure.add_subplot`.  (To test whether there is currently an\nAxes on a Figure, check whether ``figure.axes`` is empty.  To test\nwhether there is currently a Figure on the pyplot figure stack, check\nwhether `.pyplot.get_fignums()` is empty.)", "short_docstring": "Get the current Axes."}
{"name": "pandas.to_numeric", "type": "callable", "signature": "(arg, errors: 'DateTimeErrorChoices' = 'raise', downcast: \"Literal['integer', 'signed', 'unsigned', 'float'] | None\" = None, dtype_backend: 'DtypeBackend | lib.NoDefault' = <no_default>)", "docstring": "Convert argument to a numeric type.\n\nThe default return dtype is `float64` or `int64`\ndepending on the data supplied. Use the `downcast` parameter\nto obtain other dtypes.\n\nPlease note that precision loss may occur if really large numbers\nare passed in. Due to the internal limitations of `ndarray`, if\nnumbers smaller than `-9223372036854775808` (np.iinfo(np.int64).min)\nor larger than `18446744073709551615` (np.iinfo(np.uint64).max) are\npassed in, it is very likely they will be converted to float so that\nthey can be stored in an `ndarray`. These warnings apply similarly to\n`Series` since it internally leverages `ndarray`.\n\nParameters\n----------\narg : scalar, list, tuple, 1-d array, or Series\n    Argument to be converted.\nerrors : {'ignore', 'raise', 'coerce'}, default 'raise'\n    - If 'raise', then invalid parsing will raise an exception.\n    - If 'coerce', then invalid parsing will be set as NaN.\n    - If 'ignore', then invalid parsing will return the input.\n\n    .. versionchanged:: 2.2\n\n    \"ignore\" is deprecated. Catch exceptions explicitly instead.\n\ndowncast : str, default None\n    Can be 'integer', 'signed', 'unsigned', or 'float'.\n    If not None, and if the data has been successfully cast to a\n    numerical dtype (or if the data was numeric to begin with),\n    downcast that resulting data to the smallest numerical dtype\n    possible according to the following rules:\n\n    - 'integer' or 'signed': smallest signed int dtype (min.: np.int8)\n    - 'unsigned': smallest unsigned int dtype (min.: np.uint8)\n    - 'float': smallest float dtype (min.: np.float32)\n\n    As this behaviour is separate from the core conversion to\n    numeric values, any errors raised during the downcasting\n    will be surfaced regardless of the value of the 'errors' input.\n\n    In addition, downcasting will only occur if the size\n    of the resulting data's dtype is strictly larger than\n    the dtype it is to be cast to, so if none of the dtypes\n    checked satisfy that specification, no downcasting will be\n    performed on the data.\ndtype_backend : {'numpy_nullable', 'pyarrow'}, default 'numpy_nullable'\n    Back-end data type applied to the resultant :class:`DataFrame`\n    (still experimental). Behaviour is as follows:\n\n    * ``\"numpy_nullable\"``: returns nullable-dtype-backed :class:`DataFrame`\n      (default).\n    * ``\"pyarrow\"``: returns pyarrow-backed nullable :class:`ArrowDtype`\n      DataFrame.\n\n    .. versionadded:: 2.0\n\nReturns\n-------\nret\n    Numeric if parsing succeeded.\n    Return type depends on input.  Series if Series, otherwise ndarray.\n\nSee Also\n--------\nDataFrame.astype : Cast argument to a specified dtype.\nto_datetime : Convert argument to datetime.\nto_timedelta : Convert argument to timedelta.\nnumpy.ndarray.astype : Cast a numpy array to a specified type.\nDataFrame.convert_dtypes : Convert dtypes.\n\nExamples\n--------\nTake separate series and convert to numeric, coercing when told to\n\n>>> s = pd.Series(['1.0', '2', -3])\n>>> pd.to_numeric(s)\n0    1.0\n1    2.0\n2   -3.0\ndtype: float64\n>>> pd.to_numeric(s, downcast='float')\n0    1.0\n1    2.0\n2   -3.0\ndtype: float32\n>>> pd.to_numeric(s, downcast='signed')\n0    1\n1    2\n2   -3\ndtype: int8\n>>> s = pd.Series(['apple', '1.0', '2', -3])\n>>> pd.to_numeric(s, errors='coerce')\n0    NaN\n1    1.0\n2    2.0\n3   -3.0\ndtype: float64\n\nDowncasting of nullable integer and floating dtypes is supported:\n\n>>> s = pd.Series([1, 2, 3], dtype=\"Int64\")\n>>> pd.to_numeric(s, downcast=\"integer\")\n0    1\n1    2\n2    3\ndtype: Int8\n>>> s = pd.Series([1.0, 2.1, 3.0], dtype=\"Float64\")\n>>> pd.to_numeric(s, downcast=\"float\")\n0    1.0\n1    2.1\n2    3.0\ndtype: Float32", "short_docstring": "Convert argument to a numeric type."}
{"name": "sklearn.decomposition.NMF", "type": "class", "signature": "(n_components='warn', *, init=None, solver='cd', beta_loss='frobenius', tol=0.0001, max_iter=200, random_state=None, alpha_W=0.0, alpha_H='same', l1_ratio=0.0, verbose=0, shuffle=False)", "docstring": "Non-Negative Matrix Factorization (NMF).\n\nFind two non-negative matrices, i.e. matrices with all non-negative elements, (W, H)\nwhose product approximates the non-negative matrix X. This factorization can be used\nfor example for dimensionality reduction, source separation or topic extraction.\n\nThe objective function is:\n\n    .. math::\n\n        L(W, H) &= 0.5 * ||X - WH||_{loss}^2\n\n        &+ alpha\\_W * l1\\_ratio * n\\_features * ||vec(W)||_1\n\n        &+ alpha\\_H * l1\\_ratio * n\\_samples * ||vec(H)||_1\n\n        &+ 0.5 * alpha\\_W * (1 - l1\\_ratio) * n\\_features * ||W||_{Fro}^2\n\n        &+ 0.5 * alpha\\_H * (1 - l1\\_ratio) * n\\_samples * ||H||_{Fro}^2\n\nWhere:\n\n:math:`||A||_{Fro}^2 = \\sum_{i,j} A_{ij}^2` (Frobenius norm)\n\n:math:`||vec(A)||_1 = \\sum_{i,j} abs(A_{ij})` (Elementwise L1 norm)\n\nThe generic norm :math:`||X - WH||_{loss}` may represent\nthe Frobenius norm or another supported beta-divergence loss.\nThe choice between options is controlled by the `beta_loss` parameter.\n\nThe regularization terms are scaled by `n_features` for `W` and by `n_samples` for\n`H` to keep their impact balanced with respect to one another and to the data fit\nterm as independent as possible of the size `n_samples` of the training set.\n\nThe objective function is minimized with an alternating minimization of W\nand H.\n\nNote that the transformed data is named W and the components matrix is named H. In\nthe NMF literature, the naming convention is usually the opposite since the data\nmatrix X is transposed.\n\nRead more in the :ref:`User Guide <NMF>`.\n\nParameters\n----------\nn_components : int or {'auto'} or None, default=None\n    Number of components, if n_components is not set all features\n    are kept.\n    If `n_components='auto'`, the number of components is automatically inferred\n    from W or H shapes.\n\n    .. versionchanged:: 1.4\n        Added `'auto'` value.\n\ninit : {'random', 'nndsvd', 'nndsvda', 'nndsvdar', 'custom'}, default=None\n    Method used to initialize the procedure.\n    Valid options:\n\n    - `None`: 'nndsvda' if n_components <= min(n_samples, n_features),\n      otherwise random.\n\n    - `'random'`: non-negative random matrices, scaled with:\n      `sqrt(X.mean() / n_components)`\n\n    - `'nndsvd'`: Nonnegative Double Singular Value Decomposition (NNDSVD)\n      initialization (better for sparseness)\n\n    - `'nndsvda'`: NNDSVD with zeros filled with the average of X\n      (better when sparsity is not desired)\n\n    - `'nndsvdar'` NNDSVD with zeros filled with small random values\n      (generally faster, less accurate alternative to NNDSVDa\n      for when sparsity is not desired)\n\n    - `'custom'`: Use custom matrices `W` and `H` which must both be provided.\n\n    .. versionchanged:: 1.1\n        When `init=None` and n_components is less than n_samples and n_features\n        defaults to `nndsvda` instead of `nndsvd`.\n\nsolver : {'cd', 'mu'}, default='cd'\n    Numerical solver to use:\n\n    - 'cd' is a Coordinate Descent solver.\n    - 'mu' is a Multiplicative Update solver.\n\n    .. versionadded:: 0.17\n       Coordinate Descent solver.\n\n    .. versionadded:: 0.19\n       Multiplicative Update solver.\n\nbeta_loss : float or {'frobenius', 'kullback-leibler',             'itakura-saito'}, default='frobenius'\n    Beta divergence to be minimized, measuring the distance between X\n    and the dot product WH. Note that values different from 'frobenius'\n    (or 2) and 'kullback-leibler' (or 1) lead to significantly slower\n    fits. Note that for beta_loss <= 0 (or 'itakura-saito'), the input\n    matrix X cannot contain zeros. Used only in 'mu' solver.\n\n    .. versionadded:: 0.19\n\ntol : float, default=1e-4\n    Tolerance of the stopping condition.\n\nmax_iter : int, default=200\n    Maximum number of iterations before timing out.\n\nrandom_state : int, RandomState instance or None, default=None\n    Used for initialisation (when ``init`` == 'nndsvdar' or\n    'random'), and in Coordinate Descent. Pass an int for reproducible\n    results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nalpha_W : float, default=0.0\n    Constant that multiplies the regularization terms of `W`. Set it to zero\n    (default) to have no regularization on `W`.\n\n    .. versionadded:: 1.0\n\nalpha_H : float or \"same\", default=\"same\"\n    Constant that multiplies the regularization terms of `H`. Set it to zero to\n    have no regularization on `H`. If \"same\" (default), it takes the same value as\n    `alpha_W`.\n\n    .. versionadded:: 1.0\n\nl1_ratio : float, default=0.0\n    The regularization mixing parameter, with 0 <= l1_ratio <= 1.\n    For l1_ratio = 0 the penalty is an elementwise L2 penalty\n    (aka Frobenius Norm).\n    For l1_ratio = 1 it is an elementwise L1 penalty.\n    For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2.\n\n    .. versionadded:: 0.17\n       Regularization parameter *l1_ratio* used in the Coordinate Descent\n       solver.\n\nverbose : int, default=0\n    Whether to be verbose.\n\nshuffle : bool, default=False\n    If true, randomize the order of coordinates in the CD solver.\n\n    .. versionadded:: 0.17\n       *shuffle* parameter used in the Coordinate Descent solver.\n\nAttributes\n----------\ncomponents_ : ndarray of shape (n_components, n_features)\n    Factorization matrix, sometimes called 'dictionary'.\n\nn_components_ : int\n    The number of components. It is same as the `n_components` parameter\n    if it was given. Otherwise, it will be same as the number of\n    features.\n\nreconstruction_err_ : float\n    Frobenius norm of the matrix difference, or beta-divergence, between\n    the training data ``X`` and the reconstructed data ``WH`` from\n    the fitted model.\n\nn_iter_ : int\n    Actual number of iterations.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nDictionaryLearning : Find a dictionary that sparsely encodes data.\nMiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\nPCA : Principal component analysis.\nSparseCoder : Find a sparse representation of data from a fixed,\n    precomputed dictionary.\nSparsePCA : Sparse Principal Components Analysis.\nTruncatedSVD : Dimensionality reduction using truncated SVD.\n\nReferences\n----------\n.. [1] :doi:`\"Fast local algorithms for large scale nonnegative matrix and tensor\n   factorizations\" <10.1587/transfun.E92.A.708>`\n   Cichocki, Andrzej, and P. H. A. N. Anh-Huy. IEICE transactions on fundamentals\n   of electronics, communications and computer sciences 92.3: 708-721, 2009.\n\n.. [2] :doi:`\"Algorithms for nonnegative matrix factorization with the\n   beta-divergence\" <10.1162/NECO_a_00168>`\n   Fevotte, C., & Idier, J. (2011). Neural Computation, 23(9).\n\nExamples\n--------\n>>> import numpy as np\n>>> X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n>>> from sklearn.decomposition import NMF\n>>> model = NMF(n_components=2, init='random', random_state=0)\n>>> W = model.fit_transform(X)\n>>> H = model.components_", "short_docstring": "Non-Negative Matrix Factorization (NMF)."}
{"name": "warnings.simplefilter", "type": "callable", "signature": "(action, category=<class 'Warning'>, lineno=0, append=False)", "docstring": "Insert a simple entry into the list of warnings filters (at the front).\n\nA simple filter matches all modules and messages.\n'action' -- one of \"error\", \"ignore\", \"always\", \"default\", \"module\",\n            or \"once\"\n'category' -- a class that the warning must be a subclass of\n'lineno' -- an integer line number, 0 matches all warnings\n'append' -- if true, append to the list of filters", "short_docstring": "Insert a simple entry into the list of warnings filters (at the front)."}
{"name": "os.makedirs", "type": "callable", "signature": "(name, mode=511, exist_ok=False)", "docstring": "makedirs(name [, mode=0o777][, exist_ok=False])\n\nSuper-mkdir; create a leaf directory and all intermediate ones.  Works like\nmkdir, except that any intermediate path segment (not just the rightmost)\nwill be created if it does not exist. If the target directory already\nexists, raise an OSError if exist_ok is False. Otherwise no exception is\nraised.  This is recursive.", "short_docstring": "makedirs(name [, mode=0o777][, exist_ok=False])"}
{"name": "pandas.api.types.is_numeric_dtype", "type": "callable", "signature": "(arr_or_dtype) -> 'bool'", "docstring": "Check whether the provided array or dtype is of a numeric dtype.\n\nParameters\n----------\narr_or_dtype : array-like or dtype\n    The array or dtype to check.\n\nReturns\n-------\nboolean\n    Whether or not the array or dtype is of a numeric dtype.\n\nExamples\n--------\n>>> from pandas.api.types import is_numeric_dtype\n>>> is_numeric_dtype(str)\nFalse\n>>> is_numeric_dtype(int)\nTrue\n>>> is_numeric_dtype(float)\nTrue\n>>> is_numeric_dtype(np.uint64)\nTrue\n>>> is_numeric_dtype(np.datetime64)\nFalse\n>>> is_numeric_dtype(np.timedelta64)\nFalse\n>>> is_numeric_dtype(np.array(['a', 'b']))\nFalse\n>>> is_numeric_dtype(pd.Series([1, 2]))\nTrue\n>>> is_numeric_dtype(pd.Index([1, 2.]))\nTrue\n>>> is_numeric_dtype(np.array([], dtype=np.timedelta64))\nFalse", "short_docstring": "Check whether the provided array or dtype is of a numeric dtype."}
{"name": "numpy.histogram_bin_edges", "type": "callable", "signature": "(a, bins=10, range=None, weights=None)", "docstring": "Function to calculate only the edges of the bins used by the `histogram`\nfunction.\n\nParameters\n----------\na : array_like\n    Input data. The histogram is computed over the flattened array.\nbins : int or sequence of scalars or str, optional\n    If `bins` is an int, it defines the number of equal-width\n    bins in the given range (10, by default). If `bins` is a\n    sequence, it defines the bin edges, including the rightmost\n    edge, allowing for non-uniform bin widths.\n\n    If `bins` is a string from the list below, `histogram_bin_edges` will use\n    the method chosen to calculate the optimal bin width and\n    consequently the number of bins (see `Notes` for more detail on\n    the estimators) from the data that falls within the requested\n    range. While the bin width will be optimal for the actual data\n    in the range, the number of bins will be computed to fill the\n    entire range, including the empty portions. For visualisation,\n    using the 'auto' option is suggested. Weighted data is not\n    supported for automated bin size selection.\n\n    'auto'\n        Maximum of the 'sturges' and 'fd' estimators. Provides good\n        all around performance.\n\n    'fd' (Freedman Diaconis Estimator)\n        Robust (resilient to outliers) estimator that takes into\n        account data variability and data size.\n\n    'doane'\n        An improved version of Sturges' estimator that works better\n        with non-normal datasets.\n\n    'scott'\n        Less robust estimator that takes into account data variability\n        and data size.\n\n    'stone'\n        Estimator based on leave-one-out cross-validation estimate of\n        the integrated squared error. Can be regarded as a generalization\n        of Scott's rule.\n\n    'rice'\n        Estimator does not take variability into account, only data\n        size. Commonly overestimates number of bins required.\n\n    'sturges'\n        R's default method, only accounts for data size. Only\n        optimal for gaussian data and underestimates number of bins\n        for large non-gaussian datasets.\n\n    'sqrt'\n        Square root (of data size) estimator, used by Excel and\n        other programs for its speed and simplicity.\n\nrange : (float, float), optional\n    The lower and upper range of the bins.  If not provided, range\n    is simply ``(a.min(), a.max())``.  Values outside the range are\n    ignored. The first element of the range must be less than or\n    equal to the second. `range` affects the automatic bin\n    computation as well. While bin width is computed to be optimal\n    based on the actual data within `range`, the bin count will fill\n    the entire range including portions containing no data.\n\nweights : array_like, optional\n    An array of weights, of the same shape as `a`.  Each value in\n    `a` only contributes its associated weight towards the bin count\n    (instead of 1). This is currently not used by any of the bin estimators,\n    but may be in the future.\n\nReturns\n-------\nbin_edges : array of dtype float\n    The edges to pass into `histogram`\n\nSee Also\n--------\nhistogram\n\nNotes\n-----\nThe methods to estimate the optimal number of bins are well founded\nin literature, and are inspired by the choices R provides for\nhistogram visualisation. Note that having the number of bins\nproportional to :math:`n^{1/3}` is asymptotically optimal, which is\nwhy it appears in most estimators. These are simply plug-in methods\nthat give good starting points for number of bins. In the equations\nbelow, :math:`h` is the binwidth and :math:`n_h` is the number of\nbins. All estimators that compute bin counts are recast to bin width\nusing the `ptp` of the data. The final bin count is obtained from\n``np.round(np.ceil(range / h))``. The final bin width is often less\nthan what is returned by the estimators below.\n\n'auto' (maximum of the 'sturges' and 'fd' estimators)\n    A compromise to get a good value. For small datasets the Sturges\n    value will usually be chosen, while larger datasets will usually\n    default to FD.  Avoids the overly conservative behaviour of FD\n    and Sturges for small and large datasets respectively.\n    Switchover point is usually :math:`a.size \\approx 1000`.\n\n'fd' (Freedman Diaconis Estimator)\n    .. math:: h = 2 \\frac{IQR}{n^{1/3}}\n\n    The binwidth is proportional to the interquartile range (IQR)\n    and inversely proportional to cube root of a.size. Can be too\n    conservative for small datasets, but is quite good for large\n    datasets. The IQR is very robust to outliers.\n\n'scott'\n    .. math:: h = \\sigma \\sqrt[3]{\\frac{24 \\sqrt{\\pi}}{n}}\n\n    The binwidth is proportional to the standard deviation of the\n    data and inversely proportional to cube root of ``x.size``. Can\n    be too conservative for small datasets, but is quite good for\n    large datasets. The standard deviation is not very robust to\n    outliers. Values are very similar to the Freedman-Diaconis\n    estimator in the absence of outliers.\n\n'rice'\n    .. math:: n_h = 2n^{1/3}\n\n    The number of bins is only proportional to cube root of\n    ``a.size``. It tends to overestimate the number of bins and it\n    does not take into account data variability.\n\n'sturges'\n    .. math:: n_h = \\log _{2}(n) + 1\n\n    The number of bins is the base 2 log of ``a.size``.  This\n    estimator assumes normality of data and is too conservative for\n    larger, non-normal datasets. This is the default method in R's\n    ``hist`` method.\n\n'doane'\n    .. math:: n_h = 1 + \\log_{2}(n) +\n                    \\log_{2}\\left(1 + \\frac{|g_1|}{\\sigma_{g_1}}\\right)\n\n        g_1 = mean\\left[\\left(\\frac{x - \\mu}{\\sigma}\\right)^3\\right]\n\n        \\sigma_{g_1} = \\sqrt{\\frac{6(n - 2)}{(n + 1)(n + 3)}}\n\n    An improved version of Sturges' formula that produces better\n    estimates for non-normal datasets. This estimator attempts to\n    account for the skew of the data.\n\n'sqrt'\n    .. math:: n_h = \\sqrt n\n\n    The simplest and fastest estimator. Only takes into account the\n    data size.\n\nExamples\n--------\n>>> arr = np.array([0, 0, 0, 1, 2, 3, 3, 4, 5])\n>>> np.histogram_bin_edges(arr, bins='auto', range=(0, 1))\narray([0.  , 0.25, 0.5 , 0.75, 1.  ])\n>>> np.histogram_bin_edges(arr, bins=2)\narray([0. , 2.5, 5. ])\n\nFor consistency with histogram, an array of pre-computed bins is\npassed through unmodified:\n\n>>> np.histogram_bin_edges(arr, [1, 2])\narray([1, 2])\n\nThis function allows one set of bins to be computed, and reused across\nmultiple histograms:\n\n>>> shared_bins = np.histogram_bin_edges(arr, bins='auto')\n>>> shared_bins\narray([0., 1., 2., 3., 4., 5.])\n\n>>> group_id = np.array([0, 1, 1, 0, 1, 1, 0, 1, 1])\n>>> hist_0, _ = np.histogram(arr[group_id == 0], bins=shared_bins)\n>>> hist_1, _ = np.histogram(arr[group_id == 1], bins=shared_bins)\n\n>>> hist_0; hist_1\narray([1, 1, 0, 1, 0])\narray([2, 0, 1, 1, 2])\n\nWhich gives more easily comparable results than using separate bins for\neach histogram:\n\n>>> hist_0, bins_0 = np.histogram(arr[group_id == 0], bins='auto')\n>>> hist_1, bins_1 = np.histogram(arr[group_id == 1], bins='auto')\n>>> hist_0; hist_1\narray([1, 1, 1])\narray([2, 1, 1, 2])\n>>> bins_0; bins_1\narray([0., 1., 2., 3.])\narray([0.  , 1.25, 2.5 , 3.75, 5.  ])", "short_docstring": "Function to calculate only the edges of the bins used by the `histogram`\nfunction."}
{"name": "chardet.detect", "type": "callable", "signature": "(byte_str: Union[bytes, bytearray], should_rename_legacy: bool = False) -> dict", "docstring": "Detect the encoding of the given byte string.\n\n:param byte_str:     The byte sequence to examine.\n:type byte_str:      ``bytes`` or ``bytearray``\n:param should_rename_legacy:  Should we rename legacy encodings\n                              to their more modern equivalents?\n:type should_rename_legacy:   ``bool``", "short_docstring": "Detect the encoding of the given byte string."}
{"name": "sklearn.preprocessing.MinMaxScaler", "type": "class", "signature": "(feature_range=(0, 1), *, copy=True, clip=False)", "docstring": "Transform features by scaling each feature to a given range.\n\nThis estimator scales and translates each feature individually such\nthat it is in the given range on the training set, e.g. between\nzero and one.\n\nThe transformation is given by::\n\n    X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n    X_scaled = X_std * (max - min) + min\n\nwhere min, max = feature_range.\n\nThis transformation is often used as an alternative to zero mean,\nunit variance scaling.\n\n`MinMaxScaler` doesn't reduce the effect of outliers, but it linearly\nscales them down into a fixed range, where the largest occurring data point\ncorresponds to the maximum value and the smallest one corresponds to the\nminimum value. For an example visualization, refer to :ref:`Compare\nMinMaxScaler with other scalers <plot_all_scaling_minmax_scaler_section>`.\n\nRead more in the :ref:`User Guide <preprocessing_scaler>`.\n\nParameters\n----------\nfeature_range : tuple (min, max), default=(0, 1)\n    Desired range of transformed data.\n\ncopy : bool, default=True\n    Set to False to perform inplace row normalization and avoid a\n    copy (if the input is already a numpy array).\n\nclip : bool, default=False\n    Set to True to clip transformed values of held-out data to\n    provided `feature range`.\n\n    .. versionadded:: 0.24\n\nAttributes\n----------\nmin_ : ndarray of shape (n_features,)\n    Per feature adjustment for minimum. Equivalent to\n    ``min - X.min(axis=0) * self.scale_``\n\nscale_ : ndarray of shape (n_features,)\n    Per feature relative scaling of the data. Equivalent to\n    ``(max - min) / (X.max(axis=0) - X.min(axis=0))``\n\n    .. versionadded:: 0.17\n       *scale_* attribute.\n\ndata_min_ : ndarray of shape (n_features,)\n    Per feature minimum seen in the data\n\n    .. versionadded:: 0.17\n       *data_min_*\n\ndata_max_ : ndarray of shape (n_features,)\n    Per feature maximum seen in the data\n\n    .. versionadded:: 0.17\n       *data_max_*\n\ndata_range_ : ndarray of shape (n_features,)\n    Per feature range ``(data_max_ - data_min_)`` seen in the data\n\n    .. versionadded:: 0.17\n       *data_range_*\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nn_samples_seen_ : int\n    The number of samples processed by the estimator.\n    It will be reset on new calls to fit, but increments across\n    ``partial_fit`` calls.\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nminmax_scale : Equivalent function without the estimator API.\n\nNotes\n-----\nNaNs are treated as missing values: disregarded in fit, and maintained in\ntransform.\n\nExamples\n--------\n>>> from sklearn.preprocessing import MinMaxScaler\n>>> data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n>>> scaler = MinMaxScaler()\n>>> print(scaler.fit(data))\nMinMaxScaler()\n>>> print(scaler.data_max_)\n[ 1. 18.]\n>>> print(scaler.transform(data))\n[[0.   0.  ]\n [0.25 0.25]\n [0.5  0.5 ]\n [1.   1.  ]]\n>>> print(scaler.transform([[2, 2]]))\n[[1.5 0. ]]", "short_docstring": "Transform features by scaling each feature to a given range."}
{"name": "tarfile.open", "type": "callable", "signature": "(name=None, mode='r', fileobj=None, bufsize=10240, **kwargs)", "docstring": "Open a tar archive for reading, writing or appending. Return\nan appropriate TarFile class.\n\nmode:\n'r' or 'r:*' open for reading with transparent compression\n'r:'         open for reading exclusively uncompressed\n'r:gz'       open for reading with gzip compression\n'r:bz2'      open for reading with bzip2 compression\n'r:xz'       open for reading with lzma compression\n'a' or 'a:'  open for appending, creating the file if necessary\n'w' or 'w:'  open for writing without compression\n'w:gz'       open for writing with gzip compression\n'w:bz2'      open for writing with bzip2 compression\n'w:xz'       open for writing with lzma compression\n\n'x' or 'x:'  create a tarfile exclusively without compression, raise\n             an exception if the file is already created\n'x:gz'       create a gzip compressed tarfile, raise an exception\n             if the file is already created\n'x:bz2'      create a bzip2 compressed tarfile, raise an exception\n             if the file is already created\n'x:xz'       create an lzma compressed tarfile, raise an exception\n             if the file is already created\n\n'r|*'        open a stream of tar blocks with transparent compression\n'r|'         open an uncompressed stream of tar blocks for reading\n'r|gz'       open a gzip compressed stream of tar blocks\n'r|bz2'      open a bzip2 compressed stream of tar blocks\n'r|xz'       open an lzma compressed stream of tar blocks\n'w|'         open an uncompressed stream for writing\n'w|gz'       open a gzip compressed stream for writing\n'w|bz2'      open a bzip2 compressed stream for writing\n'w|xz'       open an lzma compressed stream for writing", "short_docstring": "Open a tar archive for reading, writing or appending. Return\nan appropriate TarFile class."}
{"name": "numpy.sin", "type": "callable", "signature": null, "docstring": "sin(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n\nTrigonometric sine, element-wise.\n\nParameters\n----------\nx : array_like\n    Angle, in radians (:math:`2 \\pi` rad equals 360 degrees).\nout : ndarray, None, or tuple of ndarray and None, optional\n    A location into which the result is stored. If provided, it must have\n    a shape that the inputs broadcast to. If not provided or None,\n    a freshly-allocated array is returned. A tuple (possible only as a\n    keyword argument) must have length equal to the number of outputs.\nwhere : array_like, optional\n    This condition is broadcast over the input. At locations where the\n    condition is True, the `out` array will be set to the ufunc result.\n    Elsewhere, the `out` array will retain its original value.\n    Note that if an uninitialized `out` array is created via the default\n    ``out=None``, locations within it where the condition is False will\n    remain uninitialized.\n**kwargs\n    For other keyword-only arguments, see the\n    :ref:`ufunc docs <ufuncs.kwargs>`.\n\nReturns\n-------\ny : array_like\n    The sine of each element of x.\n    This is a scalar if `x` is a scalar.\n\nSee Also\n--------\narcsin, sinh, cos\n\nNotes\n-----\nThe sine is one of the fundamental functions of trigonometry (the\nmathematical study of triangles).  Consider a circle of radius 1\ncentered on the origin.  A ray comes in from the :math:`+x` axis, makes\nan angle at the origin (measured counter-clockwise from that axis), and\ndeparts from the origin.  The :math:`y` coordinate of the outgoing\nray's intersection with the unit circle is the sine of that angle.  It\nranges from -1 for :math:`x=3\\pi / 2` to +1 for :math:`\\pi / 2.`  The\nfunction has zeroes where the angle is a multiple of :math:`\\pi`.\nSines of angles between :math:`\\pi` and :math:`2\\pi` are negative.\nThe numerous properties of the sine and related functions are included\nin any standard trigonometry text.\n\nExamples\n--------\nPrint sine of one angle:\n\n>>> np.sin(np.pi/2.)\n1.0\n\nPrint sines of an array of angles given in degrees:\n\n>>> np.sin(np.array((0., 30., 45., 60., 90.)) * np.pi / 180. )\narray([ 0.        ,  0.5       ,  0.70710678,  0.8660254 ,  1.        ])\n\nPlot the sine function:\n\n>>> import matplotlib.pylab as plt\n>>> x = np.linspace(-np.pi, np.pi, 201)\n>>> plt.plot(x, np.sin(x))\n>>> plt.xlabel('Angle [rad]')\n>>> plt.ylabel('sin(x)')\n>>> plt.axis('tight')\n>>> plt.show()", "short_docstring": "sin(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])"}
{"name": "numpy.hstack", "type": "callable", "signature": "(tup)", "docstring": "Stack arrays in sequence horizontally (column wise).\n\nThis is equivalent to concatenation along the second axis, except for 1-D\narrays where it concatenates along the first axis. Rebuilds arrays divided\nby `hsplit`.\n\nThis function makes most sense for arrays with up to 3 dimensions. For\ninstance, for pixel-data with a height (first axis), width (second axis),\nand r/g/b channels (third axis). The functions `concatenate`, `stack` and\n`block` provide more general stacking and concatenation operations.\n\nParameters\n----------\ntup : sequence of ndarrays\n    The arrays must have the same shape along all but the second axis,\n    except 1-D arrays which can be any length.\n\nReturns\n-------\nstacked : ndarray\n    The array formed by stacking the given arrays.\n\nSee Also\n--------\nconcatenate : Join a sequence of arrays along an existing axis.\nstack : Join a sequence of arrays along a new axis.\nblock : Assemble an nd-array from nested lists of blocks.\nvstack : Stack arrays in sequence vertically (row wise).\ndstack : Stack arrays in sequence depth wise (along third axis).\ncolumn_stack : Stack 1-D arrays as columns into a 2-D array.\nhsplit : Split an array into multiple sub-arrays horizontally (column-wise).\n\nExamples\n--------\n>>> a = np.array((1,2,3))\n>>> b = np.array((4,5,6))\n>>> np.hstack((a,b))\narray([1, 2, 3, 4, 5, 6])\n>>> a = np.array([[1],[2],[3]])\n>>> b = np.array([[4],[5],[6]])\n>>> np.hstack((a,b))\narray([[1, 4],\n       [2, 5],\n       [3, 6]])", "short_docstring": "Stack arrays in sequence horizontally (column wise)."}
{"name": "flask_mail.Mail", "type": "class", "signature": "(app=None)", "docstring": "Manages email messaging\n\n:param app: Flask instance", "short_docstring": "Manages email messaging"}
{"name": "os.path.isfile", "type": "callable", "signature": "(path)", "docstring": "Test whether a path is a regular file", "short_docstring": "Test whether a path is a regular file"}
{"name": "os.path.isdir", "type": "callable", "signature": "(s)", "docstring": "Return true if the pathname refers to an existing directory.", "short_docstring": "Return true if the pathname refers to an existing directory."}
{"name": "numpy.random.rand", "type": "callable", "signature": null, "docstring": "rand(d0, d1, ..., dn)\n\nRandom values in a given shape.\n\n.. note::\n    This is a convenience function for users porting code from Matlab,\n    and wraps `random_sample`. That function takes a\n    tuple to specify the size of the output, which is consistent with\n    other NumPy functions like `numpy.zeros` and `numpy.ones`.\n\nCreate an array of the given shape and populate it with\nrandom samples from a uniform distribution\nover ``[0, 1)``.\n\nParameters\n----------\nd0, d1, ..., dn : int, optional\n    The dimensions of the returned array, must be non-negative.\n    If no argument is given a single Python float is returned.\n\nReturns\n-------\nout : ndarray, shape ``(d0, d1, ..., dn)``\n    Random values.\n\nSee Also\n--------\nrandom\n\nExamples\n--------\n>>> np.random.rand(3,2)\narray([[ 0.14022471,  0.96360618],  #random\n       [ 0.37601032,  0.25528411],  #random\n       [ 0.49313049,  0.94909878]]) #random", "short_docstring": "rand(d0, d1, ..., dn)"}
{"name": "string.punctuation", "type": "constant", "value": "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~", "signature": null, "docstring": "str(object='') -> str\nstr(bytes_or_buffer[, encoding[, errors]]) -> str\n\nCreate a new string object from the given object. If encoding or\nerrors is specified, then the object must expose a data buffer\nthat will be decoded using the given encoding and error handler.\nOtherwise, returns the result of object.__str__() (if defined)\nor repr(object).\nencoding defaults to sys.getdefaultencoding().\nerrors defaults to 'strict'.", "short_docstring": "str(object='') -> str\nstr(bytes_or_buffer[, encoding[, errors]]) -> str"}
{"name": "itertools.permutations", "type": "class", "signature": "(iterable, r=None)", "docstring": "Return successive r-length permutations of elements in the iterable.\n\npermutations(range(3), 2) --> (0,1), (0,2), (1,0), (1,2), (2,0), (2,1)", "short_docstring": "Return successive r-length permutations of elements in the iterable."}
{"name": "shutil.copy", "type": "callable", "signature": "(src, dst, *, follow_symlinks=True)", "docstring": "Copy data and mode bits (\"cp src dst\"). Return the file's destination.\n\nThe destination may be a directory.\n\nIf follow_symlinks is false, symlinks won't be followed. This\nresembles GNU's \"cp -P src dst\".\n\nIf source and destination are the same file, a SameFileError will be\nraised.", "short_docstring": "Copy data and mode bits (\"cp src dst\"). Return the file's destination."}
{"name": "flask.redirect", "type": "callable", "signature": "(location: 'str', code: 'int' = 302, Response: 'type[BaseResponse] | None' = None) -> 'BaseResponse'", "docstring": "Create a redirect response object.\n\nIf :data:`~flask.current_app` is available, it will use its\n:meth:`~flask.Flask.redirect` method, otherwise it will use\n:func:`werkzeug.utils.redirect`.\n\n:param location: The URL to redirect to.\n:param code: The status code for the redirect.\n:param Response: The response class to use. Not used when\n    ``current_app`` is active, which uses ``app.response_class``.\n\n.. versionadded:: 2.2\n    Calls ``current_app.redirect`` if available instead of always\n    using Werkzeug's default ``redirect``.", "short_docstring": "Create a redirect response object."}
{"name": "json.load", "type": "callable", "signature": "(fp, *, cls=None, object_hook=None, parse_float=None, parse_int=None, parse_constant=None, object_pairs_hook=None, **kw)", "docstring": "Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\na JSON document) to a Python object.\n\n``object_hook`` is an optional function that will be called with the\nresult of any object literal decode (a ``dict``). The return value of\n``object_hook`` will be used instead of the ``dict``. This feature\ncan be used to implement custom decoders (e.g. JSON-RPC class hinting).\n\n``object_pairs_hook`` is an optional function that will be called with the\nresult of any object literal decoded with an ordered list of pairs.  The\nreturn value of ``object_pairs_hook`` will be used instead of the ``dict``.\nThis feature can be used to implement custom decoders.  If ``object_hook``\nis also defined, the ``object_pairs_hook`` takes priority.\n\nTo use a custom ``JSONDecoder`` subclass, specify it with the ``cls``\nkwarg; otherwise ``JSONDecoder`` is used.", "short_docstring": "Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\na JSON document) to a Python object."}
{"name": "sklearn.metrics.roc_curve", "type": "callable", "signature": "(y_true, y_score, *, pos_label=None, sample_weight=None, drop_intermediate=True)", "docstring": "Compute Receiver operating characteristic (ROC).\n\nNote: this implementation is restricted to the binary classification task.\n\nRead more in the :ref:`User Guide <roc_metrics>`.\n\nParameters\n----------\ny_true : array-like of shape (n_samples,)\n    True binary labels. If labels are not either {-1, 1} or {0, 1}, then\n    pos_label should be explicitly given.\n\ny_score : array-like of shape (n_samples,)\n    Target scores, can either be probability estimates of the positive\n    class, confidence values, or non-thresholded measure of decisions\n    (as returned by \"decision_function\" on some classifiers).\n\npos_label : int, float, bool or str, default=None\n    The label of the positive class.\n    When ``pos_label=None``, if `y_true` is in {-1, 1} or {0, 1},\n    ``pos_label`` is set to 1, otherwise an error will be raised.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\ndrop_intermediate : bool, default=True\n    Whether to drop some suboptimal thresholds which would not appear\n    on a plotted ROC curve. This is useful in order to create lighter\n    ROC curves.\n\n    .. versionadded:: 0.17\n       parameter *drop_intermediate*.\n\nReturns\n-------\nfpr : ndarray of shape (>2,)\n    Increasing false positive rates such that element i is the false\n    positive rate of predictions with score >= `thresholds[i]`.\n\ntpr : ndarray of shape (>2,)\n    Increasing true positive rates such that element `i` is the true\n    positive rate of predictions with score >= `thresholds[i]`.\n\nthresholds : ndarray of shape (n_thresholds,)\n    Decreasing thresholds on the decision function used to compute\n    fpr and tpr. `thresholds[0]` represents no instances being predicted\n    and is arbitrarily set to `np.inf`.\n\nSee Also\n--------\nRocCurveDisplay.from_estimator : Plot Receiver Operating Characteristic\n    (ROC) curve given an estimator and some data.\nRocCurveDisplay.from_predictions : Plot Receiver Operating Characteristic\n    (ROC) curve given the true and predicted values.\ndet_curve: Compute error rates for different probability thresholds.\nroc_auc_score : Compute the area under the ROC curve.\n\nNotes\n-----\nSince the thresholds are sorted from low to high values, they\nare reversed upon returning them to ensure they correspond to both ``fpr``\nand ``tpr``, which are sorted in reversed order during their calculation.\n\nAn arbitrary threshold is added for the case `tpr=0` and `fpr=0` to\nensure that the curve starts at `(0, 0)`. This threshold corresponds to the\n`np.inf`.\n\nReferences\n----------\n.. [1] `Wikipedia entry for the Receiver operating characteristic\n        <https://en.wikipedia.org/wiki/Receiver_operating_characteristic>`_\n\n.. [2] Fawcett T. An introduction to ROC analysis[J]. Pattern Recognition\n       Letters, 2006, 27(8):861-874.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn import metrics\n>>> y = np.array([1, 1, 2, 2])\n>>> scores = np.array([0.1, 0.4, 0.35, 0.8])\n>>> fpr, tpr, thresholds = metrics.roc_curve(y, scores, pos_label=2)\n>>> fpr\narray([0. , 0. , 0.5, 0.5, 1. ])\n>>> tpr\narray([0. , 0.5, 0.5, 1. , 1. ])\n>>> thresholds\narray([ inf, 0.8 , 0.4 , 0.35, 0.1 ])", "short_docstring": "Compute Receiver operating characteristic (ROC)."}
{"name": "xlwt.Workbook", "type": "class", "signature": "(encoding='ascii', style_compression=0)", "docstring": "This is a class representing a workbook and all its contents. When creating\nExcel files with xlwt, you will normally start by instantiating an\nobject of this class.", "short_docstring": "This is a class representing a workbook and all its contents. When creating\nExcel files with xlwt, you will normally start by instantiating an\nobject of this class."}
{"name": "requests.HTTPError", "type": "class", "signature": "(*args, **kwargs)", "docstring": "An HTTP error occurred.", "short_docstring": "An HTTP error occurred."}
{"name": "sklearn.datasets.load_iris", "type": "callable", "signature": "(*, return_X_y=False, as_frame=False)", "docstring": "Load and return the iris dataset (classification).\n\nThe iris dataset is a classic and very easy multi-class classification\ndataset.\n\n=================   ==============\nClasses                          3\nSamples per class               50\nSamples total                  150\nDimensionality                   4\nFeatures            real, positive\n=================   ==============\n\nRead more in the :ref:`User Guide <iris_dataset>`.\n\nParameters\n----------\nreturn_X_y : bool, default=False\n    If True, returns ``(data, target)`` instead of a Bunch object. See\n    below for more information about the `data` and `target` object.\n\n    .. versionadded:: 0.18\n\nas_frame : bool, default=False\n    If True, the data is a pandas DataFrame including columns with\n    appropriate dtypes (numeric). The target is\n    a pandas DataFrame or Series depending on the number of target columns.\n    If `return_X_y` is True, then (`data`, `target`) will be pandas\n    DataFrames or Series as described below.\n\n    .. versionadded:: 0.23\n\nReturns\n-------\ndata : :class:`~sklearn.utils.Bunch`\n    Dictionary-like object, with the following attributes.\n\n    data : {ndarray, dataframe} of shape (150, 4)\n        The data matrix. If `as_frame=True`, `data` will be a pandas\n        DataFrame.\n    target: {ndarray, Series} of shape (150,)\n        The classification target. If `as_frame=True`, `target` will be\n        a pandas Series.\n    feature_names: list\n        The names of the dataset columns.\n    target_names: list\n        The names of target classes.\n    frame: DataFrame of shape (150, 5)\n        Only present when `as_frame=True`. DataFrame with `data` and\n        `target`.\n\n        .. versionadded:: 0.23\n    DESCR: str\n        The full description of the dataset.\n    filename: str\n        The path to the location of the data.\n\n        .. versionadded:: 0.20\n\n(data, target) : tuple if ``return_X_y`` is True\n    A tuple of two ndarray. The first containing a 2D array of shape\n    (n_samples, n_features) with each row representing one sample and\n    each column representing the features. The second ndarray of shape\n    (n_samples,) containing the target samples.\n\n    .. versionadded:: 0.18\n\nNotes\n-----\n    .. versionchanged:: 0.20\n        Fixed two wrong data points according to Fisher's paper.\n        The new version is the same as in R, but not as in the UCI\n        Machine Learning Repository.\n\nExamples\n--------\nLet's say you are interested in the samples 10, 25, and 50, and want to\nknow their class name.\n\n>>> from sklearn.datasets import load_iris\n>>> data = load_iris()\n>>> data.target[[10, 25, 50]]\narray([0, 0, 1])\n>>> list(data.target_names)\n['setosa', 'versicolor', 'virginica']\n\nSee :ref:`sphx_glr_auto_examples_datasets_plot_iris_dataset.py` for a more\ndetailed example of how to work with the iris dataset.", "short_docstring": "Load and return the iris dataset (classification)."}
{"name": "shutil.move", "type": "callable", "signature": "(src, dst, copy_function=<function copy2 at 0x7ffa64d3eb90>)", "docstring": "Recursively move a file or directory to another location. This is\nsimilar to the Unix \"mv\" command. Return the file or directory's\ndestination.\n\nIf the destination is a directory or a symlink to a directory, the source\nis moved inside the directory. The destination path must not already\nexist.\n\nIf the destination already exists but is not a directory, it may be\noverwritten depending on os.rename() semantics.\n\nIf the destination is on our current filesystem, then rename() is used.\nOtherwise, src is copied to the destination and then removed. Symlinks are\nrecreated under the new name if os.rename() fails because of cross\nfilesystem renames.\n\nThe optional `copy_function` argument is a callable that will be used\nto copy the source or it will be delegated to `copytree`.\nBy default, copy2() is used, but any function that supports the same\nsignature (like copy()) can be used.\n\nA lot more could be done here...  A look at a mv.c shows a lot of\nthe issues this implementation glosses over.", "short_docstring": "Recursively move a file or directory to another location. This is\nsimilar to the Unix \"mv\" command. Return the file or directory's\ndestination."}
{"name": "pandas.read_json", "type": "callable", "signature": "(path_or_buf: 'FilePath | ReadBuffer[str] | ReadBuffer[bytes]', *, orient: 'str | None' = None, typ: \"Literal['frame', 'series']\" = 'frame', dtype: 'DtypeArg | None' = None, convert_axes: 'bool | None' = None, convert_dates: 'bool | list[str]' = True, keep_default_dates: 'bool' = True, precise_float: 'bool' = False, date_unit: 'str | None' = None, encoding: 'str | None' = None, encoding_errors: 'str | None' = 'strict', lines: 'bool' = False, chunksize: 'int | None' = None, compression: 'CompressionOptions' = 'infer', nrows: 'int | None' = None, storage_options: 'StorageOptions | None' = None, dtype_backend: 'DtypeBackend | lib.NoDefault' = <no_default>, engine: 'JSONEngine' = 'ujson') -> 'DataFrame | Series | JsonReader'", "docstring": "Convert a JSON string to pandas object.\n\nParameters\n----------\npath_or_buf : a valid JSON str, path object or file-like object\n    Any valid string path is acceptable. The string could be a URL. Valid\n    URL schemes include http, ftp, s3, and file. For file URLs, a host is\n    expected. A local file could be:\n    ``file://localhost/path/to/table.json``.\n\n    If you want to pass in a path object, pandas accepts any\n    ``os.PathLike``.\n\n    By file-like object, we refer to objects with a ``read()`` method,\n    such as a file handle (e.g. via builtin ``open`` function)\n    or ``StringIO``.\n\n    .. deprecated:: 2.1.0\n        Passing json literal strings is deprecated.\n\norient : str, optional\n    Indication of expected JSON string format.\n    Compatible JSON strings can be produced by ``to_json()`` with a\n    corresponding orient value.\n    The set of possible orients is:\n\n    - ``'split'`` : dict like\n      ``{index -> [index], columns -> [columns], data -> [values]}``\n    - ``'records'`` : list like\n      ``[{column -> value}, ... , {column -> value}]``\n    - ``'index'`` : dict like ``{index -> {column -> value}}``\n    - ``'columns'`` : dict like ``{column -> {index -> value}}``\n    - ``'values'`` : just the values array\n    - ``'table'`` : dict like ``{'schema': {schema}, 'data': {data}}``\n\n    The allowed and default values depend on the value\n    of the `typ` parameter.\n\n    * when ``typ == 'series'``,\n\n      - allowed orients are ``{'split','records','index'}``\n      - default is ``'index'``\n      - The Series index must be unique for orient ``'index'``.\n\n    * when ``typ == 'frame'``,\n\n      - allowed orients are ``{'split','records','index',\n        'columns','values', 'table'}``\n      - default is ``'columns'``\n      - The DataFrame index must be unique for orients ``'index'`` and\n        ``'columns'``.\n      - The DataFrame columns must be unique for orients ``'index'``,\n        ``'columns'``, and ``'records'``.\n\ntyp : {'frame', 'series'}, default 'frame'\n    The type of object to recover.\n\ndtype : bool or dict, default None\n    If True, infer dtypes; if a dict of column to dtype, then use those;\n    if False, then don't infer dtypes at all, applies only to the data.\n\n    For all ``orient`` values except ``'table'``, default is True.\n\nconvert_axes : bool, default None\n    Try to convert the axes to the proper dtypes.\n\n    For all ``orient`` values except ``'table'``, default is True.\n\nconvert_dates : bool or list of str, default True\n    If True then default datelike columns may be converted (depending on\n    keep_default_dates).\n    If False, no dates will be converted.\n    If a list of column names, then those columns will be converted and\n    default datelike columns may also be converted (depending on\n    keep_default_dates).\n\nkeep_default_dates : bool, default True\n    If parsing dates (convert_dates is not False), then try to parse the\n    default datelike columns.\n    A column label is datelike if\n\n    * it ends with ``'_at'``,\n\n    * it ends with ``'_time'``,\n\n    * it begins with ``'timestamp'``,\n\n    * it is ``'modified'``, or\n\n    * it is ``'date'``.\n\nprecise_float : bool, default False\n    Set to enable usage of higher precision (strtod) function when\n    decoding string to double values. Default (False) is to use fast but\n    less precise builtin functionality.\n\ndate_unit : str, default None\n    The timestamp unit to detect if converting dates. The default behaviour\n    is to try and detect the correct precision, but if this is not desired\n    then pass one of 's', 'ms', 'us' or 'ns' to force parsing only seconds,\n    milliseconds, microseconds or nanoseconds respectively.\n\nencoding : str, default is 'utf-8'\n    The encoding to use to decode py3 bytes.\n\nencoding_errors : str, optional, default \"strict\"\n    How encoding errors are treated. `List of possible values\n    <https://docs.python.org/3/library/codecs.html#error-handlers>`_ .\n\n    .. versionadded:: 1.3.0\n\nlines : bool, default False\n    Read the file as a json object per line.\n\nchunksize : int, optional\n    Return JsonReader object for iteration.\n    See the `line-delimited json docs\n    <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#line-delimited-json>`_\n    for more information on ``chunksize``.\n    This can only be passed if `lines=True`.\n    If this is None, the file will be read into memory all at once.\ncompression : str or dict, default 'infer'\n    For on-the-fly decompression of on-disk data. If 'infer' and 'path_or_buf' is\n    path-like, then detect compression from the following extensions: '.gz',\n    '.bz2', '.zip', '.xz', '.zst', '.tar', '.tar.gz', '.tar.xz' or '.tar.bz2'\n    (otherwise no compression).\n    If using 'zip' or 'tar', the ZIP file must contain only one data file to be read in.\n    Set to ``None`` for no decompression.\n    Can also be a dict with key ``'method'`` set\n    to one of {``'zip'``, ``'gzip'``, ``'bz2'``, ``'zstd'``, ``'xz'``, ``'tar'``} and\n    other key-value pairs are forwarded to\n    ``zipfile.ZipFile``, ``gzip.GzipFile``,\n    ``bz2.BZ2File``, ``zstandard.ZstdDecompressor``, ``lzma.LZMAFile`` or\n    ``tarfile.TarFile``, respectively.\n    As an example, the following could be passed for Zstandard decompression using a\n    custom compression dictionary:\n    ``compression={'method': 'zstd', 'dict_data': my_compression_dict}``.\n\n    .. versionadded:: 1.5.0\n        Added support for `.tar` files.\n\n    .. versionchanged:: 1.4.0 Zstandard support.\n\nnrows : int, optional\n    The number of lines from the line-delimited jsonfile that has to be read.\n    This can only be passed if `lines=True`.\n    If this is None, all the rows will be returned.\n\nstorage_options : dict, optional\n    Extra options that make sense for a particular storage connection, e.g.\n    host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n    are forwarded to ``urllib.request.Request`` as header options. For other\n    URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n    forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n    details, and for more examples on storage options refer `here\n    <https://pandas.pydata.org/docs/user_guide/io.html?\n    highlight=storage_options#reading-writing-remote-files>`_.\n\ndtype_backend : {'numpy_nullable', 'pyarrow'}, default 'numpy_nullable'\n    Back-end data type applied to the resultant :class:`DataFrame`\n    (still experimental). Behaviour is as follows:\n\n    * ``\"numpy_nullable\"``: returns nullable-dtype-backed :class:`DataFrame`\n      (default).\n    * ``\"pyarrow\"``: returns pyarrow-backed nullable :class:`ArrowDtype`\n      DataFrame.\n\n    .. versionadded:: 2.0\n\nengine : {\"ujson\", \"pyarrow\"}, default \"ujson\"\n    Parser engine to use. The ``\"pyarrow\"`` engine is only available when\n    ``lines=True``.\n\n    .. versionadded:: 2.0\n\nReturns\n-------\nSeries, DataFrame, or pandas.api.typing.JsonReader\n    A JsonReader is returned when ``chunksize`` is not ``0`` or ``None``.\n    Otherwise, the type returned depends on the value of ``typ``.\n\nSee Also\n--------\nDataFrame.to_json : Convert a DataFrame to a JSON string.\nSeries.to_json : Convert a Series to a JSON string.\njson_normalize : Normalize semi-structured JSON data into a flat table.\n\nNotes\n-----\nSpecific to ``orient='table'``, if a :class:`DataFrame` with a literal\n:class:`Index` name of `index` gets written with :func:`to_json`, the\nsubsequent read operation will incorrectly set the :class:`Index` name to\n``None``. This is because `index` is also used by :func:`DataFrame.to_json`\nto denote a missing :class:`Index` name, and the subsequent\n:func:`read_json` operation cannot distinguish between the two. The same\nlimitation is encountered with a :class:`MultiIndex` and any names\nbeginning with ``'level_'``.\n\nExamples\n--------\n>>> from io import StringIO\n>>> df = pd.DataFrame([['a', 'b'], ['c', 'd']],\n...                   index=['row 1', 'row 2'],\n...                   columns=['col 1', 'col 2'])\n\nEncoding/decoding a Dataframe using ``'split'`` formatted JSON:\n\n>>> df.to_json(orient='split')\n    '{\"columns\":[\"col 1\",\"col 2\"],\"index\":[\"row 1\",\"row 2\"],\"data\":[[\"a\",\"b\"],[\"c\",\"d\"]]}'\n>>> pd.read_json(StringIO(_), orient='split')\n      col 1 col 2\nrow 1     a     b\nrow 2     c     d\n\nEncoding/decoding a Dataframe using ``'index'`` formatted JSON:\n\n>>> df.to_json(orient='index')\n'{\"row 1\":{\"col 1\":\"a\",\"col 2\":\"b\"},\"row 2\":{\"col 1\":\"c\",\"col 2\":\"d\"}}'\n\n>>> pd.read_json(StringIO(_), orient='index')\n      col 1 col 2\nrow 1     a     b\nrow 2     c     d\n\nEncoding/decoding a Dataframe using ``'records'`` formatted JSON.\nNote that index labels are not preserved with this encoding.\n\n>>> df.to_json(orient='records')\n'[{\"col 1\":\"a\",\"col 2\":\"b\"},{\"col 1\":\"c\",\"col 2\":\"d\"}]'\n>>> pd.read_json(StringIO(_), orient='records')\n  col 1 col 2\n0     a     b\n1     c     d\n\nEncoding with Table Schema\n\n>>> df.to_json(orient='table')\n    '{\"schema\":{\"fields\":[{\"name\":\"index\",\"type\":\"string\"},{\"name\":\"col 1\",\"type\":\"string\"},{\"name\":\"col 2\",\"type\":\"string\"}],\"primaryKey\":[\"index\"],\"pandas_version\":\"1.4.0\"},\"data\":[{\"index\":\"row 1\",\"col 1\":\"a\",\"col 2\":\"b\"},{\"index\":\"row 2\",\"col 1\":\"c\",\"col 2\":\"d\"}]}'\n\nThe following example uses ``dtype_backend=\"numpy_nullable\"``\n\n>>> data = '''{\"index\": {\"0\": 0, \"1\": 1},\n...        \"a\": {\"0\": 1, \"1\": null},\n...        \"b\": {\"0\": 2.5, \"1\": 4.5},\n...        \"c\": {\"0\": true, \"1\": false},\n...        \"d\": {\"0\": \"a\", \"1\": \"b\"},\n...        \"e\": {\"0\": 1577.2, \"1\": 1577.1}}'''\n>>> pd.read_json(StringIO(data), dtype_backend=\"numpy_nullable\")\n   index     a    b      c  d       e\n0      0     1  2.5   True  a  1577.2\n1      1  <NA>  4.5  False  b  1577.1", "short_docstring": "Convert a JSON string to pandas object."}
{"name": "time.time", "type": "callable", "signature": null, "docstring": "time() -> floating point number\n\nReturn the current time in seconds since the Epoch.\nFractions of a second may be present if the system clock provides them.", "short_docstring": "time() -> floating point number"}
{"name": "statistics.mean", "type": "callable", "signature": "(data)", "docstring": "Return the sample arithmetic mean of data.\n\n>>> mean([1, 2, 3, 4, 4])\n2.8\n\n>>> from fractions import Fraction as F\n>>> mean([F(3, 7), F(1, 21), F(5, 3), F(1, 3)])\nFraction(13, 21)\n\n>>> from decimal import Decimal as D\n>>> mean([D(\"0.5\"), D(\"0.75\"), D(\"0.625\"), D(\"0.375\")])\nDecimal('0.5625')\n\nIf ``data`` is empty, StatisticsError will be raised.", "short_docstring": "Return the sample arithmetic mean of data."}
{"name": "numpy.zeros", "type": "callable", "signature": null, "docstring": "zeros(shape, dtype=float, order='C', *, like=None)\n\nReturn a new array of given shape and type, filled with zeros.\n\nParameters\n----------\nshape : int or tuple of ints\n    Shape of the new array, e.g., ``(2, 3)`` or ``2``.\ndtype : data-type, optional\n    The desired data-type for the array, e.g., `numpy.int8`.  Default is\n    `numpy.float64`.\norder : {'C', 'F'}, optional, default: 'C'\n    Whether to store multi-dimensional data in row-major\n    (C-style) or column-major (Fortran-style) order in\n    memory.\nlike : array_like, optional\n    Reference object to allow the creation of arrays which are not\n    NumPy arrays. If an array-like passed in as ``like`` supports\n    the ``__array_function__`` protocol, the result will be defined\n    by it. In this case, it ensures the creation of an array object\n    compatible with that passed in via this argument.\n\n    .. versionadded:: 1.20.0\n\nReturns\n-------\nout : ndarray\n    Array of zeros with the given shape, dtype, and order.\n\nSee Also\n--------\nzeros_like : Return an array of zeros with shape and type of input.\nempty : Return a new uninitialized array.\nones : Return a new array setting values to one.\nfull : Return a new array of given shape filled with value.\n\nExamples\n--------\n>>> np.zeros(5)\narray([ 0.,  0.,  0.,  0.,  0.])\n\n>>> np.zeros((5,), dtype=int)\narray([0, 0, 0, 0, 0])\n\n>>> np.zeros((2, 1))\narray([[ 0.],\n       [ 0.]])\n\n>>> s = (2,2)\n>>> np.zeros(s)\narray([[ 0.,  0.],\n       [ 0.,  0.]])\n\n>>> np.zeros((2,), dtype=[('x', 'i4'), ('y', 'i4')]) # custom dtype\narray([(0, 0), (0, 0)],\n      dtype=[('x', '<i4'), ('y', '<i4')])", "short_docstring": "zeros(shape, dtype=float, order='C', *, like=None)"}
{"name": "numpy.concatenate", "type": "callable", "signature": null, "docstring": "concatenate((a1, a2, ...), axis=0, out=None, dtype=None, casting=\"same_kind\")\n\nJoin a sequence of arrays along an existing axis.\n\nParameters\n----------\na1, a2, ... : sequence of array_like\n    The arrays must have the same shape, except in the dimension\n    corresponding to `axis` (the first, by default).\naxis : int, optional\n    The axis along which the arrays will be joined.  If axis is None,\n    arrays are flattened before use.  Default is 0.\nout : ndarray, optional\n    If provided, the destination to place the result. The shape must be\n    correct, matching that of what concatenate would have returned if no\n    out argument were specified.\ndtype : str or dtype\n    If provided, the destination array will have this dtype. Cannot be\n    provided together with `out`.\n\n    .. versionadded:: 1.20.0\n\ncasting : {'no', 'equiv', 'safe', 'same_kind', 'unsafe'}, optional\n    Controls what kind of data casting may occur. Defaults to 'same_kind'.\n\n    .. versionadded:: 1.20.0\n\nReturns\n-------\nres : ndarray\n    The concatenated array.\n\nSee Also\n--------\nma.concatenate : Concatenate function that preserves input masks.\narray_split : Split an array into multiple sub-arrays of equal or\n              near-equal size.\nsplit : Split array into a list of multiple sub-arrays of equal size.\nhsplit : Split array into multiple sub-arrays horizontally (column wise).\nvsplit : Split array into multiple sub-arrays vertically (row wise).\ndsplit : Split array into multiple sub-arrays along the 3rd axis (depth).\nstack : Stack a sequence of arrays along a new axis.\nblock : Assemble arrays from blocks.\nhstack : Stack arrays in sequence horizontally (column wise).\nvstack : Stack arrays in sequence vertically (row wise).\ndstack : Stack arrays in sequence depth wise (along third dimension).\ncolumn_stack : Stack 1-D arrays as columns into a 2-D array.\n\nNotes\n-----\nWhen one or more of the arrays to be concatenated is a MaskedArray,\nthis function will return a MaskedArray object instead of an ndarray,\nbut the input masks are *not* preserved. In cases where a MaskedArray\nis expected as input, use the ma.concatenate function from the masked\narray module instead.\n\nExamples\n--------\n>>> a = np.array([[1, 2], [3, 4]])\n>>> b = np.array([[5, 6]])\n>>> np.concatenate((a, b), axis=0)\narray([[1, 2],\n       [3, 4],\n       [5, 6]])\n>>> np.concatenate((a, b.T), axis=1)\narray([[1, 2, 5],\n       [3, 4, 6]])\n>>> np.concatenate((a, b), axis=None)\narray([1, 2, 3, 4, 5, 6])\n\nThis function will not preserve masking of MaskedArray inputs.\n\n>>> a = np.ma.arange(3)\n>>> a[1] = np.ma.masked\n>>> b = np.arange(2, 5)\n>>> a\nmasked_array(data=[0, --, 2],\n             mask=[False,  True, False],\n       fill_value=999999)\n>>> b\narray([2, 3, 4])\n>>> np.concatenate([a, b])\nmasked_array(data=[0, 1, 2, 2, 3, 4],\n             mask=False,\n       fill_value=999999)\n>>> np.ma.concatenate([a, b])\nmasked_array(data=[0, --, 2, 2, 3, 4],\n             mask=[False,  True, False, False, False, False],\n       fill_value=999999)", "short_docstring": "concatenate((a1, a2, ...), axis=0, out=None, dtype=None, casting=\"same_kind\")"}
{"name": "ftplib.FTP", "type": "class", "signature": "(host='', user='', passwd='', acct='', timeout=<object object at 0x7ffa6511cb10>, source_address=None, *, encoding='utf-8')", "docstring": "An FTP client class.\n\nTo create a connection, call the class using these arguments:\n        host, user, passwd, acct, timeout, source_address, encoding\n\nThe first four arguments are all strings, and have default value ''.\nThe parameter \u00b4timeout\u00b4 must be numeric and defaults to None if not\npassed, meaning that no timeout will be set on any ftp socket(s).\nIf a timeout is passed, then this is now the default timeout for all ftp\nsocket operations for this instance.\nThe last parameter is the encoding of filenames, which defaults to utf-8.\n\nThen use self.connect() with optional host and port argument.\n\nTo download a file, use ftp.retrlines('RETR ' + filename),\nor ftp.retrbinary() with slightly different arguments.\nTo upload a file, use ftp.storlines() or ftp.storbinary(),\nwhich have an open file as argument (see their definitions\nbelow for details).\nThe download/upload functions first issue appropriate TYPE\nand PORT or PASV commands.", "short_docstring": "An FTP client class."}
{"name": "scipy.optimize.curve_fit", "type": "callable", "signature": "(f, xdata, ydata, p0=None, sigma=None, absolute_sigma=False, check_finite=None, bounds=(-inf, inf), method=None, jac=None, *, full_output=False, nan_policy=None, **kwargs)", "docstring": "Use non-linear least squares to fit a function, f, to data.\n\nAssumes ``ydata = f(xdata, *params) + eps``.\n\nParameters\n----------\nf : callable\n    The model function, f(x, ...). It must take the independent\n    variable as the first argument and the parameters to fit as\n    separate remaining arguments.\nxdata : array_like\n    The independent variable where the data is measured.\n    Should usually be an M-length sequence or an (k,M)-shaped array for\n    functions with k predictors, and each element should be float\n    convertible if it is an array like object.\nydata : array_like\n    The dependent data, a length M array - nominally ``f(xdata, ...)``.\np0 : array_like, optional\n    Initial guess for the parameters (length N). If None, then the\n    initial values will all be 1 (if the number of parameters for the\n    function can be determined using introspection, otherwise a\n    ValueError is raised).\nsigma : None or scalar or M-length sequence or MxM array, optional\n    Determines the uncertainty in `ydata`. If we define residuals as\n    ``r = ydata - f(xdata, *popt)``, then the interpretation of `sigma`\n    depends on its number of dimensions:\n\n        - A scalar or 1-D `sigma` should contain values of standard deviations of\n          errors in `ydata`. In this case, the optimized function is\n          ``chisq = sum((r / sigma) ** 2)``.\n\n        - A 2-D `sigma` should contain the covariance matrix of\n          errors in `ydata`. In this case, the optimized function is\n          ``chisq = r.T @ inv(sigma) @ r``.\n\n          .. versionadded:: 0.19\n\n    None (default) is equivalent of 1-D `sigma` filled with ones.\nabsolute_sigma : bool, optional\n    If True, `sigma` is used in an absolute sense and the estimated parameter\n    covariance `pcov` reflects these absolute values.\n\n    If False (default), only the relative magnitudes of the `sigma` values matter.\n    The returned parameter covariance matrix `pcov` is based on scaling\n    `sigma` by a constant factor. This constant is set by demanding that the\n    reduced `chisq` for the optimal parameters `popt` when using the\n    *scaled* `sigma` equals unity. In other words, `sigma` is scaled to\n    match the sample variance of the residuals after the fit. Default is False.\n    Mathematically,\n    ``pcov(absolute_sigma=False) = pcov(absolute_sigma=True) * chisq(popt)/(M-N)``\ncheck_finite : bool, optional\n    If True, check that the input arrays do not contain nans of infs,\n    and raise a ValueError if they do. Setting this parameter to\n    False may silently produce nonsensical results if the input arrays\n    do contain nans. Default is True if `nan_policy` is not specified\n    explicitly and False otherwise.\nbounds : 2-tuple of array_like or `Bounds`, optional\n    Lower and upper bounds on parameters. Defaults to no bounds.\n    There are two ways to specify the bounds:\n\n        - Instance of `Bounds` class.\n\n        - 2-tuple of array_like: Each element of the tuple must be either\n          an array with the length equal to the number of parameters, or a\n          scalar (in which case the bound is taken to be the same for all\n          parameters). Use ``np.inf`` with an appropriate sign to disable\n          bounds on all or some parameters.\n\nmethod : {'lm', 'trf', 'dogbox'}, optional\n    Method to use for optimization. See `least_squares` for more details.\n    Default is 'lm' for unconstrained problems and 'trf' if `bounds` are\n    provided. The method 'lm' won't work when the number of observations\n    is less than the number of variables, use 'trf' or 'dogbox' in this\n    case.\n\n    .. versionadded:: 0.17\njac : callable, string or None, optional\n    Function with signature ``jac(x, ...)`` which computes the Jacobian\n    matrix of the model function with respect to parameters as a dense\n    array_like structure. It will be scaled according to provided `sigma`.\n    If None (default), the Jacobian will be estimated numerically.\n    String keywords for 'trf' and 'dogbox' methods can be used to select\n    a finite difference scheme, see `least_squares`.\n\n    .. versionadded:: 0.18\nfull_output : boolean, optional\n    If True, this function returns additioal information: `infodict`,\n    `mesg`, and `ier`.\n\n    .. versionadded:: 1.9\nnan_policy : {'raise', 'omit', None}, optional\n    Defines how to handle when input contains nan.\n    The following options are available (default is None):\n\n      * 'raise': throws an error\n      * 'omit': performs the calculations ignoring nan values\n      * None: no special handling of NaNs is performed\n        (except what is done by check_finite); the behavior when NaNs\n        are present is implementation-dependent and may change.\n\n    Note that if this value is specified explicitly (not None),\n    `check_finite` will be set as False.\n\n    .. versionadded:: 1.11\n**kwargs\n    Keyword arguments passed to `leastsq` for ``method='lm'`` or\n    `least_squares` otherwise.\n\nReturns\n-------\npopt : array\n    Optimal values for the parameters so that the sum of the squared\n    residuals of ``f(xdata, *popt) - ydata`` is minimized.\npcov : 2-D array\n    The estimated approximate covariance of popt. The diagonals provide\n    the variance of the parameter estimate. To compute one standard\n    deviation errors on the parameters, use\n    ``perr = np.sqrt(np.diag(pcov))``. Note that the relationship between\n    `cov` and parameter error estimates is derived based on a linear\n    approximation to the model function around the optimum [1].\n    When this approximation becomes inaccurate, `cov` may not provide an\n    accurate measure of uncertainty.\n\n    How the `sigma` parameter affects the estimated covariance\n    depends on `absolute_sigma` argument, as described above.\n\n    If the Jacobian matrix at the solution doesn't have a full rank, then\n    'lm' method returns a matrix filled with ``np.inf``, on the other hand\n    'trf'  and 'dogbox' methods use Moore-Penrose pseudoinverse to compute\n    the covariance matrix. Covariance matrices with large condition numbers\n    (e.g. computed with `numpy.linalg.cond`) may indicate that results are\n    unreliable.\ninfodict : dict (returned only if `full_output` is True)\n    a dictionary of optional outputs with the keys:\n\n    ``nfev``\n        The number of function calls. Methods 'trf' and 'dogbox' do not\n        count function calls for numerical Jacobian approximation,\n        as opposed to 'lm' method.\n    ``fvec``\n        The residual values evaluated at the solution, for a 1-D `sigma`\n        this is ``(f(x, *popt) - ydata)/sigma``.\n    ``fjac``\n        A permutation of the R matrix of a QR\n        factorization of the final approximate\n        Jacobian matrix, stored column wise.\n        Together with ipvt, the covariance of the\n        estimate can be approximated.\n        Method 'lm' only provides this information.\n    ``ipvt``\n        An integer array of length N which defines\n        a permutation matrix, p, such that\n        fjac*p = q*r, where r is upper triangular\n        with diagonal elements of nonincreasing\n        magnitude. Column j of p is column ipvt(j)\n        of the identity matrix.\n        Method 'lm' only provides this information.\n    ``qtf``\n        The vector (transpose(q) * fvec).\n        Method 'lm' only provides this information.\n\n    .. versionadded:: 1.9\nmesg : str (returned only if `full_output` is True)\n    A string message giving information about the solution.\n\n    .. versionadded:: 1.9\nier : int (returned only if `full_output` is True)\n    An integer flag. If it is equal to 1, 2, 3 or 4, the solution was\n    found. Otherwise, the solution was not found. In either case, the\n    optional output variable `mesg` gives more information.\n\n    .. versionadded:: 1.9\n\nRaises\n------\nValueError\n    if either `ydata` or `xdata` contain NaNs, or if incompatible options\n    are used.\n\nRuntimeError\n    if the least-squares minimization fails.\n\nOptimizeWarning\n    if covariance of the parameters can not be estimated.\n\nSee Also\n--------\nleast_squares : Minimize the sum of squares of nonlinear functions.\nscipy.stats.linregress : Calculate a linear least squares regression for\n                         two sets of measurements.\n\nNotes\n-----\nUsers should ensure that inputs `xdata`, `ydata`, and the output of `f`\nare ``float64``, or else the optimization may return incorrect results.\n\nWith ``method='lm'``, the algorithm uses the Levenberg-Marquardt algorithm\nthrough `leastsq`. Note that this algorithm can only deal with\nunconstrained problems.\n\nBox constraints can be handled by methods 'trf' and 'dogbox'. Refer to\nthe docstring of `least_squares` for more information.\n\nParameters to be fitted must have similar scale. Differences of multiple\norders of magnitude can lead to incorrect results. For the 'trf' and\n'dogbox' methods, the `x_scale` keyword argument can be used to scale\nthe parameters.\n\nReferences\n----------\n[1] K. Vugrin et al. Confidence region estimation techniques for nonlinear\n    regression in groundwater flow: Three case studies. Water Resources\n    Research, Vol. 43, W03423, :doi:`10.1029/2005WR004804`\n\nExamples\n--------\n>>> import numpy as np\n>>> import matplotlib.pyplot as plt\n>>> from scipy.optimize import curve_fit\n\n>>> def func(x, a, b, c):\n...     return a * np.exp(-b * x) + c\n\nDefine the data to be fit with some noise:\n\n>>> xdata = np.linspace(0, 4, 50)\n>>> y = func(xdata, 2.5, 1.3, 0.5)\n>>> rng = np.random.default_rng()\n>>> y_noise = 0.2 * rng.normal(size=xdata.size)\n>>> ydata = y + y_noise\n>>> plt.plot(xdata, ydata, 'b-', label='data')\n\nFit for the parameters a, b, c of the function `func`:\n\n>>> popt, pcov = curve_fit(func, xdata, ydata)\n>>> popt\narray([2.56274217, 1.37268521, 0.47427475])\n>>> plt.plot(xdata, func(xdata, *popt), 'r-',\n...          label='fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt))\n\nConstrain the optimization to the region of ``0 <= a <= 3``,\n``0 <= b <= 1`` and ``0 <= c <= 0.5``:\n\n>>> popt, pcov = curve_fit(func, xdata, ydata, bounds=(0, [3., 1., 0.5]))\n>>> popt\narray([2.43736712, 1.        , 0.34463856])\n>>> plt.plot(xdata, func(xdata, *popt), 'g--',\n...          label='fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt))\n\n>>> plt.xlabel('x')\n>>> plt.ylabel('y')\n>>> plt.legend()\n>>> plt.show()\n\nFor reliable results, the model `func` should not be overparametrized;\nredundant parameters can cause unreliable covariance matrices and, in some\ncases, poorer quality fits. As a quick check of whether the model may be\noverparameterized, calculate the condition number of the covariance matrix:\n\n>>> np.linalg.cond(pcov)\n34.571092161547405  # may vary\n\nThe value is small, so it does not raise much concern. If, however, we were\nto add a fourth parameter ``d`` to `func` with the same effect as ``a``:\n\n>>> def func2(x, a, b, c, d):\n...     return a * d * np.exp(-b * x) + c  # a and d are redundant\n>>> popt, pcov = curve_fit(func2, xdata, ydata)\n>>> np.linalg.cond(pcov)\n1.13250718925596e+32  # may vary\n\nSuch a large value is cause for concern. The diagonal elements of the\ncovariance matrix, which is related to uncertainty of the fit, gives more\ninformation:\n\n>>> np.diag(pcov)\narray([1.48814742e+29, 3.78596560e-02, 5.39253738e-03, 2.76417220e+28])  # may vary\n\nNote that the first and last terms are much larger than the other elements,\nsuggesting that the optimal values of these parameters are ambiguous and\nthat only one of these parameters is needed in the model.\n\nIf the optimal parameters of `f` differ by multiple orders of magnitude, the\nresulting fit can be inaccurate. Sometimes, `curve_fit` can fail to find any\nresults:\n\n>>> ydata = func(xdata, 500000, 0.01, 15)\n>>> try:\n...     popt, pcov = curve_fit(func, xdata, ydata, method = 'trf')\n... except RuntimeError as e:\n...     print(e)\nOptimal parameters not found: The maximum number of function evaluations is exceeded.\n\nIf parameter scale is roughly known beforehand, it can be defined in\n`x_scale` argument:\n\n>>> popt, pcov = curve_fit(func, xdata, ydata, method = 'trf',\n...                        x_scale = [1000, 1, 1])\n>>> popt\narray([5.00000000e+05, 1.00000000e-02, 1.49999999e+01])", "short_docstring": "Use non-linear least squares to fit a function, f, to data."}
{"name": "ipaddress.IPv4Network", "type": "class", "signature": "(address, strict=True)", "docstring": "This class represents and manipulates 32-bit IPv4 network + addresses..\n\nAttributes: [examples for IPv4Network('192.0.2.0/27')]\n    .network_address: IPv4Address('192.0.2.0')\n    .hostmask: IPv4Address('0.0.0.31')\n    .broadcast_address: IPv4Address('192.0.2.32')\n    .netmask: IPv4Address('255.255.255.224')\n    .prefixlen: 27", "short_docstring": "This class represents and manipulates 32-bit IPv4 network + addresses.."}
{"name": "io.StringIO", "type": "class", "signature": "(initial_value='', newline='\\n')", "docstring": "Text I/O implementation using an in-memory buffer.\n\nThe initial_value argument sets the value of object.  The newline\nargument is like the one of TextIOWrapper's constructor.", "short_docstring": "Text I/O implementation using an in-memory buffer."}
{"name": "werkzeug.security.generate_password_hash", "type": "callable", "signature": "(password: 'str', method: 'str' = 'scrypt', salt_length: 'int' = 16) -> 'str'", "docstring": "Securely hash a password for storage. A password can be compared to a stored hash\nusing :func:`check_password_hash`.\n\nThe following methods are supported:\n\n-   ``scrypt``, the default. The parameters are ``n``, ``r``, and ``p``, the default\n    is ``scrypt:32768:8:1``. See :func:`hashlib.scrypt`.\n-   ``pbkdf2``, less secure. The parameters are ``hash_method`` and ``iterations``,\n    the default is ``pbkdf2:sha256:600000``. See :func:`hashlib.pbkdf2_hmac`.\n\nDefault parameters may be updated to reflect current guidelines, and methods may be\ndeprecated and removed if they are no longer considered secure. To migrate old\nhashes, you may generate a new hash when checking an old hash, or you may contact\nusers with a link to reset their password.\n\n:param password: The plaintext password.\n:param method: The key derivation function and parameters.\n:param salt_length: The number of characters to generate for the salt.\n\n.. versionchanged:: 2.3\n    Scrypt support was added.\n\n.. versionchanged:: 2.3\n    The default iterations for pbkdf2 was increased to 600,000.\n\n.. versionchanged:: 2.3\n    All plain hashes are deprecated and will not be supported in Werkzeug 3.0.", "short_docstring": "Securely hash a password for storage. A password can be compared to a stored hash\nusing :func:`check_password_hash`."}
{"name": "openpyxl.load_workbook", "type": "callable", "signature": "(filename, read_only=False, keep_vba=False, data_only=False, keep_links=True, rich_text=False)", "docstring": "Open the given filename and return the workbook\n\n:param filename: the path to open or a file-like object\n:type filename: string or a file-like object open in binary mode c.f., :class:`zipfile.ZipFile`\n\n:param read_only: optimised for reading, content cannot be edited\n:type read_only: bool\n\n:param keep_vba: preserve vba content (this does NOT mean you can use it)\n:type keep_vba: bool\n\n:param data_only: controls whether cells with formulae have either the formula (default) or the value stored the last time Excel read the sheet\n:type data_only: bool\n\n:param keep_links: whether links to external workbooks should be preserved. The default is True\n:type keep_links: bool\n\n:param rich_text: if set to True openpyxl will preserve any rich text formatting in cells. The default is False\n:type rich_text: bool\n\n:rtype: :class:`openpyxl.workbook.Workbook`\n\n.. note::\n\n    When using lazy load, all worksheets will be :class:`openpyxl.worksheet.iter_worksheet.IterableWorksheet`\n    and the returned workbook will be read-only.", "short_docstring": "Open the given filename and return the workbook"}
{"name": "scipy.stats.zscore", "type": "callable", "signature": "(a, axis=0, ddof=0, nan_policy='propagate')", "docstring": "Compute the z score.\n\nCompute the z score of each value in the sample, relative to the\nsample mean and standard deviation.\n\nParameters\n----------\na : array_like\n    An array like object containing the sample data.\naxis : int or None, optional\n    Axis along which to operate. Default is 0. If None, compute over\n    the whole array `a`.\nddof : int, optional\n    Degrees of freedom correction in the calculation of the\n    standard deviation. Default is 0.\nnan_policy : {'propagate', 'raise', 'omit'}, optional\n    Defines how to handle when input contains nan. 'propagate' returns nan,\n    'raise' throws an error, 'omit' performs the calculations ignoring nan\n    values. Default is 'propagate'.  Note that when the value is 'omit',\n    nans in the input also propagate to the output, but they do not affect\n    the z-scores computed for the non-nan values.\n\nReturns\n-------\nzscore : array_like\n    The z-scores, standardized by mean and standard deviation of\n    input array `a`.\n\nSee Also\n--------\nnumpy.mean : Arithmetic average\nnumpy.std : Arithmetic standard deviation\nscipy.stats.gzscore : Geometric standard score\n\nNotes\n-----\nThis function preserves ndarray subclasses, and works also with\nmatrices and masked arrays (it uses `asanyarray` instead of\n`asarray` for parameters).\n\nReferences\n----------\n.. [1] \"Standard score\", *Wikipedia*,\n       https://en.wikipedia.org/wiki/Standard_score.\n.. [2] Huck, S. W., Cross, T. L., Clark, S. B, \"Overcoming misconceptions\n       about Z-scores\", Teaching Statistics, vol. 8, pp. 38-40, 1986\n\nExamples\n--------\n>>> import numpy as np\n>>> a = np.array([ 0.7972,  0.0767,  0.4383,  0.7866,  0.8091,\n...                0.1954,  0.6307,  0.6599,  0.1065,  0.0508])\n>>> from scipy import stats\n>>> stats.zscore(a)\narray([ 1.1273, -1.247 , -0.0552,  1.0923,  1.1664, -0.8559,  0.5786,\n        0.6748, -1.1488, -1.3324])\n\nComputing along a specified axis, using n-1 degrees of freedom\n(``ddof=1``) to calculate the standard deviation:\n\n>>> b = np.array([[ 0.3148,  0.0478,  0.6243,  0.4608],\n...               [ 0.7149,  0.0775,  0.6072,  0.9656],\n...               [ 0.6341,  0.1403,  0.9759,  0.4064],\n...               [ 0.5918,  0.6948,  0.904 ,  0.3721],\n...               [ 0.0921,  0.2481,  0.1188,  0.1366]])\n>>> stats.zscore(b, axis=1, ddof=1)\narray([[-0.19264823, -1.28415119,  1.07259584,  0.40420358],\n       [ 0.33048416, -1.37380874,  0.04251374,  1.00081084],\n       [ 0.26796377, -1.12598418,  1.23283094, -0.37481053],\n       [-0.22095197,  0.24468594,  1.19042819, -1.21416216],\n       [-0.82780366,  1.4457416 , -0.43867764, -0.1792603 ]])\n\nAn example with `nan_policy='omit'`:\n\n>>> x = np.array([[25.11, 30.10, np.nan, 32.02, 43.15],\n...               [14.95, 16.06, 121.25, 94.35, 29.81]])\n>>> stats.zscore(x, axis=1, nan_policy='omit')\narray([[-1.13490897, -0.37830299,         nan, -0.08718406,  1.60039602],\n       [-0.91611681, -0.89090508,  1.4983032 ,  0.88731639, -0.5785977 ]])", "short_docstring": "Compute the z score."}
{"name": "seaborn.barplot", "type": "callable", "signature": "(data=None, *, x=None, y=None, hue=None, order=None, hue_order=None, estimator='mean', errorbar=('ci', 95), n_boot=1000, seed=None, units=None, weights=None, orient=None, color=None, palette=None, saturation=0.75, fill=True, hue_norm=None, width=0.8, dodge='auto', gap=0, log_scale=None, native_scale=False, formatter=None, legend='auto', capsize=0, err_kws=None, ci=<deprecated>, errcolor=<deprecated>, errwidth=<deprecated>, ax=None, **kwargs)", "docstring": "Show point estimates and errors as rectangular bars.\n\nA bar plot represents an aggregate or statistical estimate for a numeric\nvariable with the height of each rectangle and indicates the uncertainty\naround that estimate using an error bar. Bar plots include 0 in the\naxis range, and they are a good choice when 0 is a meaningful value\nfor the variable to take.\n\nSee the :ref:`tutorial <categorical_tutorial>` for more information.\n\n.. note::\n    By default, this function treats one of the variables as categorical\n    and draws data at ordinal positions (0, 1, ... n) on the relevant axis.\n    As of version 0.13.0, this can be disabled by setting `native_scale=True`.\n\n\nParameters\n----------\ndata : DataFrame, Series, dict, array, or list of arrays\n    Dataset for plotting. If `x` and `y` are absent, this is\n    interpreted as wide-form. Otherwise it is expected to be long-form.    \nx, y, hue : names of variables in `data` or vector data\n    Inputs for plotting long-form data. See examples for interpretation.    \norder, hue_order : lists of strings\n    Order to plot the categorical levels in; otherwise the levels are\n    inferred from the data objects.    \nestimator : string or callable that maps vector -> scalar\n    Statistical function to estimate within each categorical bin.\nerrorbar : string, (string, number) tuple, callable or None\n    Name of errorbar method (either \"ci\", \"pi\", \"se\", or \"sd\"), or a tuple\n    with a method name and a level parameter, or a function that maps from a\n    vector to a (min, max) interval, or None to hide errorbar. See the\n    :doc:`errorbar tutorial </tutorial/error_bars>` for more information.\n\n    .. versionadded:: v0.12.0\nn_boot : int\n    Number of bootstrap samples used to compute confidence intervals.\nseed : int, `numpy.random.Generator`, or `numpy.random.RandomState`\n    Seed or random number generator for reproducible bootstrapping.\nunits : name of variable in `data` or vector data\n    Identifier of sampling units; used by the errorbar function to\n    perform a multilevel bootstrap and account for repeated measures\nweights : name of variable in `data` or vector data\n    Data values or column used to compute weighted statistics.\n    Note that the use of weights may limit other statistical options.\n\n    .. versionadded:: v0.13.1    \norient : \"v\" | \"h\" | \"x\" | \"y\"\n    Orientation of the plot (vertical or horizontal). This is usually\n    inferred based on the type of the input variables, but it can be used\n    to resolve ambiguity when both `x` and `y` are numeric or when\n    plotting wide-form data.\n\n    .. versionchanged:: v0.13.0\n        Added 'x'/'y' as options, equivalent to 'v'/'h'.    \ncolor : matplotlib color\n    Single color for the elements in the plot.    \npalette : palette name, list, or dict\n    Colors to use for the different levels of the ``hue`` variable. Should\n    be something that can be interpreted by :func:`color_palette`, or a\n    dictionary mapping hue levels to matplotlib colors.    \nsaturation : float\n    Proportion of the original saturation to draw fill colors in. Large\n    patches often look better with desaturated colors, but set this to\n    `1` if you want the colors to perfectly match the input values.    \nfill : bool\n    If True, use a solid patch. Otherwise, draw as line art.\n\n    .. versionadded:: v0.13.0    \nhue_norm : tuple or :class:`matplotlib.colors.Normalize` object\n    Normalization in data units for colormap applied to the `hue`\n    variable when it is numeric. Not relevant if `hue` is categorical.\n\n    .. versionadded:: v0.12.0    \nwidth : float\n    Width allotted to each element on the orient axis. When `native_scale=True`,\n    it is relative to the minimum distance between two values in the native scale.    \ndodge : \"auto\" or bool\n    When hue mapping is used, whether elements should be narrowed and shifted along\n    the orient axis to eliminate overlap. If `\"auto\"`, set to `True` when the\n    orient variable is crossed with the categorical variable or `False` otherwise.\n\n    .. versionchanged:: 0.13.0\n\n        Added `\"auto\"` mode as a new default.    \ngap : float\n    Shrink on the orient axis by this factor to add a gap between dodged elements.\n\n    .. versionadded:: 0.13.0    \nlog_scale : bool or number, or pair of bools or numbers\n    Set axis scale(s) to log. A single value sets the data axis for any numeric\n    axes in the plot. A pair of values sets each axis independently.\n    Numeric values are interpreted as the desired base (default 10).\n    When `None` or `False`, seaborn defers to the existing Axes scale.\n\n    .. versionadded:: v0.13.0    \nnative_scale : bool\n    When True, numeric or datetime values on the categorical axis will maintain\n    their original scaling rather than being converted to fixed indices.\n\n    .. versionadded:: v0.13.0    \nformatter : callable\n    Function for converting categorical data into strings. Affects both grouping\n    and tick labels.\n\n    .. versionadded:: v0.13.0    \nlegend : \"auto\", \"brief\", \"full\", or False\n    How to draw the legend. If \"brief\", numeric `hue` and `size`\n    variables will be represented with a sample of evenly spaced values.\n    If \"full\", every group will get an entry in the legend. If \"auto\",\n    choose between brief or full representation based on number of levels.\n    If `False`, no legend data is added and no legend is drawn.\n\n    .. versionadded:: v0.13.0    \ncapsize : float\n    Width of the \"caps\" on error bars, relative to bar spacing.    \nerr_kws : dict\n    Parameters of :class:`matplotlib.lines.Line2D`, for the error bar artists.\n\n    .. versionadded:: v0.13.0    \nci : float\n    Level of the confidence interval to show, in [0, 100].\n\n    .. deprecated:: v0.12.0\n        Use `errorbar=(\"ci\", ...)`.    \nerrcolor : matplotlib color\n    Color used for the error bar lines.\n\n    .. deprecated:: 0.13.0\n        Use `err_kws={'color': ...}`.    \nerrwidth : float\n    Thickness of error bar lines (and caps), in points.\n\n    .. deprecated:: 0.13.0\n        Use `err_kws={'linewidth': ...}`.    \nax : matplotlib Axes\n    Axes object to draw the plot onto, otherwise uses the current Axes.    \nkwargs : key, value mappings\n    Other parameters are passed through to :class:`matplotlib.patches.Rectangle`.\n\nReturns\n-------\nax : matplotlib Axes\n    Returns the Axes object with the plot drawn onto it.    \n\nSee Also\n--------\ncountplot : Show the counts of observations in each categorical bin.    \npointplot : Show point estimates and confidence intervals using dots.    \ncatplot : Combine a categorical plot with a :class:`FacetGrid`.    \n\nNotes\n-----\n\nFor datasets where 0 is not a meaningful value, a :func:`pointplot` will\nallow you to focus on differences between levels of one or more categorical\nvariables.\n\nIt is also important to keep in mind that a bar plot shows only the mean (or\nother aggregate) value, but it is often more informative to show the\ndistribution of values at each level of the categorical variables. In those\ncases, approaches such as a :func:`boxplot` or :func:`violinplot` may be\nmore appropriate.\n\nExamples\n--------\n.. include:: ../docstrings/barplot.rst", "short_docstring": "Show point estimates and errors as rectangular bars."}
{"name": "seaborn.pairplot", "type": "callable", "signature": "(data, *, hue=None, hue_order=None, palette=None, vars=None, x_vars=None, y_vars=None, kind='scatter', diag_kind='auto', markers=None, height=2.5, aspect=1, corner=False, dropna=False, plot_kws=None, diag_kws=None, grid_kws=None, size=None)", "docstring": "Plot pairwise relationships in a dataset.\n\nBy default, this function will create a grid of Axes such that each numeric\nvariable in ``data`` will by shared across the y-axes across a single row and\nthe x-axes across a single column. The diagonal plots are treated\ndifferently: a univariate distribution plot is drawn to show the marginal\ndistribution of the data in each column.\n\nIt is also possible to show a subset of variables or plot different\nvariables on the rows and columns.\n\nThis is a high-level interface for :class:`PairGrid` that is intended to\nmake it easy to draw a few common styles. You should use :class:`PairGrid`\ndirectly if you need more flexibility.\n\nParameters\n----------\ndata : `pandas.DataFrame`\n    Tidy (long-form) dataframe where each column is a variable and\n    each row is an observation.\nhue : name of variable in ``data``\n    Variable in ``data`` to map plot aspects to different colors.\nhue_order : list of strings\n    Order for the levels of the hue variable in the palette\npalette : dict or seaborn color palette\n    Set of colors for mapping the ``hue`` variable. If a dict, keys\n    should be values  in the ``hue`` variable.\nvars : list of variable names\n    Variables within ``data`` to use, otherwise use every column with\n    a numeric datatype.\n{x, y}_vars : lists of variable names\n    Variables within ``data`` to use separately for the rows and\n    columns of the figure; i.e. to make a non-square plot.\nkind : {'scatter', 'kde', 'hist', 'reg'}\n    Kind of plot to make.\ndiag_kind : {'auto', 'hist', 'kde', None}\n    Kind of plot for the diagonal subplots. If 'auto', choose based on\n    whether or not ``hue`` is used.\nmarkers : single matplotlib marker code or list\n    Either the marker to use for all scatterplot points or a list of markers\n    with a length the same as the number of levels in the hue variable so that\n    differently colored points will also have different scatterplot\n    markers.\nheight : scalar\n    Height (in inches) of each facet.\naspect : scalar\n    Aspect * height gives the width (in inches) of each facet.\ncorner : bool\n    If True, don't add axes to the upper (off-diagonal) triangle of the\n    grid, making this a \"corner\" plot.\ndropna : boolean\n    Drop missing values from the data before plotting.\n{plot, diag, grid}_kws : dicts\n    Dictionaries of keyword arguments. ``plot_kws`` are passed to the\n    bivariate plotting function, ``diag_kws`` are passed to the univariate\n    plotting function, and ``grid_kws`` are passed to the :class:`PairGrid`\n    constructor.\n\nReturns\n-------\ngrid : :class:`PairGrid`\n    Returns the underlying :class:`PairGrid` instance for further tweaking.\n\nSee Also\n--------\nPairGrid : Subplot grid for more flexible plotting of pairwise relationships.\nJointGrid : Grid for plotting joint and marginal distributions of two variables.\n\nExamples\n--------\n\n.. include:: ../docstrings/pairplot.rst", "short_docstring": "Plot pairwise relationships in a dataset."}
{"name": "cgi.parse_header", "type": "callable", "signature": "(line)", "docstring": "Parse a Content-type like header.\n\nReturn the main content-type and a dictionary of options.", "short_docstring": "Parse a Content-type like header."}
{"name": "zipfile.ZipFile", "type": "class", "signature": "(file, mode='r', compression=0, allowZip64=True, compresslevel=None, *, strict_timestamps=True)", "docstring": "Class with methods to open, read, write, close, list zip files.\n\nz = ZipFile(file, mode=\"r\", compression=ZIP_STORED, allowZip64=True,\n            compresslevel=None)\n\nfile: Either the path to the file, or a file-like object.\n      If it is a path, the file will be opened and closed by ZipFile.\nmode: The mode can be either read 'r', write 'w', exclusive create 'x',\n      or append 'a'.\ncompression: ZIP_STORED (no compression), ZIP_DEFLATED (requires zlib),\n             ZIP_BZIP2 (requires bz2) or ZIP_LZMA (requires lzma).\nallowZip64: if True ZipFile will create files with ZIP64 extensions when\n            needed, otherwise it will raise an exception when this would\n            be necessary.\ncompresslevel: None (default for the given compression type) or an integer\n               specifying the level to pass to the compressor.\n               When using ZIP_STORED or ZIP_LZMA this keyword has no effect.\n               When using ZIP_DEFLATED integers 0 through 9 are accepted.\n               When using ZIP_BZIP2 integers 1 through 9 are accepted.", "short_docstring": "Class with methods to open, read, write, close, list zip files."}
{"name": "scipy.spatial.Voronoi", "type": "class", "signature": "(points, furthest_site=False, incremental=False, qhull_options=None)", "docstring": "Voronoi(points, furthest_site=False, incremental=False, qhull_options=None)\n\nVoronoi diagrams in N dimensions.\n\n.. versionadded:: 0.12.0\n\nParameters\n----------\npoints : ndarray of floats, shape (npoints, ndim)\n    Coordinates of points to construct a Voronoi diagram from\nfurthest_site : bool, optional\n    Whether to compute a furthest-site Voronoi diagram. Default: False\nincremental : bool, optional\n    Allow adding new points incrementally. This takes up some additional\n    resources.\nqhull_options : str, optional\n    Additional options to pass to Qhull. See Qhull manual\n    for details. (Default: \"Qbb Qc Qz Qx\" for ndim > 4 and\n    \"Qbb Qc Qz\" otherwise. Incremental mode omits \"Qz\".)\n\nAttributes\n----------\npoints : ndarray of double, shape (npoints, ndim)\n    Coordinates of input points.\nvertices : ndarray of double, shape (nvertices, ndim)\n    Coordinates of the Voronoi vertices.\nridge_points : ndarray of ints, shape ``(nridges, 2)``\n    Indices of the points between which each Voronoi ridge lies.\nridge_vertices : list of list of ints, shape ``(nridges, *)``\n    Indices of the Voronoi vertices forming each Voronoi ridge.\nregions : list of list of ints, shape ``(nregions, *)``\n    Indices of the Voronoi vertices forming each Voronoi region.\n    -1 indicates vertex outside the Voronoi diagram.\n    When qhull option \"Qz\" was specified, an empty sublist\n    represents the Voronoi region for a point at infinity that\n    was added internally.\npoint_region : array of ints, shape (npoints)\n    Index of the Voronoi region for each input point.\n    If qhull option \"Qc\" was not specified, the list will contain -1\n    for points that are not associated with a Voronoi region.\n    If qhull option \"Qz\" was specified, there will be one less\n    element than the number of regions because an extra point\n    at infinity is added internally to facilitate computation.\nfurthest_site\n    True if this was a furthest site triangulation and False if not.\n\n    .. versionadded:: 1.4.0\n\nRaises\n------\nQhullError\n    Raised when Qhull encounters an error condition, such as\n    geometrical degeneracy when options to resolve are not enabled.\nValueError\n    Raised if an incompatible array is given as input.\n\nNotes\n-----\nThe Voronoi diagram is computed using the\n`Qhull library <http://www.qhull.org/>`__.\n\nExamples\n--------\nVoronoi diagram for a set of point:\n\n>>> import numpy as np\n>>> points = np.array([[0, 0], [0, 1], [0, 2], [1, 0], [1, 1], [1, 2],\n...                    [2, 0], [2, 1], [2, 2]])\n>>> from scipy.spatial import Voronoi, voronoi_plot_2d\n>>> vor = Voronoi(points)\n\nPlot it:\n\n>>> import matplotlib.pyplot as plt\n>>> fig = voronoi_plot_2d(vor)\n>>> plt.show()\n\nThe Voronoi vertices:\n\n>>> vor.vertices\narray([[0.5, 0.5],\n       [0.5, 1.5],\n       [1.5, 0.5],\n       [1.5, 1.5]])\n\nThere is a single finite Voronoi region, and four finite Voronoi\nridges:\n\n>>> vor.regions\n[[], [-1, 0], [-1, 1], [1, -1, 0], [3, -1, 2], [-1, 3], [-1, 2], [0, 1, 3, 2], [2, -1, 0], [3, -1, 1]]\n>>> vor.ridge_vertices\n[[-1, 0], [-1, 0], [-1, 1], [-1, 1], [0, 1], [-1, 3], [-1, 2], [2, 3], [-1, 3], [-1, 2], [1, 3], [0, 2]]\n\nThe ridges are perpendicular between lines drawn between the following\ninput points:\n\n>>> vor.ridge_points\narray([[0, 3],\n       [0, 1],\n       [2, 5],\n       [2, 1],\n       [1, 4],\n       [7, 8],\n       [7, 6],\n       [7, 4],\n       [8, 5],\n       [6, 3],\n       [4, 5],\n       [4, 3]], dtype=int32)", "short_docstring": "Voronoi(points, furthest_site=False, incremental=False, qhull_options=None)"}
{"name": "numpy.arange", "type": "callable", "signature": null, "docstring": "arange([start,] stop[, step,], dtype=None, *, like=None)\n\nReturn evenly spaced values within a given interval.\n\n``arange`` can be called with a varying number of positional arguments:\n\n* ``arange(stop)``: Values are generated within the half-open interval\n  ``[0, stop)`` (in other words, the interval including `start` but\n  excluding `stop`).\n* ``arange(start, stop)``: Values are generated within the half-open\n  interval ``[start, stop)``.\n* ``arange(start, stop, step)`` Values are generated within the half-open\n  interval ``[start, stop)``, with spacing between values given by\n  ``step``.\n\nFor integer arguments the function is roughly equivalent to the Python\nbuilt-in :py:class:`range`, but returns an ndarray rather than a ``range``\ninstance.\n\nWhen using a non-integer step, such as 0.1, it is often better to use\n`numpy.linspace`.\n\nSee the Warning sections below for more information.\n\nParameters\n----------\nstart : integer or real, optional\n    Start of interval.  The interval includes this value.  The default\n    start value is 0.\nstop : integer or real\n    End of interval.  The interval does not include this value, except\n    in some cases where `step` is not an integer and floating point\n    round-off affects the length of `out`.\nstep : integer or real, optional\n    Spacing between values.  For any output `out`, this is the distance\n    between two adjacent values, ``out[i+1] - out[i]``.  The default\n    step size is 1.  If `step` is specified as a position argument,\n    `start` must also be given.\ndtype : dtype, optional\n    The type of the output array.  If `dtype` is not given, infer the data\n    type from the other input arguments.\nlike : array_like, optional\n    Reference object to allow the creation of arrays which are not\n    NumPy arrays. If an array-like passed in as ``like`` supports\n    the ``__array_function__`` protocol, the result will be defined\n    by it. In this case, it ensures the creation of an array object\n    compatible with that passed in via this argument.\n\n    .. versionadded:: 1.20.0\n\nReturns\n-------\narange : ndarray\n    Array of evenly spaced values.\n\n    For floating point arguments, the length of the result is\n    ``ceil((stop - start)/step)``.  Because of floating point overflow,\n    this rule may result in the last element of `out` being greater\n    than `stop`.\n\nWarnings\n--------\nThe length of the output might not be numerically stable.\n\nAnother stability issue is due to the internal implementation of\n`numpy.arange`.\nThe actual step value used to populate the array is\n``dtype(start + step) - dtype(start)`` and not `step`. Precision loss\ncan occur here, due to casting or due to using floating points when\n`start` is much larger than `step`. This can lead to unexpected\nbehaviour. For example::\n\n  >>> np.arange(0, 5, 0.5, dtype=int)\n  array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n  >>> np.arange(-3, 3, 0.5, dtype=int)\n  array([-3, -2, -1,  0,  1,  2,  3,  4,  5,  6,  7,  8])\n\nIn such cases, the use of `numpy.linspace` should be preferred.\n\nThe built-in :py:class:`range` generates :std:doc:`Python built-in integers\nthat have arbitrary size <c-api/long>`, while `numpy.arange` produces\n`numpy.int32` or `numpy.int64` numbers. This may result in incorrect\nresults for large integer values::\n\n  >>> power = 40\n  >>> modulo = 10000\n  >>> x1 = [(n ** power) % modulo for n in range(8)]\n  >>> x2 = [(n ** power) % modulo for n in np.arange(8)]\n  >>> print(x1)\n  [0, 1, 7776, 8801, 6176, 625, 6576, 4001]  # correct\n  >>> print(x2)\n  [0, 1, 7776, 7185, 0, 5969, 4816, 3361]  # incorrect\n\nSee Also\n--------\nnumpy.linspace : Evenly spaced numbers with careful handling of endpoints.\nnumpy.ogrid: Arrays of evenly spaced numbers in N-dimensions.\nnumpy.mgrid: Grid-shaped arrays of evenly spaced numbers in N-dimensions.\n\nExamples\n--------\n>>> np.arange(3)\narray([0, 1, 2])\n>>> np.arange(3.0)\narray([ 0.,  1.,  2.])\n>>> np.arange(3,7)\narray([3, 4, 5, 6])\n>>> np.arange(3,7,2)\narray([3, 5])", "short_docstring": "arange([start,] stop[, step,], dtype=None, *, like=None)"}
{"name": "numpy.tensordot", "type": "callable", "signature": "(a, b, axes=2)", "docstring": "Compute tensor dot product along specified axes.\n\nGiven two tensors, `a` and `b`, and an array_like object containing\ntwo array_like objects, ``(a_axes, b_axes)``, sum the products of\n`a`'s and `b`'s elements (components) over the axes specified by\n``a_axes`` and ``b_axes``. The third argument can be a single non-negative\ninteger_like scalar, ``N``; if it is such, then the last ``N`` dimensions\nof `a` and the first ``N`` dimensions of `b` are summed over.\n\nParameters\n----------\na, b : array_like\n    Tensors to \"dot\".\n\naxes : int or (2,) array_like\n    * integer_like\n      If an int N, sum over the last N axes of `a` and the first N axes\n      of `b` in order. The sizes of the corresponding axes must match.\n    * (2,) array_like\n      Or, a list of axes to be summed over, first sequence applying to `a`,\n      second to `b`. Both elements array_like must be of the same length.\n\nReturns\n-------\noutput : ndarray\n    The tensor dot product of the input.\n\nSee Also\n--------\ndot, einsum\n\nNotes\n-----\nThree common use cases are:\n    * ``axes = 0`` : tensor product :math:`a\\otimes b`\n    * ``axes = 1`` : tensor dot product :math:`a\\cdot b`\n    * ``axes = 2`` : (default) tensor double contraction :math:`a:b`\n\nWhen `axes` is integer_like, the sequence for evaluation will be: first\nthe -Nth axis in `a` and 0th axis in `b`, and the -1th axis in `a` and\nNth axis in `b` last.\n\nWhen there is more than one axis to sum over - and they are not the last\n(first) axes of `a` (`b`) - the argument `axes` should consist of\ntwo sequences of the same length, with the first axis to sum over given\nfirst in both sequences, the second axis second, and so forth.\n\nThe shape of the result consists of the non-contracted axes of the\nfirst tensor, followed by the non-contracted axes of the second.\n\nExamples\n--------\nA \"traditional\" example:\n\n>>> a = np.arange(60.).reshape(3,4,5)\n>>> b = np.arange(24.).reshape(4,3,2)\n>>> c = np.tensordot(a,b, axes=([1,0],[0,1]))\n>>> c.shape\n(5, 2)\n>>> c\narray([[4400., 4730.],\n       [4532., 4874.],\n       [4664., 5018.],\n       [4796., 5162.],\n       [4928., 5306.]])\n>>> # A slower but equivalent way of computing the same...\n>>> d = np.zeros((5,2))\n>>> for i in range(5):\n...   for j in range(2):\n...     for k in range(3):\n...       for n in range(4):\n...         d[i,j] += a[k,n,i] * b[n,k,j]\n>>> c == d\narray([[ True,  True],\n       [ True,  True],\n       [ True,  True],\n       [ True,  True],\n       [ True,  True]])\n\nAn extended example taking advantage of the overloading of + and \\*:\n\n>>> a = np.array(range(1, 9))\n>>> a.shape = (2, 2, 2)\n>>> A = np.array(('a', 'b', 'c', 'd'), dtype=object)\n>>> A.shape = (2, 2)\n>>> a; A\narray([[[1, 2],\n        [3, 4]],\n       [[5, 6],\n        [7, 8]]])\narray([['a', 'b'],\n       ['c', 'd']], dtype=object)\n\n>>> np.tensordot(a, A) # third argument default is 2 for double-contraction\narray(['abbcccdddd', 'aaaaabbbbbbcccccccdddddddd'], dtype=object)\n\n>>> np.tensordot(a, A, 1)\narray([[['acc', 'bdd'],\n        ['aaacccc', 'bbbdddd']],\n       [['aaaaacccccc', 'bbbbbdddddd'],\n        ['aaaaaaacccccccc', 'bbbbbbbdddddddd']]], dtype=object)\n\n>>> np.tensordot(a, A, 0) # tensor product (result too long to incl.)\narray([[[[['a', 'b'],\n          ['c', 'd']],\n          ...\n\n>>> np.tensordot(a, A, (0, 1))\narray([[['abbbbb', 'cddddd'],\n        ['aabbbbbb', 'ccdddddd']],\n       [['aaabbbbbbb', 'cccddddddd'],\n        ['aaaabbbbbbbb', 'ccccdddddddd']]], dtype=object)\n\n>>> np.tensordot(a, A, (2, 1))\narray([[['abb', 'cdd'],\n        ['aaabbbb', 'cccdddd']],\n       [['aaaaabbbbbb', 'cccccdddddd'],\n        ['aaaaaaabbbbbbbb', 'cccccccdddddddd']]], dtype=object)\n\n>>> np.tensordot(a, A, ((0, 1), (0, 1)))\narray(['abbbcccccddddddd', 'aabbbbccccccdddddddd'], dtype=object)\n\n>>> np.tensordot(a, A, ((2, 1), (1, 0)))\narray(['acccbbdddd', 'aaaaacccccccbbbbbbdddddddd'], dtype=object)", "short_docstring": "Compute tensor dot product along specified axes."}
{"name": "select.select", "type": "callable", "signature": "(rlist, wlist, xlist, timeout=None, /)", "docstring": "Wait until one or more file descriptors are ready for some kind of I/O.\n\nThe first three arguments are iterables of file descriptors to be waited for:\nrlist -- wait until ready for reading\nwlist -- wait until ready for writing\nxlist -- wait for an \"exceptional condition\"\nIf only one kind of condition is required, pass [] for the other lists.\n\nA file descriptor is either a socket or file object, or a small integer\ngotten from a fileno() method call on one of those.\n\nThe optional 4th argument specifies a timeout in seconds; it may be\na floating point number to specify fractions of seconds.  If it is absent\nor None, the call will never time out.\n\nThe return value is a tuple of three lists corresponding to the first three\narguments; each contains the subset of the corresponding file descriptors\nthat are ready.\n\n*** IMPORTANT NOTICE ***\nOn Windows, only sockets are supported; on Unix, all file\ndescriptors can be used.", "short_docstring": "Wait until one or more file descriptors are ready for some kind of I/O."}
{"name": "cv2.cvtColor", "type": "callable", "signature": null, "docstring": "cvtColor(src, code[, dst[, dstCn]]) -> dst\n.   @brief Converts an image from one color space to another.\n.   \n.   The function converts an input image from one color space to another. In case of a transformation\n.   to-from RGB color space, the order of the channels should be specified explicitly (RGB or BGR). Note\n.   that the default color format in OpenCV is often referred to as RGB but it is actually BGR (the\n.   bytes are reversed). So the first byte in a standard (24-bit) color image will be an 8-bit Blue\n.   component, the second byte will be Green, and the third byte will be Red. The fourth, fifth, and\n.   sixth bytes would then be the second pixel (Blue, then Green, then Red), and so on.\n.   \n.   The conventional ranges for R, G, and B channel values are:\n.   -   0 to 255 for CV_8U images\n.   -   0 to 65535 for CV_16U images\n.   -   0 to 1 for CV_32F images\n.   \n.   In case of linear transformations, the range does not matter. But in case of a non-linear\n.   transformation, an input RGB image should be normalized to the proper value range to get the correct\n.   results, for example, for RGB \\f$\\rightarrow\\f$ L\\*u\\*v\\* transformation. For example, if you have a\n.   32-bit floating-point image directly converted from an 8-bit image without any scaling, then it will\n.   have the 0..255 value range instead of 0..1 assumed by the function. So, before calling #cvtColor ,\n.   you need first to scale the image down:\n.   @code\n.       img *= 1./255;\n.       cvtColor(img, img, COLOR_BGR2Luv);\n.   @endcode\n.   If you use #cvtColor with 8-bit images, the conversion will have some information lost. For many\n.   applications, this will not be noticeable but it is recommended to use 32-bit images in applications\n.   that need the full range of colors or that convert an image before an operation and then convert\n.   back.\n.   \n.   If conversion adds the alpha channel, its value will set to the maximum of corresponding channel\n.   range: 255 for CV_8U, 65535 for CV_16U, 1 for CV_32F.\n.   \n.   @param src input image: 8-bit unsigned, 16-bit unsigned ( CV_16UC... ), or single-precision\n.   floating-point.\n.   @param dst output image of the same size and depth as src.\n.   @param code color space conversion code (see #ColorConversionCodes).\n.   @param dstCn number of channels in the destination image; if the parameter is 0, the number of the\n.   channels is derived automatically from src and code.\n.   \n.   @see @ref imgproc_color_conversions", "short_docstring": "cvtColor(src, code[, dst[, dstCn]]) -> dst\n.   @brief Converts an image from one color space to another.\n.   \n.   The function converts an input image from one color space to another. In case of a transformation\n.   to-from RGB color space, the order of the channels should be specified explicitly (RGB or BGR). Note\n.   that the default color format in OpenCV is often referred to as RGB but it is actually BGR (the\n.   bytes are reversed). So the first byte in a standard (24-bit) color image will be an 8-bit Blue\n.   component, the second byte will be Green, and the third byte will be Red. The fourth, fifth, and\n.   sixth bytes would then be the second pixel (Blue, then Green, then Red), and so on.\n.   \n.   The conventional ranges for R, G, and B channel values are:\n.   -   0 to 255 for CV_8U images\n.   -   0 to 65535 for CV_16U images\n.   -   0 to 1 for CV_32F images\n.   \n.   In case of linear transformations, the range does not matter. But in case of a non-linear\n.   transformation, an input RGB image should be normalized to the proper value range to get the correct\n.   results, for example, for RGB \\f$\\rightarrow\\f$ L\\*u\\*v\\* transformation. For example, if you have a\n.   32-bit floating-point image directly converted from an 8-bit image without any scaling, then it will\n.   have the 0..255 value range instead of 0..1 assumed by the function. So, before calling #cvtColor ,\n.   you need first to scale the image down:\n.   @code\n.       img *= 1./255;\n.       cvtColor(img, img, COLOR_BGR2Luv);\n.   @endcode\n.   If you use #cvtColor with 8-bit images, the conversion will have some information lost. For many\n.   applications, this will not be noticeable but it is recommended to use 32-bit images in applications\n.   that need the full range of colors or that convert an image before an operation and then convert\n.   back.\n.   \n.   If conversion adds the alpha channel, its value will set to the maximum of corresponding channel\n.   range: 255 for CV_8U, 65535 for CV_16U, 1 for CV_32F.\n.   \n.   @param src input image: 8-bit unsigned, 16-bit unsigned ( CV_16UC... ), or single-precision\n.   floating-point.\n.   @param dst output image of the same size and depth as src.\n.   @param code color space conversion code (see #ColorConversionCodes).\n.   @param dstCn number of channels in the destination image; if the parameter is 0, the number of the\n.   channels is derived automatically from src and code.\n.   \n.   @see @ref imgproc_color_conversions"}
{"name": "csv.DictWriter", "type": "class", "signature": "(f, fieldnames, restval='', extrasaction='raise', dialect='excel', *args, **kwds)", "docstring": null, "short_docstring": ""}
{"name": "smtplib.SMTP", "type": "class", "signature": "(host='', port=0, local_hostname=None, timeout=<object object at 0x7ffa6511cb10>, source_address=None)", "docstring": "This class manages a connection to an SMTP or ESMTP server.\nSMTP Objects:\n    SMTP objects have the following attributes:\n        helo_resp\n            This is the message given by the server in response to the\n            most recent HELO command.\n\n        ehlo_resp\n            This is the message given by the server in response to the\n            most recent EHLO command. This is usually multiline.\n\n        does_esmtp\n            This is a True value _after you do an EHLO command_, if the\n            server supports ESMTP.\n\n        esmtp_features\n            This is a dictionary, which, if the server supports ESMTP,\n            will _after you do an EHLO command_, contain the names of the\n            SMTP service extensions this server supports, and their\n            parameters (if any).\n\n            Note, all extension names are mapped to lower case in the\n            dictionary.\n\n    See each method's docstrings for details.  In general, there is a\n    method of the same name to perform each SMTP command.  There is also a\n    method called 'sendmail' that will do an entire mail transaction.\n    ", "short_docstring": "This class manages a connection to an SMTP or ESMTP server.\nSMTP Objects:\n    SMTP objects have the following attributes:\n        helo_resp\n            This is the message given by the server in response to the\n            most recent HELO command."}
{"name": "json.JSONDecodeError", "type": "class", "signature": "(msg, doc, pos)", "docstring": "Subclass of ValueError with the following additional properties:\n\nmsg: The unformatted error message\ndoc: The JSON document being parsed\npos: The start index of doc where parsing failed\nlineno: The line corresponding to pos\ncolno: The column corresponding to pos", "short_docstring": "Subclass of ValueError with the following additional properties:"}
{"name": "pandas.Series", "type": "class", "signature": "(data=None, index=None, dtype: 'Dtype | None' = None, name=None, copy: 'bool | None' = None, fastpath: 'bool | lib.NoDefault' = <no_default>) -> 'None'", "docstring": "One-dimensional ndarray with axis labels (including time series).\n\nLabels need not be unique but must be a hashable type. The object\nsupports both integer- and label-based indexing and provides a host of\nmethods for performing operations involving the index. Statistical\nmethods from ndarray have been overridden to automatically exclude\nmissing data (currently represented as NaN).\n\nOperations between Series (+, -, /, \\*, \\*\\*) align values based on their\nassociated index values-- they need not be the same length. The result\nindex will be the sorted union of the two indexes.\n\nParameters\n----------\ndata : array-like, Iterable, dict, or scalar value\n    Contains data stored in Series. If data is a dict, argument order is\n    maintained.\nindex : array-like or Index (1d)\n    Values must be hashable and have the same length as `data`.\n    Non-unique index values are allowed. Will default to\n    RangeIndex (0, 1, 2, ..., n) if not provided. If data is dict-like\n    and index is None, then the keys in the data are used as the index. If the\n    index is not None, the resulting Series is reindexed with the index values.\ndtype : str, numpy.dtype, or ExtensionDtype, optional\n    Data type for the output Series. If not specified, this will be\n    inferred from `data`.\n    See the :ref:`user guide <basics.dtypes>` for more usages.\nname : Hashable, default None\n    The name to give to the Series.\ncopy : bool, default False\n    Copy input data. Only affects Series or 1d ndarray input. See examples.\n\nNotes\n-----\nPlease reference the :ref:`User Guide <basics.series>` for more information.\n\nExamples\n--------\nConstructing Series from a dictionary with an Index specified\n\n>>> d = {'a': 1, 'b': 2, 'c': 3}\n>>> ser = pd.Series(data=d, index=['a', 'b', 'c'])\n>>> ser\na   1\nb   2\nc   3\ndtype: int64\n\nThe keys of the dictionary match with the Index values, hence the Index\nvalues have no effect.\n\n>>> d = {'a': 1, 'b': 2, 'c': 3}\n>>> ser = pd.Series(data=d, index=['x', 'y', 'z'])\n>>> ser\nx   NaN\ny   NaN\nz   NaN\ndtype: float64\n\nNote that the Index is first build with the keys from the dictionary.\nAfter this the Series is reindexed with the given Index values, hence we\nget all NaN as a result.\n\nConstructing Series from a list with `copy=False`.\n\n>>> r = [1, 2]\n>>> ser = pd.Series(r, copy=False)\n>>> ser.iloc[0] = 999\n>>> r\n[1, 2]\n>>> ser\n0    999\n1      2\ndtype: int64\n\nDue to input data type the Series has a `copy` of\nthe original data even though `copy=False`, so\nthe data is unchanged.\n\nConstructing Series from a 1d ndarray with `copy=False`.\n\n>>> r = np.array([1, 2])\n>>> ser = pd.Series(r, copy=False)\n>>> ser.iloc[0] = 999\n>>> r\narray([999,   2])\n>>> ser\n0    999\n1      2\ndtype: int64\n\nDue to input data type the Series has a `view` on\nthe original data, so\nthe data is changed as well.", "short_docstring": "One-dimensional ndarray with axis labels (including time series)."}
{"name": "librosa.stft", "type": "callable", "signature": "(y: 'np.ndarray', *, n_fft: 'int' = 2048, hop_length: 'Optional[int]' = None, win_length: 'Optional[int]' = None, window: '_WindowSpec' = 'hann', center: 'bool' = True, dtype: 'Optional[DTypeLike]' = None, pad_mode: '_PadModeSTFT' = 'constant', out: 'Optional[np.ndarray]' = None) -> 'np.ndarray'", "docstring": "Short-time Fourier transform (STFT).\n\nThe STFT represents a signal in the time-frequency domain by\ncomputing discrete Fourier transforms (DFT) over short overlapping\nwindows.\n\nThis function returns a complex-valued matrix D such that\n\n- ``np.abs(D[..., f, t])`` is the magnitude of frequency bin ``f``\n  at frame ``t``, and\n\n- ``np.angle(D[..., f, t])`` is the phase of frequency bin ``f``\n  at frame ``t``.\n\nThe integers ``t`` and ``f`` can be converted to physical units by means\nof the utility functions `frames_to_samples` and `fft_frequencies`.\n\nParameters\n----------\ny : np.ndarray [shape=(..., n)], real-valued\n    input signal. Multi-channel is supported.\n\nn_fft : int > 0 [scalar]\n    length of the windowed signal after padding with zeros.\n    The number of rows in the STFT matrix ``D`` is ``(1 + n_fft/2)``.\n    The default value, ``n_fft=2048`` samples, corresponds to a physical\n    duration of 93 milliseconds at a sample rate of 22050 Hz, i.e. the\n    default sample rate in librosa. This value is well adapted for music\n    signals. However, in speech processing, the recommended value is 512,\n    corresponding to 23 milliseconds at a sample rate of 22050 Hz.\n    In any case, we recommend setting ``n_fft`` to a power of two for\n    optimizing the speed of the fast Fourier transform (FFT) algorithm.\n\nhop_length : int > 0 [scalar]\n    number of audio samples between adjacent STFT columns.\n\n    Smaller values increase the number of columns in ``D`` without\n    affecting the frequency resolution of the STFT.\n\n    If unspecified, defaults to ``win_length // 4`` (see below).\n\nwin_length : int <= n_fft [scalar]\n    Each frame of audio is windowed by ``window`` of length ``win_length``\n    and then padded with zeros to match ``n_fft``.  Padding is added on\n    both the left- and the right-side of the window so that the window\n    is centered within the frame.\n\n    Smaller values improve the temporal resolution of the STFT (i.e. the\n    ability to discriminate impulses that are closely spaced in time)\n    at the expense of frequency resolution (i.e. the ability to discriminate\n    pure tones that are closely spaced in frequency). This effect is known\n    as the time-frequency localization trade-off and needs to be adjusted\n    according to the properties of the input signal ``y``.\n\n    If unspecified, defaults to ``win_length = n_fft``.\n\nwindow : string, tuple, number, function, or np.ndarray [shape=(n_fft,)]\n    Either:\n\n    - a window specification (string, tuple, or number);\n      see `scipy.signal.get_window`\n    - a window function, such as `scipy.signal.windows.hann`\n    - a vector or array of length ``n_fft``\n\n    Defaults to a raised cosine window (`'hann'`), which is adequate for\n    most applications in audio signal processing.\n\n    .. see also:: `filters.get_window`\n\ncenter : boolean\n    If ``True``, the signal ``y`` is padded so that frame\n    ``D[:, t]`` is centered at ``y[t * hop_length]``.\n\n    If ``False``, then ``D[:, t]`` begins at ``y[t * hop_length]``.\n\n    Defaults to ``True``,  which simplifies the alignment of ``D`` onto a\n    time grid by means of `librosa.frames_to_samples`.\n    Note, however, that ``center`` must be set to `False` when analyzing\n    signals with `librosa.stream`.\n\n    .. see also:: `librosa.stream`\n\ndtype : np.dtype, optional\n    Complex numeric type for ``D``.  Default is inferred to match the\n    precision of the input signal.\n\npad_mode : string or function\n    If ``center=True``, this argument is passed to `np.pad` for padding\n    the edges of the signal ``y``. By default (``pad_mode=\"constant\"``),\n    ``y`` is padded on both sides with zeros.\n\n    .. note:: Not all padding modes supported by `numpy.pad` are supported here.\n        `wrap`, `mean`, `maximum`, `median`, and `minimum` are not supported.\n\n        Other modes that depend at most on input values at the edges of the\n        signal (e.g., `constant`, `edge`, `linear_ramp`) are supported.\n\n    If ``center=False``,  this argument is ignored.\n\n    .. see also:: `numpy.pad`\n\nout : np.ndarray or None\n    A pre-allocated, complex-valued array to store the STFT results.\n    This must be of compatible shape and dtype for the given input parameters.\n\n    If `out` is larger than necessary for the provided input signal, then only\n    a prefix slice of `out` will be used.\n\n    If not provided, a new array is allocated and returned.\n\nReturns\n-------\nD : np.ndarray [shape=(..., 1 + n_fft/2, n_frames), dtype=dtype]\n    Complex-valued matrix of short-term Fourier transform\n    coefficients.\n\n    If a pre-allocated `out` array is provided, then `D` will be\n    a reference to `out`.\n\n    If `out` is larger than necessary, then `D` will be a sliced\n    view: `D = out[..., :n_frames]`.\n\nSee Also\n--------\nistft : Inverse STFT\nreassigned_spectrogram : Time-frequency reassigned spectrogram\n\nNotes\n-----\nThis function caches at level 20.\n\nExamples\n--------\n>>> y, sr = librosa.load(librosa.ex('trumpet'))\n>>> S = np.abs(librosa.stft(y))\n>>> S\narray([[5.395e-03, 3.332e-03, ..., 9.862e-07, 1.201e-05],\n       [3.244e-03, 2.690e-03, ..., 9.536e-07, 1.201e-05],\n       ...,\n       [7.523e-05, 3.722e-05, ..., 1.188e-04, 1.031e-03],\n       [7.640e-05, 3.944e-05, ..., 5.180e-04, 1.346e-03]],\n      dtype=float32)\n\nUse left-aligned frames, instead of centered frames\n\n>>> S_left = librosa.stft(y, center=False)\n\nUse a shorter hop length\n\n>>> D_short = librosa.stft(y, hop_length=64)\n\nDisplay a spectrogram\n\n>>> import matplotlib.pyplot as plt\n>>> fig, ax = plt.subplots()\n>>> img = librosa.display.specshow(librosa.amplitude_to_db(S,\n...                                                        ref=np.max),\n...                                y_axis='log', x_axis='time', ax=ax)\n>>> ax.set_title('Power spectrogram')\n>>> fig.colorbar(img, ax=ax, format=\"%+2.0f dB\")", "short_docstring": "Short-time Fourier transform (STFT)."}
{"name": "rsa.encrypt", "type": "callable", "signature": "(message: bytes, pub_key: rsa.key.PublicKey) -> bytes", "docstring": "Encrypts the given message using PKCS#1 v1.5\n\n:param message: the message to encrypt. Must be a byte string no longer than\n    ``k-11`` bytes, where ``k`` is the number of bytes needed to encode\n    the ``n`` component of the public key.\n:param pub_key: the :py:class:`rsa.PublicKey` to encrypt with.\n:raise OverflowError: when the message is too large to fit in the padded\n    block.\n\n>>> from rsa import key, common\n>>> (pub_key, priv_key) = key.newkeys(256)\n>>> message = b'hello'\n>>> crypto = encrypt(message, pub_key)\n\nThe crypto text should be just as long as the public key 'n' component:\n\n>>> len(crypto) == common.byte_size(pub_key.n)\nTrue", "short_docstring": "Encrypts the given message using PKCS#1 v1.5"}
{"name": "seaborn.countplot", "type": "callable", "signature": "(data=None, *, x=None, y=None, hue=None, order=None, hue_order=None, orient=None, color=None, palette=None, saturation=0.75, fill=True, hue_norm=None, stat='count', width=0.8, dodge='auto', gap=0, log_scale=None, native_scale=False, formatter=None, legend='auto', ax=None, **kwargs)", "docstring": "Show the counts of observations in each categorical bin using bars.\n\nA count plot can be thought of as a histogram across a categorical, instead\nof quantitative, variable. The basic API and options are identical to those\nfor :func:`barplot`, so you can compare counts across nested variables.\n\nNote that :func:`histplot` function offers similar functionality with additional\nfeatures (e.g. bar stacking), although its default behavior is somewhat different.\n\nSee the :ref:`tutorial <categorical_tutorial>` for more information.\n\n.. note::\n    By default, this function treats one of the variables as categorical\n    and draws data at ordinal positions (0, 1, ... n) on the relevant axis.\n    As of version 0.13.0, this can be disabled by setting `native_scale=True`.\n\n\nParameters\n----------\ndata : DataFrame, Series, dict, array, or list of arrays\n    Dataset for plotting. If `x` and `y` are absent, this is\n    interpreted as wide-form. Otherwise it is expected to be long-form.    \nx, y, hue : names of variables in `data` or vector data\n    Inputs for plotting long-form data. See examples for interpretation.    \norder, hue_order : lists of strings\n    Order to plot the categorical levels in; otherwise the levels are\n    inferred from the data objects.    \norient : \"v\" | \"h\" | \"x\" | \"y\"\n    Orientation of the plot (vertical or horizontal). This is usually\n    inferred based on the type of the input variables, but it can be used\n    to resolve ambiguity when both `x` and `y` are numeric or when\n    plotting wide-form data.\n\n    .. versionchanged:: v0.13.0\n        Added 'x'/'y' as options, equivalent to 'v'/'h'.    \ncolor : matplotlib color\n    Single color for the elements in the plot.    \npalette : palette name, list, or dict\n    Colors to use for the different levels of the ``hue`` variable. Should\n    be something that can be interpreted by :func:`color_palette`, or a\n    dictionary mapping hue levels to matplotlib colors.    \nsaturation : float\n    Proportion of the original saturation to draw fill colors in. Large\n    patches often look better with desaturated colors, but set this to\n    `1` if you want the colors to perfectly match the input values.    \nhue_norm : tuple or :class:`matplotlib.colors.Normalize` object\n    Normalization in data units for colormap applied to the `hue`\n    variable when it is numeric. Not relevant if `hue` is categorical.\n\n    .. versionadded:: v0.12.0    \nstat : {'count', 'percent', 'proportion', 'probability'}\n    Statistic to compute; when not `'count'`, bar heights will be normalized so that\n    they sum to 100 (for `'percent'`) or 1 (otherwise) across the plot.\n\n    .. versionadded:: v0.13.0\nwidth : float\n    Width allotted to each element on the orient axis. When `native_scale=True`,\n    it is relative to the minimum distance between two values in the native scale.    \ndodge : \"auto\" or bool\n    When hue mapping is used, whether elements should be narrowed and shifted along\n    the orient axis to eliminate overlap. If `\"auto\"`, set to `True` when the\n    orient variable is crossed with the categorical variable or `False` otherwise.\n\n    .. versionchanged:: 0.13.0\n\n        Added `\"auto\"` mode as a new default.    \nlog_scale : bool or number, or pair of bools or numbers\n    Set axis scale(s) to log. A single value sets the data axis for any numeric\n    axes in the plot. A pair of values sets each axis independently.\n    Numeric values are interpreted as the desired base (default 10).\n    When `None` or `False`, seaborn defers to the existing Axes scale.\n\n    .. versionadded:: v0.13.0    \nnative_scale : bool\n    When True, numeric or datetime values on the categorical axis will maintain\n    their original scaling rather than being converted to fixed indices.\n\n    .. versionadded:: v0.13.0    \nformatter : callable\n    Function for converting categorical data into strings. Affects both grouping\n    and tick labels.\n\n    .. versionadded:: v0.13.0    \nlegend : \"auto\", \"brief\", \"full\", or False\n    How to draw the legend. If \"brief\", numeric `hue` and `size`\n    variables will be represented with a sample of evenly spaced values.\n    If \"full\", every group will get an entry in the legend. If \"auto\",\n    choose between brief or full representation based on number of levels.\n    If `False`, no legend data is added and no legend is drawn.\n\n    .. versionadded:: v0.13.0    \nax : matplotlib Axes\n    Axes object to draw the plot onto, otherwise uses the current Axes.    \nkwargs : key, value mappings\n    Other parameters are passed through to :class:`matplotlib.patches.Rectangle`.\n\nReturns\n-------\nax : matplotlib Axes\n    Returns the Axes object with the plot drawn onto it.    \n\nSee Also\n--------\nhistplot : Bin and count observations with additional options.\nbarplot : Show point estimates and confidence intervals using bars.    \ncatplot : Combine a categorical plot with a :class:`FacetGrid`.    \n\nExamples\n--------\n.. include:: ../docstrings/countplot.rst", "short_docstring": "Show the counts of observations in each categorical bin using bars."}
{"name": "sklearn.ensemble.RandomForestClassifier", "type": "class", "signature": "(n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None, monotonic_cst=None)", "docstring": "A random forest classifier.\n\nA random forest is a meta estimator that fits a number of decision tree\nclassifiers on various sub-samples of the dataset and uses averaging to\nimprove the predictive accuracy and control over-fitting.\nTrees in the forest use the best split strategy, i.e. equivalent to passing\n`splitter=\"best\"` to the underlying :class:`~sklearn.tree.DecisionTreeRegressor`.\nThe sub-sample size is controlled with the `max_samples` parameter if\n`bootstrap=True` (default), otherwise the whole dataset is used to build\neach tree.\n\nFor a comparison between tree-based ensemble models see the example\n:ref:`sphx_glr_auto_examples_ensemble_plot_forest_hist_grad_boosting_comparison.py`.\n\nRead more in the :ref:`User Guide <forest>`.\n\nParameters\n----------\nn_estimators : int, default=100\n    The number of trees in the forest.\n\n    .. versionchanged:: 0.22\n       The default value of ``n_estimators`` changed from 10 to 100\n       in 0.22.\n\ncriterion : {\"gini\", \"entropy\", \"log_loss\"}, default=\"gini\"\n    The function to measure the quality of a split. Supported criteria are\n    \"gini\" for the Gini impurity and \"log_loss\" and \"entropy\" both for the\n    Shannon information gain, see :ref:`tree_mathematical_formulation`.\n    Note: This parameter is tree-specific.\n\nmax_depth : int, default=None\n    The maximum depth of the tree. If None, then nodes are expanded until\n    all leaves are pure or until all leaves contain less than\n    min_samples_split samples.\n\nmin_samples_split : int or float, default=2\n    The minimum number of samples required to split an internal node:\n\n    - If int, then consider `min_samples_split` as the minimum number.\n    - If float, then `min_samples_split` is a fraction and\n      `ceil(min_samples_split * n_samples)` are the minimum\n      number of samples for each split.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_samples_leaf : int or float, default=1\n    The minimum number of samples required to be at a leaf node.\n    A split point at any depth will only be considered if it leaves at\n    least ``min_samples_leaf`` training samples in each of the left and\n    right branches.  This may have the effect of smoothing the model,\n    especially in regression.\n\n    - If int, then consider `min_samples_leaf` as the minimum number.\n    - If float, then `min_samples_leaf` is a fraction and\n      `ceil(min_samples_leaf * n_samples)` are the minimum\n      number of samples for each node.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_weight_fraction_leaf : float, default=0.0\n    The minimum weighted fraction of the sum total of weights (of all\n    the input samples) required to be at a leaf node. Samples have\n    equal weight when sample_weight is not provided.\n\nmax_features : {\"sqrt\", \"log2\", None}, int or float, default=\"sqrt\"\n    The number of features to consider when looking for the best split:\n\n    - If int, then consider `max_features` features at each split.\n    - If float, then `max_features` is a fraction and\n      `max(1, int(max_features * n_features_in_))` features are considered at each\n      split.\n    - If \"sqrt\", then `max_features=sqrt(n_features)`.\n    - If \"log2\", then `max_features=log2(n_features)`.\n    - If None, then `max_features=n_features`.\n\n    .. versionchanged:: 1.1\n        The default of `max_features` changed from `\"auto\"` to `\"sqrt\"`.\n\n    Note: the search for a split does not stop until at least one\n    valid partition of the node samples is found, even if it requires to\n    effectively inspect more than ``max_features`` features.\n\nmax_leaf_nodes : int, default=None\n    Grow trees with ``max_leaf_nodes`` in best-first fashion.\n    Best nodes are defined as relative reduction in impurity.\n    If None then unlimited number of leaf nodes.\n\nmin_impurity_decrease : float, default=0.0\n    A node will be split if this split induces a decrease of the impurity\n    greater than or equal to this value.\n\n    The weighted impurity decrease equation is the following::\n\n        N_t / N * (impurity - N_t_R / N_t * right_impurity\n                            - N_t_L / N_t * left_impurity)\n\n    where ``N`` is the total number of samples, ``N_t`` is the number of\n    samples at the current node, ``N_t_L`` is the number of samples in the\n    left child, and ``N_t_R`` is the number of samples in the right child.\n\n    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n    if ``sample_weight`` is passed.\n\n    .. versionadded:: 0.19\n\nbootstrap : bool, default=True\n    Whether bootstrap samples are used when building trees. If False, the\n    whole dataset is used to build each tree.\n\noob_score : bool or callable, default=False\n    Whether to use out-of-bag samples to estimate the generalization score.\n    By default, :func:`~sklearn.metrics.accuracy_score` is used.\n    Provide a callable with signature `metric(y_true, y_pred)` to use a\n    custom metric. Only available if `bootstrap=True`.\n\nn_jobs : int, default=None\n    The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n    :meth:`decision_path` and :meth:`apply` are all parallelized over the\n    trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n    context. ``-1`` means using all processors. See :term:`Glossary\n    <n_jobs>` for more details.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls both the randomness of the bootstrapping of the samples used\n    when building trees (if ``bootstrap=True``) and the sampling of the\n    features to consider when looking for the best split at each node\n    (if ``max_features < n_features``).\n    See :term:`Glossary <random_state>` for details.\n\nverbose : int, default=0\n    Controls the verbosity when fitting and predicting.\n\nwarm_start : bool, default=False\n    When set to ``True``, reuse the solution of the previous call to fit\n    and add more estimators to the ensemble, otherwise, just fit a whole\n    new forest. See :term:`Glossary <warm_start>` and\n    :ref:`gradient_boosting_warm_start` for details.\n\nclass_weight : {\"balanced\", \"balanced_subsample\"}, dict or list of dicts,             default=None\n    Weights associated with classes in the form ``{class_label: weight}``.\n    If not given, all classes are supposed to have weight one. For\n    multi-output problems, a list of dicts can be provided in the same\n    order as the columns of y.\n\n    Note that for multioutput (including multilabel) weights should be\n    defined for each class of every column in its own dict. For example,\n    for four-class multilabel classification weights should be\n    [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n    [{1:1}, {2:5}, {3:1}, {4:1}].\n\n    The \"balanced\" mode uses the values of y to automatically adjust\n    weights inversely proportional to class frequencies in the input data\n    as ``n_samples / (n_classes * np.bincount(y))``\n\n    The \"balanced_subsample\" mode is the same as \"balanced\" except that\n    weights are computed based on the bootstrap sample for every tree\n    grown.\n\n    For multi-output, the weights of each column of y will be multiplied.\n\n    Note that these weights will be multiplied with sample_weight (passed\n    through the fit method) if sample_weight is specified.\n\nccp_alpha : non-negative float, default=0.0\n    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n    subtree with the largest cost complexity that is smaller than\n    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n    :ref:`minimal_cost_complexity_pruning` for details.\n\n    .. versionadded:: 0.22\n\nmax_samples : int or float, default=None\n    If bootstrap is True, the number of samples to draw from X\n    to train each base estimator.\n\n    - If None (default), then draw `X.shape[0]` samples.\n    - If int, then draw `max_samples` samples.\n    - If float, then draw `max(round(n_samples * max_samples), 1)` samples. Thus,\n      `max_samples` should be in the interval `(0.0, 1.0]`.\n\n    .. versionadded:: 0.22\n\nmonotonic_cst : array-like of int of shape (n_features), default=None\n    Indicates the monotonicity constraint to enforce on each feature.\n      - 1: monotonic increase\n      - 0: no constraint\n      - -1: monotonic decrease\n\n    If monotonic_cst is None, no constraints are applied.\n\n    Monotonicity constraints are not supported for:\n      - multiclass classifications (i.e. when `n_classes > 2`),\n      - multioutput classifications (i.e. when `n_outputs_ > 1`),\n      - classifications trained on data with missing values.\n\n    The constraints hold over the probability of the positive class.\n\n    Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.\n\n    .. versionadded:: 1.4\n\nAttributes\n----------\nestimator_ : :class:`~sklearn.tree.DecisionTreeClassifier`\n    The child estimator template used to create the collection of fitted\n    sub-estimators.\n\n    .. versionadded:: 1.2\n       `base_estimator_` was renamed to `estimator_`.\n\nestimators_ : list of DecisionTreeClassifier\n    The collection of fitted sub-estimators.\n\nclasses_ : ndarray of shape (n_classes,) or a list of such arrays\n    The classes labels (single output problem), or a list of arrays of\n    class labels (multi-output problem).\n\nn_classes_ : int or list\n    The number of classes (single output problem), or a list containing the\n    number of classes for each output (multi-output problem).\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nn_outputs_ : int\n    The number of outputs when ``fit`` is performed.\n\nfeature_importances_ : ndarray of shape (n_features,)\n    The impurity-based feature importances.\n    The higher, the more important the feature.\n    The importance of a feature is computed as the (normalized)\n    total reduction of the criterion brought by that feature.  It is also\n    known as the Gini importance.\n\n    Warning: impurity-based feature importances can be misleading for\n    high cardinality features (many unique values). See\n    :func:`sklearn.inspection.permutation_importance` as an alternative.\n\noob_score_ : float\n    Score of the training dataset obtained using an out-of-bag estimate.\n    This attribute exists only when ``oob_score`` is True.\n\noob_decision_function_ : ndarray of shape (n_samples, n_classes) or             (n_samples, n_classes, n_outputs)\n    Decision function computed with out-of-bag estimate on the training\n    set. If n_estimators is small it might be possible that a data point\n    was never left out during the bootstrap. In this case,\n    `oob_decision_function_` might contain NaN. This attribute exists\n    only when ``oob_score`` is True.\n\nestimators_samples_ : list of arrays\n    The subset of drawn samples (i.e., the in-bag samples) for each base\n    estimator. Each subset is defined by an array of the indices selected.\n\n    .. versionadded:: 1.4\n\nSee Also\n--------\nsklearn.tree.DecisionTreeClassifier : A decision tree classifier.\nsklearn.ensemble.ExtraTreesClassifier : Ensemble of extremely randomized\n    tree classifiers.\nsklearn.ensemble.HistGradientBoostingClassifier : A Histogram-based Gradient\n    Boosting Classification Tree, very fast for big datasets (n_samples >=\n    10_000).\n\nNotes\n-----\nThe default values for the parameters controlling the size of the trees\n(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\nunpruned trees which can potentially be very large on some data sets. To\nreduce memory consumption, the complexity and size of the trees should be\ncontrolled by setting those parameter values.\n\nThe features are always randomly permuted at each split. Therefore,\nthe best found split may vary, even with the same training data,\n``max_features=n_features`` and ``bootstrap=False``, if the improvement\nof the criterion is identical for several splits enumerated during the\nsearch of the best split. To obtain a deterministic behaviour during\nfitting, ``random_state`` has to be fixed.\n\nReferences\n----------\n.. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n\nExamples\n--------\n>>> from sklearn.ensemble import RandomForestClassifier\n>>> from sklearn.datasets import make_classification\n>>> X, y = make_classification(n_samples=1000, n_features=4,\n...                            n_informative=2, n_redundant=0,\n...                            random_state=0, shuffle=False)\n>>> clf = RandomForestClassifier(max_depth=2, random_state=0)\n>>> clf.fit(X, y)\nRandomForestClassifier(...)\n>>> print(clf.predict([[0, 0, 0, 0]]))\n[1]", "short_docstring": "A random forest classifier."}
{"name": "pandas.date_range", "type": "callable", "signature": "(start=None, end=None, periods=None, freq=None, tz=None, normalize: 'bool' = False, name: 'Hashable | None' = None, inclusive: 'IntervalClosedType' = 'both', *, unit: 'str | None' = None, **kwargs) -> 'DatetimeIndex'", "docstring": "Return a fixed frequency DatetimeIndex.\n\nReturns the range of equally spaced time points (where the difference between any\ntwo adjacent points is specified by the given frequency) such that they all\nsatisfy `start <[=] x <[=] end`, where the first one and the last one are, resp.,\nthe first and last time points in that range that fall on the boundary of ``freq``\n(if given as a frequency string) or that are valid for ``freq`` (if given as a\n:class:`pandas.tseries.offsets.DateOffset`). (If exactly one of ``start``,\n``end``, or ``freq`` is *not* specified, this missing parameter can be computed\ngiven ``periods``, the number of timesteps in the range. See the note below.)\n\nParameters\n----------\nstart : str or datetime-like, optional\n    Left bound for generating dates.\nend : str or datetime-like, optional\n    Right bound for generating dates.\nperiods : int, optional\n    Number of periods to generate.\nfreq : str, Timedelta, datetime.timedelta, or DateOffset, default 'D'\n    Frequency strings can have multiples, e.g. '5h'. See\n    :ref:`here <timeseries.offset_aliases>` for a list of\n    frequency aliases.\ntz : str or tzinfo, optional\n    Time zone name for returning localized DatetimeIndex, for example\n    'Asia/Hong_Kong'. By default, the resulting DatetimeIndex is\n    timezone-naive unless timezone-aware datetime-likes are passed.\nnormalize : bool, default False\n    Normalize start/end dates to midnight before generating date range.\nname : str, default None\n    Name of the resulting DatetimeIndex.\ninclusive : {\"both\", \"neither\", \"left\", \"right\"}, default \"both\"\n    Include boundaries; Whether to set each bound as closed or open.\n\n    .. versionadded:: 1.4.0\nunit : str, default None\n    Specify the desired resolution of the result.\n\n    .. versionadded:: 2.0.0\n**kwargs\n    For compatibility. Has no effect on the result.\n\nReturns\n-------\nDatetimeIndex\n\nSee Also\n--------\nDatetimeIndex : An immutable container for datetimes.\ntimedelta_range : Return a fixed frequency TimedeltaIndex.\nperiod_range : Return a fixed frequency PeriodIndex.\ninterval_range : Return a fixed frequency IntervalIndex.\n\nNotes\n-----\nOf the four parameters ``start``, ``end``, ``periods``, and ``freq``,\nexactly three must be specified. If ``freq`` is omitted, the resulting\n``DatetimeIndex`` will have ``periods`` linearly spaced elements between\n``start`` and ``end`` (closed on both sides).\n\nTo learn more about the frequency strings, please see `this link\n<https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases>`__.\n\nExamples\n--------\n**Specifying the values**\n\nThe next four examples generate the same `DatetimeIndex`, but vary\nthe combination of `start`, `end` and `periods`.\n\nSpecify `start` and `end`, with the default daily frequency.\n\n>>> pd.date_range(start='1/1/2018', end='1/08/2018')\nDatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03', '2018-01-04',\n               '2018-01-05', '2018-01-06', '2018-01-07', '2018-01-08'],\n              dtype='datetime64[ns]', freq='D')\n\nSpecify timezone-aware `start` and `end`, with the default daily frequency.\n\n>>> pd.date_range(\n...     start=pd.to_datetime(\"1/1/2018\").tz_localize(\"Europe/Berlin\"),\n...     end=pd.to_datetime(\"1/08/2018\").tz_localize(\"Europe/Berlin\"),\n... )\nDatetimeIndex(['2018-01-01 00:00:00+01:00', '2018-01-02 00:00:00+01:00',\n               '2018-01-03 00:00:00+01:00', '2018-01-04 00:00:00+01:00',\n               '2018-01-05 00:00:00+01:00', '2018-01-06 00:00:00+01:00',\n               '2018-01-07 00:00:00+01:00', '2018-01-08 00:00:00+01:00'],\n              dtype='datetime64[ns, Europe/Berlin]', freq='D')\n\nSpecify `start` and `periods`, the number of periods (days).\n\n>>> pd.date_range(start='1/1/2018', periods=8)\nDatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03', '2018-01-04',\n               '2018-01-05', '2018-01-06', '2018-01-07', '2018-01-08'],\n              dtype='datetime64[ns]', freq='D')\n\nSpecify `end` and `periods`, the number of periods (days).\n\n>>> pd.date_range(end='1/1/2018', periods=8)\nDatetimeIndex(['2017-12-25', '2017-12-26', '2017-12-27', '2017-12-28',\n               '2017-12-29', '2017-12-30', '2017-12-31', '2018-01-01'],\n              dtype='datetime64[ns]', freq='D')\n\nSpecify `start`, `end`, and `periods`; the frequency is generated\nautomatically (linearly spaced).\n\n>>> pd.date_range(start='2018-04-24', end='2018-04-27', periods=3)\nDatetimeIndex(['2018-04-24 00:00:00', '2018-04-25 12:00:00',\n               '2018-04-27 00:00:00'],\n              dtype='datetime64[ns]', freq=None)\n\n**Other Parameters**\n\nChanged the `freq` (frequency) to ``'ME'`` (month end frequency).\n\n>>> pd.date_range(start='1/1/2018', periods=5, freq='ME')\nDatetimeIndex(['2018-01-31', '2018-02-28', '2018-03-31', '2018-04-30',\n               '2018-05-31'],\n              dtype='datetime64[ns]', freq='ME')\n\nMultiples are allowed\n\n>>> pd.date_range(start='1/1/2018', periods=5, freq='3ME')\nDatetimeIndex(['2018-01-31', '2018-04-30', '2018-07-31', '2018-10-31',\n               '2019-01-31'],\n              dtype='datetime64[ns]', freq='3ME')\n\n`freq` can also be specified as an Offset object.\n\n>>> pd.date_range(start='1/1/2018', periods=5, freq=pd.offsets.MonthEnd(3))\nDatetimeIndex(['2018-01-31', '2018-04-30', '2018-07-31', '2018-10-31',\n               '2019-01-31'],\n              dtype='datetime64[ns]', freq='3ME')\n\nSpecify `tz` to set the timezone.\n\n>>> pd.date_range(start='1/1/2018', periods=5, tz='Asia/Tokyo')\nDatetimeIndex(['2018-01-01 00:00:00+09:00', '2018-01-02 00:00:00+09:00',\n               '2018-01-03 00:00:00+09:00', '2018-01-04 00:00:00+09:00',\n               '2018-01-05 00:00:00+09:00'],\n              dtype='datetime64[ns, Asia/Tokyo]', freq='D')\n\n`inclusive` controls whether to include `start` and `end` that are on the\nboundary. The default, \"both\", includes boundary points on either end.\n\n>>> pd.date_range(start='2017-01-01', end='2017-01-04', inclusive=\"both\")\nDatetimeIndex(['2017-01-01', '2017-01-02', '2017-01-03', '2017-01-04'],\n              dtype='datetime64[ns]', freq='D')\n\nUse ``inclusive='left'`` to exclude `end` if it falls on the boundary.\n\n>>> pd.date_range(start='2017-01-01', end='2017-01-04', inclusive='left')\nDatetimeIndex(['2017-01-01', '2017-01-02', '2017-01-03'],\n              dtype='datetime64[ns]', freq='D')\n\nUse ``inclusive='right'`` to exclude `start` if it falls on the boundary, and\nsimilarly ``inclusive='neither'`` will exclude both `start` and `end`.\n\n>>> pd.date_range(start='2017-01-01', end='2017-01-04', inclusive='right')\nDatetimeIndex(['2017-01-02', '2017-01-03', '2017-01-04'],\n              dtype='datetime64[ns]', freq='D')\n\n**Specify a unit**\n\n>>> pd.date_range(start=\"2017-01-01\", periods=10, freq=\"100YS\", unit=\"s\")\nDatetimeIndex(['2017-01-01', '2117-01-01', '2217-01-01', '2317-01-01',\n               '2417-01-01', '2517-01-01', '2617-01-01', '2717-01-01',\n               '2817-01-01', '2917-01-01'],\n              dtype='datetime64[s]', freq='100YS-JAN')", "short_docstring": "Return a fixed frequency DatetimeIndex."}
{"name": "keras.layers.Dense", "error": "Import error: No module named 'tensorflow'"}
{"name": "binascii.hexlify", "type": "callable", "signature": null, "docstring": "Hexadecimal representation of binary data.\n\n  sep\n    An optional single character or byte to separate hex bytes.\n  bytes_per_sep\n    How many bytes between separators.  Positive values count from the\n    right, negative values count from the left.\n\nThe return value is a bytes object.  This function is also\navailable as \"b2a_hex()\".", "short_docstring": "Hexadecimal representation of binary data."}
{"name": "typing.Tuple", "type": "callable", "signature": "(*args, **kwargs)", "docstring": "Tuple type; Tuple[X, Y] is the cross-product type of X and Y.\n\nExample: Tuple[T1, T2] is a tuple of two elements corresponding\nto type variables T1 and T2.  Tuple[int, float, str] is a tuple\nof an int, a float and a string.\n\nTo specify a variable-length tuple of homogeneous type, use Tuple[T, ...].", "short_docstring": "Tuple type; Tuple[X, Y] is the cross-product type of X and Y."}
{"name": "wtforms.validators.DataRequired", "type": "class", "signature": "(message=None)", "docstring": "Checks the field's data is 'truthy' otherwise stops the validation chain.\n\nThis validator checks that the ``data`` attribute on the field is a 'true'\nvalue (effectively, it does ``if field.data``.) Furthermore, if the data\nis a string type, a string containing only whitespace characters is\nconsidered false.\n\nIf the data is empty, also removes prior errors (such as processing errors)\nfrom the field.\n\n**NOTE** this validator used to be called `Required` but the way it behaved\n(requiring coerced data, not input data) meant it functioned in a way\nwhich was not symmetric to the `Optional` validator and furthermore caused\nconfusion with certain fields which coerced data to 'falsey' values like\n``0``, ``Decimal(0)``, ``time(0)`` etc. Unless a very specific reason\nexists, we recommend using the :class:`InputRequired` instead.\n\n:param message:\n    Error message to raise in case of a validation error.\n\nSets the `required` attribute on widgets.", "short_docstring": "Checks the field's data is 'truthy' otherwise stops the validation chain."}
{"name": "os.remove", "type": "callable", "signature": "(path, *, dir_fd=None)", "docstring": "Remove a file (same as unlink()).\n\nIf dir_fd is not None, it should be a file descriptor open to a directory,\n  and path should be relative; path will then be relative to that directory.\ndir_fd may not be implemented on your platform.\n  If it is unavailable, using it will raise a NotImplementedError.", "short_docstring": "Remove a file (same as unlink())."}
{"name": "os.path.dirname", "type": "callable", "signature": "(p)", "docstring": "Returns the directory component of a pathname", "short_docstring": "Returns the directory component of a pathname"}
{"name": "flask_login.logout_user", "type": "callable", "signature": "()", "docstring": "Logs a user out. (You do not need to pass the actual user.) This will\nalso clean up the remember me cookie if it exists.", "short_docstring": "Logs a user out. (You do not need to pass the actual user.) This will\nalso clean up the remember me cookie if it exists."}
{"name": "pandas.read_csv", "type": "callable", "signature": "(filepath_or_buffer: 'FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str]', *, sep: 'str | None | lib.NoDefault' = <no_default>, delimiter: 'str | None | lib.NoDefault' = None, header: \"int | Sequence[int] | None | Literal['infer']\" = 'infer', names: 'Sequence[Hashable] | None | lib.NoDefault' = <no_default>, index_col: 'IndexLabel | Literal[False] | None' = None, usecols: 'UsecolsArgType' = None, dtype: 'DtypeArg | None' = None, engine: 'CSVEngine | None' = None, converters: 'Mapping[Hashable, Callable] | None' = None, true_values: 'list | None' = None, false_values: 'list | None' = None, skipinitialspace: 'bool' = False, skiprows: 'list[int] | int | Callable[[Hashable], bool] | None' = None, skipfooter: 'int' = 0, nrows: 'int | None' = None, na_values: 'Hashable | Iterable[Hashable] | Mapping[Hashable, Iterable[Hashable]] | None' = None, keep_default_na: 'bool' = True, na_filter: 'bool' = True, verbose: 'bool | lib.NoDefault' = <no_default>, skip_blank_lines: 'bool' = True, parse_dates: 'bool | Sequence[Hashable] | None' = None, infer_datetime_format: 'bool | lib.NoDefault' = <no_default>, keep_date_col: 'bool | lib.NoDefault' = <no_default>, date_parser: 'Callable | lib.NoDefault' = <no_default>, date_format: 'str | dict[Hashable, str] | None' = None, dayfirst: 'bool' = False, cache_dates: 'bool' = True, iterator: 'bool' = False, chunksize: 'int | None' = None, compression: 'CompressionOptions' = 'infer', thousands: 'str | None' = None, decimal: 'str' = '.', lineterminator: 'str | None' = None, quotechar: 'str' = '\"', quoting: 'int' = 0, doublequote: 'bool' = True, escapechar: 'str | None' = None, comment: 'str | None' = None, encoding: 'str | None' = None, encoding_errors: 'str | None' = 'strict', dialect: 'str | csv.Dialect | None' = None, on_bad_lines: 'str' = 'error', delim_whitespace: 'bool | lib.NoDefault' = <no_default>, low_memory: 'bool' = True, memory_map: 'bool' = False, float_precision: \"Literal['high', 'legacy'] | None\" = None, storage_options: 'StorageOptions | None' = None, dtype_backend: 'DtypeBackend | lib.NoDefault' = <no_default>) -> 'DataFrame | TextFileReader'", "docstring": "Read a comma-separated values (csv) file into DataFrame.\n\nAlso supports optionally iterating or breaking of the file\ninto chunks.\n\nAdditional help can be found in the online docs for\n`IO Tools <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html>`_.\n\nParameters\n----------\nfilepath_or_buffer : str, path object or file-like object\n    Any valid string path is acceptable. The string could be a URL. Valid\n    URL schemes include http, ftp, s3, gs, and file. For file URLs, a host is\n    expected. A local file could be: file://localhost/path/to/table.csv.\n\n    If you want to pass in a path object, pandas accepts any ``os.PathLike``.\n\n    By file-like object, we refer to objects with a ``read()`` method, such as\n    a file handle (e.g. via builtin ``open`` function) or ``StringIO``.\nsep : str, default ','\n    Character or regex pattern to treat as the delimiter. If ``sep=None``, the\n    C engine cannot automatically detect\n    the separator, but the Python parsing engine can, meaning the latter will\n    be used and automatically detect the separator from only the first valid\n    row of the file by Python's builtin sniffer tool, ``csv.Sniffer``.\n    In addition, separators longer than 1 character and different from\n    ``'\\s+'`` will be interpreted as regular expressions and will also force\n    the use of the Python parsing engine. Note that regex delimiters are prone\n    to ignoring quoted data. Regex example: ``'\\r\\t'``.\ndelimiter : str, optional\n    Alias for ``sep``.\nheader : int, Sequence of int, 'infer' or None, default 'infer'\n    Row number(s) containing column labels and marking the start of the\n    data (zero-indexed). Default behavior is to infer the column names: if no ``names``\n    are passed the behavior is identical to ``header=0`` and column\n    names are inferred from the first line of the file, if column\n    names are passed explicitly to ``names`` then the behavior is identical to\n    ``header=None``. Explicitly pass ``header=0`` to be able to\n    replace existing names. The header can be a list of integers that\n    specify row locations for a :class:`~pandas.MultiIndex` on the columns\n    e.g. ``[0, 1, 3]``. Intervening rows that are not specified will be\n    skipped (e.g. 2 in this example is skipped). Note that this\n    parameter ignores commented lines and empty lines if\n    ``skip_blank_lines=True``, so ``header=0`` denotes the first line of\n    data rather than the first line of the file.\nnames : Sequence of Hashable, optional\n    Sequence of column labels to apply. If the file contains a header row,\n    then you should explicitly pass ``header=0`` to override the column names.\n    Duplicates in this list are not allowed.\nindex_col : Hashable, Sequence of Hashable or False, optional\n  Column(s) to use as row label(s), denoted either by column labels or column\n  indices.  If a sequence of labels or indices is given, :class:`~pandas.MultiIndex`\n  will be formed for the row labels.\n\n  Note: ``index_col=False`` can be used to force pandas to *not* use the first\n  column as the index, e.g., when you have a malformed file with delimiters at\n  the end of each line.\nusecols : Sequence of Hashable or Callable, optional\n    Subset of columns to select, denoted either by column labels or column indices.\n    If list-like, all elements must either\n    be positional (i.e. integer indices into the document columns) or strings\n    that correspond to column names provided either by the user in ``names`` or\n    inferred from the document header row(s). If ``names`` are given, the document\n    header row(s) are not taken into account. For example, a valid list-like\n    ``usecols`` parameter would be ``[0, 1, 2]`` or ``['foo', 'bar', 'baz']``.\n    Element order is ignored, so ``usecols=[0, 1]`` is the same as ``[1, 0]``.\n    To instantiate a :class:`~pandas.DataFrame` from ``data`` with element order\n    preserved use ``pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']]``\n    for columns in ``['foo', 'bar']`` order or\n    ``pd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']]``\n    for ``['bar', 'foo']`` order.\n\n    If callable, the callable function will be evaluated against the column\n    names, returning names where the callable function evaluates to ``True``. An\n    example of a valid callable argument would be ``lambda x: x.upper() in\n    ['AAA', 'BBB', 'DDD']``. Using this parameter results in much faster\n    parsing time and lower memory usage.\ndtype : dtype or dict of {Hashable : dtype}, optional\n    Data type(s) to apply to either the whole dataset or individual columns.\n    E.g., ``{'a': np.float64, 'b': np.int32, 'c': 'Int64'}``\n    Use ``str`` or ``object`` together with suitable ``na_values`` settings\n    to preserve and not interpret ``dtype``.\n    If ``converters`` are specified, they will be applied INSTEAD\n    of ``dtype`` conversion.\n\n    .. versionadded:: 1.5.0\n\n        Support for ``defaultdict`` was added. Specify a ``defaultdict`` as input where\n        the default determines the ``dtype`` of the columns which are not explicitly\n        listed.\nengine : {'c', 'python', 'pyarrow'}, optional\n    Parser engine to use. The C and pyarrow engines are faster, while the python engine\n    is currently more feature-complete. Multithreading is currently only supported by\n    the pyarrow engine.\n\n    .. versionadded:: 1.4.0\n\n        The 'pyarrow' engine was added as an *experimental* engine, and some features\n        are unsupported, or may not work correctly, with this engine.\nconverters : dict of {Hashable : Callable}, optional\n    Functions for converting values in specified columns. Keys can either\n    be column labels or column indices.\ntrue_values : list, optional\n    Values to consider as ``True`` in addition to case-insensitive variants of 'True'.\nfalse_values : list, optional\n    Values to consider as ``False`` in addition to case-insensitive variants of 'False'.\nskipinitialspace : bool, default False\n    Skip spaces after delimiter.\nskiprows : int, list of int or Callable, optional\n    Line numbers to skip (0-indexed) or number of lines to skip (``int``)\n    at the start of the file.\n\n    If callable, the callable function will be evaluated against the row\n    indices, returning ``True`` if the row should be skipped and ``False`` otherwise.\n    An example of a valid callable argument would be ``lambda x: x in [0, 2]``.\nskipfooter : int, default 0\n    Number of lines at bottom of file to skip (Unsupported with ``engine='c'``).\nnrows : int, optional\n    Number of rows of file to read. Useful for reading pieces of large files.\nna_values : Hashable, Iterable of Hashable or dict of {Hashable : Iterable}, optional\n    Additional strings to recognize as ``NA``/``NaN``. If ``dict`` passed, specific\n    per-column ``NA`` values.  By default the following values are interpreted as\n    ``NaN``: \" \", \"#N/A\", \"#N/A N/A\", \"#NA\", \"-1.#IND\", \"-1.#QNAN\", \"-NaN\", \"-nan\",\n    \"1.#IND\", \"1.#QNAN\", \"<NA>\", \"N/A\", \"NA\", \"NULL\", \"NaN\", \"None\",\n    \"n/a\", \"nan\", \"null \".\n\nkeep_default_na : bool, default True\n    Whether or not to include the default ``NaN`` values when parsing the data.\n    Depending on whether ``na_values`` is passed in, the behavior is as follows:\n\n    * If ``keep_default_na`` is ``True``, and ``na_values`` are specified, ``na_values``\n      is appended to the default ``NaN`` values used for parsing.\n    * If ``keep_default_na`` is ``True``, and ``na_values`` are not specified, only\n      the default ``NaN`` values are used for parsing.\n    * If ``keep_default_na`` is ``False``, and ``na_values`` are specified, only\n      the ``NaN`` values specified ``na_values`` are used for parsing.\n    * If ``keep_default_na`` is ``False``, and ``na_values`` are not specified, no\n      strings will be parsed as ``NaN``.\n\n    Note that if ``na_filter`` is passed in as ``False``, the ``keep_default_na`` and\n    ``na_values`` parameters will be ignored.\nna_filter : bool, default True\n    Detect missing value markers (empty strings and the value of ``na_values``). In\n    data without any ``NA`` values, passing ``na_filter=False`` can improve the\n    performance of reading a large file.\nverbose : bool, default False\n    Indicate number of ``NA`` values placed in non-numeric columns.\n\n    .. deprecated:: 2.2.0\nskip_blank_lines : bool, default True\n    If ``True``, skip over blank lines rather than interpreting as ``NaN`` values.\nparse_dates : bool, list of Hashable, list of lists or dict of {Hashable : list}, default False\n    The behavior is as follows:\n\n    * ``bool``. If ``True`` -> try parsing the index. Note: Automatically set to\n      ``True`` if ``date_format`` or ``date_parser`` arguments have been passed.\n    * ``list`` of ``int`` or names. e.g. If ``[1, 2, 3]`` -> try parsing columns 1, 2, 3\n      each as a separate date column.\n    * ``list`` of ``list``. e.g.  If ``[[1, 3]]`` -> combine columns 1 and 3 and parse\n      as a single date column. Values are joined with a space before parsing.\n    * ``dict``, e.g. ``{'foo' : [1, 3]}`` -> parse columns 1, 3 as date and call\n      result 'foo'. Values are joined with a space before parsing.\n\n    If a column or index cannot be represented as an array of ``datetime``,\n    say because of an unparsable value or a mixture of timezones, the column\n    or index will be returned unaltered as an ``object`` data type. For\n    non-standard ``datetime`` parsing, use :func:`~pandas.to_datetime` after\n    :func:`~pandas.read_csv`.\n\n    Note: A fast-path exists for iso8601-formatted dates.\ninfer_datetime_format : bool, default False\n    If ``True`` and ``parse_dates`` is enabled, pandas will attempt to infer the\n    format of the ``datetime`` strings in the columns, and if it can be inferred,\n    switch to a faster method of parsing them. In some cases this can increase\n    the parsing speed by 5-10x.\n\n    .. deprecated:: 2.0.0\n        A strict version of this argument is now the default, passing it has no effect.\n\nkeep_date_col : bool, default False\n    If ``True`` and ``parse_dates`` specifies combining multiple columns then\n    keep the original columns.\ndate_parser : Callable, optional\n    Function to use for converting a sequence of string columns to an array of\n    ``datetime`` instances. The default uses ``dateutil.parser.parser`` to do the\n    conversion. pandas will try to call ``date_parser`` in three different ways,\n    advancing to the next if an exception occurs: 1) Pass one or more arrays\n    (as defined by ``parse_dates``) as arguments; 2) concatenate (row-wise) the\n    string values from the columns defined by ``parse_dates`` into a single array\n    and pass that; and 3) call ``date_parser`` once for each row using one or\n    more strings (corresponding to the columns defined by ``parse_dates``) as\n    arguments.\n\n    .. deprecated:: 2.0.0\n       Use ``date_format`` instead, or read in as ``object`` and then apply\n       :func:`~pandas.to_datetime` as-needed.\ndate_format : str or dict of column -> format, optional\n    Format to use for parsing dates when used in conjunction with ``parse_dates``.\n    The strftime to parse time, e.g. :const:`\"%d/%m/%Y\"`. See\n    `strftime documentation\n    <https://docs.python.org/3/library/datetime.html\n    #strftime-and-strptime-behavior>`_ for more information on choices, though\n    note that :const:`\"%f\"` will parse all the way up to nanoseconds.\n    You can also pass:\n\n    - \"ISO8601\", to parse any `ISO8601 <https://en.wikipedia.org/wiki/ISO_8601>`_\n        time string (not necessarily in exactly the same format);\n    - \"mixed\", to infer the format for each element individually. This is risky,\n        and you should probably use it along with `dayfirst`.\n\n    .. versionadded:: 2.0.0\ndayfirst : bool, default False\n    DD/MM format dates, international and European format.\ncache_dates : bool, default True\n    If ``True``, use a cache of unique, converted dates to apply the ``datetime``\n    conversion. May produce significant speed-up when parsing duplicate\n    date strings, especially ones with timezone offsets.\n\niterator : bool, default False\n    Return ``TextFileReader`` object for iteration or getting chunks with\n    ``get_chunk()``.\nchunksize : int, optional\n    Number of lines to read from the file per chunk. Passing a value will cause the\n    function to return a ``TextFileReader`` object for iteration.\n    See the `IO Tools docs\n    <https://pandas.pydata.org/pandas-docs/stable/io.html#io-chunking>`_\n    for more information on ``iterator`` and ``chunksize``.\n\ncompression : str or dict, default 'infer'\n    For on-the-fly decompression of on-disk data. If 'infer' and 'filepath_or_buffer' is\n    path-like, then detect compression from the following extensions: '.gz',\n    '.bz2', '.zip', '.xz', '.zst', '.tar', '.tar.gz', '.tar.xz' or '.tar.bz2'\n    (otherwise no compression).\n    If using 'zip' or 'tar', the ZIP file must contain only one data file to be read in.\n    Set to ``None`` for no decompression.\n    Can also be a dict with key ``'method'`` set\n    to one of {``'zip'``, ``'gzip'``, ``'bz2'``, ``'zstd'``, ``'xz'``, ``'tar'``} and\n    other key-value pairs are forwarded to\n    ``zipfile.ZipFile``, ``gzip.GzipFile``,\n    ``bz2.BZ2File``, ``zstandard.ZstdDecompressor``, ``lzma.LZMAFile`` or\n    ``tarfile.TarFile``, respectively.\n    As an example, the following could be passed for Zstandard decompression using a\n    custom compression dictionary:\n    ``compression={'method': 'zstd', 'dict_data': my_compression_dict}``.\n\n    .. versionadded:: 1.5.0\n        Added support for `.tar` files.\n\n    .. versionchanged:: 1.4.0 Zstandard support.\n\nthousands : str (length 1), optional\n    Character acting as the thousands separator in numerical values.\ndecimal : str (length 1), default '.'\n    Character to recognize as decimal point (e.g., use ',' for European data).\nlineterminator : str (length 1), optional\n    Character used to denote a line break. Only valid with C parser.\nquotechar : str (length 1), optional\n    Character used to denote the start and end of a quoted item. Quoted\n    items can include the ``delimiter`` and it will be ignored.\nquoting : {0 or csv.QUOTE_MINIMAL, 1 or csv.QUOTE_ALL, 2 or csv.QUOTE_NONNUMERIC, 3 or csv.QUOTE_NONE}, default csv.QUOTE_MINIMAL\n    Control field quoting behavior per ``csv.QUOTE_*`` constants. Default is\n    ``csv.QUOTE_MINIMAL`` (i.e., 0) which implies that only fields containing special\n    characters are quoted (e.g., characters defined in ``quotechar``, ``delimiter``,\n    or ``lineterminator``.\ndoublequote : bool, default True\n   When ``quotechar`` is specified and ``quoting`` is not ``QUOTE_NONE``, indicate\n   whether or not to interpret two consecutive ``quotechar`` elements INSIDE a\n   field as a single ``quotechar`` element.\nescapechar : str (length 1), optional\n    Character used to escape other characters.\ncomment : str (length 1), optional\n    Character indicating that the remainder of line should not be parsed.\n    If found at the beginning\n    of a line, the line will be ignored altogether. This parameter must be a\n    single character. Like empty lines (as long as ``skip_blank_lines=True``),\n    fully commented lines are ignored by the parameter ``header`` but not by\n    ``skiprows``. For example, if ``comment='#'``, parsing\n    ``#empty\\na,b,c\\n1,2,3`` with ``header=0`` will result in ``'a,b,c'`` being\n    treated as the header.\nencoding : str, optional, default 'utf-8'\n    Encoding to use for UTF when reading/writing (ex. ``'utf-8'``). `List of Python\n    standard encodings\n    <https://docs.python.org/3/library/codecs.html#standard-encodings>`_ .\n\nencoding_errors : str, optional, default 'strict'\n    How encoding errors are treated. `List of possible values\n    <https://docs.python.org/3/library/codecs.html#error-handlers>`_ .\n\n    .. versionadded:: 1.3.0\n\ndialect : str or csv.Dialect, optional\n    If provided, this parameter will override values (default or not) for the\n    following parameters: ``delimiter``, ``doublequote``, ``escapechar``,\n    ``skipinitialspace``, ``quotechar``, and ``quoting``. If it is necessary to\n    override values, a ``ParserWarning`` will be issued. See ``csv.Dialect``\n    documentation for more details.\non_bad_lines : {'error', 'warn', 'skip'} or Callable, default 'error'\n    Specifies what to do upon encountering a bad line (a line with too many fields).\n    Allowed values are :\n\n    - ``'error'``, raise an Exception when a bad line is encountered.\n    - ``'warn'``, raise a warning when a bad line is encountered and skip that line.\n    - ``'skip'``, skip bad lines without raising or warning when they are encountered.\n\n    .. versionadded:: 1.3.0\n\n    .. versionadded:: 1.4.0\n\n        - Callable, function with signature\n          ``(bad_line: list[str]) -> list[str] | None`` that will process a single\n          bad line. ``bad_line`` is a list of strings split by the ``sep``.\n          If the function returns ``None``, the bad line will be ignored.\n          If the function returns a new ``list`` of strings with more elements than\n          expected, a ``ParserWarning`` will be emitted while dropping extra elements.\n          Only supported when ``engine='python'``\n\n    .. versionchanged:: 2.2.0\n\n        - Callable, function with signature\n          as described in `pyarrow documentation\n          <https://arrow.apache.org/docs/python/generated/pyarrow.csv.ParseOptions.html\n          #pyarrow.csv.ParseOptions.invalid_row_handler>`_ when ``engine='pyarrow'``\n\ndelim_whitespace : bool, default False\n    Specifies whether or not whitespace (e.g. ``' '`` or ``'\\t'``) will be\n    used as the ``sep`` delimiter. Equivalent to setting ``sep='\\s+'``. If this option\n    is set to ``True``, nothing should be passed in for the ``delimiter``\n    parameter.\n\n    .. deprecated:: 2.2.0\n        Use ``sep=\"\\s+\"`` instead.\nlow_memory : bool, default True\n    Internally process the file in chunks, resulting in lower memory use\n    while parsing, but possibly mixed type inference.  To ensure no mixed\n    types either set ``False``, or specify the type with the ``dtype`` parameter.\n    Note that the entire file is read into a single :class:`~pandas.DataFrame`\n    regardless, use the ``chunksize`` or ``iterator`` parameter to return the data in\n    chunks. (Only valid with C parser).\nmemory_map : bool, default False\n    If a filepath is provided for ``filepath_or_buffer``, map the file object\n    directly onto memory and access the data directly from there. Using this\n    option can improve performance because there is no longer any I/O overhead.\nfloat_precision : {'high', 'legacy', 'round_trip'}, optional\n    Specifies which converter the C engine should use for floating-point\n    values. The options are ``None`` or ``'high'`` for the ordinary converter,\n    ``'legacy'`` for the original lower precision pandas converter, and\n    ``'round_trip'`` for the round-trip converter.\n\nstorage_options : dict, optional\n    Extra options that make sense for a particular storage connection, e.g.\n    host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n    are forwarded to ``urllib.request.Request`` as header options. For other\n    URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n    forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n    details, and for more examples on storage options refer `here\n    <https://pandas.pydata.org/docs/user_guide/io.html?\n    highlight=storage_options#reading-writing-remote-files>`_.\n\ndtype_backend : {'numpy_nullable', 'pyarrow'}, default 'numpy_nullable'\n    Back-end data type applied to the resultant :class:`DataFrame`\n    (still experimental). Behaviour is as follows:\n\n    * ``\"numpy_nullable\"``: returns nullable-dtype-backed :class:`DataFrame`\n      (default).\n    * ``\"pyarrow\"``: returns pyarrow-backed nullable :class:`ArrowDtype`\n      DataFrame.\n\n    .. versionadded:: 2.0\n\nReturns\n-------\nDataFrame or TextFileReader\n    A comma-separated values (csv) file is returned as two-dimensional\n    data structure with labeled axes.\n\nSee Also\n--------\nDataFrame.to_csv : Write DataFrame to a comma-separated values (csv) file.\nread_table : Read general delimited file into DataFrame.\nread_fwf : Read a table of fixed-width formatted lines into DataFrame.\n\nExamples\n--------\n>>> pd.read_csv('data.csv')  # doctest: +SKIP", "short_docstring": "Read a comma-separated values (csv) file into DataFrame."}
{"name": "re.split", "type": "callable", "signature": "(pattern, string, maxsplit=0, flags=0)", "docstring": "Split the source string by the occurrences of the pattern,\nreturning a list containing the resulting substrings.  If\ncapturing parentheses are used in pattern, then the text of all\ngroups in the pattern are also returned as part of the resulting\nlist.  If maxsplit is nonzero, at most maxsplit splits occur,\nand the remainder of the string is returned as the final element\nof the list.", "short_docstring": "Split the source string by the occurrences of the pattern,\nreturning a list containing the resulting substrings.  If\ncapturing parentheses are used in pattern, then the text of all\ngroups in the pattern are also returned as part of the resulting\nlist.  If maxsplit is nonzero, at most maxsplit splits occur,\nand the remainder of the string is returned as the final element\nof the list."}
{"name": "os.path.exists", "type": "callable", "signature": "(path)", "docstring": "Test whether a path exists.  Returns False for broken symbolic links", "short_docstring": "Test whether a path exists.  Returns False for broken symbolic links"}
{"name": "subprocess.PIPE", "type": "constant", "value": "-1", "signature": null, "docstring": "int([x]) -> integer\nint(x, base=10) -> integer\n\nConvert a number or string to an integer, or return 0 if no arguments\nare given.  If x is a number, return x.__int__().  For floating point\nnumbers, this truncates towards zero.\n\nIf x is not a number or if base is given, then x must be a string,\nbytes, or bytearray instance representing an integer literal in the\ngiven base.  The literal can be preceded by '+' or '-' and be surrounded\nby whitespace.  The base defaults to 10.  Valid bases are 0 and 2-36.\nBase 0 means to interpret the base from the string as an integer literal.\n>>> int('0b100', base=0)\n4", "short_docstring": "int([x]) -> integer\nint(x, base=10) -> integer"}
{"name": "os.listdir", "type": "callable", "signature": "(path=None)", "docstring": "Return a list containing the names of the files in the directory.\n\npath can be specified as either str, bytes, or a path-like object.  If path is bytes,\n  the filenames returned will also be bytes; in all other circumstances\n  the filenames returned will be str.\nIf path is None, uses the path='.'.\nOn some platforms, path may also be specified as an open file descriptor;\\\n  the file descriptor must refer to a directory.\n  If this functionality is unavailable, using it raises NotImplementedError.\n\nThe list is in arbitrary order.  It does not include the special\nentries '.' and '..' even if they are present in the directory.", "short_docstring": "Return a list containing the names of the files in the directory."}
{"name": "matplotlib.pyplot.scatter", "type": "callable", "signature": "(x: 'float | ArrayLike', y: 'float | ArrayLike', s: 'float | ArrayLike | None' = None, c: 'ArrayLike | Sequence[ColorType] | ColorType | None' = None, marker: 'MarkerType | None' = None, cmap: 'str | Colormap | None' = None, norm: 'str | Normalize | None' = None, vmin: 'float | None' = None, vmax: 'float | None' = None, alpha: 'float | None' = None, linewidths: 'float | Sequence[float] | None' = None, *, edgecolors: \"Literal['face', 'none'] | ColorType | Sequence[ColorType] | None\" = None, plotnonfinite: 'bool' = False, data=None, **kwargs) -> 'PathCollection'", "docstring": "A scatter plot of *y* vs. *x* with varying marker size and/or color.\n\nParameters\n----------\nx, y : float or array-like, shape (n, )\n    The data positions.\n\ns : float or array-like, shape (n, ), optional\n    The marker size in points**2 (typographic points are 1/72 in.).\n    Default is ``rcParams['lines.markersize'] ** 2``.\n\n    The linewidth and edgecolor can visually interact with the marker\n    size, and can lead to artifacts if the marker size is smaller than\n    the linewidth.\n\n    If the linewidth is greater than 0 and the edgecolor is anything\n    but *'none'*, then the effective size of the marker will be\n    increased by half the linewidth because the stroke will be centered\n    on the edge of the shape.\n\n    To eliminate the marker edge either set *linewidth=0* or\n    *edgecolor='none'*.\n\nc : array-like or list of colors or color, optional\n    The marker colors. Possible values:\n\n    - A scalar or sequence of n numbers to be mapped to colors using\n      *cmap* and *norm*.\n    - A 2D array in which the rows are RGB or RGBA.\n    - A sequence of colors of length n.\n    - A single color format string.\n\n    Note that *c* should not be a single numeric RGB or RGBA sequence\n    because that is indistinguishable from an array of values to be\n    colormapped. If you want to specify the same RGB or RGBA value for\n    all points, use a 2D array with a single row.  Otherwise,\n    value-matching will have precedence in case of a size matching with\n    *x* and *y*.\n\n    If you wish to specify a single color for all points\n    prefer the *color* keyword argument.\n\n    Defaults to `None`. In that case the marker color is determined\n    by the value of *color*, *facecolor* or *facecolors*. In case\n    those are not specified or `None`, the marker color is determined\n    by the next color of the ``Axes``' current \"shape and fill\" color\n    cycle. This cycle defaults to :rc:`axes.prop_cycle`.\n\nmarker : `~.markers.MarkerStyle`, default: :rc:`scatter.marker`\n    The marker style. *marker* can be either an instance of the class\n    or the text shorthand for a particular marker.\n    See :mod:`matplotlib.markers` for more information about marker\n    styles.\n\ncmap : str or `~matplotlib.colors.Colormap`, default: :rc:`image.cmap`\n    The Colormap instance or registered colormap name used to map scalar data\n    to colors.\n\n    This parameter is ignored if *c* is RGB(A).\n\nnorm : str or `~matplotlib.colors.Normalize`, optional\n    The normalization method used to scale scalar data to the [0, 1] range\n    before mapping to colors using *cmap*. By default, a linear scaling is\n    used, mapping the lowest value to 0 and the highest to 1.\n\n    If given, this can be one of the following:\n\n    - An instance of `.Normalize` or one of its subclasses\n      (see :ref:`colormapnorms`).\n    - A scale name, i.e. one of \"linear\", \"log\", \"symlog\", \"logit\", etc.  For a\n      list of available scales, call `matplotlib.scale.get_scale_names()`.\n      In that case, a suitable `.Normalize` subclass is dynamically generated\n      and instantiated.\n\n    This parameter is ignored if *c* is RGB(A).\n\nvmin, vmax : float, optional\n    When using scalar data and no explicit *norm*, *vmin* and *vmax* define\n    the data range that the colormap covers. By default, the colormap covers\n    the complete value range of the supplied data. It is an error to use\n    *vmin*/*vmax* when a *norm* instance is given (but using a `str` *norm*\n    name together with *vmin*/*vmax* is acceptable).\n\n    This parameter is ignored if *c* is RGB(A).\n\nalpha : float, default: None\n    The alpha blending value, between 0 (transparent) and 1 (opaque).\n\nlinewidths : float or array-like, default: :rc:`lines.linewidth`\n    The linewidth of the marker edges. Note: The default *edgecolors*\n    is 'face'. You may want to change this as well.\n\nedgecolors : {'face', 'none', *None*} or color or sequence of color, default: :rc:`scatter.edgecolors`\n    The edge color of the marker. Possible values:\n\n    - 'face': The edge color will always be the same as the face color.\n    - 'none': No patch boundary will be drawn.\n    - A color or sequence of colors.\n\n    For non-filled markers, *edgecolors* is ignored. Instead, the color\n    is determined like with 'face', i.e. from *c*, *colors*, or\n    *facecolors*.\n\nplotnonfinite : bool, default: False\n    Whether to plot points with nonfinite *c* (i.e. ``inf``, ``-inf``\n    or ``nan``). If ``True`` the points are drawn with the *bad*\n    colormap color (see `.Colormap.set_bad`).\n\nReturns\n-------\n`~matplotlib.collections.PathCollection`\n\nOther Parameters\n----------------\ndata : indexable object, optional\n    If given, the following parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` (unless this raises an exception):\n\n    *x*, *y*, *s*, *linewidths*, *edgecolors*, *c*, *facecolor*, *facecolors*, *color*\n**kwargs : `~matplotlib.collections.Collection` properties\n\nSee Also\n--------\nplot : To plot scatter plots when markers are identical in size and\n    color.\n\nNotes\n-----\n* The `.plot` function will be faster for scatterplots where markers\n  don't vary in size or color.\n\n* Any or all of *x*, *y*, *s*, and *c* may be masked arrays, in which\n  case all masks will be combined and only unmasked points will be\n  plotted.\n\n* Fundamentally, scatter works with 1D arrays; *x*, *y*, *s*, and *c*\n  may be input as N-D arrays, but within scatter they will be\n  flattened. The exception is *c*, which will be flattened only if its\n  size matches the size of *x* and *y*.", "short_docstring": "A scatter plot of *y* vs. *x* with varying marker size and/or color."}
{"name": "numpy.cos", "type": "callable", "signature": null, "docstring": "cos(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n\nCosine element-wise.\n\nParameters\n----------\nx : array_like\n    Input array in radians.\nout : ndarray, None, or tuple of ndarray and None, optional\n    A location into which the result is stored. If provided, it must have\n    a shape that the inputs broadcast to. If not provided or None,\n    a freshly-allocated array is returned. A tuple (possible only as a\n    keyword argument) must have length equal to the number of outputs.\nwhere : array_like, optional\n    This condition is broadcast over the input. At locations where the\n    condition is True, the `out` array will be set to the ufunc result.\n    Elsewhere, the `out` array will retain its original value.\n    Note that if an uninitialized `out` array is created via the default\n    ``out=None``, locations within it where the condition is False will\n    remain uninitialized.\n**kwargs\n    For other keyword-only arguments, see the\n    :ref:`ufunc docs <ufuncs.kwargs>`.\n\nReturns\n-------\ny : ndarray\n    The corresponding cosine values.\n    This is a scalar if `x` is a scalar.\n\nNotes\n-----\nIf `out` is provided, the function writes the result into it,\nand returns a reference to `out`.  (See Examples)\n\nReferences\n----------\nM. Abramowitz and I. A. Stegun, Handbook of Mathematical Functions.\nNew York, NY: Dover, 1972.\n\nExamples\n--------\n>>> np.cos(np.array([0, np.pi/2, np.pi]))\narray([  1.00000000e+00,   6.12303177e-17,  -1.00000000e+00])\n>>>\n>>> # Example of providing the optional output parameter\n>>> out1 = np.array([0], dtype='d')\n>>> out2 = np.cos([0.1], out1)\n>>> out2 is out1\nTrue\n>>>\n>>> # Example of ValueError due to provision of shape mis-matched `out`\n>>> np.cos(np.zeros((3,3)),np.zeros((2,2)))\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nValueError: operands could not be broadcast together with shapes (3,3) (2,2)", "short_docstring": "cos(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])"}
{"name": "matplotlib.pyplot.close", "type": "callable", "signature": "(fig: \"None | int | str | Figure | Literal['all']\" = None) -> 'None'", "docstring": "Close a figure window.\n\nParameters\n----------\nfig : None or int or str or `.Figure`\n    The figure to close. There are a number of ways to specify this:\n\n    - *None*: the current figure\n    - `.Figure`: the given `.Figure` instance\n    - ``int``: a figure number\n    - ``str``: a figure name\n    - 'all': all figures", "short_docstring": "Close a figure window."}
{"name": "pytz.UTC", "type": "constant", "value": "UTC", "signature": null, "docstring": "UTC\n\nOptimized UTC implementation. It unpickles using the single module global\ninstance defined beneath this class declaration.", "short_docstring": "UTC"}
{"name": "bs4.BeautifulSoup", "type": "class", "signature": "(markup='', features=None, builder=None, parse_only=None, from_encoding=None, exclude_encodings=None, element_classes=None, **kwargs)", "docstring": "A data structure representing a parsed HTML or XML document.\n\nMost of the methods you'll call on a BeautifulSoup object are inherited from\nPageElement or Tag.\n\nInternally, this class defines the basic interface called by the\ntree builders when converting an HTML/XML document into a data\nstructure. The interface abstracts away the differences between\nparsers. To write a new tree builder, you'll need to understand\nthese methods as a whole.\n\nThese methods will be called by the BeautifulSoup constructor:\n  * reset()\n  * feed(markup)\n\nThe tree builder may call these methods from its feed() implementation:\n  * handle_starttag(name, attrs) # See note about return value\n  * handle_endtag(name)\n  * handle_data(data) # Appends to the current data node\n  * endData(containerClass) # Ends the current data node\n\nNo matter how complicated the underlying parser is, you should be\nable to build a tree using 'start tag' events, 'end tag' events,\n'data' events, and \"done with data\" events.\n\nIf you encounter an empty-element tag (aka a self-closing tag,\nlike HTML's <br> tag), call handle_starttag and then\nhandle_endtag.", "short_docstring": "A data structure representing a parsed HTML or XML document."}
{"name": "gensim.models.Word2Vec", "error": "Import error: cannot import name 'triu' from 'scipy.linalg' (/home/aiops/zhuoyt/.local/lib/python3.10/site-packages/scipy/linalg/__init__.py)"}
{"name": "wordcloud.WordCloud", "type": "class", "signature": "(font_path=None, width=400, height=200, margin=2, ranks_only=None, prefer_horizontal=0.9, mask=None, scale=1, color_func=None, max_words=200, min_font_size=4, stopwords=None, random_state=None, background_color='black', max_font_size=None, font_step=1, mode='RGB', relative_scaling='auto', regexp=None, collocations=True, colormap=None, normalize_plurals=True, contour_width=0, contour_color='black', repeat=False, include_numbers=False, min_word_length=0, collocation_threshold=30)", "docstring": "Word cloud object for generating and drawing.\n\nParameters\n----------\nfont_path : string\n    Font path to the font that will be used (OTF or TTF).\n    Defaults to DroidSansMono path on a Linux machine. If you are on\n    another OS or don't have this font, you need to adjust this path.\n\nwidth : int (default=400)\n    Width of the canvas.\n\nheight : int (default=200)\n    Height of the canvas.\n\nprefer_horizontal : float (default=0.90)\n    The ratio of times to try horizontal fitting as opposed to vertical.\n    If prefer_horizontal < 1, the algorithm will try rotating the word\n    if it doesn't fit. (There is currently no built-in way to get only\n    vertical words.)\n\nmask : nd-array or None (default=None)\n    If not None, gives a binary mask on where to draw words. If mask is not\n    None, width and height will be ignored and the shape of mask will be\n    used instead. All white (#FF or #FFFFFF) entries will be considerd\n    \"masked out\" while other entries will be free to draw on. [This\n    changed in the most recent version!]\n\ncontour_width: float (default=0)\n    If mask is not None and contour_width > 0, draw the mask contour.\n\ncontour_color: color value (default=\"black\")\n    Mask contour color.\n\nscale : float (default=1)\n    Scaling between computation and drawing. For large word-cloud images,\n    using scale instead of larger canvas size is significantly faster, but\n    might lead to a coarser fit for the words.\n\nmin_font_size : int (default=4)\n    Smallest font size to use. Will stop when there is no more room in this\n    size.\n\nfont_step : int (default=1)\n    Step size for the font. font_step > 1 might speed up computation but\n    give a worse fit.\n\nmax_words : number (default=200)\n    The maximum number of words.\n\nstopwords : set of strings or None\n    The words that will be eliminated. If None, the build-in STOPWORDS\n    list will be used. Ignored if using generate_from_frequencies.\n\nbackground_color : color value (default=\"black\")\n    Background color for the word cloud image.\n\nmax_font_size : int or None (default=None)\n    Maximum font size for the largest word. If None, height of the image is\n    used.\n\nmode : string (default=\"RGB\")\n    Transparent background will be generated when mode is \"RGBA\" and\n    background_color is None.\n\nrelative_scaling : float (default='auto')\n    Importance of relative word frequencies for font-size.  With\n    relative_scaling=0, only word-ranks are considered.  With\n    relative_scaling=1, a word that is twice as frequent will have twice\n    the size.  If you want to consider the word frequencies and not only\n    their rank, relative_scaling around .5 often looks good.\n    If 'auto' it will be set to 0.5 unless repeat is true, in which\n    case it will be set to 0.\n\n    .. versionchanged: 2.0\n        Default is now 'auto'.\n\ncolor_func : callable, default=None\n    Callable with parameters word, font_size, position, orientation,\n    font_path, random_state that returns a PIL color for each word.\n    Overwrites \"colormap\".\n    See colormap for specifying a matplotlib colormap instead.\n    To create a word cloud with a single color, use\n    ``color_func=lambda *args, **kwargs: \"white\"``.\n    The single color can also be specified using RGB code. For example\n    ``color_func=lambda *args, **kwargs: (255,0,0)`` sets color to red.\n\nregexp : string or None (optional)\n    Regular expression to split the input text into tokens in process_text.\n    If None is specified, ``r\"\\w[\\w']+\"`` is used. Ignored if using\n    generate_from_frequencies.\n\ncollocations : bool, default=True\n    Whether to include collocations (bigrams) of two words. Ignored if using\n    generate_from_frequencies.\n\n\n    .. versionadded: 2.0\n\ncolormap : string or matplotlib colormap, default=\"viridis\"\n    Matplotlib colormap to randomly draw colors from for each word.\n    Ignored if \"color_func\" is specified.\n\n    .. versionadded: 2.0\n\nnormalize_plurals : bool, default=True\n    Whether to remove trailing 's' from words. If True and a word\n    appears with and without a trailing 's', the one with trailing 's'\n    is removed and its counts are added to the version without\n    trailing 's' -- unless the word ends with 'ss'. Ignored if using\n    generate_from_frequencies.\n\nrepeat : bool, default=False\n    Whether to repeat words and phrases until max_words or min_font_size\n    is reached.\n\ninclude_numbers : bool, default=False\n    Whether to include numbers as phrases or not.\n\nmin_word_length : int, default=0\n    Minimum number of letters a word must have to be included.\n\ncollocation_threshold: int, default=30\n    Bigrams must have a Dunning likelihood collocation score greater than this\n    parameter to be counted as bigrams. Default of 30 is arbitrary.\n\n    See Manning, C.D., Manning, C.D. and Sch\u00fctze, H., 1999. Foundations of\n    Statistical Natural Language Processing. MIT press, p. 162\n    https://nlp.stanford.edu/fsnlp/promo/colloc.pdf#page=22\n\nAttributes\n----------\n``words_`` : dict of string to float\n    Word tokens with associated frequency.\n\n    .. versionchanged: 2.0\n        ``words_`` is now a dictionary\n\n``layout_`` : list of tuples ((string, float), int, (int, int), int, color))\n    Encodes the fitted word cloud. For each word, it encodes the string,\n    normalized frequency, font size, position, orientation, and color.\n    The frequencies are normalized by the most commonly occurring word.\n    The color is in the format of 'rgb(R, G, B).'\n\nNotes\n-----\nLarger canvases make the code significantly slower. If you need a\nlarge word cloud, try a lower canvas size, and set the scale parameter.\n\nThe algorithm might give more weight to the ranking of the words\nthan their actual frequencies, depending on the ``max_font_size`` and the\nscaling heuristic.", "short_docstring": "Word cloud object for generating and drawing."}
{"name": "numpy.abs", "type": "callable", "signature": null, "docstring": "absolute(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n\nCalculate the absolute value element-wise.\n\n``np.abs`` is a shorthand for this function.\n\nParameters\n----------\nx : array_like\n    Input array.\nout : ndarray, None, or tuple of ndarray and None, optional\n    A location into which the result is stored. If provided, it must have\n    a shape that the inputs broadcast to. If not provided or None,\n    a freshly-allocated array is returned. A tuple (possible only as a\n    keyword argument) must have length equal to the number of outputs.\nwhere : array_like, optional\n    This condition is broadcast over the input. At locations where the\n    condition is True, the `out` array will be set to the ufunc result.\n    Elsewhere, the `out` array will retain its original value.\n    Note that if an uninitialized `out` array is created via the default\n    ``out=None``, locations within it where the condition is False will\n    remain uninitialized.\n**kwargs\n    For other keyword-only arguments, see the\n    :ref:`ufunc docs <ufuncs.kwargs>`.\n\nReturns\n-------\nabsolute : ndarray\n    An ndarray containing the absolute value of\n    each element in `x`.  For complex input, ``a + ib``, the\n    absolute value is :math:`\\sqrt{ a^2 + b^2 }`.\n    This is a scalar if `x` is a scalar.\n\nExamples\n--------\n>>> x = np.array([-1.2, 1.2])\n>>> np.absolute(x)\narray([ 1.2,  1.2])\n>>> np.absolute(1.2 + 1j)\n1.5620499351813308\n\nPlot the function over ``[-10, 10]``:\n\n>>> import matplotlib.pyplot as plt\n\n>>> x = np.linspace(start=-10, stop=10, num=101)\n>>> plt.plot(x, np.absolute(x))\n>>> plt.show()\n\nPlot the function over the complex plane:\n\n>>> xx = x + 1j * x[:, np.newaxis]\n>>> plt.imshow(np.abs(xx), extent=[-10, 10, -10, 10], cmap='gray')\n>>> plt.show()\n\nThe `abs` function can be used as a shorthand for ``np.absolute`` on\nndarrays.\n\n>>> x = np.array([-1.2, 1.2])\n>>> abs(x)\narray([1.2, 1.2])", "short_docstring": "absolute(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])"}
{"name": "sklearn.feature_extraction.text.CountVectorizer", "type": "class", "signature": "(*, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.int64'>)", "docstring": "Convert a collection of text documents to a matrix of token counts.\n\nThis implementation produces a sparse representation of the counts using\nscipy.sparse.csr_matrix.\n\nIf you do not provide an a-priori dictionary and you do not use an analyzer\nthat does some kind of feature selection then the number of features will\nbe equal to the vocabulary size found by analyzing the data.\n\nFor an efficiency comparison of the different feature extractors, see\n:ref:`sphx_glr_auto_examples_text_plot_hashing_vs_dict_vectorizer.py`.\n\nRead more in the :ref:`User Guide <text_feature_extraction>`.\n\nParameters\n----------\ninput : {'filename', 'file', 'content'}, default='content'\n    - If `'filename'`, the sequence passed as an argument to fit is\n      expected to be a list of filenames that need reading to fetch\n      the raw content to analyze.\n\n    - If `'file'`, the sequence items must have a 'read' method (file-like\n      object) that is called to fetch the bytes in memory.\n\n    - If `'content'`, the input is expected to be a sequence of items that\n      can be of type string or byte.\n\nencoding : str, default='utf-8'\n    If bytes or files are given to analyze, this encoding is used to\n    decode.\n\ndecode_error : {'strict', 'ignore', 'replace'}, default='strict'\n    Instruction on what to do if a byte sequence is given to analyze that\n    contains characters not of the given `encoding`. By default, it is\n    'strict', meaning that a UnicodeDecodeError will be raised. Other\n    values are 'ignore' and 'replace'.\n\nstrip_accents : {'ascii', 'unicode'} or callable, default=None\n    Remove accents and perform other character normalization\n    during the preprocessing step.\n    'ascii' is a fast method that only works on characters that have\n    a direct ASCII mapping.\n    'unicode' is a slightly slower method that works on any characters.\n    None (default) means no character normalization is performed.\n\n    Both 'ascii' and 'unicode' use NFKD normalization from\n    :func:`unicodedata.normalize`.\n\nlowercase : bool, default=True\n    Convert all characters to lowercase before tokenizing.\n\npreprocessor : callable, default=None\n    Override the preprocessing (strip_accents and lowercase) stage while\n    preserving the tokenizing and n-grams generation steps.\n    Only applies if ``analyzer`` is not callable.\n\ntokenizer : callable, default=None\n    Override the string tokenization step while preserving the\n    preprocessing and n-grams generation steps.\n    Only applies if ``analyzer == 'word'``.\n\nstop_words : {'english'}, list, default=None\n    If 'english', a built-in stop word list for English is used.\n    There are several known issues with 'english' and you should\n    consider an alternative (see :ref:`stop_words`).\n\n    If a list, that list is assumed to contain stop words, all of which\n    will be removed from the resulting tokens.\n    Only applies if ``analyzer == 'word'``.\n\n    If None, no stop words will be used. In this case, setting `max_df`\n    to a higher value, such as in the range (0.7, 1.0), can automatically detect\n    and filter stop words based on intra corpus document frequency of terms.\n\ntoken_pattern : str or None, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n    Regular expression denoting what constitutes a \"token\", only used\n    if ``analyzer == 'word'``. The default regexp select tokens of 2\n    or more alphanumeric characters (punctuation is completely ignored\n    and always treated as a token separator).\n\n    If there is a capturing group in token_pattern then the\n    captured group content, not the entire match, becomes the token.\n    At most one capturing group is permitted.\n\nngram_range : tuple (min_n, max_n), default=(1, 1)\n    The lower and upper boundary of the range of n-values for different\n    word n-grams or char n-grams to be extracted. All values of n such\n    such that min_n <= n <= max_n will be used. For example an\n    ``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means\n    unigrams and bigrams, and ``(2, 2)`` means only bigrams.\n    Only applies if ``analyzer`` is not callable.\n\nanalyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n    Whether the feature should be made of word n-gram or character\n    n-grams.\n    Option 'char_wb' creates character n-grams only from text inside\n    word boundaries; n-grams at the edges of words are padded with space.\n\n    If a callable is passed it is used to extract the sequence of features\n    out of the raw, unprocessed input.\n\n    .. versionchanged:: 0.21\n\n    Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n    first read from the file and then passed to the given callable\n    analyzer.\n\nmax_df : float in range [0.0, 1.0] or int, default=1.0\n    When building the vocabulary ignore terms that have a document\n    frequency strictly higher than the given threshold (corpus-specific\n    stop words).\n    If float, the parameter represents a proportion of documents, integer\n    absolute counts.\n    This parameter is ignored if vocabulary is not None.\n\nmin_df : float in range [0.0, 1.0] or int, default=1\n    When building the vocabulary ignore terms that have a document\n    frequency strictly lower than the given threshold. This value is also\n    called cut-off in the literature.\n    If float, the parameter represents a proportion of documents, integer\n    absolute counts.\n    This parameter is ignored if vocabulary is not None.\n\nmax_features : int, default=None\n    If not None, build a vocabulary that only consider the top\n    `max_features` ordered by term frequency across the corpus.\n    Otherwise, all features are used.\n\n    This parameter is ignored if vocabulary is not None.\n\nvocabulary : Mapping or iterable, default=None\n    Either a Mapping (e.g., a dict) where keys are terms and values are\n    indices in the feature matrix, or an iterable over terms. If not\n    given, a vocabulary is determined from the input documents. Indices\n    in the mapping should not be repeated and should not have any gap\n    between 0 and the largest index.\n\nbinary : bool, default=False\n    If True, all non zero counts are set to 1. This is useful for discrete\n    probabilistic models that model binary events rather than integer\n    counts.\n\ndtype : dtype, default=np.int64\n    Type of the matrix returned by fit_transform() or transform().\n\nAttributes\n----------\nvocabulary_ : dict\n    A mapping of terms to feature indices.\n\nfixed_vocabulary_ : bool\n    True if a fixed vocabulary of term to indices mapping\n    is provided by the user.\n\nstop_words_ : set\n    Terms that were ignored because they either:\n\n      - occurred in too many documents (`max_df`)\n      - occurred in too few documents (`min_df`)\n      - were cut off by feature selection (`max_features`).\n\n    This is only available if no vocabulary was given.\n\nSee Also\n--------\nHashingVectorizer : Convert a collection of text documents to a\n    matrix of token counts.\n\nTfidfVectorizer : Convert a collection of raw documents to a matrix\n    of TF-IDF features.\n\nNotes\n-----\nThe ``stop_words_`` attribute can get large and increase the model size\nwhen pickling. This attribute is provided only for introspection and can\nbe safely removed using delattr or set to None before pickling.\n\nExamples\n--------\n>>> from sklearn.feature_extraction.text import CountVectorizer\n>>> corpus = [\n...     'This is the first document.',\n...     'This document is the second document.',\n...     'And this is the third one.',\n...     'Is this the first document?',\n... ]\n>>> vectorizer = CountVectorizer()\n>>> X = vectorizer.fit_transform(corpus)\n>>> vectorizer.get_feature_names_out()\narray(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n       'this'], ...)\n>>> print(X.toarray())\n[[0 1 1 1 0 0 1 0 1]\n [0 2 0 1 0 1 1 0 1]\n [1 0 0 1 1 0 1 1 1]\n [0 1 1 1 0 0 1 0 1]]\n>>> vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n>>> X2 = vectorizer2.fit_transform(corpus)\n>>> vectorizer2.get_feature_names_out()\narray(['and this', 'document is', 'first document', 'is the', 'is this',\n       'second document', 'the first', 'the second', 'the third', 'third one',\n       'this document', 'this is', 'this the'], ...)\n >>> print(X2.toarray())\n [[0 0 1 1 0 0 1 0 0 0 0 1 0]\n [0 1 0 1 0 1 0 1 0 0 1 0 0]\n [1 0 0 1 0 0 0 0 1 1 0 1 0]\n [0 0 1 0 1 0 1 0 0 0 0 0 1]]", "short_docstring": "Convert a collection of text documents to a matrix of token counts."}
{"name": "getpass.getpass", "type": "callable", "signature": "(prompt='Password: ', stream=None)", "docstring": "Prompt for a password, with echo turned off.\n\nArgs:\n  prompt: Written on stream to ask for the input.  Default: 'Password: '\n  stream: A writable file object to display the prompt.  Defaults to\n          the tty.  If no tty is available defaults to sys.stderr.\nReturns:\n  The seKr3t input.\nRaises:\n  EOFError: If our input tty or stdin was closed.\n  GetPassWarning: When we were unable to turn echo off on the input.\n\nAlways restores terminal settings before returning.", "short_docstring": "Prompt for a password, with echo turned off."}
{"name": "librosa.display.specshow", "type": "callable", "signature": "(data: 'np.ndarray', *, x_coords: 'Optional[np.ndarray]' = None, y_coords: 'Optional[np.ndarray]' = None, x_axis: 'Optional[str]' = None, y_axis: 'Optional[str]' = None, sr: 'float' = 22050, hop_length: 'int' = 512, n_fft: 'Optional[int]' = None, win_length: 'Optional[int]' = None, fmin: 'Optional[float]' = None, fmax: 'Optional[float]' = None, tempo_min: 'Optional[float]' = 16, tempo_max: 'Optional[float]' = 480, tuning: 'float' = 0.0, bins_per_octave: 'int' = 12, key: 'str' = 'C:maj', Sa: 'Optional[Union[float, int]]' = None, mela: 'Optional[Union[str, int]]' = None, thaat: 'Optional[str]' = None, auto_aspect: 'bool' = True, htk: 'bool' = False, unicode: 'bool' = True, intervals: 'Optional[Union[str, np.ndarray]]' = None, unison: 'Optional[str]' = None, ax: 'Optional[mplaxes.Axes]' = None, **kwargs: 'Any') -> 'QuadMesh'", "docstring": "Display a spectrogram/chromagram/cqt/etc.\n\nFor a detailed overview of this function, see :ref:`sphx_glr_auto_examples_plot_display.py`\n\nParameters\n----------\ndata : np.ndarray [shape=(d, n)]\n    Matrix to display (e.g., spectrogram)\n\nsr : number > 0 [scalar]\n    Sample rate used to determine time scale in x-axis.\n\nhop_length : int > 0 [scalar]\n    Hop length, also used to determine time scale in x-axis\n\nn_fft : int > 0 or None\n    Number of samples per frame in STFT/spectrogram displays.\n    By default, this will be inferred from the shape of ``data``\n    as ``2 * (d - 1)``.\n    If ``data`` was generated using an odd frame length, the correct\n    value can be specified here.\n\nwin_length : int > 0 or None\n    The number of samples per window.\n    By default, this will be inferred to match ``n_fft``.\n    This is primarily useful for specifying odd window lengths in\n    Fourier tempogram displays.\n\nx_axis, y_axis : None or str\n    Range for the x- and y-axes.\n\n    Valid types are:\n\n    - None, 'none', or 'off' : no axis decoration is displayed.\n\n    Frequency types:\n\n    - 'linear', 'fft', 'hz' : frequency range is determined by\n      the FFT window and sampling rate.\n    - 'log' : the spectrum is displayed on a log scale.\n    - 'fft_note': the spectrum is displayed on a log scale with pitches marked.\n    - 'fft_svara': the spectrum is displayed on a log scale with svara marked.\n    - 'mel' : frequencies are determined by the mel scale.\n    - 'cqt_hz' : frequencies are determined by the CQT scale.\n    - 'cqt_note' : pitches are determined by the CQT scale.\n    - 'cqt_svara' : like `cqt_note` but using Hindustani or Carnatic svara\n    - 'vqt_fjs' : like `cqt_note` but using Functional Just System (FJS)\n      notation.  This requires a just intonation-based variable-Q\n      transform representation.\n\n    All frequency types are plotted in units of Hz.\n\n    Any spectrogram parameters (hop_length, sr, bins_per_octave, etc.)\n    used to generate the input data should also be provided when\n    calling `specshow`.\n\n    Categorical types:\n\n    - 'chroma' : pitches are determined by the chroma filters.\n      Pitch classes are arranged at integer locations (0-11) according to\n      a given key.\n\n    - `chroma_h`, `chroma_c`: pitches are determined by chroma filters,\n      and labeled as svara in the Hindustani (`chroma_h`) or Carnatic (`chroma_c`)\n      according to a given thaat (Hindustani) or melakarta raga (Carnatic).\n\n    - 'chroma_fjs': pitches are determined by chroma filters using just\n      intonation.  All pitch classes are annotated.\n\n    - 'tonnetz' : axes are labeled by Tonnetz dimensions (0-5)\n    - 'frames' : markers are shown as frame counts.\n\n    Time types:\n\n    - 'time' : markers are shown as milliseconds, seconds, minutes, or hours.\n            Values are plotted in units of seconds.\n    - 'h' : markers are shown as hours, minutes, and seconds.\n    - 'm' : markers are shown as minutes and seconds.\n    - 's' : markers are shown as seconds.\n    - 'ms' : markers are shown as milliseconds.\n    - 'lag' : like time, but past the halfway point counts as negative values.\n    - 'lag_h' : same as lag, but in hours, minutes and seconds.\n    - 'lag_m' : same as lag, but in minutes and seconds.\n    - 'lag_s' : same as lag, but in seconds.\n    - 'lag_ms' : same as lag, but in milliseconds.\n\n    Rhythm:\n\n    - 'tempo' : markers are shown as beats-per-minute (BPM)\n        using a logarithmic scale.  This is useful for\n        visualizing the outputs of `feature.tempogram`.\n\n    - 'fourier_tempo' : same as `'tempo'`, but used when\n        tempograms are calculated in the Frequency domain\n        using `feature.fourier_tempogram`.\n\nx_coords, y_coords : np.ndarray [shape=data.shape[0 or 1]]\n    Optional positioning coordinates of the input data.\n    These can be use to explicitly set the location of each\n    element ``data[i, j]``, e.g., for displaying beat-synchronous\n    features in natural time coordinates.\n\n    If not provided, they are inferred from ``x_axis`` and ``y_axis``.\n\nfmin : float > 0 [scalar] or None\n    Frequency of the lowest spectrogram bin.  Used for Mel, CQT, and VQT\n    scales.\n\n    If ``y_axis`` is `cqt_hz` or `cqt_note` and ``fmin`` is not given,\n    it is set by default to ``note_to_hz('C1')``.\n\nfmax : float > 0 [scalar] or None\n    Used for setting the Mel frequency scales\n\ntempo_min : float > 0 [scalar]\n    Lowest tempo (in beats per minute) for tempogram display.\n\ntempo_max : float > 0 [scalar]\n    Highest tempo (in beats per minute) for tempogram display.\n\ntuning : float\n    Tuning deviation from A440, in fractions of a bin.\n\n    This is used for CQT frequency scales, so that ``fmin`` is adjusted\n    to ``fmin * 2**(tuning / bins_per_octave)``.\n\nbins_per_octave : int > 0 [scalar]\n    Number of bins per octave.  Used for CQT frequency scale.\n\nkey : str\n    The reference key to use when using note axes (`cqt_note`, `chroma`).\n\nSa : float or int\n    If using Hindustani or Carnatic svara axis decorations, specify Sa.\n\n    For `cqt_svara`, ``Sa`` should be specified as a frequency in Hz.\n\n    For `chroma_c` or `chroma_h`, ``Sa`` should correspond to the position\n    of Sa within the chromagram.\n    If not provided, Sa will default to 0 (equivalent to `C`)\n\nmela : str or int, optional\n    If using `chroma_c` or `cqt_svara` display mode, specify the melakarta raga.\n\nthaat : str, optional\n    If using `chroma_h` display mode, specify the parent thaat.\n\nintervals : str or array of floats in [1, 2), optional\n    If using an FJS notation (`chroma_fjs`, `vqt_fjs`), the interval specification.\n\n    See `core.interval_frequencies` for a description of supported values.\n\nunison : str, optional\n    If using an FJS notation (`chroma_fjs`, `vqt_fjs`), the pitch name of the unison\n    interval.  If not provided, it will be inferred from `fmin` (for VQT display) or\n    assumed as `'C'` (for chroma display).\n\nauto_aspect : bool\n    Axes will have 'equal' aspect if the horizontal and vertical dimensions\n    cover the same extent and their types match.\n\n    To override, set to `False`.\n\nhtk : bool\n    If plotting on a mel frequency axis, specify which version of the mel\n    scale to use.\n\n        - `False`: use Slaney formula (default)\n        - `True`: use HTK formula\n\n    See `core.mel_frequencies` for more information.\n\nunicode : bool\n    If using note or svara decorations, setting `unicode=True`\n    will use unicode glyphs for accidentals and octave encoding.\n\n    Setting `unicode=False` will use ASCII glyphs.  This can be helpful\n    if your font does not support musical notation symbols.\n\nax : matplotlib.axes.Axes or None\n    Axes to plot on instead of the default `plt.gca()`.\n\n**kwargs : additional keyword arguments\n    Arguments passed through to `matplotlib.pyplot.pcolormesh`.\n\n    By default, the following options are set:\n\n        - ``rasterized=True``\n        - ``shading='auto'``\n        - ``edgecolors='None'``\n\n    The ``cmap`` option if not provided, is inferred from data automatically.\n    Set ``cmap=None`` to use matplotlib's default colormap.\n\nReturns\n-------\ncolormesh : `matplotlib.collections.QuadMesh`\n    The color mesh object produced by `matplotlib.pyplot.pcolormesh`\n\nSee Also\n--------\ncmap : Automatic colormap detection\nmatplotlib.pyplot.pcolormesh\n\nExamples\n--------\nVisualize an STFT power spectrum using default parameters\n\n>>> import matplotlib.pyplot as plt\n>>> y, sr = librosa.load(librosa.ex('choice'), duration=15)\n>>> fig, ax = plt.subplots(nrows=2, ncols=1, sharex=True)\n>>> D = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)\n>>> img = librosa.display.specshow(D, y_axis='linear', x_axis='time',\n...                                sr=sr, ax=ax[0])\n>>> ax[0].set(title='Linear-frequency power spectrogram')\n>>> ax[0].label_outer()\n\nOr on a logarithmic scale, and using a larger hop\n\n>>> hop_length = 1024\n>>> D = librosa.amplitude_to_db(np.abs(librosa.stft(y, hop_length=hop_length)),\n...                             ref=np.max)\n>>> librosa.display.specshow(D, y_axis='log', sr=sr, hop_length=hop_length,\n...                          x_axis='time', ax=ax[1])\n>>> ax[1].set(title='Log-frequency power spectrogram')\n>>> ax[1].label_outer()\n>>> fig.colorbar(img, ax=ax, format=\"%+2.f dB\")", "short_docstring": "Display a spectrogram/chromagram/cqt/etc."}
{"name": "queue.Queue", "type": "class", "signature": "(maxsize=0)", "docstring": "Create a queue object with a given maximum size.\n\nIf maxsize is <= 0, the queue size is infinite.", "short_docstring": "Create a queue object with a given maximum size."}
{"name": "time.sleep", "type": "callable", "signature": null, "docstring": "sleep(seconds)\n\nDelay execution for a given number of seconds.  The argument may be\na floating point number for subsecond precision.", "short_docstring": "sleep(seconds)"}
{"name": "requests.get", "type": "callable", "signature": "(url, params=None, **kwargs)", "docstring": "Sends a GET request.\n\n:param url: URL for the new :class:`Request` object.\n:param params: (optional) Dictionary, list of tuples or bytes to send\n    in the query string for the :class:`Request`.\n:param \\*\\*kwargs: Optional arguments that ``request`` takes.\n:return: :class:`Response <Response>` object\n:rtype: requests.Response", "short_docstring": "Sends a GET request."}
{"name": "wtforms.SubmitField", "type": "class", "signature": "(*args, **kwargs)", "docstring": "Represents an ``<input type=\"submit\">``.  This allows checking if a given\nsubmit button has been pressed.", "short_docstring": "Represents an ``<input type=\"submit\">``.  This allows checking if a given\nsubmit button has been pressed."}
{"name": "subprocess.STDOUT", "type": "constant", "value": "-2", "signature": null, "docstring": "int([x]) -> integer\nint(x, base=10) -> integer\n\nConvert a number or string to an integer, or return 0 if no arguments\nare given.  If x is a number, return x.__int__().  For floating point\nnumbers, this truncates towards zero.\n\nIf x is not a number or if base is given, then x must be a string,\nbytes, or bytearray instance representing an integer literal in the\ngiven base.  The literal can be preceded by '+' or '-' and be surrounded\nby whitespace.  The base defaults to 10.  Valid bases are 0 and 2-36.\nBase 0 means to interpret the base from the string as an integer literal.\n>>> int('0b100', base=0)\n4", "short_docstring": "int([x]) -> integer\nint(x, base=10) -> integer"}
{"name": "numpy.log10", "type": "callable", "signature": null, "docstring": "log10(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n\nReturn the base 10 logarithm of the input array, element-wise.\n\nParameters\n----------\nx : array_like\n    Input values.\nout : ndarray, None, or tuple of ndarray and None, optional\n    A location into which the result is stored. If provided, it must have\n    a shape that the inputs broadcast to. If not provided or None,\n    a freshly-allocated array is returned. A tuple (possible only as a\n    keyword argument) must have length equal to the number of outputs.\nwhere : array_like, optional\n    This condition is broadcast over the input. At locations where the\n    condition is True, the `out` array will be set to the ufunc result.\n    Elsewhere, the `out` array will retain its original value.\n    Note that if an uninitialized `out` array is created via the default\n    ``out=None``, locations within it where the condition is False will\n    remain uninitialized.\n**kwargs\n    For other keyword-only arguments, see the\n    :ref:`ufunc docs <ufuncs.kwargs>`.\n\nReturns\n-------\ny : ndarray\n    The logarithm to the base 10 of `x`, element-wise. NaNs are\n    returned where x is negative.\n    This is a scalar if `x` is a scalar.\n\nSee Also\n--------\nemath.log10\n\nNotes\n-----\nLogarithm is a multivalued function: for each `x` there is an infinite\nnumber of `z` such that `10**z = x`. The convention is to return the\n`z` whose imaginary part lies in `[-pi, pi]`.\n\nFor real-valued input data types, `log10` always returns real output.\nFor each value that cannot be expressed as a real number or infinity,\nit yields ``nan`` and sets the `invalid` floating point error flag.\n\nFor complex-valued input, `log10` is a complex analytical function that\nhas a branch cut `[-inf, 0]` and is continuous from above on it.\n`log10` handles the floating-point negative zero as an infinitesimal\nnegative number, conforming to the C99 standard.\n\nReferences\n----------\n.. [1] M. Abramowitz and I.A. Stegun, \"Handbook of Mathematical Functions\",\n       10th printing, 1964, pp. 67.\n       https://personal.math.ubc.ca/~cbm/aands/page_67.htm\n.. [2] Wikipedia, \"Logarithm\". https://en.wikipedia.org/wiki/Logarithm\n\nExamples\n--------\n>>> np.log10([1e-15, -3.])\narray([-15.,  nan])", "short_docstring": "log10(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])"}
{"name": "requests.RequestException", "type": "class", "signature": "(*args, **kwargs)", "docstring": "There was an ambiguous exception that occurred while handling your\nrequest.", "short_docstring": "There was an ambiguous exception that occurred while handling your\nrequest."}
{"name": "sklearn.feature_extraction.text.TfidfVectorizer", "type": "class", "signature": "(*, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, analyzer='word', stop_words=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.float64'>, norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)", "docstring": "Convert a collection of raw documents to a matrix of TF-IDF features.\n\nEquivalent to :class:`CountVectorizer` followed by\n:class:`TfidfTransformer`.\n\nFor an example of usage, see\n:ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`.\n\nFor an efficiency comparison of the different feature extractors, see\n:ref:`sphx_glr_auto_examples_text_plot_hashing_vs_dict_vectorizer.py`.\n\nRead more in the :ref:`User Guide <text_feature_extraction>`.\n\nParameters\n----------\ninput : {'filename', 'file', 'content'}, default='content'\n    - If `'filename'`, the sequence passed as an argument to fit is\n      expected to be a list of filenames that need reading to fetch\n      the raw content to analyze.\n\n    - If `'file'`, the sequence items must have a 'read' method (file-like\n      object) that is called to fetch the bytes in memory.\n\n    - If `'content'`, the input is expected to be a sequence of items that\n      can be of type string or byte.\n\nencoding : str, default='utf-8'\n    If bytes or files are given to analyze, this encoding is used to\n    decode.\n\ndecode_error : {'strict', 'ignore', 'replace'}, default='strict'\n    Instruction on what to do if a byte sequence is given to analyze that\n    contains characters not of the given `encoding`. By default, it is\n    'strict', meaning that a UnicodeDecodeError will be raised. Other\n    values are 'ignore' and 'replace'.\n\nstrip_accents : {'ascii', 'unicode'} or callable, default=None\n    Remove accents and perform other character normalization\n    during the preprocessing step.\n    'ascii' is a fast method that only works on characters that have\n    a direct ASCII mapping.\n    'unicode' is a slightly slower method that works on any characters.\n    None (default) means no character normalization is performed.\n\n    Both 'ascii' and 'unicode' use NFKD normalization from\n    :func:`unicodedata.normalize`.\n\nlowercase : bool, default=True\n    Convert all characters to lowercase before tokenizing.\n\npreprocessor : callable, default=None\n    Override the preprocessing (string transformation) stage while\n    preserving the tokenizing and n-grams generation steps.\n    Only applies if ``analyzer`` is not callable.\n\ntokenizer : callable, default=None\n    Override the string tokenization step while preserving the\n    preprocessing and n-grams generation steps.\n    Only applies if ``analyzer == 'word'``.\n\nanalyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n    Whether the feature should be made of word or character n-grams.\n    Option 'char_wb' creates character n-grams only from text inside\n    word boundaries; n-grams at the edges of words are padded with space.\n\n    If a callable is passed it is used to extract the sequence of features\n    out of the raw, unprocessed input.\n\n    .. versionchanged:: 0.21\n        Since v0.21, if ``input`` is ``'filename'`` or ``'file'``, the data\n        is first read from the file and then passed to the given callable\n        analyzer.\n\nstop_words : {'english'}, list, default=None\n    If a string, it is passed to _check_stop_list and the appropriate stop\n    list is returned. 'english' is currently the only supported string\n    value.\n    There are several known issues with 'english' and you should\n    consider an alternative (see :ref:`stop_words`).\n\n    If a list, that list is assumed to contain stop words, all of which\n    will be removed from the resulting tokens.\n    Only applies if ``analyzer == 'word'``.\n\n    If None, no stop words will be used. In this case, setting `max_df`\n    to a higher value, such as in the range (0.7, 1.0), can automatically detect\n    and filter stop words based on intra corpus document frequency of terms.\n\ntoken_pattern : str, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n    Regular expression denoting what constitutes a \"token\", only used\n    if ``analyzer == 'word'``. The default regexp selects tokens of 2\n    or more alphanumeric characters (punctuation is completely ignored\n    and always treated as a token separator).\n\n    If there is a capturing group in token_pattern then the\n    captured group content, not the entire match, becomes the token.\n    At most one capturing group is permitted.\n\nngram_range : tuple (min_n, max_n), default=(1, 1)\n    The lower and upper boundary of the range of n-values for different\n    n-grams to be extracted. All values of n such that min_n <= n <= max_n\n    will be used. For example an ``ngram_range`` of ``(1, 1)`` means only\n    unigrams, ``(1, 2)`` means unigrams and bigrams, and ``(2, 2)`` means\n    only bigrams.\n    Only applies if ``analyzer`` is not callable.\n\nmax_df : float or int, default=1.0\n    When building the vocabulary ignore terms that have a document\n    frequency strictly higher than the given threshold (corpus-specific\n    stop words).\n    If float in range [0.0, 1.0], the parameter represents a proportion of\n    documents, integer absolute counts.\n    This parameter is ignored if vocabulary is not None.\n\nmin_df : float or int, default=1\n    When building the vocabulary ignore terms that have a document\n    frequency strictly lower than the given threshold. This value is also\n    called cut-off in the literature.\n    If float in range of [0.0, 1.0], the parameter represents a proportion\n    of documents, integer absolute counts.\n    This parameter is ignored if vocabulary is not None.\n\nmax_features : int, default=None\n    If not None, build a vocabulary that only consider the top\n    `max_features` ordered by term frequency across the corpus.\n    Otherwise, all features are used.\n\n    This parameter is ignored if vocabulary is not None.\n\nvocabulary : Mapping or iterable, default=None\n    Either a Mapping (e.g., a dict) where keys are terms and values are\n    indices in the feature matrix, or an iterable over terms. If not\n    given, a vocabulary is determined from the input documents.\n\nbinary : bool, default=False\n    If True, all non-zero term counts are set to 1. This does not mean\n    outputs will have only 0/1 values, only that the tf term in tf-idf\n    is binary. (Set `binary` to True, `use_idf` to False and\n    `norm` to None to get 0/1 outputs).\n\ndtype : dtype, default=float64\n    Type of the matrix returned by fit_transform() or transform().\n\nnorm : {'l1', 'l2'} or None, default='l2'\n    Each output row will have unit norm, either:\n\n    - 'l2': Sum of squares of vector elements is 1. The cosine\n      similarity between two vectors is their dot product when l2 norm has\n      been applied.\n    - 'l1': Sum of absolute values of vector elements is 1.\n      See :func:`~sklearn.preprocessing.normalize`.\n    - None: No normalization.\n\nuse_idf : bool, default=True\n    Enable inverse-document-frequency reweighting. If False, idf(t) = 1.\n\nsmooth_idf : bool, default=True\n    Smooth idf weights by adding one to document frequencies, as if an\n    extra document was seen containing every term in the collection\n    exactly once. Prevents zero divisions.\n\nsublinear_tf : bool, default=False\n    Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).\n\nAttributes\n----------\nvocabulary_ : dict\n    A mapping of terms to feature indices.\n\nfixed_vocabulary_ : bool\n    True if a fixed vocabulary of term to indices mapping\n    is provided by the user.\n\nidf_ : array of shape (n_features,)\n    The inverse document frequency (IDF) vector; only defined\n    if ``use_idf`` is True.\n\nstop_words_ : set\n    Terms that were ignored because they either:\n\n      - occurred in too many documents (`max_df`)\n      - occurred in too few documents (`min_df`)\n      - were cut off by feature selection (`max_features`).\n\n    This is only available if no vocabulary was given.\n\nSee Also\n--------\nCountVectorizer : Transforms text into a sparse matrix of n-gram counts.\n\nTfidfTransformer : Performs the TF-IDF transformation from a provided\n    matrix of counts.\n\nNotes\n-----\nThe ``stop_words_`` attribute can get large and increase the model size\nwhen pickling. This attribute is provided only for introspection and can\nbe safely removed using delattr or set to None before pickling.\n\nExamples\n--------\n>>> from sklearn.feature_extraction.text import TfidfVectorizer\n>>> corpus = [\n...     'This is the first document.',\n...     'This document is the second document.',\n...     'And this is the third one.',\n...     'Is this the first document?',\n... ]\n>>> vectorizer = TfidfVectorizer()\n>>> X = vectorizer.fit_transform(corpus)\n>>> vectorizer.get_feature_names_out()\narray(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n       'this'], ...)\n>>> print(X.shape)\n(4, 9)", "short_docstring": "Convert a collection of raw documents to a matrix of TF-IDF features."}
{"name": "wtforms.StringField", "type": "class", "signature": "(*args, **kwargs)", "docstring": "This field is the base for most of the more complicated fields, and\nrepresents an ``<input type=\"text\">``.", "short_docstring": "This field is the base for most of the more complicated fields, and\nrepresents an ``<input type=\"text\">``."}
{"name": "numpy.cumsum", "type": "callable", "signature": "(a, axis=None, dtype=None, out=None)", "docstring": "Return the cumulative sum of the elements along a given axis.\n\nParameters\n----------\na : array_like\n    Input array.\naxis : int, optional\n    Axis along which the cumulative sum is computed. The default\n    (None) is to compute the cumsum over the flattened array.\ndtype : dtype, optional\n    Type of the returned array and of the accumulator in which the\n    elements are summed.  If `dtype` is not specified, it defaults\n    to the dtype of `a`, unless `a` has an integer dtype with a\n    precision less than that of the default platform integer.  In\n    that case, the default platform integer is used.\nout : ndarray, optional\n    Alternative output array in which to place the result. It must\n    have the same shape and buffer length as the expected output\n    but the type will be cast if necessary. See :ref:`ufuncs-output-type` for\n    more details.\n\nReturns\n-------\ncumsum_along_axis : ndarray.\n    A new array holding the result is returned unless `out` is\n    specified, in which case a reference to `out` is returned. The\n    result has the same size as `a`, and the same shape as `a` if\n    `axis` is not None or `a` is a 1-d array.\n\nSee Also\n--------\nsum : Sum array elements.\ntrapz : Integration of array values using the composite trapezoidal rule.\ndiff : Calculate the n-th discrete difference along given axis.\n\nNotes\n-----\nArithmetic is modular when using integer types, and no error is\nraised on overflow.\n\n``cumsum(a)[-1]`` may not be equal to ``sum(a)`` for floating-point\nvalues since ``sum`` may use a pairwise summation routine, reducing\nthe roundoff-error. See `sum` for more information.\n\nExamples\n--------\n>>> a = np.array([[1,2,3], [4,5,6]])\n>>> a\narray([[1, 2, 3],\n       [4, 5, 6]])\n>>> np.cumsum(a)\narray([ 1,  3,  6, 10, 15, 21])\n>>> np.cumsum(a, dtype=float)     # specifies type of output value(s)\narray([  1.,   3.,   6.,  10.,  15.,  21.])\n\n>>> np.cumsum(a,axis=0)      # sum over rows for each of the 3 columns\narray([[1, 2, 3],\n       [5, 7, 9]])\n>>> np.cumsum(a,axis=1)      # sum over columns for each of the 2 rows\narray([[ 1,  3,  6],\n       [ 4,  9, 15]])\n\n``cumsum(b)[-1]`` may not be equal to ``sum(b)``\n\n>>> b = np.array([1, 2e-9, 3e-9] * 1000000)\n>>> b.cumsum()[-1]\n1000000.0050045159\n>>> b.sum()\n1000000.0050000029", "short_docstring": "Return the cumulative sum of the elements along a given axis."}
{"name": "subprocess.call", "type": "callable", "signature": "(*popenargs, timeout=None, **kwargs)", "docstring": "Run command with arguments.  Wait for command to complete or\ntimeout, then return the returncode attribute.\n\nThe arguments are the same as for the Popen constructor.  Example:\n\nretcode = call([\"ls\", \"-l\"])", "short_docstring": "Run command with arguments.  Wait for command to complete or\ntimeout, then return the returncode attribute."}
{"name": "numpy.where", "type": "callable", "signature": null, "docstring": "where(condition, [x, y], /)\n\nReturn elements chosen from `x` or `y` depending on `condition`.\n\n.. note::\n    When only `condition` is provided, this function is a shorthand for\n    ``np.asarray(condition).nonzero()``. Using `nonzero` directly should be\n    preferred, as it behaves correctly for subclasses. The rest of this\n    documentation covers only the case where all three arguments are\n    provided.\n\nParameters\n----------\ncondition : array_like, bool\n    Where True, yield `x`, otherwise yield `y`.\nx, y : array_like\n    Values from which to choose. `x`, `y` and `condition` need to be\n    broadcastable to some shape.\n\nReturns\n-------\nout : ndarray\n    An array with elements from `x` where `condition` is True, and elements\n    from `y` elsewhere.\n\nSee Also\n--------\nchoose\nnonzero : The function that is called when x and y are omitted\n\nNotes\n-----\nIf all the arrays are 1-D, `where` is equivalent to::\n\n    [xv if c else yv\n     for c, xv, yv in zip(condition, x, y)]\n\nExamples\n--------\n>>> a = np.arange(10)\n>>> a\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n>>> np.where(a < 5, a, 10*a)\narray([ 0,  1,  2,  3,  4, 50, 60, 70, 80, 90])\n\nThis can be used on multidimensional arrays too:\n\n>>> np.where([[True, False], [True, True]],\n...          [[1, 2], [3, 4]],\n...          [[9, 8], [7, 6]])\narray([[1, 8],\n       [3, 4]])\n\nThe shapes of x, y, and the condition are broadcast together:\n\n>>> x, y = np.ogrid[:3, :4]\n>>> np.where(x < y, x, 10 + y)  # both x and 10+y are broadcast\narray([[10,  0,  0,  0],\n       [10, 11,  1,  1],\n       [10, 11, 12,  2]])\n\n>>> a = np.array([[0, 1, 2],\n...               [0, 2, 4],\n...               [0, 3, 6]])\n>>> np.where(a < 4, a, -1)  # -1 is broadcast\narray([[ 0,  1,  2],\n       [ 0,  2, -1],\n       [ 0,  3, -1]])", "short_docstring": "where(condition, [x, y], /)"}
{"name": "socket.SOCK_STREAM", "type": "constant", "value": "SocketKind.SOCK_STREAM", "signature": null, "docstring": "An enumeration.", "short_docstring": "An enumeration."}
{"name": "cryptography.hazmat.backends.default_backend", "type": "callable", "signature": "()", "docstring": null, "short_docstring": ""}
{"name": "cryptography.hazmat.primitives.ciphers.modes.CBC", "type": "class", "signature": "(initialization_vector: bytes)", "docstring": null, "short_docstring": ""}
{"name": "datetime.datetime.today", "type": "callable", "signature": null, "docstring": "Current date or datetime:  same as self.__class__.fromtimestamp(time.time()).", "short_docstring": "Current date or datetime:  same as self.__class__.fromtimestamp(time.time())."}
{"name": "sklearn.preprocessing.normalize", "type": "callable", "signature": "(X, norm='l2', *, axis=1, copy=True, return_norm=False)", "docstring": "Scale input vectors individually to unit norm (vector length).\n\nRead more in the :ref:`User Guide <preprocessing_normalization>`.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The data to normalize, element by element.\n    scipy.sparse matrices should be in CSR format to avoid an\n    un-necessary copy.\n\nnorm : {'l1', 'l2', 'max'}, default='l2'\n    The norm to use to normalize each non zero sample (or each non-zero\n    feature if axis is 0).\n\naxis : {0, 1}, default=1\n    Define axis used to normalize the data along. If 1, independently\n    normalize each sample, otherwise (if 0) normalize each feature.\n\ncopy : bool, default=True\n    If False, try to avoid a copy and normalize in place.\n    This is not guaranteed to always work in place; e.g. if the data is\n    a numpy array with an int dtype, a copy will be returned even with\n    copy=False.\n\nreturn_norm : bool, default=False\n    Whether to return the computed norms.\n\nReturns\n-------\nX : {ndarray, sparse matrix} of shape (n_samples, n_features)\n    Normalized input X.\n\nnorms : ndarray of shape (n_samples, ) if axis=1 else (n_features, )\n    An array of norms along given axis for X.\n    When X is sparse, a NotImplementedError will be raised\n    for norm 'l1' or 'l2'.\n\nSee Also\n--------\nNormalizer : Performs normalization using the Transformer API\n    (e.g. as part of a preprocessing :class:`~sklearn.pipeline.Pipeline`).\n\nNotes\n-----\nFor a comparison of the different scalers, transformers, and normalizers,\nsee: :ref:`sphx_glr_auto_examples_preprocessing_plot_all_scaling.py`.\n\nExamples\n--------\n>>> from sklearn.preprocessing import normalize\n>>> X = [[-2, 1, 2], [-1, 0, 1]]\n>>> normalize(X, norm=\"l1\")  # L1 normalization each row independently\narray([[-0.4,  0.2,  0.4],\n       [-0.5,  0. ,  0.5]])\n>>> normalize(X, norm=\"l2\")  # L2 normalization each row independently\narray([[-0.66...,  0.33...,  0.66...],\n       [-0.70...,  0.     ,  0.70...]])", "short_docstring": "Scale input vectors individually to unit norm (vector length)."}
{"name": "numpy.random.uniform", "type": "callable", "signature": null, "docstring": "uniform(low=0.0, high=1.0, size=None)\n\nDraw samples from a uniform distribution.\n\nSamples are uniformly distributed over the half-open interval\n``[low, high)`` (includes low, but excludes high).  In other words,\nany value within the given interval is equally likely to be drawn\nby `uniform`.\n\n.. note::\n    New code should use the ``uniform`` method of a ``default_rng()``\n    instance instead; please see the :ref:`random-quick-start`.\n\nParameters\n----------\nlow : float or array_like of floats, optional\n    Lower boundary of the output interval.  All values generated will be\n    greater than or equal to low.  The default value is 0.\nhigh : float or array_like of floats\n    Upper boundary of the output interval.  All values generated will be\n    less than or equal to high.  The high limit may be included in the \n    returned array of floats due to floating-point rounding in the \n    equation ``low + (high-low) * random_sample()``.  The default value \n    is 1.0.\nsize : int or tuple of ints, optional\n    Output shape.  If the given shape is, e.g., ``(m, n, k)``, then\n    ``m * n * k`` samples are drawn.  If size is ``None`` (default),\n    a single value is returned if ``low`` and ``high`` are both scalars.\n    Otherwise, ``np.broadcast(low, high).size`` samples are drawn.\n\nReturns\n-------\nout : ndarray or scalar\n    Drawn samples from the parameterized uniform distribution.\n\nSee Also\n--------\nrandint : Discrete uniform distribution, yielding integers.\nrandom_integers : Discrete uniform distribution over the closed\n                  interval ``[low, high]``.\nrandom_sample : Floats uniformly distributed over ``[0, 1)``.\nrandom : Alias for `random_sample`.\nrand : Convenience function that accepts dimensions as input, e.g.,\n       ``rand(2,2)`` would generate a 2-by-2 array of floats,\n       uniformly distributed over ``[0, 1)``.\nrandom.Generator.uniform: which should be used for new code.\n\nNotes\n-----\nThe probability density function of the uniform distribution is\n\n.. math:: p(x) = \\frac{1}{b - a}\n\nanywhere within the interval ``[a, b)``, and zero elsewhere.\n\nWhen ``high`` == ``low``, values of ``low`` will be returned.\nIf ``high`` < ``low``, the results are officially undefined\nand may eventually raise an error, i.e. do not rely on this\nfunction to behave when passed arguments satisfying that\ninequality condition. The ``high`` limit may be included in the\nreturned array of floats due to floating-point rounding in the\nequation ``low + (high-low) * random_sample()``. For example:\n\n>>> x = np.float32(5*0.99999999)\n>>> x\n5.0\n\n\nExamples\n--------\nDraw samples from the distribution:\n\n>>> s = np.random.uniform(-1,0,1000)\n\nAll values are within the given interval:\n\n>>> np.all(s >= -1)\nTrue\n>>> np.all(s < 0)\nTrue\n\nDisplay the histogram of the samples, along with the\nprobability density function:\n\n>>> import matplotlib.pyplot as plt\n>>> count, bins, ignored = plt.hist(s, 15, density=True)\n>>> plt.plot(bins, np.ones_like(bins), linewidth=2, color='r')\n>>> plt.show()", "short_docstring": "uniform(low=0.0, high=1.0, size=None)"}
{"name": "matplotlib.pyplot.Axes", "type": "class", "signature": "(fig, *args, facecolor=None, frameon=True, sharex=None, sharey=None, label='', xscale=None, yscale=None, box_aspect=None, **kwargs)", "docstring": "An Axes object encapsulates all the elements of an individual (sub-)plot in\na figure.\n\nIt contains most of the (sub-)plot elements: `~.axis.Axis`,\n`~.axis.Tick`, `~.lines.Line2D`, `~.text.Text`, `~.patches.Polygon`, etc.,\nand sets the coordinate system.\n\nLike all visible elements in a figure, Axes is an `.Artist` subclass.\n\nThe `Axes` instance supports callbacks through a callbacks attribute which\nis a `~.cbook.CallbackRegistry` instance.  The events you can connect to\nare 'xlim_changed' and 'ylim_changed' and the callback will be called with\nfunc(*ax*) where *ax* is the `Axes` instance.\n\n.. note::\n\n    As a user, you do not instantiate Axes directly, but use Axes creation\n    methods instead; e.g. from `.pyplot` or `.Figure`:\n    `~.pyplot.subplots`, `~.pyplot.subplot_mosaic` or `.Figure.add_axes`.\n\nAttributes\n----------\ndataLim : `.Bbox`\n    The bounding box enclosing all data displayed in the Axes.\nviewLim : `.Bbox`\n    The view limits in data coordinates.", "short_docstring": "An Axes object encapsulates all the elements of an individual (sub-)plot in\na figure."}
{"name": "numpy.outer", "type": "callable", "signature": "(a, b, out=None)", "docstring": "Compute the outer product of two vectors.\n\nGiven two vectors, ``a = [a0, a1, ..., aM]`` and\n``b = [b0, b1, ..., bN]``,\nthe outer product [1]_ is::\n\n  [[a0*b0  a0*b1 ... a0*bN ]\n   [a1*b0    .\n   [ ...          .\n   [aM*b0            aM*bN ]]\n\nParameters\n----------\na : (M,) array_like\n    First input vector.  Input is flattened if\n    not already 1-dimensional.\nb : (N,) array_like\n    Second input vector.  Input is flattened if\n    not already 1-dimensional.\nout : (M, N) ndarray, optional\n    A location where the result is stored\n\n    .. versionadded:: 1.9.0\n\nReturns\n-------\nout : (M, N) ndarray\n    ``out[i, j] = a[i] * b[j]``\n\nSee also\n--------\ninner\neinsum : ``einsum('i,j->ij', a.ravel(), b.ravel())`` is the equivalent.\nufunc.outer : A generalization to dimensions other than 1D and other\n              operations. ``np.multiply.outer(a.ravel(), b.ravel())``\n              is the equivalent.\ntensordot : ``np.tensordot(a.ravel(), b.ravel(), axes=((), ()))``\n            is the equivalent.\n\nReferences\n----------\n.. [1] : G. H. Golub and C. F. Van Loan, *Matrix Computations*, 3rd\n         ed., Baltimore, MD, Johns Hopkins University Press, 1996,\n         pg. 8.\n\nExamples\n--------\nMake a (*very* coarse) grid for computing a Mandelbrot set:\n\n>>> rl = np.outer(np.ones((5,)), np.linspace(-2, 2, 5))\n>>> rl\narray([[-2., -1.,  0.,  1.,  2.],\n       [-2., -1.,  0.,  1.,  2.],\n       [-2., -1.,  0.,  1.,  2.],\n       [-2., -1.,  0.,  1.,  2.],\n       [-2., -1.,  0.,  1.,  2.]])\n>>> im = np.outer(1j*np.linspace(2, -2, 5), np.ones((5,)))\n>>> im\narray([[0.+2.j, 0.+2.j, 0.+2.j, 0.+2.j, 0.+2.j],\n       [0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j],\n       [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n       [0.-1.j, 0.-1.j, 0.-1.j, 0.-1.j, 0.-1.j],\n       [0.-2.j, 0.-2.j, 0.-2.j, 0.-2.j, 0.-2.j]])\n>>> grid = rl + im\n>>> grid\narray([[-2.+2.j, -1.+2.j,  0.+2.j,  1.+2.j,  2.+2.j],\n       [-2.+1.j, -1.+1.j,  0.+1.j,  1.+1.j,  2.+1.j],\n       [-2.+0.j, -1.+0.j,  0.+0.j,  1.+0.j,  2.+0.j],\n       [-2.-1.j, -1.-1.j,  0.-1.j,  1.-1.j,  2.-1.j],\n       [-2.-2.j, -1.-2.j,  0.-2.j,  1.-2.j,  2.-2.j]])\n\nAn example using a \"vector\" of letters:\n\n>>> x = np.array(['a', 'b', 'c'], dtype=object)\n>>> np.outer(x, [1, 2, 3])\narray([['a', 'aa', 'aaa'],\n       ['b', 'bb', 'bbb'],\n       ['c', 'cc', 'ccc']], dtype=object)", "short_docstring": "Compute the outer product of two vectors."}
{"name": "tensorflow.keras.layers.Dense", "error": "Import error: No module named 'tensorflow'"}
{"name": "pandas.read_excel", "type": "callable", "signature": "(io, sheet_name: 'str | int | list[IntStrT] | None' = 0, *, header: 'int | Sequence[int] | None' = 0, names: 'SequenceNotStr[Hashable] | range | None' = None, index_col: 'int | str | Sequence[int] | None' = None, usecols: 'int | str | Sequence[int] | Sequence[str] | Callable[[str], bool] | None' = None, dtype: 'DtypeArg | None' = None, engine: \"Literal['xlrd', 'openpyxl', 'odf', 'pyxlsb', 'calamine'] | None\" = None, converters: 'dict[str, Callable] | dict[int, Callable] | None' = None, true_values: 'Iterable[Hashable] | None' = None, false_values: 'Iterable[Hashable] | None' = None, skiprows: 'Sequence[int] | int | Callable[[int], object] | None' = None, nrows: 'int | None' = None, na_values=None, keep_default_na: 'bool' = True, na_filter: 'bool' = True, verbose: 'bool' = False, parse_dates: 'list | dict | bool' = False, date_parser: 'Callable | lib.NoDefault' = <no_default>, date_format: 'dict[Hashable, str] | str | None' = None, thousands: 'str | None' = None, decimal: 'str' = '.', comment: 'str | None' = None, skipfooter: 'int' = 0, storage_options: 'StorageOptions | None' = None, dtype_backend: 'DtypeBackend | lib.NoDefault' = <no_default>, engine_kwargs: 'dict | None' = None) -> 'DataFrame | dict[IntStrT, DataFrame]'", "docstring": "Read an Excel file into a ``pandas`` ``DataFrame``.\n\nSupports `xls`, `xlsx`, `xlsm`, `xlsb`, `odf`, `ods` and `odt` file extensions\nread from a local filesystem or URL. Supports an option to read\na single sheet or a list of sheets.\n\nParameters\n----------\nio : str, bytes, ExcelFile, xlrd.Book, path object, or file-like object\n    Any valid string path is acceptable. The string could be a URL. Valid\n    URL schemes include http, ftp, s3, and file. For file URLs, a host is\n    expected. A local file could be: ``file://localhost/path/to/table.xlsx``.\n\n    If you want to pass in a path object, pandas accepts any ``os.PathLike``.\n\n    By file-like object, we refer to objects with a ``read()`` method,\n    such as a file handle (e.g. via builtin ``open`` function)\n    or ``StringIO``.\n\n    .. deprecated:: 2.1.0\n        Passing byte strings is deprecated. To read from a\n        byte string, wrap it in a ``BytesIO`` object.\nsheet_name : str, int, list, or None, default 0\n    Strings are used for sheet names. Integers are used in zero-indexed\n    sheet positions (chart sheets do not count as a sheet position).\n    Lists of strings/integers are used to request multiple sheets.\n    Specify ``None`` to get all worksheets.\n\n    Available cases:\n\n    * Defaults to ``0``: 1st sheet as a `DataFrame`\n    * ``1``: 2nd sheet as a `DataFrame`\n    * ``\"Sheet1\"``: Load sheet with name \"Sheet1\"\n    * ``[0, 1, \"Sheet5\"]``: Load first, second and sheet named \"Sheet5\"\n      as a dict of `DataFrame`\n    * ``None``: All worksheets.\n\nheader : int, list of int, default 0\n    Row (0-indexed) to use for the column labels of the parsed\n    DataFrame. If a list of integers is passed those row positions will\n    be combined into a ``MultiIndex``. Use None if there is no header.\nnames : array-like, default None\n    List of column names to use. If file contains no header row,\n    then you should explicitly pass header=None.\nindex_col : int, str, list of int, default None\n    Column (0-indexed) to use as the row labels of the DataFrame.\n    Pass None if there is no such column.  If a list is passed,\n    those columns will be combined into a ``MultiIndex``.  If a\n    subset of data is selected with ``usecols``, index_col\n    is based on the subset.\n\n    Missing values will be forward filled to allow roundtripping with\n    ``to_excel`` for ``merged_cells=True``. To avoid forward filling the\n    missing values use ``set_index`` after reading the data instead of\n    ``index_col``.\nusecols : str, list-like, or callable, default None\n    * If None, then parse all columns.\n    * If str, then indicates comma separated list of Excel column letters\n      and column ranges (e.g. \"A:E\" or \"A,C,E:F\"). Ranges are inclusive of\n      both sides.\n    * If list of int, then indicates list of column numbers to be parsed\n      (0-indexed).\n    * If list of string, then indicates list of column names to be parsed.\n    * If callable, then evaluate each column name against it and parse the\n      column if the callable returns ``True``.\n\n    Returns a subset of the columns according to behavior above.\ndtype : Type name or dict of column -> type, default None\n    Data type for data or columns. E.g. {'a': np.float64, 'b': np.int32}\n    Use ``object`` to preserve data as stored in Excel and not interpret dtype,\n    which will necessarily result in ``object`` dtype.\n    If converters are specified, they will be applied INSTEAD\n    of dtype conversion.\n    If you use ``None``, it will infer the dtype of each column based on the data.\nengine : {'openpyxl', 'calamine', 'odf', 'pyxlsb', 'xlrd'}, default None\n    If io is not a buffer or path, this must be set to identify io.\n    Engine compatibility :\n\n    - ``openpyxl`` supports newer Excel file formats.\n    - ``calamine`` supports Excel (.xls, .xlsx, .xlsm, .xlsb)\n      and OpenDocument (.ods) file formats.\n    - ``odf`` supports OpenDocument file formats (.odf, .ods, .odt).\n    - ``pyxlsb`` supports Binary Excel files.\n    - ``xlrd`` supports old-style Excel files (.xls).\n\n    When ``engine=None``, the following logic will be used to determine the engine:\n\n    - If ``path_or_buffer`` is an OpenDocument format (.odf, .ods, .odt),\n      then `odf <https://pypi.org/project/odfpy/>`_ will be used.\n    - Otherwise if ``path_or_buffer`` is an xls format, ``xlrd`` will be used.\n    - Otherwise if ``path_or_buffer`` is in xlsb format, ``pyxlsb`` will be used.\n    - Otherwise ``openpyxl`` will be used.\nconverters : dict, default None\n    Dict of functions for converting values in certain columns. Keys can\n    either be integers or column labels, values are functions that take one\n    input argument, the Excel cell content, and return the transformed\n    content.\ntrue_values : list, default None\n    Values to consider as True.\nfalse_values : list, default None\n    Values to consider as False.\nskiprows : list-like, int, or callable, optional\n    Line numbers to skip (0-indexed) or number of lines to skip (int) at the\n    start of the file. If callable, the callable function will be evaluated\n    against the row indices, returning True if the row should be skipped and\n    False otherwise. An example of a valid callable argument would be ``lambda\n    x: x in [0, 2]``.\nnrows : int, default None\n    Number of rows to parse.\nna_values : scalar, str, list-like, or dict, default None\n    Additional strings to recognize as NA/NaN. If dict passed, specific\n    per-column NA values. By default the following values are interpreted\n    as NaN: '', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan',\n    '1.#IND', '1.#QNAN', '<NA>', 'N/A', 'NA', 'NULL', 'NaN', 'None',\n    'n/a', 'nan', 'null'.\nkeep_default_na : bool, default True\n    Whether or not to include the default NaN values when parsing the data.\n    Depending on whether ``na_values`` is passed in, the behavior is as follows:\n\n    * If ``keep_default_na`` is True, and ``na_values`` are specified,\n      ``na_values`` is appended to the default NaN values used for parsing.\n    * If ``keep_default_na`` is True, and ``na_values`` are not specified, only\n      the default NaN values are used for parsing.\n    * If ``keep_default_na`` is False, and ``na_values`` are specified, only\n      the NaN values specified ``na_values`` are used for parsing.\n    * If ``keep_default_na`` is False, and ``na_values`` are not specified, no\n      strings will be parsed as NaN.\n\n    Note that if `na_filter` is passed in as False, the ``keep_default_na`` and\n    ``na_values`` parameters will be ignored.\nna_filter : bool, default True\n    Detect missing value markers (empty strings and the value of na_values). In\n    data without any NAs, passing ``na_filter=False`` can improve the\n    performance of reading a large file.\nverbose : bool, default False\n    Indicate number of NA values placed in non-numeric columns.\nparse_dates : bool, list-like, or dict, default False\n    The behavior is as follows:\n\n    * ``bool``. If True -> try parsing the index.\n    * ``list`` of int or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3\n      each as a separate date column.\n    * ``list`` of lists. e.g.  If [[1, 3]] -> combine columns 1 and 3 and parse as\n      a single date column.\n    * ``dict``, e.g. {'foo' : [1, 3]} -> parse columns 1, 3 as date and call\n      result 'foo'\n\n    If a column or index contains an unparsable date, the entire column or\n    index will be returned unaltered as an object data type. If you don`t want to\n    parse some cells as date just change their type in Excel to \"Text\".\n    For non-standard datetime parsing, use ``pd.to_datetime`` after ``pd.read_excel``.\n\n    Note: A fast-path exists for iso8601-formatted dates.\ndate_parser : function, optional\n    Function to use for converting a sequence of string columns to an array of\n    datetime instances. The default uses ``dateutil.parser.parser`` to do the\n    conversion. Pandas will try to call `date_parser` in three different ways,\n    advancing to the next if an exception occurs: 1) Pass one or more arrays\n    (as defined by `parse_dates`) as arguments; 2) concatenate (row-wise) the\n    string values from the columns defined by `parse_dates` into a single array\n    and pass that; and 3) call `date_parser` once for each row using one or\n    more strings (corresponding to the columns defined by `parse_dates`) as\n    arguments.\n\n    .. deprecated:: 2.0.0\n       Use ``date_format`` instead, or read in as ``object`` and then apply\n       :func:`to_datetime` as-needed.\ndate_format : str or dict of column -> format, default ``None``\n   If used in conjunction with ``parse_dates``, will parse dates according to this\n   format. For anything more complex,\n   please read in as ``object`` and then apply :func:`to_datetime` as-needed.\n\n   .. versionadded:: 2.0.0\nthousands : str, default None\n    Thousands separator for parsing string columns to numeric.  Note that\n    this parameter is only necessary for columns stored as TEXT in Excel,\n    any numeric columns will automatically be parsed, regardless of display\n    format.\ndecimal : str, default '.'\n    Character to recognize as decimal point for parsing string columns to numeric.\n    Note that this parameter is only necessary for columns stored as TEXT in Excel,\n    any numeric columns will automatically be parsed, regardless of display\n    format.(e.g. use ',' for European data).\n\n    .. versionadded:: 1.4.0\n\ncomment : str, default None\n    Comments out remainder of line. Pass a character or characters to this\n    argument to indicate comments in the input file. Any data between the\n    comment string and the end of the current line is ignored.\nskipfooter : int, default 0\n    Rows at the end to skip (0-indexed).\nstorage_options : dict, optional\n    Extra options that make sense for a particular storage connection, e.g.\n    host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n    are forwarded to ``urllib.request.Request`` as header options. For other\n    URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n    forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n    details, and for more examples on storage options refer `here\n    <https://pandas.pydata.org/docs/user_guide/io.html?\n    highlight=storage_options#reading-writing-remote-files>`_.\n\ndtype_backend : {'numpy_nullable', 'pyarrow'}, default 'numpy_nullable'\n    Back-end data type applied to the resultant :class:`DataFrame`\n    (still experimental). Behaviour is as follows:\n\n    * ``\"numpy_nullable\"``: returns nullable-dtype-backed :class:`DataFrame`\n      (default).\n    * ``\"pyarrow\"``: returns pyarrow-backed nullable :class:`ArrowDtype`\n      DataFrame.\n\n    .. versionadded:: 2.0\n\nengine_kwargs : dict, optional\n    Arbitrary keyword arguments passed to excel engine.\n\nReturns\n-------\nDataFrame or dict of DataFrames\n    DataFrame from the passed in Excel file. See notes in sheet_name\n    argument for more information on when a dict of DataFrames is returned.\n\nSee Also\n--------\nDataFrame.to_excel : Write DataFrame to an Excel file.\nDataFrame.to_csv : Write DataFrame to a comma-separated values (csv) file.\nread_csv : Read a comma-separated values (csv) file into DataFrame.\nread_fwf : Read a table of fixed-width formatted lines into DataFrame.\n\nNotes\n-----\nFor specific information on the methods used for each Excel engine, refer to the pandas\n:ref:`user guide <io.excel_reader>`\n\nExamples\n--------\nThe file can be read using the file name as string or an open file object:\n\n>>> pd.read_excel('tmp.xlsx', index_col=0)  # doctest: +SKIP\n       Name  Value\n0   string1      1\n1   string2      2\n2  #Comment      3\n\n>>> pd.read_excel(open('tmp.xlsx', 'rb'),\n...               sheet_name='Sheet3')  # doctest: +SKIP\n   Unnamed: 0      Name  Value\n0           0   string1      1\n1           1   string2      2\n2           2  #Comment      3\n\nIndex and header can be specified via the `index_col` and `header` arguments\n\n>>> pd.read_excel('tmp.xlsx', index_col=None, header=None)  # doctest: +SKIP\n     0         1      2\n0  NaN      Name  Value\n1  0.0   string1      1\n2  1.0   string2      2\n3  2.0  #Comment      3\n\nColumn types are inferred but can be explicitly specified\n\n>>> pd.read_excel('tmp.xlsx', index_col=0,\n...               dtype={'Name': str, 'Value': float})  # doctest: +SKIP\n       Name  Value\n0   string1    1.0\n1   string2    2.0\n2  #Comment    3.0\n\nTrue, False, and NA values, and thousands separators have defaults,\nbut can be explicitly specified, too. Supply the values you would like\nas strings or lists of strings!\n\n>>> pd.read_excel('tmp.xlsx', index_col=0,\n...               na_values=['string1', 'string2'])  # doctest: +SKIP\n       Name  Value\n0       NaN      1\n1       NaN      2\n2  #Comment      3\n\nComment lines in the excel input file can be skipped using the\n``comment`` kwarg.\n\n>>> pd.read_excel('tmp.xlsx', index_col=0, comment='#')  # doctest: +SKIP\n      Name  Value\n0  string1    1.0\n1  string2    2.0\n2     None    NaN", "short_docstring": "Read an Excel file into a ``pandas`` ``DataFrame``."}
{"name": "urllib.parse.urlparse", "type": "callable", "signature": "(url, scheme='', allow_fragments=True)", "docstring": "Parse a URL into 6 components:\n<scheme>://<netloc>/<path>;<params>?<query>#<fragment>\n\nThe result is a named 6-tuple with fields corresponding to the\nabove. It is either a ParseResult or ParseResultBytes object,\ndepending on the type of the url parameter.\n\nThe username, password, hostname, and port sub-components of netloc\ncan also be accessed as attributes of the returned object.\n\nThe scheme argument provides the default value of the scheme\ncomponent when no scheme is found in url.\n\nIf allow_fragments is False, no attempt is made to separate the\nfragment component from the previous component, which can be either\npath or query.\n\nNote that % escapes are not expanded.", "short_docstring": "Parse a URL into 6 components:\n<scheme>://<netloc>/<path>;<params>?<query>#<fragment>"}
{"name": "pandas.crosstab", "type": "callable", "signature": "(index, columns, values=None, rownames=None, colnames=None, aggfunc=None, margins: 'bool' = False, margins_name: 'Hashable' = 'All', dropna: 'bool' = True, normalize: \"bool | Literal[0, 1, 'all', 'index', 'columns']\" = False) -> 'DataFrame'", "docstring": "Compute a simple cross tabulation of two (or more) factors.\n\nBy default, computes a frequency table of the factors unless an\narray of values and an aggregation function are passed.\n\nParameters\n----------\nindex : array-like, Series, or list of arrays/Series\n    Values to group by in the rows.\ncolumns : array-like, Series, or list of arrays/Series\n    Values to group by in the columns.\nvalues : array-like, optional\n    Array of values to aggregate according to the factors.\n    Requires `aggfunc` be specified.\nrownames : sequence, default None\n    If passed, must match number of row arrays passed.\ncolnames : sequence, default None\n    If passed, must match number of column arrays passed.\naggfunc : function, optional\n    If specified, requires `values` be specified as well.\nmargins : bool, default False\n    Add row/column margins (subtotals).\nmargins_name : str, default 'All'\n    Name of the row/column that will contain the totals\n    when margins is True.\ndropna : bool, default True\n    Do not include columns whose entries are all NaN.\nnormalize : bool, {'all', 'index', 'columns'}, or {0,1}, default False\n    Normalize by dividing all values by the sum of values.\n\n    - If passed 'all' or `True`, will normalize over all values.\n    - If passed 'index' will normalize over each row.\n    - If passed 'columns' will normalize over each column.\n    - If margins is `True`, will also normalize margin values.\n\nReturns\n-------\nDataFrame\n    Cross tabulation of the data.\n\nSee Also\n--------\nDataFrame.pivot : Reshape data based on column values.\npivot_table : Create a pivot table as a DataFrame.\n\nNotes\n-----\nAny Series passed will have their name attributes used unless row or column\nnames for the cross-tabulation are specified.\n\nAny input passed containing Categorical data will have **all** of its\ncategories included in the cross-tabulation, even if the actual data does\nnot contain any instances of a particular category.\n\nIn the event that there aren't overlapping indexes an empty DataFrame will\nbe returned.\n\nReference :ref:`the user guide <reshaping.crosstabulations>` for more examples.\n\nExamples\n--------\n>>> a = np.array([\"foo\", \"foo\", \"foo\", \"foo\", \"bar\", \"bar\",\n...               \"bar\", \"bar\", \"foo\", \"foo\", \"foo\"], dtype=object)\n>>> b = np.array([\"one\", \"one\", \"one\", \"two\", \"one\", \"one\",\n...               \"one\", \"two\", \"two\", \"two\", \"one\"], dtype=object)\n>>> c = np.array([\"dull\", \"dull\", \"shiny\", \"dull\", \"dull\", \"shiny\",\n...               \"shiny\", \"dull\", \"shiny\", \"shiny\", \"shiny\"],\n...              dtype=object)\n>>> pd.crosstab(a, [b, c], rownames=['a'], colnames=['b', 'c'])\nb   one        two\nc   dull shiny dull shiny\na\nbar    1     2    1     0\nfoo    2     2    1     2\n\nHere 'c' and 'f' are not represented in the data and will not be\nshown in the output because dropna is True by default. Set\ndropna=False to preserve categories with no data.\n\n>>> foo = pd.Categorical(['a', 'b'], categories=['a', 'b', 'c'])\n>>> bar = pd.Categorical(['d', 'e'], categories=['d', 'e', 'f'])\n>>> pd.crosstab(foo, bar)\ncol_0  d  e\nrow_0\na      1  0\nb      0  1\n>>> pd.crosstab(foo, bar, dropna=False)\ncol_0  d  e  f\nrow_0\na      1  0  0\nb      0  1  0\nc      0  0  0", "short_docstring": "Compute a simple cross tabulation of two (or more) factors."}
{"name": "cv2.imread", "type": "callable", "signature": null, "docstring": "imread(filename[, flags]) -> retval\n.   @brief Loads an image from a file.\n.   \n.   @anchor imread\n.   \n.   The function imread loads an image from the specified file and returns it. If the image cannot be\n.   read (because of missing file, improper permissions, unsupported or invalid format), the function\n.   returns an empty matrix ( Mat::data==NULL ).\n.   \n.   Currently, the following file formats are supported:\n.   \n.   -   Windows bitmaps - \\*.bmp, \\*.dib (always supported)\n.   -   JPEG files - \\*.jpeg, \\*.jpg, \\*.jpe (see the *Note* section)\n.   -   JPEG 2000 files - \\*.jp2 (see the *Note* section)\n.   -   Portable Network Graphics - \\*.png (see the *Note* section)\n.   -   WebP - \\*.webp (see the *Note* section)\n.   -   AVIF - \\*.avif (see the *Note* section)\n.   -   Portable image format - \\*.pbm, \\*.pgm, \\*.ppm \\*.pxm, \\*.pnm (always supported)\n.   -   PFM files - \\*.pfm (see the *Note* section)\n.   -   Sun rasters - \\*.sr, \\*.ras (always supported)\n.   -   TIFF files - \\*.tiff, \\*.tif (see the *Note* section)\n.   -   OpenEXR Image files - \\*.exr (see the *Note* section)\n.   -   Radiance HDR - \\*.hdr, \\*.pic (always supported)\n.   -   Raster and Vector geospatial data supported by GDAL (see the *Note* section)\n.   \n.   @note\n.   -   The function determines the type of an image by the content, not by the file extension.\n.   -   In the case of color images, the decoded images will have the channels stored in **B G R** order.\n.   -   When using IMREAD_GRAYSCALE, the codec's internal grayscale conversion will be used, if available.\n.       Results may differ to the output of cvtColor()\n.   -   On Microsoft Windows\\* OS and MacOSX\\*, the codecs shipped with an OpenCV image (libjpeg,\n.       libpng, libtiff, and libjasper) are used by default. So, OpenCV can always read JPEGs, PNGs,\n.       and TIFFs. On MacOSX, there is also an option to use native MacOSX image readers. But beware\n.       that currently these native image loaders give images with different pixel values because of\n.       the color management embedded into MacOSX.\n.   -   On Linux\\*, BSD flavors and other Unix-like open-source operating systems, OpenCV looks for\n.       codecs supplied with an OS image. Install the relevant packages (do not forget the development\n.       files, for example, \"libjpeg-dev\", in Debian\\* and Ubuntu\\*) to get the codec support or turn\n.       on the OPENCV_BUILD_3RDPARTY_LIBS flag in CMake.\n.   -   In the case you set *WITH_GDAL* flag to true in CMake and @ref IMREAD_LOAD_GDAL to load the image,\n.       then the [GDAL](http://www.gdal.org) driver will be used in order to decode the image, supporting\n.       the following formats: [Raster](http://www.gdal.org/formats_list.html),\n.       [Vector](http://www.gdal.org/ogr_formats.html).\n.   -   If EXIF information is embedded in the image file, the EXIF orientation will be taken into account\n.       and thus the image will be rotated accordingly except if the flags @ref IMREAD_IGNORE_ORIENTATION\n.       or @ref IMREAD_UNCHANGED are passed.\n.   -   Use the IMREAD_UNCHANGED flag to keep the floating point values from PFM image.\n.   -   By default number of pixels must be less than 2^30. Limit can be set using system\n.       variable OPENCV_IO_MAX_IMAGE_PIXELS\n.   \n.   @param filename Name of file to be loaded.\n.   @param flags Flag that can take values of cv::ImreadModes", "short_docstring": "imread(filename[, flags]) -> retval\n.   @brief Loads an image from a file.\n.   \n.   @anchor imread\n.   \n.   The function imread loads an image from the specified file and returns it. If the image cannot be\n.   read (because of missing file, improper permissions, unsupported or invalid format), the function\n.   returns an empty matrix ( Mat::data==NULL ).\n.   \n.   Currently, the following file formats are supported:\n.   \n.   -   Windows bitmaps - \\*.bmp, \\*.dib (always supported)\n.   -   JPEG files - \\*.jpeg, \\*.jpg, \\*.jpe (see the *Note* section)\n.   -   JPEG 2000 files - \\*.jp2 (see the *Note* section)\n.   -   Portable Network Graphics - \\*.png (see the *Note* section)\n.   -   WebP - \\*.webp (see the *Note* section)\n.   -   AVIF - \\*.avif (see the *Note* section)\n.   -   Portable image format - \\*.pbm, \\*.pgm, \\*.ppm \\*.pxm, \\*.pnm (always supported)\n.   -   PFM files - \\*.pfm (see the *Note* section)\n.   -   Sun rasters - \\*.sr, \\*.ras (always supported)\n.   -   TIFF files - \\*.tiff, \\*.tif (see the *Note* section)\n.   -   OpenEXR Image files - \\*.exr (see the *Note* section)\n.   -   Radiance HDR - \\*.hdr, \\*.pic (always supported)\n.   -   Raster and Vector geospatial data supported by GDAL (see the *Note* section)\n.   \n.   @note\n.   -   The function determines the type of an image by the content, not by the file extension.\n.   -   In the case of color images, the decoded images will have the channels stored in **B G R** order.\n.   -   When using IMREAD_GRAYSCALE, the codec's internal grayscale conversion will be used, if available.\n.       Results may differ to the output of cvtColor()\n.   -   On Microsoft Windows\\* OS and MacOSX\\*, the codecs shipped with an OpenCV image (libjpeg,\n.       libpng, libtiff, and libjasper) are used by default. So, OpenCV can always read JPEGs, PNGs,\n.       and TIFFs. On MacOSX, there is also an option to use native MacOSX image readers. But beware\n.       that currently these native image loaders give images with different pixel values because of\n.       the color management embedded into MacOSX.\n.   -   On Linux\\*, BSD flavors and other Unix-like open-source operating systems, OpenCV looks for\n.       codecs supplied with an OS image. Install the relevant packages (do not forget the development\n.       files, for example, \"libjpeg-dev\", in Debian\\* and Ubuntu\\*) to get the codec support or turn\n.       on the OPENCV_BUILD_3RDPARTY_LIBS flag in CMake.\n.   -   In the case you set *WITH_GDAL* flag to true in CMake and @ref IMREAD_LOAD_GDAL to load the image,\n.       then the [GDAL](http://www.gdal.org) driver will be used in order to decode the image, supporting\n.       the following formats: [Raster](http://www.gdal.org/formats_list.html),\n.       [Vector](http://www.gdal.org/ogr_formats.html).\n.   -   If EXIF information is embedded in the image file, the EXIF orientation will be taken into account\n.       and thus the image will be rotated accordingly except if the flags @ref IMREAD_IGNORE_ORIENTATION\n.       or @ref IMREAD_UNCHANGED are passed.\n.   -   Use the IMREAD_UNCHANGED flag to keep the floating point values from PFM image.\n.   -   By default number of pixels must be less than 2^30. Limit can be set using system\n.       variable OPENCV_IO_MAX_IMAGE_PIXELS\n.   \n.   @param filename Name of file to be loaded.\n.   @param flags Flag that can take values of cv::ImreadModes"}
{"name": "csv.DictReader", "type": "class", "signature": "(f, fieldnames=None, restkey=None, restval=None, dialect='excel', *args, **kwds)", "docstring": null, "short_docstring": ""}
{"name": "numpy.ndarray", "type": "class", "signature": null, "docstring": "ndarray(shape, dtype=float, buffer=None, offset=0,\n        strides=None, order=None)\n\nAn array object represents a multidimensional, homogeneous array\nof fixed-size items.  An associated data-type object describes the\nformat of each element in the array (its byte-order, how many bytes it\noccupies in memory, whether it is an integer, a floating point number,\nor something else, etc.)\n\nArrays should be constructed using `array`, `zeros` or `empty` (refer\nto the See Also section below).  The parameters given here refer to\na low-level method (`ndarray(...)`) for instantiating an array.\n\nFor more information, refer to the `numpy` module and examine the\nmethods and attributes of an array.\n\nParameters\n----------\n(for the __new__ method; see Notes below)\n\nshape : tuple of ints\n    Shape of created array.\ndtype : data-type, optional\n    Any object that can be interpreted as a numpy data type.\nbuffer : object exposing buffer interface, optional\n    Used to fill the array with data.\noffset : int, optional\n    Offset of array data in buffer.\nstrides : tuple of ints, optional\n    Strides of data in memory.\norder : {'C', 'F'}, optional\n    Row-major (C-style) or column-major (Fortran-style) order.\n\nAttributes\n----------\nT : ndarray\n    Transpose of the array.\ndata : buffer\n    The array's elements, in memory.\ndtype : dtype object\n    Describes the format of the elements in the array.\nflags : dict\n    Dictionary containing information related to memory use, e.g.,\n    'C_CONTIGUOUS', 'OWNDATA', 'WRITEABLE', etc.\nflat : numpy.flatiter object\n    Flattened version of the array as an iterator.  The iterator\n    allows assignments, e.g., ``x.flat = 3`` (See `ndarray.flat` for\n    assignment examples; TODO).\nimag : ndarray\n    Imaginary part of the array.\nreal : ndarray\n    Real part of the array.\nsize : int\n    Number of elements in the array.\nitemsize : int\n    The memory use of each array element in bytes.\nnbytes : int\n    The total number of bytes required to store the array data,\n    i.e., ``itemsize * size``.\nndim : int\n    The array's number of dimensions.\nshape : tuple of ints\n    Shape of the array.\nstrides : tuple of ints\n    The step-size required to move from one element to the next in\n    memory. For example, a contiguous ``(3, 4)`` array of type\n    ``int16`` in C-order has strides ``(8, 2)``.  This implies that\n    to move from element to element in memory requires jumps of 2 bytes.\n    To move from row-to-row, one needs to jump 8 bytes at a time\n    (``2 * 4``).\nctypes : ctypes object\n    Class containing properties of the array needed for interaction\n    with ctypes.\nbase : ndarray\n    If the array is a view into another array, that array is its `base`\n    (unless that array is also a view).  The `base` array is where the\n    array data is actually stored.\n\nSee Also\n--------\narray : Construct an array.\nzeros : Create an array, each element of which is zero.\nempty : Create an array, but leave its allocated memory unchanged (i.e.,\n        it contains \"garbage\").\ndtype : Create a data-type.\nnumpy.typing.NDArray : An ndarray alias :term:`generic <generic type>`\n                       w.r.t. its `dtype.type <numpy.dtype.type>`.\n\nNotes\n-----\nThere are two modes of creating an array using ``__new__``:\n\n1. If `buffer` is None, then only `shape`, `dtype`, and `order`\n   are used.\n2. If `buffer` is an object exposing the buffer interface, then\n   all keywords are interpreted.\n\nNo ``__init__`` method is needed because the array is fully initialized\nafter the ``__new__`` method.\n\nExamples\n--------\nThese examples illustrate the low-level `ndarray` constructor.  Refer\nto the `See Also` section above for easier ways of constructing an\nndarray.\n\nFirst mode, `buffer` is None:\n\n>>> np.ndarray(shape=(2,2), dtype=float, order='F')\narray([[0.0e+000, 0.0e+000], # random\n       [     nan, 2.5e-323]])\n\nSecond mode:\n\n>>> np.ndarray((2,), buffer=np.array([1,2,3]),\n...            offset=np.int_().itemsize,\n...            dtype=int) # offset = 1*itemsize, i.e. skip first element\narray([2, 3])", "short_docstring": "ndarray(shape, dtype=float, buffer=None, offset=0,\n        strides=None, order=None)"}
{"name": "datetime.datetime", "type": "class", "signature": null, "docstring": "datetime(year, month, day[, hour[, minute[, second[, microsecond[,tzinfo]]]]])\n\nThe year, month and day arguments are required. tzinfo may be None, or an\ninstance of a tzinfo subclass. The remaining arguments may be ints.", "short_docstring": "datetime(year, month, day[, hour[, minute[, second[, microsecond[,tzinfo]]]]])"}
{"name": "datetime.datetime.now", "type": "callable", "signature": "(tz=None)", "docstring": "Returns new datetime object representing current time local to tz.\n\n  tz\n    Timezone object.\n\nIf no tz is specified, uses local timezone.", "short_docstring": "Returns new datetime object representing current time local to tz."}
{"name": "numpy.issubdtype", "type": "callable", "signature": "(arg1, arg2)", "docstring": "Returns True if first argument is a typecode lower/equal in type hierarchy.\n\nThis is like the builtin :func:`issubclass`, but for `dtype`\\ s.\n\nParameters\n----------\narg1, arg2 : dtype_like\n    `dtype` or object coercible to one\n\nReturns\n-------\nout : bool\n\nSee Also\n--------\n:ref:`arrays.scalars` : Overview of the numpy type hierarchy.\nissubsctype, issubclass_\n\nExamples\n--------\n`issubdtype` can be used to check the type of arrays:\n\n>>> ints = np.array([1, 2, 3], dtype=np.int32)\n>>> np.issubdtype(ints.dtype, np.integer)\nTrue\n>>> np.issubdtype(ints.dtype, np.floating)\nFalse\n\n>>> floats = np.array([1, 2, 3], dtype=np.float32)\n>>> np.issubdtype(floats.dtype, np.integer)\nFalse\n>>> np.issubdtype(floats.dtype, np.floating)\nTrue\n\nSimilar types of different sizes are not subdtypes of each other:\n\n>>> np.issubdtype(np.float64, np.float32)\nFalse\n>>> np.issubdtype(np.float32, np.float64)\nFalse\n\nbut both are subtypes of `floating`:\n\n>>> np.issubdtype(np.float64, np.floating)\nTrue\n>>> np.issubdtype(np.float32, np.floating)\nTrue\n\nFor convenience, dtype-like objects are allowed too:\n\n>>> np.issubdtype('S1', np.string_)\nTrue\n>>> np.issubdtype('i4', np.signedinteger)\nTrue", "short_docstring": "Returns True if first argument is a typecode lower/equal in type hierarchy."}
{"name": "pathlib.Path", "type": "class", "signature": "(*args, **kwargs)", "docstring": "PurePath subclass that can make system calls.\n\nPath represents a filesystem path but unlike PurePath, also offers\nmethods to do system calls on path objects. Depending on your system,\ninstantiating a Path will return either a PosixPath or a WindowsPath\nobject. You can also instantiate a PosixPath or WindowsPath directly,\nbut cannot instantiate a WindowsPath on a POSIX system or vice versa.", "short_docstring": "PurePath subclass that can make system calls."}
{"name": "datetime.datetime.strptime", "type": "callable", "signature": null, "docstring": "string, format -> new datetime parsed from a string (like time.strptime()).", "short_docstring": "string, format -> new datetime parsed from a string (like time.strptime())."}
{"name": "Crypto.Cipher.AES.new", "type": "callable", "signature": "(key, mode, *args, **kwargs)", "docstring": "Create a new AES cipher.\n\n:param key:\n    The secret key to use in the symmetric cipher.\n\n    It must be 16, 24 or 32 bytes long (respectively for *AES-128*,\n    *AES-192* or *AES-256*).\n\n    For ``MODE_SIV`` only, it doubles to 32, 48, or 64 bytes.\n:type key: bytes/bytearray/memoryview\n\n:param mode:\n    The chaining mode to use for encryption or decryption.\n    If in doubt, use ``MODE_EAX``.\n:type mode: One of the supported ``MODE_*`` constants\n\n:Keyword Arguments:\n    *   **iv** (*bytes*, *bytearray*, *memoryview*) --\n        (Only applicable for ``MODE_CBC``, ``MODE_CFB``, ``MODE_OFB``,\n        and ``MODE_OPENPGP`` modes).\n\n        The initialization vector to use for encryption or decryption.\n\n        For ``MODE_CBC``, ``MODE_CFB``, and ``MODE_OFB`` it must be 16 bytes long.\n\n        For ``MODE_OPENPGP`` mode only,\n        it must be 16 bytes long for encryption\n        and 18 bytes for decryption (in the latter case, it is\n        actually the *encrypted* IV which was prefixed to the ciphertext).\n\n        If not provided, a random byte string is generated (you must then\n        read its value with the :attr:`iv` attribute).\n\n    *   **nonce** (*bytes*, *bytearray*, *memoryview*) --\n        (Only applicable for ``MODE_CCM``, ``MODE_EAX``, ``MODE_GCM``,\n        ``MODE_SIV``, ``MODE_OCB``, and ``MODE_CTR``).\n\n        A value that must never be reused for any other encryption done\n        with this key (except possibly for ``MODE_SIV``, see below).\n\n        For ``MODE_EAX``, ``MODE_GCM`` and ``MODE_SIV`` there are no\n        restrictions on its length (recommended: **16** bytes).\n\n        For ``MODE_CCM``, its length must be in the range **[7..13]**.\n        Bear in mind that with CCM there is a trade-off between nonce\n        length and maximum message size. Recommendation: **11** bytes.\n\n        For ``MODE_OCB``, its length must be in the range **[1..15]**\n        (recommended: **15**).\n\n        For ``MODE_CTR``, its length must be in the range **[0..15]**\n        (recommended: **8**).\n\n        For ``MODE_SIV``, the nonce is optional, if it is not specified,\n        then no nonce is being used, which renders the encryption\n        deterministic.\n\n        If not provided, for modes other than ``MODE_SIV```, a random\n        byte string of the recommended length is used (you must then\n        read its value with the :attr:`nonce` attribute).\n\n    *   **segment_size** (*integer*) --\n        (Only ``MODE_CFB``).The number of **bits** the plaintext and ciphertext\n        are segmented in. It must be a multiple of 8.\n        If not specified, it will be assumed to be 8.\n\n    *   **mac_len** : (*integer*) --\n        (Only ``MODE_EAX``, ``MODE_GCM``, ``MODE_OCB``, ``MODE_CCM``)\n        Length of the authentication tag, in bytes.\n\n        It must be even and in the range **[4..16]**.\n        The recommended value (and the default, if not specified) is **16**.\n\n    *   **msg_len** : (*integer*) --\n        (Only ``MODE_CCM``). Length of the message to (de)cipher.\n        If not specified, ``encrypt`` must be called with the entire message.\n        Similarly, ``decrypt`` can only be called once.\n\n    *   **assoc_len** : (*integer*) --\n        (Only ``MODE_CCM``). Length of the associated data.\n        If not specified, all associated data is buffered internally,\n        which may represent a problem for very large messages.\n\n    *   **initial_value** : (*integer* or *bytes/bytearray/memoryview*) --\n        (Only ``MODE_CTR``).\n        The initial value for the counter. If not present, the cipher will\n        start counting from 0. The value is incremented by one for each block.\n        The counter number is encoded in big endian mode.\n\n    *   **counter** : (*object*) --\n        Instance of ``Crypto.Util.Counter``, which allows full customization\n        of the counter block. This parameter is incompatible to both ``nonce``\n        and ``initial_value``.\n\n    *   **use_aesni** : (*boolean*) --\n        Use Intel AES-NI hardware extensions (default: use if available).\n\n:Return: an AES object, of the applicable mode.", "short_docstring": "Create a new AES cipher."}
{"name": "nltk.word_tokenize", "type": "callable", "signature": "(text, language='english', preserve_line=False)", "docstring": "Return a tokenized copy of *text*,\nusing NLTK's recommended word tokenizer\n(currently an improved :class:`.TreebankWordTokenizer`\nalong with :class:`.PunktSentenceTokenizer`\nfor the specified language).\n\n:param text: text to split into words\n:type text: str\n:param language: the model name in the Punkt corpus\n:type language: str\n:param preserve_line: A flag to decide whether to sentence tokenize the text or not.\n:type preserve_line: bool", "short_docstring": "Return a tokenized copy of *text*,\nusing NLTK's recommended word tokenizer\n(currently an improved :class:`.TreebankWordTokenizer`\nalong with :class:`.PunktSentenceTokenizer`\nfor the specified language)."}
{"name": "numpy.linspace", "type": "callable", "signature": "(start, stop, num=50, endpoint=True, retstep=False, dtype=None, axis=0)", "docstring": "Return evenly spaced numbers over a specified interval.\n\nReturns `num` evenly spaced samples, calculated over the\ninterval [`start`, `stop`].\n\nThe endpoint of the interval can optionally be excluded.\n\n.. versionchanged:: 1.16.0\n    Non-scalar `start` and `stop` are now supported.\n\n.. versionchanged:: 1.20.0\n    Values are rounded towards ``-inf`` instead of ``0`` when an\n    integer ``dtype`` is specified. The old behavior can\n    still be obtained with ``np.linspace(start, stop, num).astype(int)``\n\nParameters\n----------\nstart : array_like\n    The starting value of the sequence.\nstop : array_like\n    The end value of the sequence, unless `endpoint` is set to False.\n    In that case, the sequence consists of all but the last of ``num + 1``\n    evenly spaced samples, so that `stop` is excluded.  Note that the step\n    size changes when `endpoint` is False.\nnum : int, optional\n    Number of samples to generate. Default is 50. Must be non-negative.\nendpoint : bool, optional\n    If True, `stop` is the last sample. Otherwise, it is not included.\n    Default is True.\nretstep : bool, optional\n    If True, return (`samples`, `step`), where `step` is the spacing\n    between samples.\ndtype : dtype, optional\n    The type of the output array.  If `dtype` is not given, the data type\n    is inferred from `start` and `stop`. The inferred dtype will never be\n    an integer; `float` is chosen even if the arguments would produce an\n    array of integers.\n\n    .. versionadded:: 1.9.0\n\naxis : int, optional\n    The axis in the result to store the samples.  Relevant only if start\n    or stop are array-like.  By default (0), the samples will be along a\n    new axis inserted at the beginning. Use -1 to get an axis at the end.\n\n    .. versionadded:: 1.16.0\n\nReturns\n-------\nsamples : ndarray\n    There are `num` equally spaced samples in the closed interval\n    ``[start, stop]`` or the half-open interval ``[start, stop)``\n    (depending on whether `endpoint` is True or False).\nstep : float, optional\n    Only returned if `retstep` is True\n\n    Size of spacing between samples.\n\n\nSee Also\n--------\narange : Similar to `linspace`, but uses a step size (instead of the\n         number of samples).\ngeomspace : Similar to `linspace`, but with numbers spaced evenly on a log\n            scale (a geometric progression).\nlogspace : Similar to `geomspace`, but with the end points specified as\n           logarithms.\n\nExamples\n--------\n>>> np.linspace(2.0, 3.0, num=5)\narray([2.  , 2.25, 2.5 , 2.75, 3.  ])\n>>> np.linspace(2.0, 3.0, num=5, endpoint=False)\narray([2. ,  2.2,  2.4,  2.6,  2.8])\n>>> np.linspace(2.0, 3.0, num=5, retstep=True)\n(array([2.  ,  2.25,  2.5 ,  2.75,  3.  ]), 0.25)\n\nGraphical illustration:\n\n>>> import matplotlib.pyplot as plt\n>>> N = 8\n>>> y = np.zeros(N)\n>>> x1 = np.linspace(0, 10, N, endpoint=True)\n>>> x2 = np.linspace(0, 10, N, endpoint=False)\n>>> plt.plot(x1, y, 'o')\n[<matplotlib.lines.Line2D object at 0x...>]\n>>> plt.plot(x2, y + 0.5, 'o')\n[<matplotlib.lines.Line2D object at 0x...>]\n>>> plt.ylim([-0.5, 1])\n(-0.5, 1)\n>>> plt.show()", "short_docstring": "Return evenly spaced numbers over a specified interval."}
{"name": "os.stat", "type": "callable", "signature": "(path, *, dir_fd=None, follow_symlinks=True)", "docstring": "Perform a stat system call on the given path.\n\n  path\n    Path to be examined; can be string, bytes, a path-like object or\n    open-file-descriptor int.\n  dir_fd\n    If not None, it should be a file descriptor open to a directory,\n    and path should be a relative string; path will then be relative to\n    that directory.\n  follow_symlinks\n    If False, and the last element of the path is a symbolic link,\n    stat will examine the symbolic link itself instead of the file\n    the link points to.\n\ndir_fd and follow_symlinks may not be implemented\n  on your platform.  If they are unavailable, using them will raise a\n  NotImplementedError.\n\nIt's an error to use dir_fd or follow_symlinks when specifying path as\n  an open file descriptor.", "short_docstring": "Perform a stat system call on the given path."}
{"name": "subprocess.Popen", "type": "class", "signature": "(args, bufsize=-1, executable=None, stdin=None, stdout=None, stderr=None, preexec_fn=None, close_fds=True, shell=False, cwd=None, env=None, universal_newlines=None, startupinfo=None, creationflags=0, restore_signals=True, start_new_session=False, pass_fds=(), *, user=None, group=None, extra_groups=None, encoding=None, errors=None, text=None, umask=-1, pipesize=-1)", "docstring": "Execute a child program in a new process.\n\nFor a complete description of the arguments see the Python documentation.\n\nArguments:\n  args: A string, or a sequence of program arguments.\n\n  bufsize: supplied as the buffering argument to the open() function when\n      creating the stdin/stdout/stderr pipe file objects\n\n  executable: A replacement program to execute.\n\n  stdin, stdout and stderr: These specify the executed programs' standard\n      input, standard output and standard error file handles, respectively.\n\n  preexec_fn: (POSIX only) An object to be called in the child process\n      just before the child is executed.\n\n  close_fds: Controls closing or inheriting of file descriptors.\n\n  shell: If true, the command will be executed through the shell.\n\n  cwd: Sets the current directory before the child is executed.\n\n  env: Defines the environment variables for the new process.\n\n  text: If true, decode stdin, stdout and stderr using the given encoding\n      (if set) or the system default otherwise.\n\n  universal_newlines: Alias of text, provided for backwards compatibility.\n\n  startupinfo and creationflags (Windows only)\n\n  restore_signals (POSIX only)\n\n  start_new_session (POSIX only)\n\n  group (POSIX only)\n\n  extra_groups (POSIX only)\n\n  user (POSIX only)\n\n  umask (POSIX only)\n\n  pass_fds (POSIX only)\n\n  encoding and errors: Text mode encoding and error handling to use for\n      file objects stdin, stdout and stderr.\n\nAttributes:\n    stdin, stdout, stderr, pid, returncode", "short_docstring": "Execute a child program in a new process."}
{"name": "codecs.decode", "type": "callable", "signature": "(obj, encoding='utf-8', errors='strict')", "docstring": "Decodes obj using the codec registered for encoding.\n\nDefault encoding is 'utf-8'.  errors may be given to set a\ndifferent error handling scheme.  Default is 'strict' meaning that encoding\nerrors raise a ValueError.  Other possible values are 'ignore', 'replace'\nand 'backslashreplace' as well as any other name registered with\ncodecs.register_error that can handle ValueErrors.", "short_docstring": "Decodes obj using the codec registered for encoding."}
