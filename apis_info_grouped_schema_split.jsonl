{"task_id": "BigCodeBench/13", "data": {"name": "ftplib.FTP", "type": "class", "signature": "(host='', user='', passwd='', acct='', timeout=<object object at 0x7fa57439cb10>, source_address=None, *, encoding='utf-8')", "description": "An FTP client class.\n\nTo create a connection, call the class using these arguments:\n        host, user, passwd, acct, timeout, source_address, encoding\n\nThe first four arguments are all strings, and have default value ''.\nThe parameter \u00b4timeout\u00b4 must be numeric and defaults to None if not\npassed, meaning that no timeout will be set on any ftp socket(s).\nIf a timeout is passed, then this is now the default timeout for all ftp\nsocket operations for this instance.\nThe last parameter is the encoding of filenames, which defaults to utf-8.\n\nThen use self.connect() with optional host and port argument.\n\nTo download a file, use ftp.retrlines('RETR ' + filename),\nor ftp.retrbinary() with slightly different arguments.\nTo upload a file, use ftp.storlines() or ftp.storbinary(),\nwhich have an open file as argument (see their definitions\nbelow for details).\nThe download/upload functions first issue appropriate TYPE\nand PORT or PASV commands.", "parameters": {"type": "object", "properties": {"host": {"type": "str", "default": ""}, "user": {"type": "str", "default": ""}, "passwd": {"type": "str", "default": ""}, "acct": {"type": "str", "default": ""}, "timeout": {"type": "str", "default": "<object object at 0x7fa57439cb10>"}, "source_address": {"type": "NoneType", "default": null}, "encoding": {"type": "str", "default": "utf-8"}}}}}
{"task_id": "BigCodeBench/13", "data": {"name": "ftplib.FTP.nlst", "type": "callable", "signature": "(self)", "description": "Return a list of files in a given directory (default the current).", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/13", "data": {"name": "ftplib.FTP.quit", "type": "callable", "signature": "(self)", "description": "Quit, and close the connection.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/13", "data": {"name": "ftplib.FTP.cwd", "type": "callable", "signature": "(self, dirname)", "description": "Change to a directory.", "parameters": {"type": "object", "properties": {"dirname": {}}}}}
{"task_id": "BigCodeBench/13", "data": {"name": "ftplib.FTP.login", "type": "callable", "signature": "(self, user='', passwd='')", "description": "Login, default anonymous.", "parameters": {"type": "object", "properties": {"user": {"type": "str", "default": ""}, "passwd": {"type": "str", "default": ""}}}}}
{"task_id": "BigCodeBench/13", "data": {"name": "downloaded_files.append(filename)"}}
{"task_id": "BigCodeBench/13", "data": {"name": "os.makedirs", "type": "callable", "signature": "(name)", "description": "makedirs(name [, mode=0o777][, exist_ok=False])\n\nSuper-mkdir; create a leaf directory and all intermediate ones.  Works like\nmkdir, except that any intermediate path segment (not just the rightmost)\nwill be created if it does not exist. If the target directory already\nexists, raise an OSError if exist_ok is False. Otherwise no exception is\nraised.  This is recursive.", "parameters": {"type": "object", "properties": {"name": {}}}}}
{"task_id": "BigCodeBench/13", "data": {"name": "os.path.exists", "type": "callable", "signature": "(path)", "description": "Test whether a path exists.  Returns False for broken symbolic links", "parameters": {"type": "object", "properties": {"path": {}}}}}
{"task_id": "BigCodeBench/13", "data": {"name": "subprocess.call", "type": "callable", "signature": "(*popenargs, **kwargs)", "description": "Run command with arguments.  Wait for command to complete or\ntimeout, then return the returncode attribute.\n\nThe arguments are the same as for the Popen constructor.  Example:\n\nretcode = call([\"ls\", \"-l\"])", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/15", "data": {"name": "csv.reader", "type": "callable", "signature": "(*args, **kwargs)", "description": "csv_reader = reader(iterable [, dialect='excel']\n                        [optional keyword args])\n    for row in csv_reader:\n        process(row)\n\nThe \"iterable\" argument can be any object that returns a line\nof input for each iteration, such as a file object or a list.  The\noptional \"dialect\" parameter is discussed below.  The function\nalso accepts optional keyword arguments which override settings\nprovided by the dialect.\n\nThe returned object is an iterator.  Each iteration returns a row\nof the CSV file (which can span multiple input lines).", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/15", "data": {"name": "open.write(f'\\nError executing command, exited with code {ret_code}')"}}
{"task_id": "BigCodeBench/15", "data": {"name": "os.makedirs", "type": "callable", "signature": "(name)", "description": "makedirs(name [, mode=0o777][, exist_ok=False])\n\nSuper-mkdir; create a leaf directory and all intermediate ones.  Works like\nmkdir, except that any intermediate path segment (not just the rightmost)\nwill be created if it does not exist. If the target directory already\nexists, raise an OSError if exist_ok is False. Otherwise no exception is\nraised.  This is recursive.", "parameters": {"type": "object", "properties": {"name": {}}}}}
{"task_id": "BigCodeBench/15", "data": {"name": "os.path.exists", "type": "callable", "signature": "(path)", "description": "Test whether a path exists.  Returns False for broken symbolic links", "parameters": {"type": "object", "properties": {"path": {}}}}}
{"task_id": "BigCodeBench/15", "data": {"name": "output_files.append(output_file)"}}
{"task_id": "BigCodeBench/15", "data": {"name": "subprocess.STDOUT", "type": "constant", "signature": null, "description": "int([x]) -> integer\nint(x, base=10) -> integer\n\nConvert a number or string to an integer, or return 0 if no arguments\nare given.  If x is a number, return x.__int__().  For floating point\nnumbers, this truncates towards zero.\n\nIf x is not a number or if base is given, then x must be a string,\nbytes, or bytearray instance representing an integer literal in the\ngiven base.  The literal can be preceded by '+' or '-' and be surrounded\nby whitespace.  The base defaults to 10.  Valid bases are 0 and 2-36.\nBase 0 means to interpret the base from the string as an integer literal.\n>>> int('0b100', base=0)\n4", "value": "-2", "parameters": {"type": "constant"}}}
{"task_id": "BigCodeBench/15", "data": {"name": "subprocess.call", "type": "callable", "signature": "(*popenargs, **kwargs)", "description": "Run command with arguments.  Wait for command to complete or\ntimeout, then return the returncode attribute.\n\nThe arguments are the same as for the Popen constructor.  Example:\n\nretcode = call([\"ls\", \"-l\"])", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/17", "data": {"name": "proc.name()"}}
{"task_id": "BigCodeBench/17", "data": {"name": "proc.terminate()"}}
{"task_id": "BigCodeBench/17", "data": {"name": "psutil.process_iter", "type": "callable", "signature": "()", "description": "Return a generator yielding a Process instance for all\nrunning processes.\n\nEvery new Process instance is only created once and then cached\ninto an internal table which is updated every time this is used.\n\nCached Process instances are checked for identity so that you're\nsafe in case a PID has been reused by another process, in which\ncase the cached instance is updated.\n\nThe sorting order in which processes are yielded is based on\ntheir PIDs.\n\n*attrs* and *ad_value* have the same meaning as in\nProcess.as_dict(). If *attrs* is specified as_dict() is called\nand the resulting dict is stored as a 'info' attribute attached\nto returned Process instance.\nIf *attrs* is an empty list it will retrieve all process info\n(slow).", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/17", "data": {"name": "time.sleep", "type": "callable", "signature": "(*args, **kwargs)", "description": "sleep(seconds)\n\nDelay execution for a given number of seconds.  The argument may be\na floating point number for subsecond precision.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/19", "data": {"name": "zipfile.ZipFile", "type": "class", "signature": "(file, mode='r', compression=0, allowZip64=True, compresslevel=None, *, strict_timestamps=True)", "description": "Class with methods to open, read, write, close, list zip files.\n\nz = ZipFile(file, mode=\"r\", compression=ZIP_STORED, allowZip64=True,\n            compresslevel=None)\n\nfile: Either the path to the file, or a file-like object.\n      If it is a path, the file will be opened and closed by ZipFile.\nmode: The mode can be either read 'r', write 'w', exclusive create 'x',\n      or append 'a'.\ncompression: ZIP_STORED (no compression), ZIP_DEFLATED (requires zlib),\n             ZIP_BZIP2 (requires bz2) or ZIP_LZMA (requires lzma).\nallowZip64: if True ZipFile will create files with ZIP64 extensions when\n            needed, otherwise it will raise an exception when this would\n            be necessary.\ncompresslevel: None (default for the given compression type) or an integer\n               specifying the level to pass to the compressor.\n               When using ZIP_STORED or ZIP_LZMA this keyword has no effect.\n               When using ZIP_DEFLATED integers 0 through 9 are accepted.\n               When using ZIP_BZIP2 integers 1 through 9 are accepted.", "parameters": {"type": "object", "properties": {"file": {}, "mode": {"type": "str", "default": "r"}, "compression": {"type": "integer", "default": 0}, "allowZip64": {"type": "bool", "default": true}, "compresslevel": {"type": "NoneType", "default": null}, "strict_timestamps": {"type": "bool", "default": true}}}}}
{"task_id": "BigCodeBench/19", "data": {"name": "zipfile.ZipFile.write", "type": "callable", "signature": "(self, filename, arcname=None)", "description": "Put the bytes from filename into the archive under the name\narcname.", "parameters": {"type": "object", "properties": {"filename": {}, "arcname": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/19", "data": {"name": "glob.glob", "type": "callable", "signature": "(pathname)", "description": "Return a list of paths matching a pathname pattern.\n\nThe pattern may contain simple shell-style wildcards a la\nfnmatch. However, unlike fnmatch, filenames starting with a\ndot are special cases that are not matched by '*' and '?'\npatterns.\n\nIf recursive is true, the pattern '**' will match any files and\nzero or more directories and subdirectories.", "parameters": {"type": "object", "properties": {"pathname": {}}}}}
{"task_id": "BigCodeBench/19", "data": {"name": "os.path.isfile", "type": "callable", "signature": "(path)", "description": "Test whether a path is a regular file", "parameters": {"type": "object", "properties": {"path": {}}}}}
{"task_id": "BigCodeBench/19", "data": {"name": "os.path.basename", "type": "callable", "signature": "(p)", "description": "Returns the final component of a pathname", "parameters": {"type": "object", "properties": {"p": {}}}}}
{"task_id": "BigCodeBench/19", "data": {"name": "os.path.exists", "type": "callable", "signature": "(path)", "description": "Test whether a path exists.  Returns False for broken symbolic links", "parameters": {"type": "object", "properties": {"path": {}}}}}
{"task_id": "BigCodeBench/19", "data": {"name": "os.path.join", "type": "callable", "signature": "(a, *p)", "description": "Join two or more pathname components, inserting '/' as needed.\nIf any component is an absolute path, all previous path components\nwill be discarded.  An empty last part will result in a path that\nends with a separator.", "parameters": {"type": "object", "properties": {"a": {}}}}}
{"task_id": "BigCodeBench/34", "data": {"name": "matplotlib.pyplot.axis", "type": "callable", "signature": "(arg: 'tuple[float)", "description": "Convenience method to get or set some axis properties.\n\nCall signatures::\n\n  xmin, xmax, ymin, ymax = axis()\n  xmin, xmax, ymin, ymax = axis([xmin, xmax, ymin, ymax])\n  xmin, xmax, ymin, ymax = axis(option)\n  xmin, xmax, ymin, ymax = axis(**kwargs)\n\nParameters\n----------\nxmin, xmax, ymin, ymax : float, optional\n    The axis limits to be set.  This can also be achieved using ::\n\n        ax.set(xlim=(xmin, xmax), ylim=(ymin, ymax))\n\noption : bool or str\n    If a bool, turns axis lines and labels on or off. If a string,\n    possible values are:\n\n    ================ ===========================================================\n    Value            Description\n    ================ ===========================================================\n    'off' or `False` Hide all axis decorations, i.e. axis labels, spines,\n                     tick marks, tick labels, and grid lines.\n                     This is the same as `~.Axes.set_axis_off()`.\n    'on' or `True`   Do not hide all axis decorations, i.e. axis labels, spines,\n                     tick marks, tick labels, and grid lines.\n                     This is the same as `~.Axes.set_axis_on()`.\n    'equal'          Set equal scaling (i.e., make circles circular) by\n                     changing the axis limits. This is the same as\n                     ``ax.set_aspect('equal', adjustable='datalim')``.\n                     Explicit data limits may not be respected in this case.\n    'scaled'         Set equal scaling (i.e., make circles circular) by\n                     changing dimensions of the plot box. This is the same as\n                     ``ax.set_aspect('equal', adjustable='box', anchor='C')``.\n                     Additionally, further autoscaling will be disabled.\n    'tight'          Set limits just large enough to show all data, then\n                     disable further autoscaling.\n    'auto'           Automatic scaling (fill plot box with data).\n    'image'          'scaled' with axis limits equal to data limits.\n    'square'         Square plot; similar to 'scaled', but initially forcing\n                     ``xmax-xmin == ymax-ymin``.\n    ================ ===========================================================\n\nemit : bool, default: True\n    Whether observers are notified of the axis limit change.\n    This option is passed on to `~.Axes.set_xlim` and\n    `~.Axes.set_ylim`.\n\nReturns\n-------\nxmin, xmax, ymin, ymax : float\n    The axis limits.\n\nSee Also\n--------\nmatplotlib.axes.Axes.set_xlim\nmatplotlib.axes.Axes.set_ylim\n\nNotes\n-----\nFor 3D axes, this method additionally takes *zmin*, *zmax* as\nparameters and likewise returns them.", "parameters": {"type": "object", "properties": {"arg": {"type": "tuple[float"}}}}}
{"task_id": "BigCodeBench/34", "data": {"name": "matplotlib.pyplot.imshow", "type": "callable", "signature": "(X: 'ArrayLike | PIL.Image.Image')", "description": "Display data as an image, i.e., on a 2D regular raster.\n\nThe input may either be actual RGB(A) data, or 2D scalar data, which\nwill be rendered as a pseudocolor image. For displaying a grayscale\nimage, set up the colormapping using the parameters\n``cmap='gray', vmin=0, vmax=255``.\n\nThe number of pixels used to render an image is set by the Axes size\nand the figure *dpi*. This can lead to aliasing artifacts when\nthe image is resampled, because the displayed image size will usually\nnot match the size of *X* (see\n:doc:`/gallery/images_contours_and_fields/image_antialiasing`).\nThe resampling can be controlled via the *interpolation* parameter\nand/or :rc:`image.interpolation`.\n\nParameters\n----------\nX : array-like or PIL image\n    The image data. Supported array shapes are:\n\n    - (M, N): an image with scalar data. The values are mapped to\n      colors using normalization and a colormap. See parameters *norm*,\n      *cmap*, *vmin*, *vmax*.\n    - (M, N, 3): an image with RGB values (0-1 float or 0-255 int).\n    - (M, N, 4): an image with RGBA values (0-1 float or 0-255 int),\n      i.e. including transparency.\n\n    The first two dimensions (M, N) define the rows and columns of\n    the image.\n\n    Out-of-range RGB(A) values are clipped.\n\ncmap : str or `~matplotlib.colors.Colormap`, default: :rc:`image.cmap`\n    The Colormap instance or registered colormap name used to map scalar data\n    to colors.\n\n    This parameter is ignored if *X* is RGB(A).\n\nnorm : str or `~matplotlib.colors.Normalize`, optional\n    The normalization method used to scale scalar data to the [0, 1] range\n    before mapping to colors using *cmap*. By default, a linear scaling is\n    used, mapping the lowest value to 0 and the highest to 1.\n\n    If given, this can be one of the following:\n\n    - An instance of `.Normalize` or one of its subclasses\n      (see :ref:`colormapnorms`).\n    - A scale name, i.e. one of \"linear\", \"log\", \"symlog\", \"logit\", etc.  For a\n      list of available scales, call `matplotlib.scale.get_scale_names()`.\n      In that case, a suitable `.Normalize` subclass is dynamically generated\n      and instantiated.\n\n    This parameter is ignored if *X* is RGB(A).\n\nvmin, vmax : float, optional\n    When using scalar data and no explicit *norm*, *vmin* and *vmax* define\n    the data range that the colormap covers. By default, the colormap covers\n    the complete value range of the supplied data. It is an error to use\n    *vmin*/*vmax* when a *norm* instance is given (but using a `str` *norm*\n    name together with *vmin*/*vmax* is acceptable).\n\n    This parameter is ignored if *X* is RGB(A).\n\naspect : {'equal', 'auto'} or float or None, default: None\n    The aspect ratio of the Axes.  This parameter is particularly\n    relevant for images since it determines whether data pixels are\n    square.\n\n    This parameter is a shortcut for explicitly calling\n    `.Axes.set_aspect`. See there for further details.\n\n    - 'equal': Ensures an aspect ratio of 1. Pixels will be square\n      (unless pixel sizes are explicitly made non-square in data\n      coordinates using *extent*).\n    - 'auto': The Axes is kept fixed and the aspect is adjusted so\n      that the data fit in the Axes. In general, this will result in\n      non-square pixels.\n\n    Normally, None (the default) means to use :rc:`image.aspect`.  However, if\n    the image uses a transform that does not contain the axes data transform,\n    then None means to not modify the axes aspect at all (in that case, directly\n    call `.Axes.set_aspect` if desired).\n\ninterpolation : str, default: :rc:`image.interpolation`\n    The interpolation method used.\n\n    Supported values are 'none', 'antialiased', 'nearest', 'bilinear',\n    'bicubic', 'spline16', 'spline36', 'hanning', 'hamming', 'hermite',\n    'kaiser', 'quadric', 'catrom', 'gaussian', 'bessel', 'mitchell',\n    'sinc', 'lanczos', 'blackman'.\n\n    The data *X* is resampled to the pixel size of the image on the\n    figure canvas, using the interpolation method to either up- or\n    downsample the data.\n\n    If *interpolation* is 'none', then for the ps, pdf, and svg\n    backends no down- or upsampling occurs, and the image data is\n    passed to the backend as a native image.  Note that different ps,\n    pdf, and svg viewers may display these raw pixels differently. On\n    other backends, 'none' is the same as 'nearest'.\n\n    If *interpolation* is the default 'antialiased', then 'nearest'\n    interpolation is used if the image is upsampled by more than a\n    factor of three (i.e. the number of display pixels is at least\n    three times the size of the data array).  If the upsampling rate is\n    smaller than 3, or the image is downsampled, then 'hanning'\n    interpolation is used to act as an anti-aliasing filter, unless the\n    image happens to be upsampled by exactly a factor of two or one.\n\n    See\n    :doc:`/gallery/images_contours_and_fields/interpolation_methods`\n    for an overview of the supported interpolation methods, and\n    :doc:`/gallery/images_contours_and_fields/image_antialiasing` for\n    a discussion of image antialiasing.\n\n    Some interpolation methods require an additional radius parameter,\n    which can be set by *filterrad*. Additionally, the antigrain image\n    resize filter is controlled by the parameter *filternorm*.\n\ninterpolation_stage : {'data', 'rgba'}, default: 'data'\n    If 'data', interpolation\n    is carried out on the data provided by the user.  If 'rgba', the\n    interpolation is carried out after the colormapping has been\n    applied (visual interpolation).\n\nalpha : float or array-like, optional\n    The alpha blending value, between 0 (transparent) and 1 (opaque).\n    If *alpha* is an array, the alpha blending values are applied pixel\n    by pixel, and *alpha* must have the same shape as *X*.\n\norigin : {'upper', 'lower'}, default: :rc:`image.origin`\n    Place the [0, 0] index of the array in the upper left or lower\n    left corner of the Axes. The convention (the default) 'upper' is\n    typically used for matrices and images.\n\n    Note that the vertical axis points upward for 'lower'\n    but downward for 'upper'.\n\n    See the :ref:`imshow_extent` tutorial for\n    examples and a more detailed description.\n\nextent : floats (left, right, bottom, top), optional\n    The bounding box in data coordinates that the image will fill.\n    These values may be unitful and match the units of the Axes.\n    The image is stretched individually along x and y to fill the box.\n\n    The default extent is determined by the following conditions.\n    Pixels have unit size in data coordinates. Their centers are on\n    integer coordinates, and their center coordinates range from 0 to\n    columns-1 horizontally and from 0 to rows-1 vertically.\n\n    Note that the direction of the vertical axis and thus the default\n    values for top and bottom depend on *origin*:\n\n    - For ``origin == 'upper'`` the default is\n      ``(-0.5, numcols-0.5, numrows-0.5, -0.5)``.\n    - For ``origin == 'lower'`` the default is\n      ``(-0.5, numcols-0.5, -0.5, numrows-0.5)``.\n\n    See the :ref:`imshow_extent` tutorial for\n    examples and a more detailed description.\n\nfilternorm : bool, default: True\n    A parameter for the antigrain image resize filter (see the\n    antigrain documentation).  If *filternorm* is set, the filter\n    normalizes integer values and corrects the rounding errors. It\n    doesn't do anything with the source floating point values, it\n    corrects only integers according to the rule of 1.0 which means\n    that any sum of pixel weights must be equal to 1.0.  So, the\n    filter function must produce a graph of the proper shape.\n\nfilterrad : float > 0, default: 4.0\n    The filter radius for filters that have a radius parameter, i.e.\n    when interpolation is one of: 'sinc', 'lanczos' or 'blackman'.\n\nresample : bool, default: :rc:`image.resample`\n    When *True*, use a full resampling method.  When *False*, only\n    resample when the output image is larger than the input image.\n\nurl : str, optional\n    Set the url of the created `.AxesImage`. See `.Artist.set_url`.\n\nReturns\n-------\n`~matplotlib.image.AxesImage`\n\nOther Parameters\n----------------\ndata : indexable object, optional\n    If given, all parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` (unless this raises an exception).\n\n**kwargs : `~matplotlib.artist.Artist` properties\n    These parameters are passed on to the constructor of the\n    `.AxesImage` artist.\n\nSee Also\n--------\nmatshow : Plot a matrix or an array as an image.\n\nNotes\n-----\nUnless *extent* is used, pixel centers will be located at integer\ncoordinates. In other words: the origin will coincide with the center\nof pixel (0, 0).\n\nThere are two common representations for RGB images with an alpha\nchannel:\n\n-   Straight (unassociated) alpha: R, G, and B channels represent the\n    color of the pixel, disregarding its opacity.\n-   Premultiplied (associated) alpha: R, G, and B channels represent\n    the color of the pixel, adjusted for its opacity by multiplication.\n\n`~matplotlib.pyplot.imshow` expects RGB images adopting the straight\n(unassociated) alpha representation.", "parameters": {"type": "object", "properties": {"X": {"type": ["arraylike", "pil.image.image"]}}}}}
{"task_id": "BigCodeBench/34", "data": {"name": "matplotlib.pyplot.figure", "type": "callable", "signature": "(figsize: 'tuple[float, **kwargs) -> 'Figure)", "description": "Create a new figure, or activate an existing figure.\n\nParameters\n----------\nnum : int or str or `.Figure` or `.SubFigure`, optional\n    A unique identifier for the figure.\n\n    If a figure with that identifier already exists, this figure is made\n    active and returned. An integer refers to the ``Figure.number``\n    attribute, a string refers to the figure label.\n\n    If there is no figure with the identifier or *num* is not given, a new\n    figure is created, made active and returned.  If *num* is an int, it\n    will be used for the ``Figure.number`` attribute, otherwise, an\n    auto-generated integer value is used (starting at 1 and incremented\n    for each new figure). If *num* is a string, the figure label and the\n    window title is set to this value.  If num is a ``SubFigure``, its\n    parent ``Figure`` is activated.\n\nfigsize : (float, float), default: :rc:`figure.figsize`\n    Width, height in inches.\n\ndpi : float, default: :rc:`figure.dpi`\n    The resolution of the figure in dots-per-inch.\n\nfacecolor : color, default: :rc:`figure.facecolor`\n    The background color.\n\nedgecolor : color, default: :rc:`figure.edgecolor`\n    The border color.\n\nframeon : bool, default: True\n    If False, suppress drawing the figure frame.\n\nFigureClass : subclass of `~matplotlib.figure.Figure`\n    If set, an instance of this subclass will be created, rather than a\n    plain `.Figure`.\n\nclear : bool, default: False\n    If True and the figure already exists, then it is cleared.\n\nlayout : {'constrained', 'compressed', 'tight', 'none', `.LayoutEngine`, None}, default: None\n    The layout mechanism for positioning of plot elements to avoid\n    overlapping Axes decorations (labels, ticks, etc). Note that layout\n    managers can measurably slow down figure display.\n\n    - 'constrained': The constrained layout solver adjusts axes sizes\n      to avoid overlapping axes decorations.  Can handle complex plot\n      layouts and colorbars, and is thus recommended.\n\n      See :ref:`constrainedlayout_guide`\n      for examples.\n\n    - 'compressed': uses the same algorithm as 'constrained', but\n      removes extra space between fixed-aspect-ratio Axes.  Best for\n      simple grids of axes.\n\n    - 'tight': Use the tight layout mechanism. This is a relatively\n      simple algorithm that adjusts the subplot parameters so that\n      decorations do not overlap. See `.Figure.set_tight_layout` for\n      further details.\n\n    - 'none': Do not use a layout engine.\n\n    - A `.LayoutEngine` instance. Builtin layout classes are\n      `.ConstrainedLayoutEngine` and `.TightLayoutEngine`, more easily\n      accessible by 'constrained' and 'tight'.  Passing an instance\n      allows third parties to provide their own layout engine.\n\n    If not given, fall back to using the parameters *tight_layout* and\n    *constrained_layout*, including their config defaults\n    :rc:`figure.autolayout` and :rc:`figure.constrained_layout.use`.\n\n**kwargs\n    Additional keyword arguments are passed to the `.Figure` constructor.\n\nReturns\n-------\n`~matplotlib.figure.Figure`\n\nNotes\n-----\nA newly created figure is passed to the `~.FigureCanvasBase.new_manager`\nmethod or the `new_figure_manager` function provided by the current\nbackend, which install a canvas and a manager on the figure.\n\nOnce this is done, :rc:`figure.hooks` are called, one at a time, on the\nfigure; these hooks allow arbitrary customization of the figure (e.g.,\nattaching callbacks) or of associated elements (e.g., modifying the\ntoolbar).  See :doc:`/gallery/user_interfaces/mplcvd` for an example of\ntoolbar customization.\n\nIf you are creating many figures, make sure you explicitly call\n`.pyplot.close` on the figures you are not using, because this will\nenable pyplot to properly clean up the memory.\n\n`~matplotlib.rcParams` defines the default values, which can be modified\nin the matplotlibrc file.", "parameters": {"type": "object", "properties": {"figsize": {"type": "tuple[float"}}}}}
{"task_id": "BigCodeBench/34", "data": {"name": "re.sub", "type": "callable", "signature": "(pattern, repl, string)", "description": "Return the string obtained by replacing the leftmost\nnon-overlapping occurrences of the pattern in string by the\nreplacement repl.  repl can be either a string or a callable;\nif a string, backslash escapes in it are processed.  If it is\na callable, it's passed the Match object and must return\na replacement string to be used.", "parameters": {"type": "object", "properties": {"pattern": {}, "repl": {}, "string": {}}}}}
{"task_id": "BigCodeBench/34", "data": {"name": "re.sub.strip", "type": "callable", "signature": "()", "description": "Return the string obtained by replacing the leftmost\nnon-overlapping occurrences of the pattern in string by the\nreplacement repl.  repl can be either a string or a callable;\nif a string, backslash escapes in it are processed.  If it is\na callable, it's passed the Match object and must return\na replacement string to be used.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/37", "data": {"name": "pandas.Series.sort_values", "type": "callable", "signature": "(self, *, axis: 'Axis' = 0, ascending: 'bool | Sequence[bool]' = True, inplace: 'bool' = False, kind: 'SortKind' = 'quicksort', na_position: 'NaPosition' = 'last', ignore_index: 'bool' = False, key: 'ValueKeyFunc | None' = None) -> 'Series | None'", "description": "Sort by the values.\n\nSort a Series in ascending or descending order by some\ncriterion.\n\nParameters\n----------\naxis : {0 or 'index'}\n    Unused. Parameter needed for compatibility with DataFrame.\nascending : bool or list of bools, default True\n    If True, sort values in ascending order, otherwise descending.\ninplace : bool, default False\n    If True, perform operation in-place.\nkind : {'quicksort', 'mergesort', 'heapsort', 'stable'}, default 'quicksort'\n    Choice of sorting algorithm. See also :func:`numpy.sort` for more\n    information. 'mergesort' and 'stable' are the only stable  algorithms.\nna_position : {'first' or 'last'}, default 'last'\n    Argument 'first' puts NaNs at the beginning, 'last' puts NaNs at\n    the end.\nignore_index : bool, default False\n    If True, the resulting axis will be labeled 0, 1, \u2026, n - 1.\nkey : callable, optional\n    If not None, apply the key function to the series values\n    before sorting. This is similar to the `key` argument in the\n    builtin :meth:`sorted` function, with the notable difference that\n    this `key` function should be *vectorized*. It should expect a\n    ``Series`` and return an array-like.\n\nReturns\n-------\nSeries or None\n    Series ordered by values or None if ``inplace=True``.\n\nSee Also\n--------\nSeries.sort_index : Sort by the Series indices.\nDataFrame.sort_values : Sort DataFrame by the values along either axis.\nDataFrame.sort_index : Sort DataFrame by indices.\n\nExamples\n--------\n>>> s = pd.Series([np.nan, 1, 3, 10, 5])\n>>> s\n0     NaN\n1     1.0\n2     3.0\n3     10.0\n4     5.0\ndtype: float64\n\nSort values ascending order (default behaviour)\n\n>>> s.sort_values(ascending=True)\n1     1.0\n2     3.0\n4     5.0\n3    10.0\n0     NaN\ndtype: float64\n\nSort values descending order\n\n>>> s.sort_values(ascending=False)\n3    10.0\n4     5.0\n2     3.0\n1     1.0\n0     NaN\ndtype: float64\n\nSort values putting NAs first\n\n>>> s.sort_values(na_position='first')\n0     NaN\n1     1.0\n2     3.0\n4     5.0\n3    10.0\ndtype: float64\n\nSort a series of strings\n\n>>> s = pd.Series(['z', 'b', 'd', 'a', 'c'])\n>>> s\n0    z\n1    b\n2    d\n3    a\n4    c\ndtype: object\n\n>>> s.sort_values()\n3    a\n1    b\n4    c\n2    d\n0    z\ndtype: object\n\nSort using a key function. Your `key` function will be\ngiven the ``Series`` of values and should return an array-like.\n\n>>> s = pd.Series(['a', 'B', 'c', 'D', 'e'])\n>>> s.sort_values()\n1    B\n3    D\n0    a\n2    c\n4    e\ndtype: object\n>>> s.sort_values(key=lambda x: x.str.lower())\n0    a\n1    B\n2    c\n3    D\n4    e\ndtype: object\n\nNumPy ufuncs work well here. For example, we can\nsort by the ``sin`` of the value\n\n>>> s = pd.Series([-4, -2, 0, 2, 4])\n>>> s.sort_values(key=np.sin)\n1   -2\n4    4\n2    0\n0   -4\n3    2\ndtype: int64\n\nMore complicated user-defined functions can be used,\nas long as they expect a Series and return an array-like\n\n>>> s.sort_values(key=lambda x: (np.tan(x.cumsum())))\n0   -4\n3    2\n4    4\n1   -2\n2    0\ndtype: int64", "parameters": {"type": "object", "properties": {"axis": {"type": "axis", "default": 0}, "ascending": {"type": ["bool", "sequence[bool]"], "default": true}, "inplace": {"type": "bool", "default": false}, "kind": {"type": "sortkind", "default": "quicksort"}, "na_position": {"type": "naposition", "default": "last"}, "ignore_index": {"type": "bool", "default": false}, "key": {"type": ["valuekeyfunc", "null"], "default": null}}}}}
{"task_id": "BigCodeBench/37", "data": {"name": "pandas.Series.sort_values.index", "type": "callable", "signature": "(self, *, axis: 'Axis' = 0, ascending: 'bool | Sequence[bool]' = True, inplace: 'bool' = False, kind: 'SortKind' = 'quicksort', na_position: 'NaPosition' = 'last', ignore_index: 'bool' = False, key: 'ValueKeyFunc | None' = None) -> 'Series | None'", "description": "Sort by the values.\n\nSort a Series in ascending or descending order by some\ncriterion.\n\nParameters\n----------\naxis : {0 or 'index'}\n    Unused. Parameter needed for compatibility with DataFrame.\nascending : bool or list of bools, default True\n    If True, sort values in ascending order, otherwise descending.\ninplace : bool, default False\n    If True, perform operation in-place.\nkind : {'quicksort', 'mergesort', 'heapsort', 'stable'}, default 'quicksort'\n    Choice of sorting algorithm. See also :func:`numpy.sort` for more\n    information. 'mergesort' and 'stable' are the only stable  algorithms.\nna_position : {'first' or 'last'}, default 'last'\n    Argument 'first' puts NaNs at the beginning, 'last' puts NaNs at\n    the end.\nignore_index : bool, default False\n    If True, the resulting axis will be labeled 0, 1, \u2026, n - 1.\nkey : callable, optional\n    If not None, apply the key function to the series values\n    before sorting. This is similar to the `key` argument in the\n    builtin :meth:`sorted` function, with the notable difference that\n    this `key` function should be *vectorized*. It should expect a\n    ``Series`` and return an array-like.\n\nReturns\n-------\nSeries or None\n    Series ordered by values or None if ``inplace=True``.\n\nSee Also\n--------\nSeries.sort_index : Sort by the Series indices.\nDataFrame.sort_values : Sort DataFrame by the values along either axis.\nDataFrame.sort_index : Sort DataFrame by indices.\n\nExamples\n--------\n>>> s = pd.Series([np.nan, 1, 3, 10, 5])\n>>> s\n0     NaN\n1     1.0\n2     3.0\n3     10.0\n4     5.0\ndtype: float64\n\nSort values ascending order (default behaviour)\n\n>>> s.sort_values(ascending=True)\n1     1.0\n2     3.0\n4     5.0\n3    10.0\n0     NaN\ndtype: float64\n\nSort values descending order\n\n>>> s.sort_values(ascending=False)\n3    10.0\n4     5.0\n2     3.0\n1     1.0\n0     NaN\ndtype: float64\n\nSort values putting NAs first\n\n>>> s.sort_values(na_position='first')\n0     NaN\n1     1.0\n2     3.0\n4     5.0\n3    10.0\ndtype: float64\n\nSort a series of strings\n\n>>> s = pd.Series(['z', 'b', 'd', 'a', 'c'])\n>>> s\n0    z\n1    b\n2    d\n3    a\n4    c\ndtype: object\n\n>>> s.sort_values()\n3    a\n1    b\n4    c\n2    d\n0    z\ndtype: object\n\nSort using a key function. Your `key` function will be\ngiven the ``Series`` of values and should return an array-like.\n\n>>> s = pd.Series(['a', 'B', 'c', 'D', 'e'])\n>>> s.sort_values()\n1    B\n3    D\n0    a\n2    c\n4    e\ndtype: object\n>>> s.sort_values(key=lambda x: x.str.lower())\n0    a\n1    B\n2    c\n3    D\n4    e\ndtype: object\n\nNumPy ufuncs work well here. For example, we can\nsort by the ``sin`` of the value\n\n>>> s = pd.Series([-4, -2, 0, 2, 4])\n>>> s.sort_values(key=np.sin)\n1   -2\n4    4\n2    0\n0   -4\n3    2\ndtype: int64\n\nMore complicated user-defined functions can be used,\nas long as they expect a Series and return an array-like\n\n>>> s.sort_values(key=lambda x: (np.tan(x.cumsum())))\n0   -4\n3    2\n4    4\n1   -2\n2    0\ndtype: int64", "parameters": {"type": "object", "properties": {"axis": {"type": "axis", "default": 0}, "ascending": {"type": ["bool", "sequence[bool]"], "default": true}, "inplace": {"type": "bool", "default": false}, "kind": {"type": "sortkind", "default": "quicksort"}, "na_position": {"type": "naposition", "default": "last"}, "ignore_index": {"type": "bool", "default": false}, "key": {"type": ["valuekeyfunc", "null"], "default": null}}}}}
{"task_id": "BigCodeBench/37", "data": {"name": "RandomForestClassifier(random_state=42).fit(X, y).feature_importances_"}}
{"task_id": "BigCodeBench/37", "data": {"name": "df.drop(target_column, axis=1)"}}
{"task_id": "BigCodeBench/37", "data": {"name": "df.drop(target_column, axis=1).columns"}}
{"task_id": "BigCodeBench/37", "data": {"name": "matplotlib.pyplot.figure", "type": "callable", "signature": "(figsize: 'tuple[float, **kwargs) -> 'Figure)", "description": "Create a new figure, or activate an existing figure.\n\nParameters\n----------\nnum : int or str or `.Figure` or `.SubFigure`, optional\n    A unique identifier for the figure.\n\n    If a figure with that identifier already exists, this figure is made\n    active and returned. An integer refers to the ``Figure.number``\n    attribute, a string refers to the figure label.\n\n    If there is no figure with the identifier or *num* is not given, a new\n    figure is created, made active and returned.  If *num* is an int, it\n    will be used for the ``Figure.number`` attribute, otherwise, an\n    auto-generated integer value is used (starting at 1 and incremented\n    for each new figure). If *num* is a string, the figure label and the\n    window title is set to this value.  If num is a ``SubFigure``, its\n    parent ``Figure`` is activated.\n\nfigsize : (float, float), default: :rc:`figure.figsize`\n    Width, height in inches.\n\ndpi : float, default: :rc:`figure.dpi`\n    The resolution of the figure in dots-per-inch.\n\nfacecolor : color, default: :rc:`figure.facecolor`\n    The background color.\n\nedgecolor : color, default: :rc:`figure.edgecolor`\n    The border color.\n\nframeon : bool, default: True\n    If False, suppress drawing the figure frame.\n\nFigureClass : subclass of `~matplotlib.figure.Figure`\n    If set, an instance of this subclass will be created, rather than a\n    plain `.Figure`.\n\nclear : bool, default: False\n    If True and the figure already exists, then it is cleared.\n\nlayout : {'constrained', 'compressed', 'tight', 'none', `.LayoutEngine`, None}, default: None\n    The layout mechanism for positioning of plot elements to avoid\n    overlapping Axes decorations (labels, ticks, etc). Note that layout\n    managers can measurably slow down figure display.\n\n    - 'constrained': The constrained layout solver adjusts axes sizes\n      to avoid overlapping axes decorations.  Can handle complex plot\n      layouts and colorbars, and is thus recommended.\n\n      See :ref:`constrainedlayout_guide`\n      for examples.\n\n    - 'compressed': uses the same algorithm as 'constrained', but\n      removes extra space between fixed-aspect-ratio Axes.  Best for\n      simple grids of axes.\n\n    - 'tight': Use the tight layout mechanism. This is a relatively\n      simple algorithm that adjusts the subplot parameters so that\n      decorations do not overlap. See `.Figure.set_tight_layout` for\n      further details.\n\n    - 'none': Do not use a layout engine.\n\n    - A `.LayoutEngine` instance. Builtin layout classes are\n      `.ConstrainedLayoutEngine` and `.TightLayoutEngine`, more easily\n      accessible by 'constrained' and 'tight'.  Passing an instance\n      allows third parties to provide their own layout engine.\n\n    If not given, fall back to using the parameters *tight_layout* and\n    *constrained_layout*, including their config defaults\n    :rc:`figure.autolayout` and :rc:`figure.constrained_layout.use`.\n\n**kwargs\n    Additional keyword arguments are passed to the `.Figure` constructor.\n\nReturns\n-------\n`~matplotlib.figure.Figure`\n\nNotes\n-----\nA newly created figure is passed to the `~.FigureCanvasBase.new_manager`\nmethod or the `new_figure_manager` function provided by the current\nbackend, which install a canvas and a manager on the figure.\n\nOnce this is done, :rc:`figure.hooks` are called, one at a time, on the\nfigure; these hooks allow arbitrary customization of the figure (e.g.,\nattaching callbacks) or of associated elements (e.g., modifying the\ntoolbar).  See :doc:`/gallery/user_interfaces/mplcvd` for an example of\ntoolbar customization.\n\nIf you are creating many figures, make sure you explicitly call\n`.pyplot.close` on the figures you are not using, because this will\nenable pyplot to properly clean up the memory.\n\n`~matplotlib.rcParams` defines the default values, which can be modified\nin the matplotlibrc file.", "parameters": {"type": "object", "properties": {"figsize": {"type": "tuple[float"}}}}}
{"task_id": "BigCodeBench/37", "data": {"name": "seaborn.barplot", "type": "callable", "signature": "(x=None, y=None, **kwargs)", "description": "Show point estimates and errors as rectangular bars.\n\nA bar plot represents an aggregate or statistical estimate for a numeric\nvariable with the height of each rectangle and indicates the uncertainty\naround that estimate using an error bar. Bar plots include 0 in the\naxis range, and they are a good choice when 0 is a meaningful value\nfor the variable to take.\n\nSee the :ref:`tutorial <categorical_tutorial>` for more information.\n\n.. note::\n    By default, this function treats one of the variables as categorical\n    and draws data at ordinal positions (0, 1, ... n) on the relevant axis.\n    As of version 0.13.0, this can be disabled by setting `native_scale=True`.\n\n\nParameters\n----------\ndata : DataFrame, Series, dict, array, or list of arrays\n    Dataset for plotting. If `x` and `y` are absent, this is\n    interpreted as wide-form. Otherwise it is expected to be long-form.    \nx, y, hue : names of variables in `data` or vector data\n    Inputs for plotting long-form data. See examples for interpretation.    \norder, hue_order : lists of strings\n    Order to plot the categorical levels in; otherwise the levels are\n    inferred from the data objects.    \nestimator : string or callable that maps vector -> scalar\n    Statistical function to estimate within each categorical bin.\nerrorbar : string, (string, number) tuple, callable or None\n    Name of errorbar method (either \"ci\", \"pi\", \"se\", or \"sd\"), or a tuple\n    with a method name and a level parameter, or a function that maps from a\n    vector to a (min, max) interval, or None to hide errorbar. See the\n    :doc:`errorbar tutorial </tutorial/error_bars>` for more information.\n\n    .. versionadded:: v0.12.0\nn_boot : int\n    Number of bootstrap samples used to compute confidence intervals.\nseed : int, `numpy.random.Generator`, or `numpy.random.RandomState`\n    Seed or random number generator for reproducible bootstrapping.\nunits : name of variable in `data` or vector data\n    Identifier of sampling units; used by the errorbar function to\n    perform a multilevel bootstrap and account for repeated measures\nweights : name of variable in `data` or vector data\n    Data values or column used to compute weighted statistics.\n    Note that the use of weights may limit other statistical options.\n\n    .. versionadded:: v0.13.1    \norient : \"v\" | \"h\" | \"x\" | \"y\"\n    Orientation of the plot (vertical or horizontal). This is usually\n    inferred based on the type of the input variables, but it can be used\n    to resolve ambiguity when both `x` and `y` are numeric or when\n    plotting wide-form data.\n\n    .. versionchanged:: v0.13.0\n        Added 'x'/'y' as options, equivalent to 'v'/'h'.    \ncolor : matplotlib color\n    Single color for the elements in the plot.    \npalette : palette name, list, or dict\n    Colors to use for the different levels of the ``hue`` variable. Should\n    be something that can be interpreted by :func:`color_palette`, or a\n    dictionary mapping hue levels to matplotlib colors.    \nsaturation : float\n    Proportion of the original saturation to draw fill colors in. Large\n    patches often look better with desaturated colors, but set this to\n    `1` if you want the colors to perfectly match the input values.    \nfill : bool\n    If True, use a solid patch. Otherwise, draw as line art.\n\n    .. versionadded:: v0.13.0    \nhue_norm : tuple or :class:`matplotlib.colors.Normalize` object\n    Normalization in data units for colormap applied to the `hue`\n    variable when it is numeric. Not relevant if `hue` is categorical.\n\n    .. versionadded:: v0.12.0    \nwidth : float\n    Width allotted to each element on the orient axis. When `native_scale=True`,\n    it is relative to the minimum distance between two values in the native scale.    \ndodge : \"auto\" or bool\n    When hue mapping is used, whether elements should be narrowed and shifted along\n    the orient axis to eliminate overlap. If `\"auto\"`, set to `True` when the\n    orient variable is crossed with the categorical variable or `False` otherwise.\n\n    .. versionchanged:: 0.13.0\n\n        Added `\"auto\"` mode as a new default.    \ngap : float\n    Shrink on the orient axis by this factor to add a gap between dodged elements.\n\n    .. versionadded:: 0.13.0    \nlog_scale : bool or number, or pair of bools or numbers\n    Set axis scale(s) to log. A single value sets the data axis for any numeric\n    axes in the plot. A pair of values sets each axis independently.\n    Numeric values are interpreted as the desired base (default 10).\n    When `None` or `False`, seaborn defers to the existing Axes scale.\n\n    .. versionadded:: v0.13.0    \nnative_scale : bool\n    When True, numeric or datetime values on the categorical axis will maintain\n    their original scaling rather than being converted to fixed indices.\n\n    .. versionadded:: v0.13.0    \nformatter : callable\n    Function for converting categorical data into strings. Affects both grouping\n    and tick labels.\n\n    .. versionadded:: v0.13.0    \nlegend : \"auto\", \"brief\", \"full\", or False\n    How to draw the legend. If \"brief\", numeric `hue` and `size`\n    variables will be represented with a sample of evenly spaced values.\n    If \"full\", every group will get an entry in the legend. If \"auto\",\n    choose between brief or full representation based on number of levels.\n    If `False`, no legend data is added and no legend is drawn.\n\n    .. versionadded:: v0.13.0    \ncapsize : float\n    Width of the \"caps\" on error bars, relative to bar spacing.    \nerr_kws : dict\n    Parameters of :class:`matplotlib.lines.Line2D`, for the error bar artists.\n\n    .. versionadded:: v0.13.0    \nci : float\n    Level of the confidence interval to show, in [0, 100].\n\n    .. deprecated:: v0.12.0\n        Use `errorbar=(\"ci\", ...)`.    \nerrcolor : matplotlib color\n    Color used for the error bar lines.\n\n    .. deprecated:: 0.13.0\n        Use `err_kws={'color': ...}`.    \nerrwidth : float\n    Thickness of error bar lines (and caps), in points.\n\n    .. deprecated:: 0.13.0\n        Use `err_kws={'linewidth': ...}`.    \nax : matplotlib Axes\n    Axes object to draw the plot onto, otherwise uses the current Axes.    \nkwargs : key, value mappings\n    Other parameters are passed through to :class:`matplotlib.patches.Rectangle`.\n\nReturns\n-------\nax : matplotlib Axes\n    Returns the Axes object with the plot drawn onto it.    \n\nSee Also\n--------\ncountplot : Show the counts of observations in each categorical bin.    \npointplot : Show point estimates and confidence intervals using dots.    \ncatplot : Combine a categorical plot with a :class:`FacetGrid`.    \n\nNotes\n-----\n\nFor datasets where 0 is not a meaningful value, a :func:`pointplot` will\nallow you to focus on differences between levels of one or more categorical\nvariables.\n\nIt is also important to keep in mind that a bar plot shows only the mean (or\nother aggregate) value, but it is often more informative to show the\ndistribution of values at each level of the categorical variables. In those\ncases, approaches such as a :func:`boxplot` or :func:`violinplot` may be\nmore appropriate.\n\nExamples\n--------\n.. include:: ../docstrings/barplot.rst", "parameters": {"type": "object", "properties": {"x": {"type": "NoneType", "default": null}, "y": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/37", "data": {"name": "seaborn.barplot.set_ylabel", "type": "callable", "signature": "(data=None)", "description": "Show point estimates and errors as rectangular bars.\n\nA bar plot represents an aggregate or statistical estimate for a numeric\nvariable with the height of each rectangle and indicates the uncertainty\naround that estimate using an error bar. Bar plots include 0 in the\naxis range, and they are a good choice when 0 is a meaningful value\nfor the variable to take.\n\nSee the :ref:`tutorial <categorical_tutorial>` for more information.\n\n.. note::\n    By default, this function treats one of the variables as categorical\n    and draws data at ordinal positions (0, 1, ... n) on the relevant axis.\n    As of version 0.13.0, this can be disabled by setting `native_scale=True`.\n\n\nParameters\n----------\ndata : DataFrame, Series, dict, array, or list of arrays\n    Dataset for plotting. If `x` and `y` are absent, this is\n    interpreted as wide-form. Otherwise it is expected to be long-form.    \nx, y, hue : names of variables in `data` or vector data\n    Inputs for plotting long-form data. See examples for interpretation.    \norder, hue_order : lists of strings\n    Order to plot the categorical levels in; otherwise the levels are\n    inferred from the data objects.    \nestimator : string or callable that maps vector -> scalar\n    Statistical function to estimate within each categorical bin.\nerrorbar : string, (string, number) tuple, callable or None\n    Name of errorbar method (either \"ci\", \"pi\", \"se\", or \"sd\"), or a tuple\n    with a method name and a level parameter, or a function that maps from a\n    vector to a (min, max) interval, or None to hide errorbar. See the\n    :doc:`errorbar tutorial </tutorial/error_bars>` for more information.\n\n    .. versionadded:: v0.12.0\nn_boot : int\n    Number of bootstrap samples used to compute confidence intervals.\nseed : int, `numpy.random.Generator`, or `numpy.random.RandomState`\n    Seed or random number generator for reproducible bootstrapping.\nunits : name of variable in `data` or vector data\n    Identifier of sampling units; used by the errorbar function to\n    perform a multilevel bootstrap and account for repeated measures\nweights : name of variable in `data` or vector data\n    Data values or column used to compute weighted statistics.\n    Note that the use of weights may limit other statistical options.\n\n    .. versionadded:: v0.13.1    \norient : \"v\" | \"h\" | \"x\" | \"y\"\n    Orientation of the plot (vertical or horizontal). This is usually\n    inferred based on the type of the input variables, but it can be used\n    to resolve ambiguity when both `x` and `y` are numeric or when\n    plotting wide-form data.\n\n    .. versionchanged:: v0.13.0\n        Added 'x'/'y' as options, equivalent to 'v'/'h'.    \ncolor : matplotlib color\n    Single color for the elements in the plot.    \npalette : palette name, list, or dict\n    Colors to use for the different levels of the ``hue`` variable. Should\n    be something that can be interpreted by :func:`color_palette`, or a\n    dictionary mapping hue levels to matplotlib colors.    \nsaturation : float\n    Proportion of the original saturation to draw fill colors in. Large\n    patches often look better with desaturated colors, but set this to\n    `1` if you want the colors to perfectly match the input values.    \nfill : bool\n    If True, use a solid patch. Otherwise, draw as line art.\n\n    .. versionadded:: v0.13.0    \nhue_norm : tuple or :class:`matplotlib.colors.Normalize` object\n    Normalization in data units for colormap applied to the `hue`\n    variable when it is numeric. Not relevant if `hue` is categorical.\n\n    .. versionadded:: v0.12.0    \nwidth : float\n    Width allotted to each element on the orient axis. When `native_scale=True`,\n    it is relative to the minimum distance between two values in the native scale.    \ndodge : \"auto\" or bool\n    When hue mapping is used, whether elements should be narrowed and shifted along\n    the orient axis to eliminate overlap. If `\"auto\"`, set to `True` when the\n    orient variable is crossed with the categorical variable or `False` otherwise.\n\n    .. versionchanged:: 0.13.0\n\n        Added `\"auto\"` mode as a new default.    \ngap : float\n    Shrink on the orient axis by this factor to add a gap between dodged elements.\n\n    .. versionadded:: 0.13.0    \nlog_scale : bool or number, or pair of bools or numbers\n    Set axis scale(s) to log. A single value sets the data axis for any numeric\n    axes in the plot. A pair of values sets each axis independently.\n    Numeric values are interpreted as the desired base (default 10).\n    When `None` or `False`, seaborn defers to the existing Axes scale.\n\n    .. versionadded:: v0.13.0    \nnative_scale : bool\n    When True, numeric or datetime values on the categorical axis will maintain\n    their original scaling rather than being converted to fixed indices.\n\n    .. versionadded:: v0.13.0    \nformatter : callable\n    Function for converting categorical data into strings. Affects both grouping\n    and tick labels.\n\n    .. versionadded:: v0.13.0    \nlegend : \"auto\", \"brief\", \"full\", or False\n    How to draw the legend. If \"brief\", numeric `hue` and `size`\n    variables will be represented with a sample of evenly spaced values.\n    If \"full\", every group will get an entry in the legend. If \"auto\",\n    choose between brief or full representation based on number of levels.\n    If `False`, no legend data is added and no legend is drawn.\n\n    .. versionadded:: v0.13.0    \ncapsize : float\n    Width of the \"caps\" on error bars, relative to bar spacing.    \nerr_kws : dict\n    Parameters of :class:`matplotlib.lines.Line2D`, for the error bar artists.\n\n    .. versionadded:: v0.13.0    \nci : float\n    Level of the confidence interval to show, in [0, 100].\n\n    .. deprecated:: v0.12.0\n        Use `errorbar=(\"ci\", ...)`.    \nerrcolor : matplotlib color\n    Color used for the error bar lines.\n\n    .. deprecated:: 0.13.0\n        Use `err_kws={'color': ...}`.    \nerrwidth : float\n    Thickness of error bar lines (and caps), in points.\n\n    .. deprecated:: 0.13.0\n        Use `err_kws={'linewidth': ...}`.    \nax : matplotlib Axes\n    Axes object to draw the plot onto, otherwise uses the current Axes.    \nkwargs : key, value mappings\n    Other parameters are passed through to :class:`matplotlib.patches.Rectangle`.\n\nReturns\n-------\nax : matplotlib Axes\n    Returns the Axes object with the plot drawn onto it.    \n\nSee Also\n--------\ncountplot : Show the counts of observations in each categorical bin.    \npointplot : Show point estimates and confidence intervals using dots.    \ncatplot : Combine a categorical plot with a :class:`FacetGrid`.    \n\nNotes\n-----\n\nFor datasets where 0 is not a meaningful value, a :func:`pointplot` will\nallow you to focus on differences between levels of one or more categorical\nvariables.\n\nIt is also important to keep in mind that a bar plot shows only the mean (or\nother aggregate) value, but it is often more informative to show the\ndistribution of values at each level of the categorical variables. In those\ncases, approaches such as a :func:`boxplot` or :func:`violinplot` may be\nmore appropriate.\n\nExamples\n--------\n.. include:: ../docstrings/barplot.rst", "parameters": {"type": "object", "properties": {"data": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/37", "data": {"name": "seaborn.barplot.set_xlabel", "type": "callable", "signature": "(data=None)", "description": "Show point estimates and errors as rectangular bars.\n\nA bar plot represents an aggregate or statistical estimate for a numeric\nvariable with the height of each rectangle and indicates the uncertainty\naround that estimate using an error bar. Bar plots include 0 in the\naxis range, and they are a good choice when 0 is a meaningful value\nfor the variable to take.\n\nSee the :ref:`tutorial <categorical_tutorial>` for more information.\n\n.. note::\n    By default, this function treats one of the variables as categorical\n    and draws data at ordinal positions (0, 1, ... n) on the relevant axis.\n    As of version 0.13.0, this can be disabled by setting `native_scale=True`.\n\n\nParameters\n----------\ndata : DataFrame, Series, dict, array, or list of arrays\n    Dataset for plotting. If `x` and `y` are absent, this is\n    interpreted as wide-form. Otherwise it is expected to be long-form.    \nx, y, hue : names of variables in `data` or vector data\n    Inputs for plotting long-form data. See examples for interpretation.    \norder, hue_order : lists of strings\n    Order to plot the categorical levels in; otherwise the levels are\n    inferred from the data objects.    \nestimator : string or callable that maps vector -> scalar\n    Statistical function to estimate within each categorical bin.\nerrorbar : string, (string, number) tuple, callable or None\n    Name of errorbar method (either \"ci\", \"pi\", \"se\", or \"sd\"), or a tuple\n    with a method name and a level parameter, or a function that maps from a\n    vector to a (min, max) interval, or None to hide errorbar. See the\n    :doc:`errorbar tutorial </tutorial/error_bars>` for more information.\n\n    .. versionadded:: v0.12.0\nn_boot : int\n    Number of bootstrap samples used to compute confidence intervals.\nseed : int, `numpy.random.Generator`, or `numpy.random.RandomState`\n    Seed or random number generator for reproducible bootstrapping.\nunits : name of variable in `data` or vector data\n    Identifier of sampling units; used by the errorbar function to\n    perform a multilevel bootstrap and account for repeated measures\nweights : name of variable in `data` or vector data\n    Data values or column used to compute weighted statistics.\n    Note that the use of weights may limit other statistical options.\n\n    .. versionadded:: v0.13.1    \norient : \"v\" | \"h\" | \"x\" | \"y\"\n    Orientation of the plot (vertical or horizontal). This is usually\n    inferred based on the type of the input variables, but it can be used\n    to resolve ambiguity when both `x` and `y` are numeric or when\n    plotting wide-form data.\n\n    .. versionchanged:: v0.13.0\n        Added 'x'/'y' as options, equivalent to 'v'/'h'.    \ncolor : matplotlib color\n    Single color for the elements in the plot.    \npalette : palette name, list, or dict\n    Colors to use for the different levels of the ``hue`` variable. Should\n    be something that can be interpreted by :func:`color_palette`, or a\n    dictionary mapping hue levels to matplotlib colors.    \nsaturation : float\n    Proportion of the original saturation to draw fill colors in. Large\n    patches often look better with desaturated colors, but set this to\n    `1` if you want the colors to perfectly match the input values.    \nfill : bool\n    If True, use a solid patch. Otherwise, draw as line art.\n\n    .. versionadded:: v0.13.0    \nhue_norm : tuple or :class:`matplotlib.colors.Normalize` object\n    Normalization in data units for colormap applied to the `hue`\n    variable when it is numeric. Not relevant if `hue` is categorical.\n\n    .. versionadded:: v0.12.0    \nwidth : float\n    Width allotted to each element on the orient axis. When `native_scale=True`,\n    it is relative to the minimum distance between two values in the native scale.    \ndodge : \"auto\" or bool\n    When hue mapping is used, whether elements should be narrowed and shifted along\n    the orient axis to eliminate overlap. If `\"auto\"`, set to `True` when the\n    orient variable is crossed with the categorical variable or `False` otherwise.\n\n    .. versionchanged:: 0.13.0\n\n        Added `\"auto\"` mode as a new default.    \ngap : float\n    Shrink on the orient axis by this factor to add a gap between dodged elements.\n\n    .. versionadded:: 0.13.0    \nlog_scale : bool or number, or pair of bools or numbers\n    Set axis scale(s) to log. A single value sets the data axis for any numeric\n    axes in the plot. A pair of values sets each axis independently.\n    Numeric values are interpreted as the desired base (default 10).\n    When `None` or `False`, seaborn defers to the existing Axes scale.\n\n    .. versionadded:: v0.13.0    \nnative_scale : bool\n    When True, numeric or datetime values on the categorical axis will maintain\n    their original scaling rather than being converted to fixed indices.\n\n    .. versionadded:: v0.13.0    \nformatter : callable\n    Function for converting categorical data into strings. Affects both grouping\n    and tick labels.\n\n    .. versionadded:: v0.13.0    \nlegend : \"auto\", \"brief\", \"full\", or False\n    How to draw the legend. If \"brief\", numeric `hue` and `size`\n    variables will be represented with a sample of evenly spaced values.\n    If \"full\", every group will get an entry in the legend. If \"auto\",\n    choose between brief or full representation based on number of levels.\n    If `False`, no legend data is added and no legend is drawn.\n\n    .. versionadded:: v0.13.0    \ncapsize : float\n    Width of the \"caps\" on error bars, relative to bar spacing.    \nerr_kws : dict\n    Parameters of :class:`matplotlib.lines.Line2D`, for the error bar artists.\n\n    .. versionadded:: v0.13.0    \nci : float\n    Level of the confidence interval to show, in [0, 100].\n\n    .. deprecated:: v0.12.0\n        Use `errorbar=(\"ci\", ...)`.    \nerrcolor : matplotlib color\n    Color used for the error bar lines.\n\n    .. deprecated:: 0.13.0\n        Use `err_kws={'color': ...}`.    \nerrwidth : float\n    Thickness of error bar lines (and caps), in points.\n\n    .. deprecated:: 0.13.0\n        Use `err_kws={'linewidth': ...}`.    \nax : matplotlib Axes\n    Axes object to draw the plot onto, otherwise uses the current Axes.    \nkwargs : key, value mappings\n    Other parameters are passed through to :class:`matplotlib.patches.Rectangle`.\n\nReturns\n-------\nax : matplotlib Axes\n    Returns the Axes object with the plot drawn onto it.    \n\nSee Also\n--------\ncountplot : Show the counts of observations in each categorical bin.    \npointplot : Show point estimates and confidence intervals using dots.    \ncatplot : Combine a categorical plot with a :class:`FacetGrid`.    \n\nNotes\n-----\n\nFor datasets where 0 is not a meaningful value, a :func:`pointplot` will\nallow you to focus on differences between levels of one or more categorical\nvariables.\n\nIt is also important to keep in mind that a bar plot shows only the mean (or\nother aggregate) value, but it is often more informative to show the\ndistribution of values at each level of the categorical variables. In those\ncases, approaches such as a :func:`boxplot` or :func:`violinplot` may be\nmore appropriate.\n\nExamples\n--------\n.. include:: ../docstrings/barplot.rst", "parameters": {"type": "object", "properties": {"data": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/37", "data": {"name": "seaborn.barplot.set_title", "type": "callable", "signature": "(data=None)", "description": "Show point estimates and errors as rectangular bars.\n\nA bar plot represents an aggregate or statistical estimate for a numeric\nvariable with the height of each rectangle and indicates the uncertainty\naround that estimate using an error bar. Bar plots include 0 in the\naxis range, and they are a good choice when 0 is a meaningful value\nfor the variable to take.\n\nSee the :ref:`tutorial <categorical_tutorial>` for more information.\n\n.. note::\n    By default, this function treats one of the variables as categorical\n    and draws data at ordinal positions (0, 1, ... n) on the relevant axis.\n    As of version 0.13.0, this can be disabled by setting `native_scale=True`.\n\n\nParameters\n----------\ndata : DataFrame, Series, dict, array, or list of arrays\n    Dataset for plotting. If `x` and `y` are absent, this is\n    interpreted as wide-form. Otherwise it is expected to be long-form.    \nx, y, hue : names of variables in `data` or vector data\n    Inputs for plotting long-form data. See examples for interpretation.    \norder, hue_order : lists of strings\n    Order to plot the categorical levels in; otherwise the levels are\n    inferred from the data objects.    \nestimator : string or callable that maps vector -> scalar\n    Statistical function to estimate within each categorical bin.\nerrorbar : string, (string, number) tuple, callable or None\n    Name of errorbar method (either \"ci\", \"pi\", \"se\", or \"sd\"), or a tuple\n    with a method name and a level parameter, or a function that maps from a\n    vector to a (min, max) interval, or None to hide errorbar. See the\n    :doc:`errorbar tutorial </tutorial/error_bars>` for more information.\n\n    .. versionadded:: v0.12.0\nn_boot : int\n    Number of bootstrap samples used to compute confidence intervals.\nseed : int, `numpy.random.Generator`, or `numpy.random.RandomState`\n    Seed or random number generator for reproducible bootstrapping.\nunits : name of variable in `data` or vector data\n    Identifier of sampling units; used by the errorbar function to\n    perform a multilevel bootstrap and account for repeated measures\nweights : name of variable in `data` or vector data\n    Data values or column used to compute weighted statistics.\n    Note that the use of weights may limit other statistical options.\n\n    .. versionadded:: v0.13.1    \norient : \"v\" | \"h\" | \"x\" | \"y\"\n    Orientation of the plot (vertical or horizontal). This is usually\n    inferred based on the type of the input variables, but it can be used\n    to resolve ambiguity when both `x` and `y` are numeric or when\n    plotting wide-form data.\n\n    .. versionchanged:: v0.13.0\n        Added 'x'/'y' as options, equivalent to 'v'/'h'.    \ncolor : matplotlib color\n    Single color for the elements in the plot.    \npalette : palette name, list, or dict\n    Colors to use for the different levels of the ``hue`` variable. Should\n    be something that can be interpreted by :func:`color_palette`, or a\n    dictionary mapping hue levels to matplotlib colors.    \nsaturation : float\n    Proportion of the original saturation to draw fill colors in. Large\n    patches often look better with desaturated colors, but set this to\n    `1` if you want the colors to perfectly match the input values.    \nfill : bool\n    If True, use a solid patch. Otherwise, draw as line art.\n\n    .. versionadded:: v0.13.0    \nhue_norm : tuple or :class:`matplotlib.colors.Normalize` object\n    Normalization in data units for colormap applied to the `hue`\n    variable when it is numeric. Not relevant if `hue` is categorical.\n\n    .. versionadded:: v0.12.0    \nwidth : float\n    Width allotted to each element on the orient axis. When `native_scale=True`,\n    it is relative to the minimum distance between two values in the native scale.    \ndodge : \"auto\" or bool\n    When hue mapping is used, whether elements should be narrowed and shifted along\n    the orient axis to eliminate overlap. If `\"auto\"`, set to `True` when the\n    orient variable is crossed with the categorical variable or `False` otherwise.\n\n    .. versionchanged:: 0.13.0\n\n        Added `\"auto\"` mode as a new default.    \ngap : float\n    Shrink on the orient axis by this factor to add a gap between dodged elements.\n\n    .. versionadded:: 0.13.0    \nlog_scale : bool or number, or pair of bools or numbers\n    Set axis scale(s) to log. A single value sets the data axis for any numeric\n    axes in the plot. A pair of values sets each axis independently.\n    Numeric values are interpreted as the desired base (default 10).\n    When `None` or `False`, seaborn defers to the existing Axes scale.\n\n    .. versionadded:: v0.13.0    \nnative_scale : bool\n    When True, numeric or datetime values on the categorical axis will maintain\n    their original scaling rather than being converted to fixed indices.\n\n    .. versionadded:: v0.13.0    \nformatter : callable\n    Function for converting categorical data into strings. Affects both grouping\n    and tick labels.\n\n    .. versionadded:: v0.13.0    \nlegend : \"auto\", \"brief\", \"full\", or False\n    How to draw the legend. If \"brief\", numeric `hue` and `size`\n    variables will be represented with a sample of evenly spaced values.\n    If \"full\", every group will get an entry in the legend. If \"auto\",\n    choose between brief or full representation based on number of levels.\n    If `False`, no legend data is added and no legend is drawn.\n\n    .. versionadded:: v0.13.0    \ncapsize : float\n    Width of the \"caps\" on error bars, relative to bar spacing.    \nerr_kws : dict\n    Parameters of :class:`matplotlib.lines.Line2D`, for the error bar artists.\n\n    .. versionadded:: v0.13.0    \nci : float\n    Level of the confidence interval to show, in [0, 100].\n\n    .. deprecated:: v0.12.0\n        Use `errorbar=(\"ci\", ...)`.    \nerrcolor : matplotlib color\n    Color used for the error bar lines.\n\n    .. deprecated:: 0.13.0\n        Use `err_kws={'color': ...}`.    \nerrwidth : float\n    Thickness of error bar lines (and caps), in points.\n\n    .. deprecated:: 0.13.0\n        Use `err_kws={'linewidth': ...}`.    \nax : matplotlib Axes\n    Axes object to draw the plot onto, otherwise uses the current Axes.    \nkwargs : key, value mappings\n    Other parameters are passed through to :class:`matplotlib.patches.Rectangle`.\n\nReturns\n-------\nax : matplotlib Axes\n    Returns the Axes object with the plot drawn onto it.    \n\nSee Also\n--------\ncountplot : Show the counts of observations in each categorical bin.    \npointplot : Show point estimates and confidence intervals using dots.    \ncatplot : Combine a categorical plot with a :class:`FacetGrid`.    \n\nNotes\n-----\n\nFor datasets where 0 is not a meaningful value, a :func:`pointplot` will\nallow you to focus on differences between levels of one or more categorical\nvariables.\n\nIt is also important to keep in mind that a bar plot shows only the mean (or\nother aggregate) value, but it is often more informative to show the\ndistribution of values at each level of the categorical variables. In those\ncases, approaches such as a :func:`boxplot` or :func:`violinplot` may be\nmore appropriate.\n\nExamples\n--------\n.. include:: ../docstrings/barplot.rst", "parameters": {"type": "object", "properties": {"data": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/37", "data": {"name": "sklearn.ensemble.RandomForestClassifier", "type": "class", "signature": "(random_state=None)", "description": "A random forest classifier.\n\nA random forest is a meta estimator that fits a number of decision tree\nclassifiers on various sub-samples of the dataset and uses averaging to\nimprove the predictive accuracy and control over-fitting.\nTrees in the forest use the best split strategy, i.e. equivalent to passing\n`splitter=\"best\"` to the underlying :class:`~sklearn.tree.DecisionTreeRegressor`.\nThe sub-sample size is controlled with the `max_samples` parameter if\n`bootstrap=True` (default), otherwise the whole dataset is used to build\neach tree.\n\nFor a comparison between tree-based ensemble models see the example\n:ref:`sphx_glr_auto_examples_ensemble_plot_forest_hist_grad_boosting_comparison.py`.\n\nRead more in the :ref:`User Guide <forest>`.\n\nParameters\n----------\nn_estimators : int, default=100\n    The number of trees in the forest.\n\n    .. versionchanged:: 0.22\n       The default value of ``n_estimators`` changed from 10 to 100\n       in 0.22.\n\ncriterion : {\"gini\", \"entropy\", \"log_loss\"}, default=\"gini\"\n    The function to measure the quality of a split. Supported criteria are\n    \"gini\" for the Gini impurity and \"log_loss\" and \"entropy\" both for the\n    Shannon information gain, see :ref:`tree_mathematical_formulation`.\n    Note: This parameter is tree-specific.\n\nmax_depth : int, default=None\n    The maximum depth of the tree. If None, then nodes are expanded until\n    all leaves are pure or until all leaves contain less than\n    min_samples_split samples.\n\nmin_samples_split : int or float, default=2\n    The minimum number of samples required to split an internal node:\n\n    - If int, then consider `min_samples_split` as the minimum number.\n    - If float, then `min_samples_split` is a fraction and\n      `ceil(min_samples_split * n_samples)` are the minimum\n      number of samples for each split.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_samples_leaf : int or float, default=1\n    The minimum number of samples required to be at a leaf node.\n    A split point at any depth will only be considered if it leaves at\n    least ``min_samples_leaf`` training samples in each of the left and\n    right branches.  This may have the effect of smoothing the model,\n    especially in regression.\n\n    - If int, then consider `min_samples_leaf` as the minimum number.\n    - If float, then `min_samples_leaf` is a fraction and\n      `ceil(min_samples_leaf * n_samples)` are the minimum\n      number of samples for each node.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_weight_fraction_leaf : float, default=0.0\n    The minimum weighted fraction of the sum total of weights (of all\n    the input samples) required to be at a leaf node. Samples have\n    equal weight when sample_weight is not provided.\n\nmax_features : {\"sqrt\", \"log2\", None}, int or float, default=\"sqrt\"\n    The number of features to consider when looking for the best split:\n\n    - If int, then consider `max_features` features at each split.\n    - If float, then `max_features` is a fraction and\n      `max(1, int(max_features * n_features_in_))` features are considered at each\n      split.\n    - If \"sqrt\", then `max_features=sqrt(n_features)`.\n    - If \"log2\", then `max_features=log2(n_features)`.\n    - If None, then `max_features=n_features`.\n\n    .. versionchanged:: 1.1\n        The default of `max_features` changed from `\"auto\"` to `\"sqrt\"`.\n\n    Note: the search for a split does not stop until at least one\n    valid partition of the node samples is found, even if it requires to\n    effectively inspect more than ``max_features`` features.\n\nmax_leaf_nodes : int, default=None\n    Grow trees with ``max_leaf_nodes`` in best-first fashion.\n    Best nodes are defined as relative reduction in impurity.\n    If None then unlimited number of leaf nodes.\n\nmin_impurity_decrease : float, default=0.0\n    A node will be split if this split induces a decrease of the impurity\n    greater than or equal to this value.\n\n    The weighted impurity decrease equation is the following::\n\n        N_t / N * (impurity - N_t_R / N_t * right_impurity\n                            - N_t_L / N_t * left_impurity)\n\n    where ``N`` is the total number of samples, ``N_t`` is the number of\n    samples at the current node, ``N_t_L`` is the number of samples in the\n    left child, and ``N_t_R`` is the number of samples in the right child.\n\n    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n    if ``sample_weight`` is passed.\n\n    .. versionadded:: 0.19\n\nbootstrap : bool, default=True\n    Whether bootstrap samples are used when building trees. If False, the\n    whole dataset is used to build each tree.\n\noob_score : bool or callable, default=False\n    Whether to use out-of-bag samples to estimate the generalization score.\n    By default, :func:`~sklearn.metrics.accuracy_score` is used.\n    Provide a callable with signature `metric(y_true, y_pred)` to use a\n    custom metric. Only available if `bootstrap=True`.\n\nn_jobs : int, default=None\n    The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n    :meth:`decision_path` and :meth:`apply` are all parallelized over the\n    trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n    context. ``-1`` means using all processors. See :term:`Glossary\n    <n_jobs>` for more details.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls both the randomness of the bootstrapping of the samples used\n    when building trees (if ``bootstrap=True``) and the sampling of the\n    features to consider when looking for the best split at each node\n    (if ``max_features < n_features``).\n    See :term:`Glossary <random_state>` for details.\n\nverbose : int, default=0\n    Controls the verbosity when fitting and predicting.\n\nwarm_start : bool, default=False\n    When set to ``True``, reuse the solution of the previous call to fit\n    and add more estimators to the ensemble, otherwise, just fit a whole\n    new forest. See :term:`Glossary <warm_start>` and\n    :ref:`gradient_boosting_warm_start` for details.\n\nclass_weight : {\"balanced\", \"balanced_subsample\"}, dict or list of dicts,             default=None\n    Weights associated with classes in the form ``{class_label: weight}``.\n    If not given, all classes are supposed to have weight one. For\n    multi-output problems, a list of dicts can be provided in the same\n    order as the columns of y.\n\n    Note that for multioutput (including multilabel) weights should be\n    defined for each class of every column in its own dict. For example,\n    for four-class multilabel classification weights should be\n    [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n    [{1:1}, {2:5}, {3:1}, {4:1}].\n\n    The \"balanced\" mode uses the values of y to automatically adjust\n    weights inversely proportional to class frequencies in the input data\n    as ``n_samples / (n_classes * np.bincount(y))``\n\n    The \"balanced_subsample\" mode is the same as \"balanced\" except that\n    weights are computed based on the bootstrap sample for every tree\n    grown.\n\n    For multi-output, the weights of each column of y will be multiplied.\n\n    Note that these weights will be multiplied with sample_weight (passed\n    through the fit method) if sample_weight is specified.\n\nccp_alpha : non-negative float, default=0.0\n    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n    subtree with the largest cost complexity that is smaller than\n    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n    :ref:`minimal_cost_complexity_pruning` for details.\n\n    .. versionadded:: 0.22\n\nmax_samples : int or float, default=None\n    If bootstrap is True, the number of samples to draw from X\n    to train each base estimator.\n\n    - If None (default), then draw `X.shape[0]` samples.\n    - If int, then draw `max_samples` samples.\n    - If float, then draw `max(round(n_samples * max_samples), 1)` samples. Thus,\n      `max_samples` should be in the interval `(0.0, 1.0]`.\n\n    .. versionadded:: 0.22\n\nmonotonic_cst : array-like of int of shape (n_features), default=None\n    Indicates the monotonicity constraint to enforce on each feature.\n      - 1: monotonic increase\n      - 0: no constraint\n      - -1: monotonic decrease\n\n    If monotonic_cst is None, no constraints are applied.\n\n    Monotonicity constraints are not supported for:\n      - multiclass classifications (i.e. when `n_classes > 2`),\n      - multioutput classifications (i.e. when `n_outputs_ > 1`),\n      - classifications trained on data with missing values.\n\n    The constraints hold over the probability of the positive class.\n\n    Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.\n\n    .. versionadded:: 1.4\n\nAttributes\n----------\nestimator_ : :class:`~sklearn.tree.DecisionTreeClassifier`\n    The child estimator template used to create the collection of fitted\n    sub-estimators.\n\n    .. versionadded:: 1.2\n       `base_estimator_` was renamed to `estimator_`.\n\nestimators_ : list of DecisionTreeClassifier\n    The collection of fitted sub-estimators.\n\nclasses_ : ndarray of shape (n_classes,) or a list of such arrays\n    The classes labels (single output problem), or a list of arrays of\n    class labels (multi-output problem).\n\nn_classes_ : int or list\n    The number of classes (single output problem), or a list containing the\n    number of classes for each output (multi-output problem).\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nn_outputs_ : int\n    The number of outputs when ``fit`` is performed.\n\nfeature_importances_ : ndarray of shape (n_features,)\n    The impurity-based feature importances.\n    The higher, the more important the feature.\n    The importance of a feature is computed as the (normalized)\n    total reduction of the criterion brought by that feature.  It is also\n    known as the Gini importance.\n\n    Warning: impurity-based feature importances can be misleading for\n    high cardinality features (many unique values). See\n    :func:`sklearn.inspection.permutation_importance` as an alternative.\n\noob_score_ : float\n    Score of the training dataset obtained using an out-of-bag estimate.\n    This attribute exists only when ``oob_score`` is True.\n\noob_decision_function_ : ndarray of shape (n_samples, n_classes) or             (n_samples, n_classes, n_outputs)\n    Decision function computed with out-of-bag estimate on the training\n    set. If n_estimators is small it might be possible that a data point\n    was never left out during the bootstrap. In this case,\n    `oob_decision_function_` might contain NaN. This attribute exists\n    only when ``oob_score`` is True.\n\nestimators_samples_ : list of arrays\n    The subset of drawn samples (i.e., the in-bag samples) for each base\n    estimator. Each subset is defined by an array of the indices selected.\n\n    .. versionadded:: 1.4\n\nSee Also\n--------\nsklearn.tree.DecisionTreeClassifier : A decision tree classifier.\nsklearn.ensemble.ExtraTreesClassifier : Ensemble of extremely randomized\n    tree classifiers.\nsklearn.ensemble.HistGradientBoostingClassifier : A Histogram-based Gradient\n    Boosting Classification Tree, very fast for big datasets (n_samples >=\n    10_000).\n\nNotes\n-----\nThe default values for the parameters controlling the size of the trees\n(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\nunpruned trees which can potentially be very large on some data sets. To\nreduce memory consumption, the complexity and size of the trees should be\ncontrolled by setting those parameter values.\n\nThe features are always randomly permuted at each split. Therefore,\nthe best found split may vary, even with the same training data,\n``max_features=n_features`` and ``bootstrap=False``, if the improvement\nof the criterion is identical for several splits enumerated during the\nsearch of the best split. To obtain a deterministic behaviour during\nfitting, ``random_state`` has to be fixed.\n\nReferences\n----------\n.. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n\nExamples\n--------\n>>> from sklearn.ensemble import RandomForestClassifier\n>>> from sklearn.datasets import make_classification\n>>> X, y = make_classification(n_samples=1000, n_features=4,\n...                            n_informative=2, n_redundant=0,\n...                            random_state=0, shuffle=False)\n>>> clf = RandomForestClassifier(max_depth=2, random_state=0)\n>>> clf.fit(X, y)\nRandomForestClassifier(...)\n>>> print(clf.predict([[0, 0, 0, 0]]))\n[1]", "parameters": {"type": "object", "properties": {"random_state": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/82", "data": {"name": "flask.Flask", "type": "class", "signature": "(import_name: 'str', static_url_path: 'str | None' = None, static_folder: 'str | os.PathLike[str] | None' = 'static', static_host: 'str | None' = None, host_matching: 'bool' = False, subdomain_matching: 'bool' = False, template_folder: 'str | os.PathLike[str] | None' = 'templates', instance_path: 'str | None' = None, instance_relative_config: 'bool' = False, root_path: 'str | None' = None)", "description": "The flask object implements a WSGI application and acts as the central\nobject.  It is passed the name of the module or package of the\napplication.  Once it is created it will act as a central registry for\nthe view functions, the URL rules, template configuration and much more.\n\nThe name of the package is used to resolve resources from inside the\npackage or the folder the module is contained in depending on if the\npackage parameter resolves to an actual python package (a folder with\nan :file:`__init__.py` file inside) or a standard module (just a ``.py`` file).\n\nFor more information about resource loading, see :func:`open_resource`.\n\nUsually you create a :class:`Flask` instance in your main module or\nin the :file:`__init__.py` file of your package like this::\n\n    from flask import Flask\n    app = Flask(__name__)\n\n.. admonition:: About the First Parameter\n\n    The idea of the first parameter is to give Flask an idea of what\n    belongs to your application.  This name is used to find resources\n    on the filesystem, can be used by extensions to improve debugging\n    information and a lot more.\n\n    So it's important what you provide there.  If you are using a single\n    module, `__name__` is always the correct value.  If you however are\n    using a package, it's usually recommended to hardcode the name of\n    your package there.\n\n    For example if your application is defined in :file:`yourapplication/app.py`\n    you should create it with one of the two versions below::\n\n        app = Flask('yourapplication')\n        app = Flask(__name__.split('.')[0])\n\n    Why is that?  The application will work even with `__name__`, thanks\n    to how resources are looked up.  However it will make debugging more\n    painful.  Certain extensions can make assumptions based on the\n    import name of your application.  For example the Flask-SQLAlchemy\n    extension will look for the code in your application that triggered\n    an SQL query in debug mode.  If the import name is not properly set\n    up, that debugging information is lost.  (For example it would only\n    pick up SQL queries in `yourapplication.app` and not\n    `yourapplication.views.frontend`)\n\n.. versionadded:: 0.7\n   The `static_url_path`, `static_folder`, and `template_folder`\n   parameters were added.\n\n.. versionadded:: 0.8\n   The `instance_path` and `instance_relative_config` parameters were\n   added.\n\n.. versionadded:: 0.11\n   The `root_path` parameter was added.\n\n.. versionadded:: 1.0\n   The ``host_matching`` and ``static_host`` parameters were added.\n\n.. versionadded:: 1.0\n   The ``subdomain_matching`` parameter was added. Subdomain\n   matching needs to be enabled manually now. Setting\n   :data:`SERVER_NAME` does not implicitly enable it.\n\n:param import_name: the name of the application package\n:param static_url_path: can be used to specify a different path for the\n                        static files on the web.  Defaults to the name\n                        of the `static_folder` folder.\n:param static_folder: The folder with static files that is served at\n    ``static_url_path``. Relative to the application ``root_path``\n    or an absolute path. Defaults to ``'static'``.\n:param static_host: the host to use when adding the static route.\n    Defaults to None. Required when using ``host_matching=True``\n    with a ``static_folder`` configured.\n:param host_matching: set ``url_map.host_matching`` attribute.\n    Defaults to False.\n:param subdomain_matching: consider the subdomain relative to\n    :data:`SERVER_NAME` when matching routes. Defaults to False.\n:param template_folder: the folder that contains the templates that should\n                        be used by the application.  Defaults to\n                        ``'templates'`` folder in the root path of the\n                        application.\n:param instance_path: An alternative instance path for the application.\n                      By default the folder ``'instance'`` next to the\n                      package or module is assumed to be the instance\n                      path.\n:param instance_relative_config: if set to ``True`` relative filenames\n                                 for loading the config are assumed to\n                                 be relative to the instance path instead\n                                 of the application root.\n:param root_path: The path to the root of the application files.\n    This should only be set manually when it can't be detected\n    automatically, such as for namespace packages.", "parameters": {"type": "object", "properties": {"import_name": {"type": "str"}, "static_url_path": {"type": ["str", "null"], "default": null}, "static_folder": {"type": ["null", "str", "string"], "default": "static"}, "static_host": {"type": ["str", "null"], "default": null}, "host_matching": {"type": "bool", "default": false}, "subdomain_matching": {"type": "bool", "default": false}, "template_folder": {"type": ["null", "str", "string"], "default": "templates"}, "instance_path": {"type": ["str", "null"], "default": null}, "instance_relative_config": {"type": "bool", "default": false}, "root_path": {"type": ["str", "null"], "default": null}}}}}
{"task_id": "BigCodeBench/82", "data": {"name": "flask.Flask.config", "type": "class", "signature": "(import_name: 'str', static_url_path: 'str | None' = None, static_folder: 'str | os.PathLike[str] | None' = 'static', static_host: 'str | None' = None, host_matching: 'bool' = False, subdomain_matching: 'bool' = False, template_folder: 'str | os.PathLike[str] | None' = 'templates', instance_path: 'str | None' = None, instance_relative_config: 'bool' = False, root_path: 'str | None' = None)", "description": "The flask object implements a WSGI application and acts as the central\nobject.  It is passed the name of the module or package of the\napplication.  Once it is created it will act as a central registry for\nthe view functions, the URL rules, template configuration and much more.\n\nThe name of the package is used to resolve resources from inside the\npackage or the folder the module is contained in depending on if the\npackage parameter resolves to an actual python package (a folder with\nan :file:`__init__.py` file inside) or a standard module (just a ``.py`` file).\n\nFor more information about resource loading, see :func:`open_resource`.\n\nUsually you create a :class:`Flask` instance in your main module or\nin the :file:`__init__.py` file of your package like this::\n\n    from flask import Flask\n    app = Flask(__name__)\n\n.. admonition:: About the First Parameter\n\n    The idea of the first parameter is to give Flask an idea of what\n    belongs to your application.  This name is used to find resources\n    on the filesystem, can be used by extensions to improve debugging\n    information and a lot more.\n\n    So it's important what you provide there.  If you are using a single\n    module, `__name__` is always the correct value.  If you however are\n    using a package, it's usually recommended to hardcode the name of\n    your package there.\n\n    For example if your application is defined in :file:`yourapplication/app.py`\n    you should create it with one of the two versions below::\n\n        app = Flask('yourapplication')\n        app = Flask(__name__.split('.')[0])\n\n    Why is that?  The application will work even with `__name__`, thanks\n    to how resources are looked up.  However it will make debugging more\n    painful.  Certain extensions can make assumptions based on the\n    import name of your application.  For example the Flask-SQLAlchemy\n    extension will look for the code in your application that triggered\n    an SQL query in debug mode.  If the import name is not properly set\n    up, that debugging information is lost.  (For example it would only\n    pick up SQL queries in `yourapplication.app` and not\n    `yourapplication.views.frontend`)\n\n.. versionadded:: 0.7\n   The `static_url_path`, `static_folder`, and `template_folder`\n   parameters were added.\n\n.. versionadded:: 0.8\n   The `instance_path` and `instance_relative_config` parameters were\n   added.\n\n.. versionadded:: 0.11\n   The `root_path` parameter was added.\n\n.. versionadded:: 1.0\n   The ``host_matching`` and ``static_host`` parameters were added.\n\n.. versionadded:: 1.0\n   The ``subdomain_matching`` parameter was added. Subdomain\n   matching needs to be enabled manually now. Setting\n   :data:`SERVER_NAME` does not implicitly enable it.\n\n:param import_name: the name of the application package\n:param static_url_path: can be used to specify a different path for the\n                        static files on the web.  Defaults to the name\n                        of the `static_folder` folder.\n:param static_folder: The folder with static files that is served at\n    ``static_url_path``. Relative to the application ``root_path``\n    or an absolute path. Defaults to ``'static'``.\n:param static_host: the host to use when adding the static route.\n    Defaults to None. Required when using ``host_matching=True``\n    with a ``static_folder`` configured.\n:param host_matching: set ``url_map.host_matching`` attribute.\n    Defaults to False.\n:param subdomain_matching: consider the subdomain relative to\n    :data:`SERVER_NAME` when matching routes. Defaults to False.\n:param template_folder: the folder that contains the templates that should\n                        be used by the application.  Defaults to\n                        ``'templates'`` folder in the root path of the\n                        application.\n:param instance_path: An alternative instance path for the application.\n                      By default the folder ``'instance'`` next to the\n                      package or module is assumed to be the instance\n                      path.\n:param instance_relative_config: if set to ``True`` relative filenames\n                                 for loading the config are assumed to\n                                 be relative to the instance path instead\n                                 of the application root.\n:param root_path: The path to the root of the application files.\n    This should only be set manually when it can't be detected\n    automatically, such as for namespace packages.", "parameters": {"type": "object", "properties": {"import_name": {"type": "str"}, "static_url_path": {"type": ["str", "null"], "default": null}, "static_folder": {"type": ["null", "str", "string"], "default": "static"}, "static_host": {"type": ["str", "null"], "default": null}, "host_matching": {"type": "bool", "default": false}, "subdomain_matching": {"type": "bool", "default": false}, "template_folder": {"type": ["null", "str", "string"], "default": "templates"}, "instance_path": {"type": ["str", "null"], "default": null}, "instance_relative_config": {"type": "bool", "default": false}, "root_path": {"type": ["str", "null"], "default": null}}}}}
{"task_id": "BigCodeBench/82", "data": {"name": "flask.Flask.route", "type": "callable", "signature": "(self, rule: 'str')", "description": "Decorate a view function to register it with the given URL\nrule and options. Calls :meth:`add_url_rule`, which has more\ndetails about the implementation.\n\n.. code-block:: python\n\n    @app.route(\"/\")\n    def index():\n        return \"Hello, World!\"\n\nSee :ref:`url-route-registrations`.\n\nThe endpoint name for the route defaults to the name of the view\nfunction if the ``endpoint`` parameter isn't passed.\n\nThe ``methods`` parameter defaults to ``[\"GET\"]``. ``HEAD`` and\n``OPTIONS`` are added automatically.\n\n:param rule: The URL rule string.\n:param options: Extra options passed to the\n    :class:`~werkzeug.routing.Rule` object.", "parameters": {"type": "object", "properties": {"rule": {"type": "str"}}}}}
{"task_id": "BigCodeBench/82", "data": {"name": "flask_login.UserMixin", "type": "class", "signature": "()", "description": "This provides default implementations for the methods that Flask-Login\nexpects user objects to have.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/82", "data": {"name": "flask_login.UserMixin.id", "type": "class", "signature": "()", "description": "This provides default implementations for the methods that Flask-Login\nexpects user objects to have.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/82", "data": {"name": "flask_login.UserMixin.password_hash", "type": "class", "signature": "()", "description": "This provides default implementations for the methods that Flask-Login\nexpects user objects to have.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/82", "data": {"name": "flask_login.LoginManager", "type": "class", "signature": "(app=None, add_context_processor=True)", "description": "This object is used to hold the settings used for logging in. Instances\nof :class:`LoginManager` are *not* bound to specific apps, so you can\ncreate one in the main body of your code and then bind it to your\napp in a factory function.", "parameters": {"type": "object", "properties": {"app": {"type": "NoneType", "default": null}, "add_context_processor": {"type": "bool", "default": true}}}}}
{"task_id": "BigCodeBench/82", "data": {"name": "flask_login.LoginManager.user_loader", "type": "callable", "signature": "(self, callback)", "description": "This sets the callback for reloading a user from the session. The\nfunction you set should take a user ID (a ``str``) and return a\nuser object, or ``None`` if the user does not exist.\n\n:param callback: The callback for retrieving a user object.\n:type callback: callable", "parameters": {"type": "object", "properties": {"callback": {}}}}}
{"task_id": "BigCodeBench/82", "data": {"name": "flask_login.LoginManager.init_app", "type": "callable", "signature": "(self, app)", "description": "Configures an application. This registers an `after_request` call, and\nattaches this `LoginManager` to it as `app.login_manager`.\n\n:param app: The :class:`flask.Flask` object to configure.\n:type app: :class:`flask.Flask`\n:param add_context_processor: Whether to add a context processor to\n    the app that adds a `current_user` variable to the template.\n    Defaults to ``True``.\n:type add_context_processor: bool", "parameters": {"type": "object", "properties": {"app": {}}}}}
{"task_id": "BigCodeBench/82", "data": {"name": "LoginForm().password"}}
{"task_id": "BigCodeBench/82", "data": {"name": "LoginForm().password.data"}}
{"task_id": "BigCodeBench/82", "data": {"name": "LoginForm().username"}}
{"task_id": "BigCodeBench/82", "data": {"name": "LoginForm().username.data"}}
{"task_id": "BigCodeBench/82", "data": {"name": "LoginForm().validate_on_submit()"}}
{"task_id": "BigCodeBench/82", "data": {"name": "flask.url_for", "type": "callable", "signature": "(endpoint: 'str')", "description": "Generate a URL to the given endpoint with the given values.\n\nThis requires an active request or application context, and calls\n:meth:`current_app.url_for() <flask.Flask.url_for>`. See that method\nfor full documentation.\n\n:param endpoint: The endpoint name associated with the URL to\n    generate. If this starts with a ``.``, the current blueprint\n    name (if any) will be used.\n:param _anchor: If given, append this as ``#anchor`` to the URL.\n:param _method: If given, generate the URL associated with this\n    method for the endpoint.\n:param _scheme: If given, the URL will have this scheme if it is\n    external.\n:param _external: If given, prefer the URL to be internal (False) or\n    require it to be external (True). External URLs include the\n    scheme and domain. When not in an active request, URLs are\n    external by default.\n:param values: Values to use for the variable parts of the URL rule.\n    Unknown keys are appended as query string arguments, like\n    ``?a=b&c=d``.\n\n.. versionchanged:: 2.2\n    Calls ``current_app.url_for``, allowing an app to override the\n    behavior.\n\n.. versionchanged:: 0.10\n   The ``_scheme`` parameter was added.\n\n.. versionchanged:: 0.9\n   The ``_anchor`` and ``_method`` parameters were added.\n\n.. versionchanged:: 0.9\n   Calls ``app.handle_url_build_error`` on build errors.", "parameters": {"type": "object", "properties": {"endpoint": {"type": "str"}}}}}
{"task_id": "BigCodeBench/82", "data": {"name": "flask.redirect", "type": "callable", "signature": "(location: 'str')", "description": "Create a redirect response object.\n\nIf :data:`~flask.current_app` is available, it will use its\n:meth:`~flask.Flask.redirect` method, otherwise it will use\n:func:`werkzeug.utils.redirect`.\n\n:param location: The URL to redirect to.\n:param code: The status code for the redirect.\n:param Response: The response class to use. Not used when\n    ``current_app`` is active, which uses ``app.response_class``.\n\n.. versionadded:: 2.2\n    Calls ``current_app.redirect`` if available instead of always\n    using Werkzeug's default ``redirect``.", "parameters": {"type": "object", "properties": {"location": {"type": "str"}}}}}
{"task_id": "BigCodeBench/82", "data": {"name": "flask.render_template", "type": "callable", "signature": "(template_name_or_list: 'str | Template | list[str | Template]')", "description": "Render a template by name with the given context.\n\n:param template_name_or_list: The name of the template to render. If\n    a list is given, the first name to exist will be rendered.\n:param context: The variables to make available in the template.", "parameters": {"type": "object", "properties": {"template_name_or_list": {"type": ["template", "template]", "str", "list[str"]}}}}}
{"task_id": "BigCodeBench/82", "data": {"name": "flask_login.logout_user", "type": "callable", "signature": "()", "description": "Logs a user out. (You do not need to pass the actual user.) This will\nalso clean up the remember me cookie if it exists.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/82", "data": {"name": "flask_login.current_user.id", "type": "callable", "signature": "(self, *args, **kwargs)", "description": "", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/82", "data": {"name": "flask_login.login_user", "type": "callable", "signature": "(user)", "description": "Logs a user in. You should pass the actual user object to this. If the\nuser's `is_active` property is ``False``, they will not be logged in\nunless `force` is ``True``.\n\nThis will return ``True`` if the log in attempt succeeds, and ``False`` if\nit fails (i.e. because the user is inactive).\n\n:param user: The user object to log in.\n:type user: object\n:param remember: Whether to remember the user after their session expires.\n    Defaults to ``False``.\n:type remember: bool\n:param duration: The amount of time before the remember cookie expires. If\n    ``None`` the value set in the settings is used. Defaults to ``None``.\n:type duration: :class:`datetime.timedelta`\n:param force: If the user is inactive, setting this to ``True`` will log\n    them in regardless. Defaults to ``False``.\n:type force: bool\n:param fresh: setting this to ``False`` will log in the user with a session\n    marked as not \"fresh\". Defaults to ``True``.\n:type fresh: bool", "parameters": {"type": "object", "properties": {"user": {}}}}}
{"task_id": "BigCodeBench/82", "data": {"name": "werkzeug.security.generate_password_hash", "type": "callable", "signature": "(password: 'str')", "description": "Securely hash a password for storage. A password can be compared to a stored hash\nusing :func:`check_password_hash`.\n\nThe following methods are supported:\n\n-   ``scrypt``, the default. The parameters are ``n``, ``r``, and ``p``, the default\n    is ``scrypt:32768:8:1``. See :func:`hashlib.scrypt`.\n-   ``pbkdf2``, less secure. The parameters are ``hash_method`` and ``iterations``,\n    the default is ``pbkdf2:sha256:600000``. See :func:`hashlib.pbkdf2_hmac`.\n\nDefault parameters may be updated to reflect current guidelines, and methods may be\ndeprecated and removed if they are no longer considered secure. To migrate old\nhashes, you may generate a new hash when checking an old hash, or you may contact\nusers with a link to reset their password.\n\n:param password: The plaintext password.\n:param method: The key derivation function and parameters.\n:param salt_length: The number of characters to generate for the salt.\n\n.. versionchanged:: 2.3\n    Scrypt support was added.\n\n.. versionchanged:: 2.3\n    The default iterations for pbkdf2 was increased to 600,000.\n\n.. versionchanged:: 2.3\n    All plain hashes are deprecated and will not be supported in Werkzeug 3.0.", "parameters": {"type": "object", "properties": {"password": {"type": "str"}}}}}
{"task_id": "BigCodeBench/82", "data": {"name": "werkzeug.security.check_password_hash", "type": "callable", "signature": "(pwhash: 'str', password: 'str') -> 'bool)", "description": "Securely check that the given stored password hash, previously generated using\n:func:`generate_password_hash`, matches the given password.\n\nMethods may be deprecated and removed if they are no longer considered secure. To\nmigrate old hashes, you may generate a new hash when checking an old hash, or you\nmay contact users with a link to reset their password.\n\n:param pwhash: The hashed password.\n:param password: The plaintext password.\n\n.. versionchanged:: 2.3\n    All plain hashes are deprecated and will not be supported in Werkzeug 3.0.", "parameters": {"type": "object", "properties": {"pwhash": {"type": "str"}, "password": {"type": "str"}}}}}
{"task_id": "BigCodeBench/82", "data": {"name": "wtforms.validators.DataRequired", "type": "class", "signature": "()", "description": "Checks the field's data is 'truthy' otherwise stops the validation chain.\n\nThis validator checks that the ``data`` attribute on the field is a 'true'\nvalue (effectively, it does ``if field.data``.) Furthermore, if the data\nis a string type, a string containing only whitespace characters is\nconsidered false.\n\nIf the data is empty, also removes prior errors (such as processing errors)\nfrom the field.\n\n**NOTE** this validator used to be called `Required` but the way it behaved\n(requiring coerced data, not input data) meant it functioned in a way\nwhich was not symmetric to the `Optional` validator and furthermore caused\nconfusion with certain fields which coerced data to 'falsey' values like\n``0``, ``Decimal(0)``, ``time(0)`` etc. Unless a very specific reason\nexists, we recommend using the :class:`InputRequired` instead.\n\n:param message:\n    Error message to raise in case of a validation error.\n\nSets the `required` attribute on widgets.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/82", "data": {"name": "wtforms.validators.Length", "type": "class", "signature": "(min=-1, max=-1)", "description": "Validates the length of a string.\n\n:param min:\n    The minimum required length of the string. If not provided, minimum\n    length will not be checked.\n:param max:\n    The maximum length of the string. If not provided, maximum length\n    will not be checked.\n:param message:\n    Error message to raise in case of a validation error. Can be\n    interpolated using `%(min)d` and `%(max)d` if desired. Useful defaults\n    are provided depending on the existence of min and max.\n\nWhen supported, sets the `minlength` and `maxlength` attributes on widgets.", "parameters": {"type": "object", "properties": {"min": {"type": "integer", "default": -1}, "max": {"type": "integer", "default": -1}}}}}
{"task_id": "BigCodeBench/89", "data": {"name": "column_data.reshape(-1, 1)"}}
{"task_id": "BigCodeBench/89", "data": {"name": "matplotlib.pyplot.show", "type": "callable", "signature": "()", "description": "Display all open figures.\n\nParameters\n----------\nblock : bool, optional\n    Whether to wait for all figures to be closed before returning.\n\n    If `True` block and run the GUI main loop until all figure windows\n    are closed.\n\n    If `False` ensure that all figure windows are displayed and return\n    immediately.  In this case, you are responsible for ensuring\n    that the event loop is running to have responsive figures.\n\n    Defaults to True in non-interactive mode and to False in interactive\n    mode (see `.pyplot.isinteractive`).\n\nSee Also\n--------\nion : Enable interactive mode, which shows / updates the figure after\n      every plotting command, so that calling ``show()`` is not necessary.\nioff : Disable interactive mode.\nsavefig : Save the figure to an image file instead of showing it on screen.\n\nNotes\n-----\n**Saving figures to file and showing a window at the same time**\n\nIf you want an image file as well as a user interface window, use\n`.pyplot.savefig` before `.pyplot.show`. At the end of (a blocking)\n``show()`` the figure is closed and thus unregistered from pyplot. Calling\n`.pyplot.savefig` afterwards would save a new and thus empty figure. This\nlimitation of command order does not apply if the show is non-blocking or\nif you keep a reference to the figure and use `.Figure.savefig`.\n\n**Auto-show in jupyter notebooks**\n\nThe jupyter backends (activated via ``%matplotlib inline``,\n``%matplotlib notebook``, or ``%matplotlib widget``), call ``show()`` at\nthe end of every cell by default. Thus, you usually don't have to call it\nexplicitly there.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/89", "data": {"name": "matplotlib.pyplot.subplot", "type": "callable", "signature": "(*args, **kwargs) -> 'Axes)", "description": "Add an Axes to the current figure or retrieve an existing Axes.\n\nThis is a wrapper of `.Figure.add_subplot` which provides additional\nbehavior when working with the implicit API (see the notes section).\n\nCall signatures::\n\n   subplot(nrows, ncols, index, **kwargs)\n   subplot(pos, **kwargs)\n   subplot(**kwargs)\n   subplot(ax)\n\nParameters\n----------\n*args : int, (int, int, *index*), or `.SubplotSpec`, default: (1, 1, 1)\n    The position of the subplot described by one of\n\n    - Three integers (*nrows*, *ncols*, *index*). The subplot will take the\n      *index* position on a grid with *nrows* rows and *ncols* columns.\n      *index* starts at 1 in the upper left corner and increases to the\n      right. *index* can also be a two-tuple specifying the (*first*,\n      *last*) indices (1-based, and including *last*) of the subplot, e.g.,\n      ``fig.add_subplot(3, 1, (1, 2))`` makes a subplot that spans the\n      upper 2/3 of the figure.\n    - A 3-digit integer. The digits are interpreted as if given separately\n      as three single-digit integers, i.e. ``fig.add_subplot(235)`` is the\n      same as ``fig.add_subplot(2, 3, 5)``. Note that this can only be used\n      if there are no more than 9 subplots.\n    - A `.SubplotSpec`.\n\nprojection : {None, 'aitoff', 'hammer', 'lambert', 'mollweide', 'polar', 'rectilinear', str}, optional\n    The projection type of the subplot (`~.axes.Axes`). *str* is the name\n    of a custom projection, see `~matplotlib.projections`. The default\n    None results in a 'rectilinear' projection.\n\npolar : bool, default: False\n    If True, equivalent to projection='polar'.\n\nsharex, sharey : `~matplotlib.axes.Axes`, optional\n    Share the x or y `~matplotlib.axis` with sharex and/or sharey. The\n    axis will have the same limits, ticks, and scale as the axis of the\n    shared axes.\n\nlabel : str\n    A label for the returned axes.\n\nReturns\n-------\n`~.axes.Axes`\n\n    The Axes of the subplot. The returned Axes can actually be an instance\n    of a subclass, such as `.projections.polar.PolarAxes` for polar\n    projections.\n\nOther Parameters\n----------------\n**kwargs\n    This method also takes the keyword arguments for the returned axes\n    base class; except for the *figure* argument. The keyword arguments\n    for the rectilinear base class `~.axes.Axes` can be found in\n    the following table but there might also be other keyword\n    arguments if another projection is used.\n\n    Properties:\n    adjustable: {'box', 'datalim'}\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: scalar or None\n    anchor: (float, float) or {'C', 'SW', 'S', 'SE', 'E', 'NE', ...}\n    animated: bool\n    aspect: {'auto', 'equal'} or float\n    autoscale_on: bool\n    autoscalex_on: unknown\n    autoscaley_on: unknown\n    axes_locator: Callable[[Axes, Renderer], Bbox]\n    axisbelow: bool or 'line'\n    box_aspect: float or None\n    clip_box: `~matplotlib.transforms.BboxBase` or None\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    facecolor or fc: color\n    figure: `~matplotlib.figure.Figure`\n    frame_on: bool\n    gid: str\n    in_layout: bool\n    label: object\n    mouseover: bool\n    navigate: bool\n    navigate_mode: unknown\n    path_effects: list of `.AbstractPathEffect`\n    picker: None or bool or float or callable\n    position: [left, bottom, width, height] or `~matplotlib.transforms.Bbox`\n    prop_cycle: `~cycler.Cycler`\n    rasterization_zorder: float or None\n    rasterized: bool\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    subplotspec: unknown\n    title: str\n    transform: `~matplotlib.transforms.Transform`\n    url: str\n    visible: bool\n    xbound: (lower: float, upper: float)\n    xlabel: str\n    xlim: (left: float, right: float)\n    xmargin: float greater than -0.5\n    xscale: unknown\n    xticklabels: unknown\n    xticks: unknown\n    ybound: (lower: float, upper: float)\n    ylabel: str\n    ylim: (bottom: float, top: float)\n    ymargin: float greater than -0.5\n    yscale: unknown\n    yticklabels: unknown\n    yticks: unknown\n    zorder: float\n\nNotes\n-----\nCreating a new Axes will delete any preexisting Axes that\noverlaps with it beyond sharing a boundary::\n\n    import matplotlib.pyplot as plt\n    # plot a line, implicitly creating a subplot(111)\n    plt.plot([1, 2, 3])\n    # now create a subplot which represents the top plot of a grid\n    # with 2 rows and 1 column. Since this subplot will overlap the\n    # first, the plot (and its axes) previously created, will be removed\n    plt.subplot(211)\n\nIf you do not want this behavior, use the `.Figure.add_subplot` method\nor the `.pyplot.axes` function instead.\n\nIf no *kwargs* are passed and there exists an Axes in the location\nspecified by *args* then that Axes will be returned rather than a new\nAxes being created.\n\nIf *kwargs* are passed and there exists an Axes in the location\nspecified by *args*, the projection type is the same, and the\n*kwargs* match with the existing Axes, then the existing Axes is\nreturned.  Otherwise a new Axes is created with the specified\nparameters.  We save a reference to the *kwargs* which we use\nfor this comparison.  If any of the values in *kwargs* are\nmutable we will not detect the case where they are mutated.\nIn these cases we suggest using `.Figure.add_subplot` and the\nexplicit Axes API rather than the implicit pyplot API.\n\nSee Also\n--------\n.Figure.add_subplot\n.pyplot.subplots\n.pyplot.axes\n.Figure.subplots\n\nExamples\n--------\n::\n\n    plt.subplot(221)\n\n    # equivalent but more general\n    ax1 = plt.subplot(2, 2, 1)\n\n    # add a subplot with no frame\n    ax2 = plt.subplot(222, frameon=False)\n\n    # add a polar subplot\n    plt.subplot(223, projection='polar')\n\n    # add a red subplot that shares the x-axis with ax1\n    plt.subplot(224, sharex=ax1, facecolor='red')\n\n    # delete ax2 from the figure\n    plt.delaxes(ax2)\n\n    # add ax2 to the figure again\n    plt.subplot(ax2)\n\n    # make the first axes \"current\" again\n    plt.subplot(221)", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/89", "data": {"name": "matplotlib.pyplot.figure", "type": "callable", "signature": "(figsize: 'tuple[float, **kwargs) -> 'Figure)", "description": "Create a new figure, or activate an existing figure.\n\nParameters\n----------\nnum : int or str or `.Figure` or `.SubFigure`, optional\n    A unique identifier for the figure.\n\n    If a figure with that identifier already exists, this figure is made\n    active and returned. An integer refers to the ``Figure.number``\n    attribute, a string refers to the figure label.\n\n    If there is no figure with the identifier or *num* is not given, a new\n    figure is created, made active and returned.  If *num* is an int, it\n    will be used for the ``Figure.number`` attribute, otherwise, an\n    auto-generated integer value is used (starting at 1 and incremented\n    for each new figure). If *num* is a string, the figure label and the\n    window title is set to this value.  If num is a ``SubFigure``, its\n    parent ``Figure`` is activated.\n\nfigsize : (float, float), default: :rc:`figure.figsize`\n    Width, height in inches.\n\ndpi : float, default: :rc:`figure.dpi`\n    The resolution of the figure in dots-per-inch.\n\nfacecolor : color, default: :rc:`figure.facecolor`\n    The background color.\n\nedgecolor : color, default: :rc:`figure.edgecolor`\n    The border color.\n\nframeon : bool, default: True\n    If False, suppress drawing the figure frame.\n\nFigureClass : subclass of `~matplotlib.figure.Figure`\n    If set, an instance of this subclass will be created, rather than a\n    plain `.Figure`.\n\nclear : bool, default: False\n    If True and the figure already exists, then it is cleared.\n\nlayout : {'constrained', 'compressed', 'tight', 'none', `.LayoutEngine`, None}, default: None\n    The layout mechanism for positioning of plot elements to avoid\n    overlapping Axes decorations (labels, ticks, etc). Note that layout\n    managers can measurably slow down figure display.\n\n    - 'constrained': The constrained layout solver adjusts axes sizes\n      to avoid overlapping axes decorations.  Can handle complex plot\n      layouts and colorbars, and is thus recommended.\n\n      See :ref:`constrainedlayout_guide`\n      for examples.\n\n    - 'compressed': uses the same algorithm as 'constrained', but\n      removes extra space between fixed-aspect-ratio Axes.  Best for\n      simple grids of axes.\n\n    - 'tight': Use the tight layout mechanism. This is a relatively\n      simple algorithm that adjusts the subplot parameters so that\n      decorations do not overlap. See `.Figure.set_tight_layout` for\n      further details.\n\n    - 'none': Do not use a layout engine.\n\n    - A `.LayoutEngine` instance. Builtin layout classes are\n      `.ConstrainedLayoutEngine` and `.TightLayoutEngine`, more easily\n      accessible by 'constrained' and 'tight'.  Passing an instance\n      allows third parties to provide their own layout engine.\n\n    If not given, fall back to using the parameters *tight_layout* and\n    *constrained_layout*, including their config defaults\n    :rc:`figure.autolayout` and :rc:`figure.constrained_layout.use`.\n\n**kwargs\n    Additional keyword arguments are passed to the `.Figure` constructor.\n\nReturns\n-------\n`~matplotlib.figure.Figure`\n\nNotes\n-----\nA newly created figure is passed to the `~.FigureCanvasBase.new_manager`\nmethod or the `new_figure_manager` function provided by the current\nbackend, which install a canvas and a manager on the figure.\n\nOnce this is done, :rc:`figure.hooks` are called, one at a time, on the\nfigure; these hooks allow arbitrary customization of the figure (e.g.,\nattaching callbacks) or of associated elements (e.g., modifying the\ntoolbar).  See :doc:`/gallery/user_interfaces/mplcvd` for an example of\ntoolbar customization.\n\nIf you are creating many figures, make sure you explicitly call\n`.pyplot.close` on the figures you are not using, because this will\nenable pyplot to properly clean up the memory.\n\n`~matplotlib.rcParams` defines the default values, which can be modified\nin the matplotlibrc file.", "parameters": {"type": "object", "properties": {"figsize": {"type": "tuple[float"}}}}}
{"task_id": "BigCodeBench/89", "data": {"name": "matplotlib.pyplot.title", "type": "callable", "signature": "(label: 'str')", "description": "Set a title for the Axes.\n\nSet one of the three available Axes titles. The available titles\nare positioned above the Axes in the center, flush with the left\nedge, and flush with the right edge.\n\nParameters\n----------\nlabel : str\n    Text to use for the title\n\nfontdict : dict\n\n    .. admonition:: Discouraged\n\n       The use of *fontdict* is discouraged. Parameters should be passed as\n       individual keyword arguments or using dictionary-unpacking\n       ``set_title(..., **fontdict)``.\n\n    A dictionary controlling the appearance of the title text,\n    the default *fontdict* is::\n\n       {'fontsize': rcParams['axes.titlesize'],\n        'fontweight': rcParams['axes.titleweight'],\n        'color': rcParams['axes.titlecolor'],\n        'verticalalignment': 'baseline',\n        'horizontalalignment': loc}\n\nloc : {'center', 'left', 'right'}, default: :rc:`axes.titlelocation`\n    Which title to set.\n\ny : float, default: :rc:`axes.titley`\n    Vertical Axes location for the title (1.0 is the top).  If\n    None (the default) and :rc:`axes.titley` is also None, y is\n    determined automatically to avoid decorators on the Axes.\n\npad : float, default: :rc:`axes.titlepad`\n    The offset of the title from the top of the Axes, in points.\n\nReturns\n-------\n`.Text`\n    The matplotlib text instance representing the title\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    Other keyword arguments are text properties, see `.Text` for a list\n    of valid text properties.", "parameters": {"type": "object", "properties": {"label": {"type": "str"}}}}}
{"task_id": "BigCodeBench/89", "data": {"name": "matplotlib.pyplot.scatter", "type": "callable", "signature": "(x: 'float | ArrayLike', y: 'float | ArrayLike')", "description": "A scatter plot of *y* vs. *x* with varying marker size and/or color.\n\nParameters\n----------\nx, y : float or array-like, shape (n, )\n    The data positions.\n\ns : float or array-like, shape (n, ), optional\n    The marker size in points**2 (typographic points are 1/72 in.).\n    Default is ``rcParams['lines.markersize'] ** 2``.\n\n    The linewidth and edgecolor can visually interact with the marker\n    size, and can lead to artifacts if the marker size is smaller than\n    the linewidth.\n\n    If the linewidth is greater than 0 and the edgecolor is anything\n    but *'none'*, then the effective size of the marker will be\n    increased by half the linewidth because the stroke will be centered\n    on the edge of the shape.\n\n    To eliminate the marker edge either set *linewidth=0* or\n    *edgecolor='none'*.\n\nc : array-like or list of colors or color, optional\n    The marker colors. Possible values:\n\n    - A scalar or sequence of n numbers to be mapped to colors using\n      *cmap* and *norm*.\n    - A 2D array in which the rows are RGB or RGBA.\n    - A sequence of colors of length n.\n    - A single color format string.\n\n    Note that *c* should not be a single numeric RGB or RGBA sequence\n    because that is indistinguishable from an array of values to be\n    colormapped. If you want to specify the same RGB or RGBA value for\n    all points, use a 2D array with a single row.  Otherwise,\n    value-matching will have precedence in case of a size matching with\n    *x* and *y*.\n\n    If you wish to specify a single color for all points\n    prefer the *color* keyword argument.\n\n    Defaults to `None`. In that case the marker color is determined\n    by the value of *color*, *facecolor* or *facecolors*. In case\n    those are not specified or `None`, the marker color is determined\n    by the next color of the ``Axes``' current \"shape and fill\" color\n    cycle. This cycle defaults to :rc:`axes.prop_cycle`.\n\nmarker : `~.markers.MarkerStyle`, default: :rc:`scatter.marker`\n    The marker style. *marker* can be either an instance of the class\n    or the text shorthand for a particular marker.\n    See :mod:`matplotlib.markers` for more information about marker\n    styles.\n\ncmap : str or `~matplotlib.colors.Colormap`, default: :rc:`image.cmap`\n    The Colormap instance or registered colormap name used to map scalar data\n    to colors.\n\n    This parameter is ignored if *c* is RGB(A).\n\nnorm : str or `~matplotlib.colors.Normalize`, optional\n    The normalization method used to scale scalar data to the [0, 1] range\n    before mapping to colors using *cmap*. By default, a linear scaling is\n    used, mapping the lowest value to 0 and the highest to 1.\n\n    If given, this can be one of the following:\n\n    - An instance of `.Normalize` or one of its subclasses\n      (see :ref:`colormapnorms`).\n    - A scale name, i.e. one of \"linear\", \"log\", \"symlog\", \"logit\", etc.  For a\n      list of available scales, call `matplotlib.scale.get_scale_names()`.\n      In that case, a suitable `.Normalize` subclass is dynamically generated\n      and instantiated.\n\n    This parameter is ignored if *c* is RGB(A).\n\nvmin, vmax : float, optional\n    When using scalar data and no explicit *norm*, *vmin* and *vmax* define\n    the data range that the colormap covers. By default, the colormap covers\n    the complete value range of the supplied data. It is an error to use\n    *vmin*/*vmax* when a *norm* instance is given (but using a `str` *norm*\n    name together with *vmin*/*vmax* is acceptable).\n\n    This parameter is ignored if *c* is RGB(A).\n\nalpha : float, default: None\n    The alpha blending value, between 0 (transparent) and 1 (opaque).\n\nlinewidths : float or array-like, default: :rc:`lines.linewidth`\n    The linewidth of the marker edges. Note: The default *edgecolors*\n    is 'face'. You may want to change this as well.\n\nedgecolors : {'face', 'none', *None*} or color or sequence of color, default: :rc:`scatter.edgecolors`\n    The edge color of the marker. Possible values:\n\n    - 'face': The edge color will always be the same as the face color.\n    - 'none': No patch boundary will be drawn.\n    - A color or sequence of colors.\n\n    For non-filled markers, *edgecolors* is ignored. Instead, the color\n    is determined like with 'face', i.e. from *c*, *colors*, or\n    *facecolors*.\n\nplotnonfinite : bool, default: False\n    Whether to plot points with nonfinite *c* (i.e. ``inf``, ``-inf``\n    or ``nan``). If ``True`` the points are drawn with the *bad*\n    colormap color (see `.Colormap.set_bad`).\n\nReturns\n-------\n`~matplotlib.collections.PathCollection`\n\nOther Parameters\n----------------\ndata : indexable object, optional\n    If given, the following parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` (unless this raises an exception):\n\n    *x*, *y*, *s*, *linewidths*, *edgecolors*, *c*, *facecolor*, *facecolors*, *color*\n**kwargs : `~matplotlib.collections.Collection` properties\n\nSee Also\n--------\nplot : To plot scatter plots when markers are identical in size and\n    color.\n\nNotes\n-----\n* The `.plot` function will be faster for scatterplots where markers\n  don't vary in size or color.\n\n* Any or all of *x*, *y*, *s*, and *c* may be masked arrays, in which\n  case all masks will be combined and only unmasked points will be\n  plotted.\n\n* Fundamentally, scatter works with 1D arrays; *x*, *y*, *s*, and *c*\n  may be input as N-D arrays, but within scatter they will be\n  flattened. The exception is *c*, which will be flattened only if its\n  size matches the size of *x* and *y*.", "parameters": {"type": "object", "properties": {"x": {"type": ["float", "arraylike"]}, "y": {"type": ["float", "arraylike"]}}}}}
{"task_id": "BigCodeBench/89", "data": {"name": "numpy.copy", "type": "callable", "signature": "(a)", "description": "Return an array copy of the given object.\n\nParameters\n----------\na : array_like\n    Input data.\norder : {'C', 'F', 'A', 'K'}, optional\n    Controls the memory layout of the copy. 'C' means C-order,\n    'F' means F-order, 'A' means 'F' if `a` is Fortran contiguous,\n    'C' otherwise. 'K' means match the layout of `a` as closely\n    as possible. (Note that this function and :meth:`ndarray.copy` are very\n    similar, but have different default values for their order=\n    arguments.)\nsubok : bool, optional\n    If True, then sub-classes will be passed-through, otherwise the\n    returned array will be forced to be a base-class array (defaults to False).\n\n    .. versionadded:: 1.19.0\n\nReturns\n-------\narr : ndarray\n    Array interpretation of `a`.\n\nSee Also\n--------\nndarray.copy : Preferred method for creating an array copy\n\nNotes\n-----\nThis is equivalent to:\n\n>>> np.array(a, copy=True)  #doctest: +SKIP\n\nExamples\n--------\nCreate an array x, with a reference y and a copy z:\n\n>>> x = np.array([1, 2, 3])\n>>> y = x\n>>> z = np.copy(x)\n\nNote that, when we modify x, y changes, but not z:\n\n>>> x[0] = 10\n>>> x[0] == y[0]\nTrue\n>>> x[0] == z[0]\nFalse\n\nNote that, np.copy clears previously set WRITEABLE=False flag.\n\n>>> a = np.array([1, 2, 3])\n>>> a.flags[\"WRITEABLE\"] = False\n>>> b = np.copy(a)\n>>> b.flags[\"WRITEABLE\"]\nTrue\n>>> b[0] = 3\n>>> b\narray([3, 2, 3])\n\nNote that np.copy is a shallow copy and will not copy object\nelements within arrays. This is mainly important for arrays\ncontaining Python objects. The new array will contain the\nsame object which may lead to surprises if that object can\nbe modified (is mutable):\n\n>>> a = np.array([1, 'm', [2, 3, 4]], dtype=object)\n>>> b = np.copy(a)\n>>> b[2][0] = 10\n>>> a\narray([1, 'm', list([10, 3, 4])], dtype=object)\n\nTo ensure all elements within an ``object`` array are copied,\nuse `copy.deepcopy`:\n\n>>> import copy\n>>> a = np.array([1, 'm', [2, 3, 4]], dtype=object)\n>>> c = copy.deepcopy(a)\n>>> c[2][0] = 10\n>>> c\narray([1, 'm', list([10, 3, 4])], dtype=object)\n>>> a\narray([1, 'm', list([2, 3, 4])], dtype=object)", "parameters": {"type": "object", "properties": {"a": {}}}}}
{"task_id": "BigCodeBench/89", "data": {"name": "numpy.where", "type": "callable", "signature": "(*args, **kwargs)", "description": "where(condition, [x, y], /)\n\nReturn elements chosen from `x` or `y` depending on `condition`.\n\n.. note::\n    When only `condition` is provided, this function is a shorthand for\n    ``np.asarray(condition).nonzero()``. Using `nonzero` directly should be\n    preferred, as it behaves correctly for subclasses. The rest of this\n    documentation covers only the case where all three arguments are\n    provided.\n\nParameters\n----------\ncondition : array_like, bool\n    Where True, yield `x`, otherwise yield `y`.\nx, y : array_like\n    Values from which to choose. `x`, `y` and `condition` need to be\n    broadcastable to some shape.\n\nReturns\n-------\nout : ndarray\n    An array with elements from `x` where `condition` is True, and elements\n    from `y` elsewhere.\n\nSee Also\n--------\nchoose\nnonzero : The function that is called when x and y are omitted\n\nNotes\n-----\nIf all the arrays are 1-D, `where` is equivalent to::\n\n    [xv if c else yv\n     for c, xv, yv in zip(condition, x, y)]\n\nExamples\n--------\n>>> a = np.arange(10)\n>>> a\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n>>> np.where(a < 5, a, 10*a)\narray([ 0,  1,  2,  3,  4, 50, 60, 70, 80, 90])\n\nThis can be used on multidimensional arrays too:\n\n>>> np.where([[True, False], [True, True]],\n...          [[1, 2], [3, 4]],\n...          [[9, 8], [7, 6]])\narray([[1, 8],\n       [3, 4]])\n\nThe shapes of x, y, and the condition are broadcast together:\n\n>>> x, y = np.ogrid[:3, :4]\n>>> np.where(x < y, x, 10 + y)  # both x and 10+y are broadcast\narray([[10,  0,  0,  0],\n       [10, 11,  1,  1],\n       [10, 11, 12,  2]])\n\n>>> a = np.array([[0, 1, 2],\n...               [0, 2, 4],\n...               [0, 3, 6]])\n>>> np.where(a < 4, a, -1)  # -1 is broadcast\narray([[ 0,  1,  2],\n       [ 0,  2, -1],\n       [ 0,  3, -1]])", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/89", "data": {"name": "numpy.delete", "type": "callable", "signature": "(arr, obj, axis=None)", "description": "Return a new array with sub-arrays along an axis deleted. For a one\ndimensional array, this returns those entries not returned by\n`arr[obj]`.\n\nParameters\n----------\narr : array_like\n    Input array.\nobj : slice, int or array of ints\n    Indicate indices of sub-arrays to remove along the specified axis.\n\n    .. versionchanged:: 1.19.0\n        Boolean indices are now treated as a mask of elements to remove,\n        rather than being cast to the integers 0 and 1.\n\naxis : int, optional\n    The axis along which to delete the subarray defined by `obj`.\n    If `axis` is None, `obj` is applied to the flattened array.\n\nReturns\n-------\nout : ndarray\n    A copy of `arr` with the elements specified by `obj` removed. Note\n    that `delete` does not occur in-place. If `axis` is None, `out` is\n    a flattened array.\n\nSee Also\n--------\ninsert : Insert elements into an array.\nappend : Append elements at the end of an array.\n\nNotes\n-----\nOften it is preferable to use a boolean mask. For example:\n\n>>> arr = np.arange(12) + 1\n>>> mask = np.ones(len(arr), dtype=bool)\n>>> mask[[0,2,4]] = False\n>>> result = arr[mask,...]\n\nIs equivalent to ``np.delete(arr, [0,2,4], axis=0)``, but allows further\nuse of `mask`.\n\nExamples\n--------\n>>> arr = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\n>>> arr\narray([[ 1,  2,  3,  4],\n       [ 5,  6,  7,  8],\n       [ 9, 10, 11, 12]])\n>>> np.delete(arr, 1, 0)\narray([[ 1,  2,  3,  4],\n       [ 9, 10, 11, 12]])\n\n>>> np.delete(arr, np.s_[::2], 1)\narray([[ 2,  4],\n       [ 6,  8],\n       [10, 12]])\n>>> np.delete(arr, [1,3,5], None)\narray([ 1,  3,  5,  7,  8,  9, 10, 11, 12])", "parameters": {"type": "object", "properties": {"arr": {}, "obj": {}, "axis": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/89", "data": {"name": "numpy.abs", "type": "callable", "signature": "(*args, **kwargs)", "description": "absolute(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n\nCalculate the absolute value element-wise.\n\n``np.abs`` is a shorthand for this function.\n\nParameters\n----------\nx : array_like\n    Input array.\nout : ndarray, None, or tuple of ndarray and None, optional\n    A location into which the result is stored. If provided, it must have\n    a shape that the inputs broadcast to. If not provided or None,\n    a freshly-allocated array is returned. A tuple (possible only as a\n    keyword argument) must have length equal to the number of outputs.\nwhere : array_like, optional\n    This condition is broadcast over the input. At locations where the\n    condition is True, the `out` array will be set to the ufunc result.\n    Elsewhere, the `out` array will retain its original value.\n    Note that if an uninitialized `out` array is created via the default\n    ``out=None``, locations within it where the condition is False will\n    remain uninitialized.\n**kwargs\n    For other keyword-only arguments, see the\n    :ref:`ufunc docs <ufuncs.kwargs>`.\n\nReturns\n-------\nabsolute : ndarray\n    An ndarray containing the absolute value of\n    each element in `x`.  For complex input, ``a + ib``, the\n    absolute value is :math:`\\sqrt{ a^2 + b^2 }`.\n    This is a scalar if `x` is a scalar.\n\nExamples\n--------\n>>> x = np.array([-1.2, 1.2])\n>>> np.absolute(x)\narray([ 1.2,  1.2])\n>>> np.absolute(1.2 + 1j)\n1.5620499351813308\n\nPlot the function over ``[-10, 10]``:\n\n>>> import matplotlib.pyplot as plt\n\n>>> x = np.linspace(start=-10, stop=10, num=101)\n>>> plt.plot(x, np.absolute(x))\n>>> plt.show()\n\nPlot the function over the complex plane:\n\n>>> xx = x + 1j * x[:, np.newaxis]\n>>> plt.imshow(np.abs(xx), extent=[-10, 10, -10, 10], cmap='gray')\n>>> plt.show()\n\nThe `abs` function can be used as a shorthand for ``np.absolute`` on\nndarrays.\n\n>>> x = np.array([-1.2, 1.2])\n>>> abs(x)\narray([1.2, 1.2])", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/89", "data": {"name": "scipy.stats.zscore", "type": "callable", "signature": "(a)", "description": "Compute the z score.\n\nCompute the z score of each value in the sample, relative to the\nsample mean and standard deviation.\n\nParameters\n----------\na : array_like\n    An array like object containing the sample data.\naxis : int or None, optional\n    Axis along which to operate. Default is 0. If None, compute over\n    the whole array `a`.\nddof : int, optional\n    Degrees of freedom correction in the calculation of the\n    standard deviation. Default is 0.\nnan_policy : {'propagate', 'raise', 'omit'}, optional\n    Defines how to handle when input contains nan. 'propagate' returns nan,\n    'raise' throws an error, 'omit' performs the calculations ignoring nan\n    values. Default is 'propagate'.  Note that when the value is 'omit',\n    nans in the input also propagate to the output, but they do not affect\n    the z-scores computed for the non-nan values.\n\nReturns\n-------\nzscore : array_like\n    The z-scores, standardized by mean and standard deviation of\n    input array `a`.\n\nSee Also\n--------\nnumpy.mean : Arithmetic average\nnumpy.std : Arithmetic standard deviation\nscipy.stats.gzscore : Geometric standard score\n\nNotes\n-----\nThis function preserves ndarray subclasses, and works also with\nmatrices and masked arrays (it uses `asanyarray` instead of\n`asarray` for parameters).\n\nReferences\n----------\n.. [1] \"Standard score\", *Wikipedia*,\n       https://en.wikipedia.org/wiki/Standard_score.\n.. [2] Huck, S. W., Cross, T. L., Clark, S. B, \"Overcoming misconceptions\n       about Z-scores\", Teaching Statistics, vol. 8, pp. 38-40, 1986\n\nExamples\n--------\n>>> import numpy as np\n>>> a = np.array([ 0.7972,  0.0767,  0.4383,  0.7866,  0.8091,\n...                0.1954,  0.6307,  0.6599,  0.1065,  0.0508])\n>>> from scipy import stats\n>>> stats.zscore(a)\narray([ 1.1273, -1.247 , -0.0552,  1.0923,  1.1664, -0.8559,  0.5786,\n        0.6748, -1.1488, -1.3324])\n\nComputing along a specified axis, using n-1 degrees of freedom\n(``ddof=1``) to calculate the standard deviation:\n\n>>> b = np.array([[ 0.3148,  0.0478,  0.6243,  0.4608],\n...               [ 0.7149,  0.0775,  0.6072,  0.9656],\n...               [ 0.6341,  0.1403,  0.9759,  0.4064],\n...               [ 0.5918,  0.6948,  0.904 ,  0.3721],\n...               [ 0.0921,  0.2481,  0.1188,  0.1366]])\n>>> stats.zscore(b, axis=1, ddof=1)\narray([[-0.19264823, -1.28415119,  1.07259584,  0.40420358],\n       [ 0.33048416, -1.37380874,  0.04251374,  1.00081084],\n       [ 0.26796377, -1.12598418,  1.23283094, -0.37481053],\n       [-0.22095197,  0.24468594,  1.19042819, -1.21416216],\n       [-0.82780366,  1.4457416 , -0.43867764, -0.1792603 ]])\n\nAn example with `nan_policy='omit'`:\n\n>>> x = np.array([[25.11, 30.10, np.nan, 32.02, 43.15],\n...               [14.95, 16.06, 121.25, 94.35, 29.81]])\n>>> stats.zscore(x, axis=1, nan_policy='omit')\narray([[-1.13490897, -0.37830299,         nan, -0.08718406,  1.60039602],\n       [-0.91611681, -0.89090508,  1.4983032 ,  0.88731639, -0.5785977 ]])", "parameters": {"type": "object", "properties": {"a": {}}}}}
{"task_id": "BigCodeBench/89", "data": {"name": "sklearn.preprocessing.StandardScaler", "type": "class", "signature": "()", "description": "Standardize features by removing the mean and scaling to unit variance.\n\nThe standard score of a sample `x` is calculated as:\n\n    z = (x - u) / s\n\nwhere `u` is the mean of the training samples or zero if `with_mean=False`,\nand `s` is the standard deviation of the training samples or one if\n`with_std=False`.\n\nCentering and scaling happen independently on each feature by computing\nthe relevant statistics on the samples in the training set. Mean and\nstandard deviation are then stored to be used on later data using\n:meth:`transform`.\n\nStandardization of a dataset is a common requirement for many\nmachine learning estimators: they might behave badly if the\nindividual features do not more or less look like standard normally\ndistributed data (e.g. Gaussian with 0 mean and unit variance).\n\nFor instance many elements used in the objective function of\na learning algorithm (such as the RBF kernel of Support Vector\nMachines or the L1 and L2 regularizers of linear models) assume that\nall features are centered around 0 and have variance in the same\norder. If a feature has a variance that is orders of magnitude larger\nthan others, it might dominate the objective function and make the\nestimator unable to learn from other features correctly as expected.\n\n`StandardScaler` is sensitive to outliers, and the features may scale\ndifferently from each other in the presence of outliers. For an example\nvisualization, refer to :ref:`Compare StandardScaler with other scalers\n<plot_all_scaling_standard_scaler_section>`.\n\nThis scaler can also be applied to sparse CSR or CSC matrices by passing\n`with_mean=False` to avoid breaking the sparsity structure of the data.\n\nRead more in the :ref:`User Guide <preprocessing_scaler>`.\n\nParameters\n----------\ncopy : bool, default=True\n    If False, try to avoid a copy and do inplace scaling instead.\n    This is not guaranteed to always work inplace; e.g. if the data is\n    not a NumPy array or scipy.sparse CSR matrix, a copy may still be\n    returned.\n\nwith_mean : bool, default=True\n    If True, center the data before scaling.\n    This does not work (and will raise an exception) when attempted on\n    sparse matrices, because centering them entails building a dense\n    matrix which in common use cases is likely to be too large to fit in\n    memory.\n\nwith_std : bool, default=True\n    If True, scale the data to unit variance (or equivalently,\n    unit standard deviation).\n\nAttributes\n----------\nscale_ : ndarray of shape (n_features,) or None\n    Per feature relative scaling of the data to achieve zero mean and unit\n    variance. Generally this is calculated using `np.sqrt(var_)`. If a\n    variance is zero, we can't achieve unit variance, and the data is left\n    as-is, giving a scaling factor of 1. `scale_` is equal to `None`\n    when `with_std=False`.\n\n    .. versionadded:: 0.17\n       *scale_*\n\nmean_ : ndarray of shape (n_features,) or None\n    The mean value for each feature in the training set.\n    Equal to ``None`` when ``with_mean=False`` and ``with_std=False``.\n\nvar_ : ndarray of shape (n_features,) or None\n    The variance for each feature in the training set. Used to compute\n    `scale_`. Equal to ``None`` when ``with_mean=False`` and\n    ``with_std=False``.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nn_samples_seen_ : int or ndarray of shape (n_features,)\n    The number of samples processed by the estimator for each feature.\n    If there are no missing samples, the ``n_samples_seen`` will be an\n    integer, otherwise it will be an array of dtype int. If\n    `sample_weights` are used it will be a float (if no missing data)\n    or an array of dtype float that sums the weights seen so far.\n    Will be reset on new calls to fit, but increments across\n    ``partial_fit`` calls.\n\nSee Also\n--------\nscale : Equivalent function without the estimator API.\n\n:class:`~sklearn.decomposition.PCA` : Further removes the linear\n    correlation across features with 'whiten=True'.\n\nNotes\n-----\nNaNs are treated as missing values: disregarded in fit, and maintained in\ntransform.\n\nWe use a biased estimator for the standard deviation, equivalent to\n`numpy.std(x, ddof=0)`. Note that the choice of `ddof` is unlikely to\naffect model performance.\n\nExamples\n--------\n>>> from sklearn.preprocessing import StandardScaler\n>>> data = [[0, 0], [0, 0], [1, 1], [1, 1]]\n>>> scaler = StandardScaler()\n>>> print(scaler.fit(data))\nStandardScaler()\n>>> print(scaler.mean_)\n[0.5 0.5]\n>>> print(scaler.transform(data))\n[[-1. -1.]\n [-1. -1.]\n [ 1.  1.]\n [ 1.  1.]]\n>>> print(scaler.transform([[2, 2]]))\n[[3. 3.]]", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/89", "data": {"name": "sklearn.preprocessing.StandardScaler.fit_transform", "type": "callable", "signature": "(self, X)", "description": "Fit to data, then transform it.\n\nFits transformer to `X` and `y` with optional parameters `fit_params`\nand returns a transformed version of `X`.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Input samples.\n\ny :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n    Target values (None for unsupervised transformations).\n\n**fit_params : dict\n    Additional fit parameters.\n\nReturns\n-------\nX_new : ndarray array of shape (n_samples, n_features_new)\n    Transformed array.", "parameters": {"type": "object", "properties": {"X": {}}}}}
{"task_id": "BigCodeBench/92", "data": {"name": "data.iloc"}}
{"task_id": "BigCodeBench/92", "data": {"name": "matplotlib.pyplot.subplots", "type": "class", "signature": "()", "description": "Create a figure and a set of subplots.\n\nThis utility wrapper makes it convenient to create common layouts of\nsubplots, including the enclosing figure object, in a single call.\n\nParameters\n----------\nnrows, ncols : int, default: 1\n    Number of rows/columns of the subplot grid.\n\nsharex, sharey : bool or {'none', 'all', 'row', 'col'}, default: False\n    Controls sharing of properties among x (*sharex*) or y (*sharey*)\n    axes:\n\n    - True or 'all': x- or y-axis will be shared among all subplots.\n    - False or 'none': each subplot x- or y-axis will be independent.\n    - 'row': each subplot row will share an x- or y-axis.\n    - 'col': each subplot column will share an x- or y-axis.\n\n    When subplots have a shared x-axis along a column, only the x tick\n    labels of the bottom subplot are created. Similarly, when subplots\n    have a shared y-axis along a row, only the y tick labels of the first\n    column subplot are created. To later turn other subplots' ticklabels\n    on, use `~matplotlib.axes.Axes.tick_params`.\n\n    When subplots have a shared axis that has units, calling\n    `~matplotlib.axis.Axis.set_units` will update each axis with the\n    new units.\n\nsqueeze : bool, default: True\n    - If True, extra dimensions are squeezed out from the returned\n      array of `~matplotlib.axes.Axes`:\n\n      - if only one subplot is constructed (nrows=ncols=1), the\n        resulting single Axes object is returned as a scalar.\n      - for Nx1 or 1xM subplots, the returned object is a 1D numpy\n        object array of Axes objects.\n      - for NxM, subplots with N>1 and M>1 are returned as a 2D array.\n\n    - If False, no squeezing at all is done: the returned Axes object is\n      always a 2D array containing Axes instances, even if it ends up\n      being 1x1.\n\nwidth_ratios : array-like of length *ncols*, optional\n    Defines the relative widths of the columns. Each column gets a\n    relative width of ``width_ratios[i] / sum(width_ratios)``.\n    If not given, all columns will have the same width.  Equivalent\n    to ``gridspec_kw={'width_ratios': [...]}``.\n\nheight_ratios : array-like of length *nrows*, optional\n    Defines the relative heights of the rows. Each row gets a\n    relative height of ``height_ratios[i] / sum(height_ratios)``.\n    If not given, all rows will have the same height. Convenience\n    for ``gridspec_kw={'height_ratios': [...]}``.\n\nsubplot_kw : dict, optional\n    Dict with keywords passed to the\n    `~matplotlib.figure.Figure.add_subplot` call used to create each\n    subplot.\n\ngridspec_kw : dict, optional\n    Dict with keywords passed to the `~matplotlib.gridspec.GridSpec`\n    constructor used to create the grid the subplots are placed on.\n\n**fig_kw\n    All additional keyword arguments are passed to the\n    `.pyplot.figure` call.\n\nReturns\n-------\nfig : `.Figure`\n\nax : `~matplotlib.axes.Axes` or array of Axes\n    *ax* can be either a single `~.axes.Axes` object, or an array of Axes\n    objects if more than one subplot was created.  The dimensions of the\n    resulting array can be controlled with the squeeze keyword, see above.\n\n    Typical idioms for handling the return value are::\n\n        # using the variable ax for single a Axes\n        fig, ax = plt.subplots()\n\n        # using the variable axs for multiple Axes\n        fig, axs = plt.subplots(2, 2)\n\n        # using tuple unpacking for multiple Axes\n        fig, (ax1, ax2) = plt.subplots(1, 2)\n        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n\n    The names ``ax`` and pluralized ``axs`` are preferred over ``axes``\n    because for the latter it's not clear if it refers to a single\n    `~.axes.Axes` instance or a collection of these.\n\nSee Also\n--------\n.pyplot.figure\n.pyplot.subplot\n.pyplot.axes\n.Figure.subplots\n.Figure.add_subplot\n\nExamples\n--------\n::\n\n    # First create some toy data:\n    x = np.linspace(0, 2*np.pi, 400)\n    y = np.sin(x**2)\n\n    # Create just a figure and only one subplot\n    fig, ax = plt.subplots()\n    ax.plot(x, y)\n    ax.set_title('Simple plot')\n\n    # Create two subplots and unpack the output array immediately\n    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n    ax1.plot(x, y)\n    ax1.set_title('Sharing Y axis')\n    ax2.scatter(x, y)\n\n    # Create four polar axes and access them through the returned array\n    fig, axs = plt.subplots(2, 2, subplot_kw=dict(projection=\"polar\"))\n    axs[0, 0].plot(x, y)\n    axs[1, 1].scatter(x, y)\n\n    # Share a X axis with each column of subplots\n    plt.subplots(2, 2, sharex='col')\n\n    # Share a Y axis with each row of subplots\n    plt.subplots(2, 2, sharey='row')\n\n    # Share both X and Y axes with all subplots\n    plt.subplots(2, 2, sharex='all', sharey='all')\n\n    # Note that this is the same as\n    plt.subplots(2, 2, sharex=True, sharey=True)\n\n    # Create figure number 10 with a single subplot\n    # and clears it if it already exists.\n    fig, ax = plt.subplots(num=10, clear=True)", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/92", "data": {"name": "matplotlib.pyplot.subplots[1].legend", "type": "method", "signature": "(*args, **kwargs)", "description": "Place a legend on the Axes.\n\nCall signatures::\n\n    legend()\n    legend(handles, labels)\n    legend(handles=handles)\n    legend(labels)\n\nThe call signatures correspond to the following different ways to use\nthis method:\n\n**1. Automatic detection of elements to be shown in the legend**\n\nThe elements to be added to the legend are automatically determined,\nwhen you do not pass in any extra arguments.\n\nIn this case, the labels are taken from the artist. You can specify\nthem either at artist creation or by calling the\n:meth:`~.Artist.set_label` method on the artist::\n\n    ax.plot([1, 2, 3], label='Inline label')\n    ax.legend()\n\nor::\n\n    line, = ax.plot([1, 2, 3])\n    line.set_label('Label via method')\n    ax.legend()\n\n.. note::\n    Specific artists can be excluded from the automatic legend element\n    selection by using a label starting with an underscore, \"_\".\n    A string starting with an underscore is the default label for all\n    artists, so calling `.Axes.legend` without any arguments and\n    without setting the labels manually will result in no legend being\n    drawn.\n\n\n**2. Explicitly listing the artists and labels in the legend**\n\nFor full control of which artists have a legend entry, it is possible\nto pass an iterable of legend artists followed by an iterable of\nlegend labels respectively::\n\n    ax.legend([line1, line2, line3], ['label1', 'label2', 'label3'])\n\n\n**3. Explicitly listing the artists in the legend**\n\nThis is similar to 2, but the labels are taken from the artists'\nlabel properties. Example::\n\n    line1, = ax.plot([1, 2, 3], label='label1')\n    line2, = ax.plot([1, 2, 3], label='label2')\n    ax.legend(handles=[line1, line2])\n\n\n**4. Labeling existing plot elements**\n\n.. admonition:: Discouraged\n\n    This call signature is discouraged, because the relation between\n    plot elements and labels is only implicit by their order and can\n    easily be mixed up.\n\nTo make a legend for all artists on an Axes, call this function with\nan iterable of strings, one for each legend item. For example::\n\n    ax.plot([1, 2, 3])\n    ax.plot([5, 6, 7])\n    ax.legend(['First line', 'Second line'])\n\n\nParameters\n----------\nhandles : list of (`.Artist` or tuple of `.Artist`), optional\n    A list of Artists (lines, patches) to be added to the legend.\n    Use this together with *labels*, if you need full control on what\n    is shown in the legend and the automatic mechanism described above\n    is not sufficient.\n\n    The length of handles and labels should be the same in this\n    case. If they are not, they are truncated to the smaller length.\n\n    If an entry contains a tuple, then the legend handler for all Artists in the\n    tuple will be placed alongside a single label.\n\nlabels : list of str, optional\n    A list of labels to show next to the artists.\n    Use this together with *handles*, if you need full control on what\n    is shown in the legend and the automatic mechanism described above\n    is not sufficient.\n\nReturns\n-------\n`~matplotlib.legend.Legend`\n\nOther Parameters\n----------------\n\nloc : str or pair of floats, default: :rc:`legend.loc`\n    The location of the legend.\n\n    The strings ``'upper left'``, ``'upper right'``, ``'lower left'``,\n    ``'lower right'`` place the legend at the corresponding corner of the\n    axes.\n\n    The strings ``'upper center'``, ``'lower center'``, ``'center left'``,\n    ``'center right'`` place the legend at the center of the corresponding edge\n    of the axes.\n\n    The string ``'center'`` places the legend at the center of the axes.\n\n    The string ``'best'`` places the legend at the location, among the nine\n    locations defined so far, with the minimum overlap with other drawn\n    artists.  This option can be quite slow for plots with large amounts of\n    data; your plotting speed may benefit from providing a specific location.\n\n    The location can also be a 2-tuple giving the coordinates of the lower-left\n    corner of the legend in axes coordinates (in which case *bbox_to_anchor*\n    will be ignored).\n\n    For back-compatibility, ``'center right'`` (but no other location) can also\n    be spelled ``'right'``, and each \"string\" location can also be given as a\n    numeric value:\n\n    ==================   =============\n    Location String      Location Code\n    ==================   =============\n    'best' (Axes only)   0\n    'upper right'        1\n    'upper left'         2\n    'lower left'         3\n    'lower right'        4\n    'right'              5\n    'center left'        6\n    'center right'       7\n    'lower center'       8\n    'upper center'       9\n    'center'             10\n    ==================   =============\n    \nbbox_to_anchor : `.BboxBase`, 2-tuple, or 4-tuple of floats\n    Box that is used to position the legend in conjunction with *loc*.\n    Defaults to `axes.bbox` (if called as a method to `.Axes.legend`) or\n    `figure.bbox` (if `.Figure.legend`).  This argument allows arbitrary\n    placement of the legend.\n\n    Bbox coordinates are interpreted in the coordinate system given by\n    *bbox_transform*, with the default transform\n    Axes or Figure coordinates, depending on which ``legend`` is called.\n\n    If a 4-tuple or `.BboxBase` is given, then it specifies the bbox\n    ``(x, y, width, height)`` that the legend is placed in.\n    To put the legend in the best location in the bottom right\n    quadrant of the axes (or figure)::\n\n        loc='best', bbox_to_anchor=(0.5, 0., 0.5, 0.5)\n\n    A 2-tuple ``(x, y)`` places the corner of the legend specified by *loc* at\n    x, y.  For example, to put the legend's upper right-hand corner in the\n    center of the axes (or figure) the following keywords can be used::\n\n        loc='upper right', bbox_to_anchor=(0.5, 0.5)\n\nncols : int, default: 1\n    The number of columns that the legend has.\n\n    For backward compatibility, the spelling *ncol* is also supported\n    but it is discouraged. If both are given, *ncols* takes precedence.\n\nprop : None or `~matplotlib.font_manager.FontProperties` or dict\n    The font properties of the legend. If None (default), the current\n    :data:`matplotlib.rcParams` will be used.\n\nfontsize : int or {'xx-small', 'x-small', 'small', 'medium', 'large', 'x-large', 'xx-large'}\n    The font size of the legend. If the value is numeric the size will be the\n    absolute font size in points. String values are relative to the current\n    default font size. This argument is only used if *prop* is not specified.\n\nlabelcolor : str or list, default: :rc:`legend.labelcolor`\n    The color of the text in the legend. Either a valid color string\n    (for example, 'red'), or a list of color strings. The labelcolor can\n    also be made to match the color of the line or marker using 'linecolor',\n    'markerfacecolor' (or 'mfc'), or 'markeredgecolor' (or 'mec').\n\n    Labelcolor can be set globally using :rc:`legend.labelcolor`. If None,\n    use :rc:`text.color`.\n\nnumpoints : int, default: :rc:`legend.numpoints`\n    The number of marker points in the legend when creating a legend\n    entry for a `.Line2D` (line).\n\nscatterpoints : int, default: :rc:`legend.scatterpoints`\n    The number of marker points in the legend when creating\n    a legend entry for a `.PathCollection` (scatter plot).\n\nscatteryoffsets : iterable of floats, default: ``[0.375, 0.5, 0.3125]``\n    The vertical offset (relative to the font size) for the markers\n    created for a scatter plot legend entry. 0.0 is at the base the\n    legend text, and 1.0 is at the top. To draw all markers at the\n    same height, set to ``[0.5]``.\n\nmarkerscale : float, default: :rc:`legend.markerscale`\n    The relative size of legend markers compared to the originally drawn ones.\n\nmarkerfirst : bool, default: True\n    If *True*, legend marker is placed to the left of the legend label.\n    If *False*, legend marker is placed to the right of the legend label.\n\nreverse : bool, default: False\n    If *True*, the legend labels are displayed in reverse order from the input.\n    If *False*, the legend labels are displayed in the same order as the input.\n\n    .. versionadded:: 3.7\n\nframeon : bool, default: :rc:`legend.frameon`\n    Whether the legend should be drawn on a patch (frame).\n\nfancybox : bool, default: :rc:`legend.fancybox`\n    Whether round edges should be enabled around the `.FancyBboxPatch` which\n    makes up the legend's background.\n\nshadow : None, bool or dict, default: :rc:`legend.shadow`\n    Whether to draw a shadow behind the legend.\n    The shadow can be configured using `.Patch` keywords.\n    Customization via :rc:`legend.shadow` is currently not supported.\n\nframealpha : float, default: :rc:`legend.framealpha`\n    The alpha transparency of the legend's background.\n    If *shadow* is activated and *framealpha* is ``None``, the default value is\n    ignored.\n\nfacecolor : \"inherit\" or color, default: :rc:`legend.facecolor`\n    The legend's background color.\n    If ``\"inherit\"``, use :rc:`axes.facecolor`.\n\nedgecolor : \"inherit\" or color, default: :rc:`legend.edgecolor`\n    The legend's background patch edge color.\n    If ``\"inherit\"``, use take :rc:`axes.edgecolor`.\n\nmode : {\"expand\", None}\n    If *mode* is set to ``\"expand\"`` the legend will be horizontally\n    expanded to fill the axes area (or *bbox_to_anchor* if defines\n    the legend's size).\n\nbbox_transform : None or `~matplotlib.transforms.Transform`\n    The transform for the bounding box (*bbox_to_anchor*). For a value\n    of ``None`` (default) the Axes'\n    :data:`~matplotlib.axes.Axes.transAxes` transform will be used.\n\ntitle : str or None\n    The legend's title. Default is no title (``None``).\n\ntitle_fontproperties : None or `~matplotlib.font_manager.FontProperties` or dict\n    The font properties of the legend's title. If None (default), the\n    *title_fontsize* argument will be used if present; if *title_fontsize* is\n    also None, the current :rc:`legend.title_fontsize` will be used.\n\ntitle_fontsize : int or {'xx-small', 'x-small', 'small', 'medium', 'large', 'x-large', 'xx-large'}, default: :rc:`legend.title_fontsize`\n    The font size of the legend's title.\n    Note: This cannot be combined with *title_fontproperties*. If you want\n    to set the fontsize alongside other font properties, use the *size*\n    parameter in *title_fontproperties*.\n\nalignment : {'center', 'left', 'right'}, default: 'center'\n    The alignment of the legend title and the box of entries. The entries\n    are aligned as a single block, so that markers always lined up.\n\nborderpad : float, default: :rc:`legend.borderpad`\n    The fractional whitespace inside the legend border, in font-size units.\n\nlabelspacing : float, default: :rc:`legend.labelspacing`\n    The vertical space between the legend entries, in font-size units.\n\nhandlelength : float, default: :rc:`legend.handlelength`\n    The length of the legend handles, in font-size units.\n\nhandleheight : float, default: :rc:`legend.handleheight`\n    The height of the legend handles, in font-size units.\n\nhandletextpad : float, default: :rc:`legend.handletextpad`\n    The pad between the legend handle and text, in font-size units.\n\nborderaxespad : float, default: :rc:`legend.borderaxespad`\n    The pad between the axes and legend border, in font-size units.\n\ncolumnspacing : float, default: :rc:`legend.columnspacing`\n    The spacing between columns, in font-size units.\n\nhandler_map : dict or None\n    The custom dictionary mapping instances or types to a legend\n    handler. This *handler_map* updates the default handler map\n    found at `matplotlib.legend.Legend.get_legend_handler_map`.\n\ndraggable : bool, default: False\n    Whether the legend can be dragged with the mouse.\n\n\nSee Also\n--------\n.Figure.legend\n\nNotes\n-----\nSome artists are not supported by this function.  See\n:ref:`legend_guide` for details.\n\nExamples\n--------\n.. plot:: gallery/text_labels_and_annotations/legend.py", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/92", "data": {"name": "matplotlib.pyplot.subplots[1].set_xlabel", "type": "method", "signature": "(xlabel, fontdict=None, labelpad=None, *, loc=None, **kwargs)", "description": "Set the label for the x-axis.\n\nParameters\n----------\nxlabel : str\n    The label text.\n\nlabelpad : float, default: :rc:`axes.labelpad`\n    Spacing in points from the Axes bounding box including ticks\n    and tick labels.  If None, the previous value is left as is.\n\nloc : {'left', 'center', 'right'}, default: :rc:`xaxis.labellocation`\n    The label position. This is a high-level alternative for passing\n    parameters *x* and *horizontalalignment*.\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    `.Text` properties control the appearance of the label.\n\nSee Also\n--------\ntext : Documents the properties supported by `.Text`.", "parameters": {"type": "object", "properties": {"xlabel": {}, "fontdict": {"type": "NoneType", "default": null}, "labelpad": {"type": "NoneType", "default": null}, "loc": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/92", "data": {"name": "matplotlib.pyplot.subplots[1].set_ylabel", "type": "method", "signature": "(ylabel, fontdict=None, labelpad=None, *, loc=None, **kwargs)", "description": "Set the label for the y-axis.\n\nParameters\n----------\nylabel : str\n    The label text.\n\nlabelpad : float, default: :rc:`axes.labelpad`\n    Spacing in points from the Axes bounding box including ticks\n    and tick labels.  If None, the previous value is left as is.\n\nloc : {'bottom', 'center', 'top'}, default: :rc:`yaxis.labellocation`\n    The label position. This is a high-level alternative for passing\n    parameters *y* and *horizontalalignment*.\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    `.Text` properties control the appearance of the label.\n\nSee Also\n--------\ntext : Documents the properties supported by `.Text`.", "parameters": {"type": "object", "properties": {"ylabel": {}, "fontdict": {"type": "NoneType", "default": null}, "labelpad": {"type": "NoneType", "default": null}, "loc": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/92", "data": {"name": "matplotlib.pyplot.subplots[1].set_title", "type": "method", "signature": "(label, fontdict=None, loc=None, pad=None, *, y=None, **kwargs)", "description": "Set a title for the Axes.\n\nSet one of the three available Axes titles. The available titles\nare positioned above the Axes in the center, flush with the left\nedge, and flush with the right edge.\n\nParameters\n----------\nlabel : str\n    Text to use for the title\n\nfontdict : dict\n\n    .. admonition:: Discouraged\n\n       The use of *fontdict* is discouraged. Parameters should be passed as\n       individual keyword arguments or using dictionary-unpacking\n       ``set_title(..., **fontdict)``.\n\n    A dictionary controlling the appearance of the title text,\n    the default *fontdict* is::\n\n       {'fontsize': rcParams['axes.titlesize'],\n        'fontweight': rcParams['axes.titleweight'],\n        'color': rcParams['axes.titlecolor'],\n        'verticalalignment': 'baseline',\n        'horizontalalignment': loc}\n\nloc : {'center', 'left', 'right'}, default: :rc:`axes.titlelocation`\n    Which title to set.\n\ny : float, default: :rc:`axes.titley`\n    Vertical Axes location for the title (1.0 is the top).  If\n    None (the default) and :rc:`axes.titley` is also None, y is\n    determined automatically to avoid decorators on the Axes.\n\npad : float, default: :rc:`axes.titlepad`\n    The offset of the title from the top of the Axes, in points.\n\nReturns\n-------\n`.Text`\n    The matplotlib text instance representing the title\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    Other keyword arguments are text properties, see `.Text` for a list\n    of valid text properties.", "parameters": {"type": "object", "properties": {"label": {}, "fontdict": {"type": "NoneType", "default": null}, "loc": {"type": "NoneType", "default": null}, "pad": {"type": "NoneType", "default": null}, "y": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/92", "data": {"name": "matplotlib.pyplot.subplots[1].scatter", "type": "method", "signature": "(x, y, s=None, c=None, marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=None, linewidths=None, *, edgecolors=None, plotnonfinite=False, data=None, **kwargs)", "description": "A scatter plot of *y* vs. *x* with varying marker size and/or color.\n\nParameters\n----------\nx, y : float or array-like, shape (n, )\n    The data positions.\n\ns : float or array-like, shape (n, ), optional\n    The marker size in points**2 (typographic points are 1/72 in.).\n    Default is ``rcParams['lines.markersize'] ** 2``.\n\n    The linewidth and edgecolor can visually interact with the marker\n    size, and can lead to artifacts if the marker size is smaller than\n    the linewidth.\n\n    If the linewidth is greater than 0 and the edgecolor is anything\n    but *'none'*, then the effective size of the marker will be\n    increased by half the linewidth because the stroke will be centered\n    on the edge of the shape.\n\n    To eliminate the marker edge either set *linewidth=0* or\n    *edgecolor='none'*.\n\nc : array-like or list of colors or color, optional\n    The marker colors. Possible values:\n\n    - A scalar or sequence of n numbers to be mapped to colors using\n      *cmap* and *norm*.\n    - A 2D array in which the rows are RGB or RGBA.\n    - A sequence of colors of length n.\n    - A single color format string.\n\n    Note that *c* should not be a single numeric RGB or RGBA sequence\n    because that is indistinguishable from an array of values to be\n    colormapped. If you want to specify the same RGB or RGBA value for\n    all points, use a 2D array with a single row.  Otherwise,\n    value-matching will have precedence in case of a size matching with\n    *x* and *y*.\n\n    If you wish to specify a single color for all points\n    prefer the *color* keyword argument.\n\n    Defaults to `None`. In that case the marker color is determined\n    by the value of *color*, *facecolor* or *facecolors*. In case\n    those are not specified or `None`, the marker color is determined\n    by the next color of the ``Axes``' current \"shape and fill\" color\n    cycle. This cycle defaults to :rc:`axes.prop_cycle`.\n\nmarker : `~.markers.MarkerStyle`, default: :rc:`scatter.marker`\n    The marker style. *marker* can be either an instance of the class\n    or the text shorthand for a particular marker.\n    See :mod:`matplotlib.markers` for more information about marker\n    styles.\n\ncmap : str or `~matplotlib.colors.Colormap`, default: :rc:`image.cmap`\n    The Colormap instance or registered colormap name used to map scalar data\n    to colors.\n\n    This parameter is ignored if *c* is RGB(A).\n\nnorm : str or `~matplotlib.colors.Normalize`, optional\n    The normalization method used to scale scalar data to the [0, 1] range\n    before mapping to colors using *cmap*. By default, a linear scaling is\n    used, mapping the lowest value to 0 and the highest to 1.\n\n    If given, this can be one of the following:\n\n    - An instance of `.Normalize` or one of its subclasses\n      (see :ref:`colormapnorms`).\n    - A scale name, i.e. one of \"linear\", \"log\", \"symlog\", \"logit\", etc.  For a\n      list of available scales, call `matplotlib.scale.get_scale_names()`.\n      In that case, a suitable `.Normalize` subclass is dynamically generated\n      and instantiated.\n\n    This parameter is ignored if *c* is RGB(A).\n\nvmin, vmax : float, optional\n    When using scalar data and no explicit *norm*, *vmin* and *vmax* define\n    the data range that the colormap covers. By default, the colormap covers\n    the complete value range of the supplied data. It is an error to use\n    *vmin*/*vmax* when a *norm* instance is given (but using a `str` *norm*\n    name together with *vmin*/*vmax* is acceptable).\n\n    This parameter is ignored if *c* is RGB(A).\n\nalpha : float, default: None\n    The alpha blending value, between 0 (transparent) and 1 (opaque).\n\nlinewidths : float or array-like, default: :rc:`lines.linewidth`\n    The linewidth of the marker edges. Note: The default *edgecolors*\n    is 'face'. You may want to change this as well.\n\nedgecolors : {'face', 'none', *None*} or color or sequence of color, default: :rc:`scatter.edgecolors`\n    The edge color of the marker. Possible values:\n\n    - 'face': The edge color will always be the same as the face color.\n    - 'none': No patch boundary will be drawn.\n    - A color or sequence of colors.\n\n    For non-filled markers, *edgecolors* is ignored. Instead, the color\n    is determined like with 'face', i.e. from *c*, *colors*, or\n    *facecolors*.\n\nplotnonfinite : bool, default: False\n    Whether to plot points with nonfinite *c* (i.e. ``inf``, ``-inf``\n    or ``nan``). If ``True`` the points are drawn with the *bad*\n    colormap color (see `.Colormap.set_bad`).\n\nReturns\n-------\n`~matplotlib.collections.PathCollection`\n\nOther Parameters\n----------------\ndata : indexable object, optional\n    If given, the following parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` (unless this raises an exception):\n\n    *x*, *y*, *s*, *linewidths*, *edgecolors*, *c*, *facecolor*, *facecolors*, *color*\n**kwargs : `~matplotlib.collections.Collection` properties\n\nSee Also\n--------\nplot : To plot scatter plots when markers are identical in size and\n    color.\n\nNotes\n-----\n* The `.plot` function will be faster for scatterplots where markers\n  don't vary in size or color.\n\n* Any or all of *x*, *y*, *s*, and *c* may be masked arrays, in which\n  case all masks will be combined and only unmasked points will be\n  plotted.\n\n* Fundamentally, scatter works with 1D arrays; *x*, *y*, *s*, and *c*\n  may be input as N-D arrays, but within scatter they will be\n  flattened. The exception is *c*, which will be flattened only if its\n  size matches the size of *x* and *y*.", "parameters": {"type": "object", "properties": {"x": {}, "y": {}, "s": {"type": "NoneType", "default": null}, "c": {"type": "NoneType", "default": null}, "marker": {"type": "NoneType", "default": null}, "cmap": {"type": "NoneType", "default": null}, "norm": {"type": "NoneType", "default": null}, "vmin": {"type": "NoneType", "default": null}, "vmax": {"type": "NoneType", "default": null}, "alpha": {"type": "NoneType", "default": null}, "linewidths": {"type": "NoneType", "default": null}, "edgecolors": {"type": "NoneType", "default": null}, "plotnonfinite": {"type": "bool", "default": false}, "data": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/92", "data": {"name": "sklearn.cluster.KMeans", "type": "class", "signature": "(n_clusters=8)", "description": "K-Means clustering.\n\nRead more in the :ref:`User Guide <k_means>`.\n\nParameters\n----------\n\nn_clusters : int, default=8\n    The number of clusters to form as well as the number of\n    centroids to generate.\n\n    For an example of how to choose an optimal value for `n_clusters` refer to\n    :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_silhouette_analysis.py`.\n\ninit : {'k-means++', 'random'}, callable or array-like of shape             (n_clusters, n_features), default='k-means++'\n    Method for initialization:\n\n    * 'k-means++' : selects initial cluster centroids using sampling             based on an empirical probability distribution of the points'             contribution to the overall inertia. This technique speeds up             convergence. The algorithm implemented is \"greedy k-means++\". It             differs from the vanilla k-means++ by making several trials at             each sampling step and choosing the best centroid among them.\n\n    * 'random': choose `n_clusters` observations (rows) at random from         data for the initial centroids.\n\n    * If an array is passed, it should be of shape (n_clusters, n_features)        and gives the initial centers.\n\n    * If a callable is passed, it should take arguments X, n_clusters and a        random state and return an initialization.\n\n    For an example of how to use the different `init` strategy, see the example\n    entitled :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_digits.py`.\n\nn_init : 'auto' or int, default='auto'\n    Number of times the k-means algorithm is run with different centroid\n    seeds. The final results is the best output of `n_init` consecutive runs\n    in terms of inertia. Several runs are recommended for sparse\n    high-dimensional problems (see :ref:`kmeans_sparse_high_dim`).\n\n    When `n_init='auto'`, the number of runs depends on the value of init:\n    10 if using `init='random'` or `init` is a callable;\n    1 if using `init='k-means++'` or `init` is an array-like.\n\n    .. versionadded:: 1.2\n       Added 'auto' option for `n_init`.\n\n    .. versionchanged:: 1.4\n       Default value for `n_init` changed to `'auto'`.\n\nmax_iter : int, default=300\n    Maximum number of iterations of the k-means algorithm for a\n    single run.\n\ntol : float, default=1e-4\n    Relative tolerance with regards to Frobenius norm of the difference\n    in the cluster centers of two consecutive iterations to declare\n    convergence.\n\nverbose : int, default=0\n    Verbosity mode.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for centroid initialization. Use\n    an int to make the randomness deterministic.\n    See :term:`Glossary <random_state>`.\n\ncopy_x : bool, default=True\n    When pre-computing distances it is more numerically accurate to center\n    the data first. If copy_x is True (default), then the original data is\n    not modified. If False, the original data is modified, and put back\n    before the function returns, but small numerical differences may be\n    introduced by subtracting and then adding the data mean. Note that if\n    the original data is not C-contiguous, a copy will be made even if\n    copy_x is False. If the original data is sparse, but not in CSR format,\n    a copy will be made even if copy_x is False.\n\nalgorithm : {\"lloyd\", \"elkan\"}, default=\"lloyd\"\n    K-means algorithm to use. The classical EM-style algorithm is `\"lloyd\"`.\n    The `\"elkan\"` variation can be more efficient on some datasets with\n    well-defined clusters, by using the triangle inequality. However it's\n    more memory intensive due to the allocation of an extra array of shape\n    `(n_samples, n_clusters)`.\n\n    .. versionchanged:: 0.18\n        Added Elkan algorithm\n\n    .. versionchanged:: 1.1\n        Renamed \"full\" to \"lloyd\", and deprecated \"auto\" and \"full\".\n        Changed \"auto\" to use \"lloyd\" instead of \"elkan\".\n\nAttributes\n----------\ncluster_centers_ : ndarray of shape (n_clusters, n_features)\n    Coordinates of cluster centers. If the algorithm stops before fully\n    converging (see ``tol`` and ``max_iter``), these will not be\n    consistent with ``labels_``.\n\nlabels_ : ndarray of shape (n_samples,)\n    Labels of each point\n\ninertia_ : float\n    Sum of squared distances of samples to their closest cluster center,\n    weighted by the sample weights if provided.\n\nn_iter_ : int\n    Number of iterations run.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nMiniBatchKMeans : Alternative online implementation that does incremental\n    updates of the centers positions using mini-batches.\n    For large scale learning (say n_samples > 10k) MiniBatchKMeans is\n    probably much faster than the default batch implementation.\n\nNotes\n-----\nThe k-means problem is solved using either Lloyd's or Elkan's algorithm.\n\nThe average complexity is given by O(k n T), where n is the number of\nsamples and T is the number of iteration.\n\nThe worst case complexity is given by O(n^(k+2/p)) with\nn = n_samples, p = n_features.\nRefer to :doi:`\"How slow is the k-means method?\" D. Arthur and S. Vassilvitskii -\nSoCG2006.<10.1145/1137856.1137880>` for more details.\n\nIn practice, the k-means algorithm is very fast (one of the fastest\nclustering algorithms available), but it falls in local minima. That's why\nit can be useful to restart it several times.\n\nIf the algorithm stops before fully converging (because of ``tol`` or\n``max_iter``), ``labels_`` and ``cluster_centers_`` will not be consistent,\ni.e. the ``cluster_centers_`` will not be the means of the points in each\ncluster. Also, the estimator will reassign ``labels_`` after the last\niteration to make ``labels_`` consistent with ``predict`` on the training\nset.\n\nExamples\n--------\n\n>>> from sklearn.cluster import KMeans\n>>> import numpy as np\n>>> X = np.array([[1, 2], [1, 4], [1, 0],\n...               [10, 2], [10, 4], [10, 0]])\n>>> kmeans = KMeans(n_clusters=2, random_state=0, n_init=\"auto\").fit(X)\n>>> kmeans.labels_\narray([1, 1, 1, 0, 0, 0], dtype=int32)\n>>> kmeans.predict([[0, 0], [12, 3]])\narray([1, 0], dtype=int32)\n>>> kmeans.cluster_centers_\narray([[10.,  2.],\n       [ 1.,  2.]])\n\nFor a more detailed example of K-Means using the iris dataset see\n:ref:`sphx_glr_auto_examples_cluster_plot_cluster_iris.py`.\n\nFor examples of common problems with K-Means and how to address them see\n:ref:`sphx_glr_auto_examples_cluster_plot_kmeans_assumptions.py`.\n\nFor an example of how to use K-Means to perform color quantization see\n:ref:`sphx_glr_auto_examples_cluster_plot_color_quantization.py`.\n\nFor a demonstration of how K-Means can be used to cluster text documents see\n:ref:`sphx_glr_auto_examples_text_plot_document_clustering.py`.\n\nFor a comparison between K-Means and MiniBatchKMeans refer to example\n:ref:`sphx_glr_auto_examples_cluster_plot_mini_batch_kmeans.py`.", "parameters": {"type": "object", "properties": {"n_clusters": {"type": "integer", "default": 8}}}}}
{"task_id": "BigCodeBench/92", "data": {"name": "sklearn.cluster.KMeans.cluster_centers_", "type": "class", "signature": "(n_clusters=8, *, init='k-means++', n_init='auto', max_iter=300, tol=0.0001, verbose=0, random_state=None, copy_x=True, algorithm='lloyd')", "description": "K-Means clustering.\n\nRead more in the :ref:`User Guide <k_means>`.\n\nParameters\n----------\n\nn_clusters : int, default=8\n    The number of clusters to form as well as the number of\n    centroids to generate.\n\n    For an example of how to choose an optimal value for `n_clusters` refer to\n    :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_silhouette_analysis.py`.\n\ninit : {'k-means++', 'random'}, callable or array-like of shape             (n_clusters, n_features), default='k-means++'\n    Method for initialization:\n\n    * 'k-means++' : selects initial cluster centroids using sampling             based on an empirical probability distribution of the points'             contribution to the overall inertia. This technique speeds up             convergence. The algorithm implemented is \"greedy k-means++\". It             differs from the vanilla k-means++ by making several trials at             each sampling step and choosing the best centroid among them.\n\n    * 'random': choose `n_clusters` observations (rows) at random from         data for the initial centroids.\n\n    * If an array is passed, it should be of shape (n_clusters, n_features)        and gives the initial centers.\n\n    * If a callable is passed, it should take arguments X, n_clusters and a        random state and return an initialization.\n\n    For an example of how to use the different `init` strategy, see the example\n    entitled :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_digits.py`.\n\nn_init : 'auto' or int, default='auto'\n    Number of times the k-means algorithm is run with different centroid\n    seeds. The final results is the best output of `n_init` consecutive runs\n    in terms of inertia. Several runs are recommended for sparse\n    high-dimensional problems (see :ref:`kmeans_sparse_high_dim`).\n\n    When `n_init='auto'`, the number of runs depends on the value of init:\n    10 if using `init='random'` or `init` is a callable;\n    1 if using `init='k-means++'` or `init` is an array-like.\n\n    .. versionadded:: 1.2\n       Added 'auto' option for `n_init`.\n\n    .. versionchanged:: 1.4\n       Default value for `n_init` changed to `'auto'`.\n\nmax_iter : int, default=300\n    Maximum number of iterations of the k-means algorithm for a\n    single run.\n\ntol : float, default=1e-4\n    Relative tolerance with regards to Frobenius norm of the difference\n    in the cluster centers of two consecutive iterations to declare\n    convergence.\n\nverbose : int, default=0\n    Verbosity mode.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for centroid initialization. Use\n    an int to make the randomness deterministic.\n    See :term:`Glossary <random_state>`.\n\ncopy_x : bool, default=True\n    When pre-computing distances it is more numerically accurate to center\n    the data first. If copy_x is True (default), then the original data is\n    not modified. If False, the original data is modified, and put back\n    before the function returns, but small numerical differences may be\n    introduced by subtracting and then adding the data mean. Note that if\n    the original data is not C-contiguous, a copy will be made even if\n    copy_x is False. If the original data is sparse, but not in CSR format,\n    a copy will be made even if copy_x is False.\n\nalgorithm : {\"lloyd\", \"elkan\"}, default=\"lloyd\"\n    K-means algorithm to use. The classical EM-style algorithm is `\"lloyd\"`.\n    The `\"elkan\"` variation can be more efficient on some datasets with\n    well-defined clusters, by using the triangle inequality. However it's\n    more memory intensive due to the allocation of an extra array of shape\n    `(n_samples, n_clusters)`.\n\n    .. versionchanged:: 0.18\n        Added Elkan algorithm\n\n    .. versionchanged:: 1.1\n        Renamed \"full\" to \"lloyd\", and deprecated \"auto\" and \"full\".\n        Changed \"auto\" to use \"lloyd\" instead of \"elkan\".\n\nAttributes\n----------\ncluster_centers_ : ndarray of shape (n_clusters, n_features)\n    Coordinates of cluster centers. If the algorithm stops before fully\n    converging (see ``tol`` and ``max_iter``), these will not be\n    consistent with ``labels_``.\n\nlabels_ : ndarray of shape (n_samples,)\n    Labels of each point\n\ninertia_ : float\n    Sum of squared distances of samples to their closest cluster center,\n    weighted by the sample weights if provided.\n\nn_iter_ : int\n    Number of iterations run.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nMiniBatchKMeans : Alternative online implementation that does incremental\n    updates of the centers positions using mini-batches.\n    For large scale learning (say n_samples > 10k) MiniBatchKMeans is\n    probably much faster than the default batch implementation.\n\nNotes\n-----\nThe k-means problem is solved using either Lloyd's or Elkan's algorithm.\n\nThe average complexity is given by O(k n T), where n is the number of\nsamples and T is the number of iteration.\n\nThe worst case complexity is given by O(n^(k+2/p)) with\nn = n_samples, p = n_features.\nRefer to :doi:`\"How slow is the k-means method?\" D. Arthur and S. Vassilvitskii -\nSoCG2006.<10.1145/1137856.1137880>` for more details.\n\nIn practice, the k-means algorithm is very fast (one of the fastest\nclustering algorithms available), but it falls in local minima. That's why\nit can be useful to restart it several times.\n\nIf the algorithm stops before fully converging (because of ``tol`` or\n``max_iter``), ``labels_`` and ``cluster_centers_`` will not be consistent,\ni.e. the ``cluster_centers_`` will not be the means of the points in each\ncluster. Also, the estimator will reassign ``labels_`` after the last\niteration to make ``labels_`` consistent with ``predict`` on the training\nset.\n\nExamples\n--------\n\n>>> from sklearn.cluster import KMeans\n>>> import numpy as np\n>>> X = np.array([[1, 2], [1, 4], [1, 0],\n...               [10, 2], [10, 4], [10, 0]])\n>>> kmeans = KMeans(n_clusters=2, random_state=0, n_init=\"auto\").fit(X)\n>>> kmeans.labels_\narray([1, 1, 1, 0, 0, 0], dtype=int32)\n>>> kmeans.predict([[0, 0], [12, 3]])\narray([1, 0], dtype=int32)\n>>> kmeans.cluster_centers_\narray([[10.,  2.],\n       [ 1.,  2.]])\n\nFor a more detailed example of K-Means using the iris dataset see\n:ref:`sphx_glr_auto_examples_cluster_plot_cluster_iris.py`.\n\nFor examples of common problems with K-Means and how to address them see\n:ref:`sphx_glr_auto_examples_cluster_plot_kmeans_assumptions.py`.\n\nFor an example of how to use K-Means to perform color quantization see\n:ref:`sphx_glr_auto_examples_cluster_plot_color_quantization.py`.\n\nFor a demonstration of how K-Means can be used to cluster text documents see\n:ref:`sphx_glr_auto_examples_text_plot_document_clustering.py`.\n\nFor a comparison between K-Means and MiniBatchKMeans refer to example\n:ref:`sphx_glr_auto_examples_cluster_plot_mini_batch_kmeans.py`.", "parameters": {"type": "object", "properties": {"n_clusters": {"type": "integer", "default": 8}, "init": {"type": "str", "default": "k-means++"}, "n_init": {"type": "str", "default": "auto"}, "max_iter": {"type": "integer", "default": 300}, "tol": {"type": "float", "default": 0.0001}, "verbose": {"type": "integer", "default": 0}, "random_state": {"type": "NoneType", "default": null}, "copy_x": {"type": "bool", "default": true}, "algorithm": {"type": "str", "default": "lloyd"}}}}}
{"task_id": "BigCodeBench/92", "data": {"name": "sklearn.cluster.KMeans.fit_predict", "type": "callable", "signature": "(self, X)", "description": "Compute cluster centers and predict cluster index for each sample.\n\nConvenience method; equivalent to calling fit(X) followed by\npredict(X).\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    New data to transform.\n\ny : Ignored\n    Not used, present here for API consistency by convention.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    The weights for each observation in X. If None, all observations\n    are assigned equal weight.\n\nReturns\n-------\nlabels : ndarray of shape (n_samples,)\n    Index of the cluster each sample belongs to.", "parameters": {"type": "object", "properties": {"X": {}}}}}
{"task_id": "BigCodeBench/93", "data": {"name": "matplotlib.pyplot.subplots", "type": "class", "signature": "()", "description": "Create a figure and a set of subplots.\n\nThis utility wrapper makes it convenient to create common layouts of\nsubplots, including the enclosing figure object, in a single call.\n\nParameters\n----------\nnrows, ncols : int, default: 1\n    Number of rows/columns of the subplot grid.\n\nsharex, sharey : bool or {'none', 'all', 'row', 'col'}, default: False\n    Controls sharing of properties among x (*sharex*) or y (*sharey*)\n    axes:\n\n    - True or 'all': x- or y-axis will be shared among all subplots.\n    - False or 'none': each subplot x- or y-axis will be independent.\n    - 'row': each subplot row will share an x- or y-axis.\n    - 'col': each subplot column will share an x- or y-axis.\n\n    When subplots have a shared x-axis along a column, only the x tick\n    labels of the bottom subplot are created. Similarly, when subplots\n    have a shared y-axis along a row, only the y tick labels of the first\n    column subplot are created. To later turn other subplots' ticklabels\n    on, use `~matplotlib.axes.Axes.tick_params`.\n\n    When subplots have a shared axis that has units, calling\n    `~matplotlib.axis.Axis.set_units` will update each axis with the\n    new units.\n\nsqueeze : bool, default: True\n    - If True, extra dimensions are squeezed out from the returned\n      array of `~matplotlib.axes.Axes`:\n\n      - if only one subplot is constructed (nrows=ncols=1), the\n        resulting single Axes object is returned as a scalar.\n      - for Nx1 or 1xM subplots, the returned object is a 1D numpy\n        object array of Axes objects.\n      - for NxM, subplots with N>1 and M>1 are returned as a 2D array.\n\n    - If False, no squeezing at all is done: the returned Axes object is\n      always a 2D array containing Axes instances, even if it ends up\n      being 1x1.\n\nwidth_ratios : array-like of length *ncols*, optional\n    Defines the relative widths of the columns. Each column gets a\n    relative width of ``width_ratios[i] / sum(width_ratios)``.\n    If not given, all columns will have the same width.  Equivalent\n    to ``gridspec_kw={'width_ratios': [...]}``.\n\nheight_ratios : array-like of length *nrows*, optional\n    Defines the relative heights of the rows. Each row gets a\n    relative height of ``height_ratios[i] / sum(height_ratios)``.\n    If not given, all rows will have the same height. Convenience\n    for ``gridspec_kw={'height_ratios': [...]}``.\n\nsubplot_kw : dict, optional\n    Dict with keywords passed to the\n    `~matplotlib.figure.Figure.add_subplot` call used to create each\n    subplot.\n\ngridspec_kw : dict, optional\n    Dict with keywords passed to the `~matplotlib.gridspec.GridSpec`\n    constructor used to create the grid the subplots are placed on.\n\n**fig_kw\n    All additional keyword arguments are passed to the\n    `.pyplot.figure` call.\n\nReturns\n-------\nfig : `.Figure`\n\nax : `~matplotlib.axes.Axes` or array of Axes\n    *ax* can be either a single `~.axes.Axes` object, or an array of Axes\n    objects if more than one subplot was created.  The dimensions of the\n    resulting array can be controlled with the squeeze keyword, see above.\n\n    Typical idioms for handling the return value are::\n\n        # using the variable ax for single a Axes\n        fig, ax = plt.subplots()\n\n        # using the variable axs for multiple Axes\n        fig, axs = plt.subplots(2, 2)\n\n        # using tuple unpacking for multiple Axes\n        fig, (ax1, ax2) = plt.subplots(1, 2)\n        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n\n    The names ``ax`` and pluralized ``axs`` are preferred over ``axes``\n    because for the latter it's not clear if it refers to a single\n    `~.axes.Axes` instance or a collection of these.\n\nSee Also\n--------\n.pyplot.figure\n.pyplot.subplot\n.pyplot.axes\n.Figure.subplots\n.Figure.add_subplot\n\nExamples\n--------\n::\n\n    # First create some toy data:\n    x = np.linspace(0, 2*np.pi, 400)\n    y = np.sin(x**2)\n\n    # Create just a figure and only one subplot\n    fig, ax = plt.subplots()\n    ax.plot(x, y)\n    ax.set_title('Simple plot')\n\n    # Create two subplots and unpack the output array immediately\n    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n    ax1.plot(x, y)\n    ax1.set_title('Sharing Y axis')\n    ax2.scatter(x, y)\n\n    # Create four polar axes and access them through the returned array\n    fig, axs = plt.subplots(2, 2, subplot_kw=dict(projection=\"polar\"))\n    axs[0, 0].plot(x, y)\n    axs[1, 1].scatter(x, y)\n\n    # Share a X axis with each column of subplots\n    plt.subplots(2, 2, sharex='col')\n\n    # Share a Y axis with each row of subplots\n    plt.subplots(2, 2, sharey='row')\n\n    # Share both X and Y axes with all subplots\n    plt.subplots(2, 2, sharex='all', sharey='all')\n\n    # Note that this is the same as\n    plt.subplots(2, 2, sharex=True, sharey=True)\n\n    # Create figure number 10 with a single subplot\n    # and clears it if it already exists.\n    fig, ax = plt.subplots(num=10, clear=True)", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/93", "data": {"name": "matplotlib.pyplot.subplots[1].scatter", "type": "method", "signature": "(x, y, s=None, c=None, marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=None, linewidths=None, *, edgecolors=None, plotnonfinite=False, data=None, **kwargs)", "description": "A scatter plot of *y* vs. *x* with varying marker size and/or color.\n\nParameters\n----------\nx, y : float or array-like, shape (n, )\n    The data positions.\n\ns : float or array-like, shape (n, ), optional\n    The marker size in points**2 (typographic points are 1/72 in.).\n    Default is ``rcParams['lines.markersize'] ** 2``.\n\n    The linewidth and edgecolor can visually interact with the marker\n    size, and can lead to artifacts if the marker size is smaller than\n    the linewidth.\n\n    If the linewidth is greater than 0 and the edgecolor is anything\n    but *'none'*, then the effective size of the marker will be\n    increased by half the linewidth because the stroke will be centered\n    on the edge of the shape.\n\n    To eliminate the marker edge either set *linewidth=0* or\n    *edgecolor='none'*.\n\nc : array-like or list of colors or color, optional\n    The marker colors. Possible values:\n\n    - A scalar or sequence of n numbers to be mapped to colors using\n      *cmap* and *norm*.\n    - A 2D array in which the rows are RGB or RGBA.\n    - A sequence of colors of length n.\n    - A single color format string.\n\n    Note that *c* should not be a single numeric RGB or RGBA sequence\n    because that is indistinguishable from an array of values to be\n    colormapped. If you want to specify the same RGB or RGBA value for\n    all points, use a 2D array with a single row.  Otherwise,\n    value-matching will have precedence in case of a size matching with\n    *x* and *y*.\n\n    If you wish to specify a single color for all points\n    prefer the *color* keyword argument.\n\n    Defaults to `None`. In that case the marker color is determined\n    by the value of *color*, *facecolor* or *facecolors*. In case\n    those are not specified or `None`, the marker color is determined\n    by the next color of the ``Axes``' current \"shape and fill\" color\n    cycle. This cycle defaults to :rc:`axes.prop_cycle`.\n\nmarker : `~.markers.MarkerStyle`, default: :rc:`scatter.marker`\n    The marker style. *marker* can be either an instance of the class\n    or the text shorthand for a particular marker.\n    See :mod:`matplotlib.markers` for more information about marker\n    styles.\n\ncmap : str or `~matplotlib.colors.Colormap`, default: :rc:`image.cmap`\n    The Colormap instance or registered colormap name used to map scalar data\n    to colors.\n\n    This parameter is ignored if *c* is RGB(A).\n\nnorm : str or `~matplotlib.colors.Normalize`, optional\n    The normalization method used to scale scalar data to the [0, 1] range\n    before mapping to colors using *cmap*. By default, a linear scaling is\n    used, mapping the lowest value to 0 and the highest to 1.\n\n    If given, this can be one of the following:\n\n    - An instance of `.Normalize` or one of its subclasses\n      (see :ref:`colormapnorms`).\n    - A scale name, i.e. one of \"linear\", \"log\", \"symlog\", \"logit\", etc.  For a\n      list of available scales, call `matplotlib.scale.get_scale_names()`.\n      In that case, a suitable `.Normalize` subclass is dynamically generated\n      and instantiated.\n\n    This parameter is ignored if *c* is RGB(A).\n\nvmin, vmax : float, optional\n    When using scalar data and no explicit *norm*, *vmin* and *vmax* define\n    the data range that the colormap covers. By default, the colormap covers\n    the complete value range of the supplied data. It is an error to use\n    *vmin*/*vmax* when a *norm* instance is given (but using a `str` *norm*\n    name together with *vmin*/*vmax* is acceptable).\n\n    This parameter is ignored if *c* is RGB(A).\n\nalpha : float, default: None\n    The alpha blending value, between 0 (transparent) and 1 (opaque).\n\nlinewidths : float or array-like, default: :rc:`lines.linewidth`\n    The linewidth of the marker edges. Note: The default *edgecolors*\n    is 'face'. You may want to change this as well.\n\nedgecolors : {'face', 'none', *None*} or color or sequence of color, default: :rc:`scatter.edgecolors`\n    The edge color of the marker. Possible values:\n\n    - 'face': The edge color will always be the same as the face color.\n    - 'none': No patch boundary will be drawn.\n    - A color or sequence of colors.\n\n    For non-filled markers, *edgecolors* is ignored. Instead, the color\n    is determined like with 'face', i.e. from *c*, *colors*, or\n    *facecolors*.\n\nplotnonfinite : bool, default: False\n    Whether to plot points with nonfinite *c* (i.e. ``inf``, ``-inf``\n    or ``nan``). If ``True`` the points are drawn with the *bad*\n    colormap color (see `.Colormap.set_bad`).\n\nReturns\n-------\n`~matplotlib.collections.PathCollection`\n\nOther Parameters\n----------------\ndata : indexable object, optional\n    If given, the following parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` (unless this raises an exception):\n\n    *x*, *y*, *s*, *linewidths*, *edgecolors*, *c*, *facecolor*, *facecolors*, *color*\n**kwargs : `~matplotlib.collections.Collection` properties\n\nSee Also\n--------\nplot : To plot scatter plots when markers are identical in size and\n    color.\n\nNotes\n-----\n* The `.plot` function will be faster for scatterplots where markers\n  don't vary in size or color.\n\n* Any or all of *x*, *y*, *s*, and *c* may be masked arrays, in which\n  case all masks will be combined and only unmasked points will be\n  plotted.\n\n* Fundamentally, scatter works with 1D arrays; *x*, *y*, *s*, and *c*\n  may be input as N-D arrays, but within scatter they will be\n  flattened. The exception is *c*, which will be flattened only if its\n  size matches the size of *x* and *y*.", "parameters": {"type": "object", "properties": {"x": {}, "y": {}, "s": {"type": "NoneType", "default": null}, "c": {"type": "NoneType", "default": null}, "marker": {"type": "NoneType", "default": null}, "cmap": {"type": "NoneType", "default": null}, "norm": {"type": "NoneType", "default": null}, "vmin": {"type": "NoneType", "default": null}, "vmax": {"type": "NoneType", "default": null}, "alpha": {"type": "NoneType", "default": null}, "linewidths": {"type": "NoneType", "default": null}, "edgecolors": {"type": "NoneType", "default": null}, "plotnonfinite": {"type": "bool", "default": false}, "data": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/93", "data": {"name": "numpy.random.seed", "type": "callable", "signature": "(*args, **kwargs)", "description": "seed(self, seed=None)\n\nReseed a legacy MT19937 BitGenerator\n\nNotes\n-----\nThis is a convenience, legacy function.\n\nThe best practice is to **not** reseed a BitGenerator, rather to\nrecreate a new one. This method is here for legacy reasons.\nThis example demonstrates best practice.\n\n>>> from numpy.random import MT19937\n>>> from numpy.random import RandomState, SeedSequence\n>>> rs = RandomState(MT19937(SeedSequence(123456789)))\n# Later, you want to restart the stream\n>>> rs = RandomState(MT19937(SeedSequence(987654321)))", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/93", "data": {"name": "sklearn.decomposition.PCA", "type": "class", "signature": "(n_components=None)", "description": "Principal component analysis (PCA).\n\nLinear dimensionality reduction using Singular Value Decomposition of the\ndata to project it to a lower dimensional space. The input data is centered\nbut not scaled for each feature before applying the SVD.\n\nIt uses the LAPACK implementation of the full SVD or a randomized truncated\nSVD by the method of Halko et al. 2009, depending on the shape of the input\ndata and the number of components to extract.\n\nIt can also use the scipy.sparse.linalg ARPACK implementation of the\ntruncated SVD.\n\nNotice that this class does not support sparse input. See\n:class:`TruncatedSVD` for an alternative with sparse data.\n\nFor a usage example, see\n:ref:`sphx_glr_auto_examples_decomposition_plot_pca_iris.py`\n\nRead more in the :ref:`User Guide <PCA>`.\n\nParameters\n----------\nn_components : int, float or 'mle', default=None\n    Number of components to keep.\n    if n_components is not set all components are kept::\n\n        n_components == min(n_samples, n_features)\n\n    If ``n_components == 'mle'`` and ``svd_solver == 'full'``, Minka's\n    MLE is used to guess the dimension. Use of ``n_components == 'mle'``\n    will interpret ``svd_solver == 'auto'`` as ``svd_solver == 'full'``.\n\n    If ``0 < n_components < 1`` and ``svd_solver == 'full'``, select the\n    number of components such that the amount of variance that needs to be\n    explained is greater than the percentage specified by n_components.\n\n    If ``svd_solver == 'arpack'``, the number of components must be\n    strictly less than the minimum of n_features and n_samples.\n\n    Hence, the None case results in::\n\n        n_components == min(n_samples, n_features) - 1\n\ncopy : bool, default=True\n    If False, data passed to fit are overwritten and running\n    fit(X).transform(X) will not yield the expected results,\n    use fit_transform(X) instead.\n\nwhiten : bool, default=False\n    When True (False by default) the `components_` vectors are multiplied\n    by the square root of n_samples and then divided by the singular values\n    to ensure uncorrelated outputs with unit component-wise variances.\n\n    Whitening will remove some information from the transformed signal\n    (the relative variance scales of the components) but can sometime\n    improve the predictive accuracy of the downstream estimators by\n    making their data respect some hard-wired assumptions.\n\nsvd_solver : {'auto', 'full', 'arpack', 'randomized'}, default='auto'\n    If auto :\n        The solver is selected by a default policy based on `X.shape` and\n        `n_components`: if the input data is larger than 500x500 and the\n        number of components to extract is lower than 80% of the smallest\n        dimension of the data, then the more efficient 'randomized'\n        method is enabled. Otherwise the exact full SVD is computed and\n        optionally truncated afterwards.\n    If full :\n        run exact full SVD calling the standard LAPACK solver via\n        `scipy.linalg.svd` and select the components by postprocessing\n    If arpack :\n        run SVD truncated to n_components calling ARPACK solver via\n        `scipy.sparse.linalg.svds`. It requires strictly\n        0 < n_components < min(X.shape)\n    If randomized :\n        run randomized SVD by the method of Halko et al.\n\n    .. versionadded:: 0.18.0\n\ntol : float, default=0.0\n    Tolerance for singular values computed by svd_solver == 'arpack'.\n    Must be of range [0.0, infinity).\n\n    .. versionadded:: 0.18.0\n\niterated_power : int or 'auto', default='auto'\n    Number of iterations for the power method computed by\n    svd_solver == 'randomized'.\n    Must be of range [0, infinity).\n\n    .. versionadded:: 0.18.0\n\nn_oversamples : int, default=10\n    This parameter is only relevant when `svd_solver=\"randomized\"`.\n    It corresponds to the additional number of random vectors to sample the\n    range of `X` so as to ensure proper conditioning. See\n    :func:`~sklearn.utils.extmath.randomized_svd` for more details.\n\n    .. versionadded:: 1.1\n\npower_iteration_normalizer : {'auto', 'QR', 'LU', 'none'}, default='auto'\n    Power iteration normalizer for randomized SVD solver.\n    Not used by ARPACK. See :func:`~sklearn.utils.extmath.randomized_svd`\n    for more details.\n\n    .. versionadded:: 1.1\n\nrandom_state : int, RandomState instance or None, default=None\n    Used when the 'arpack' or 'randomized' solvers are used. Pass an int\n    for reproducible results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\n    .. versionadded:: 0.18.0\n\nAttributes\n----------\ncomponents_ : ndarray of shape (n_components, n_features)\n    Principal axes in feature space, representing the directions of\n    maximum variance in the data. Equivalently, the right singular\n    vectors of the centered input data, parallel to its eigenvectors.\n    The components are sorted by decreasing ``explained_variance_``.\n\nexplained_variance_ : ndarray of shape (n_components,)\n    The amount of variance explained by each of the selected components.\n    The variance estimation uses `n_samples - 1` degrees of freedom.\n\n    Equal to n_components largest eigenvalues\n    of the covariance matrix of X.\n\n    .. versionadded:: 0.18\n\nexplained_variance_ratio_ : ndarray of shape (n_components,)\n    Percentage of variance explained by each of the selected components.\n\n    If ``n_components`` is not set then all components are stored and the\n    sum of the ratios is equal to 1.0.\n\nsingular_values_ : ndarray of shape (n_components,)\n    The singular values corresponding to each of the selected components.\n    The singular values are equal to the 2-norms of the ``n_components``\n    variables in the lower-dimensional space.\n\n    .. versionadded:: 0.19\n\nmean_ : ndarray of shape (n_features,)\n    Per-feature empirical mean, estimated from the training set.\n\n    Equal to `X.mean(axis=0)`.\n\nn_components_ : int\n    The estimated number of components. When n_components is set\n    to 'mle' or a number between 0 and 1 (with svd_solver == 'full') this\n    number is estimated from input data. Otherwise it equals the parameter\n    n_components, or the lesser value of n_features and n_samples\n    if n_components is None.\n\nn_samples_ : int\n    Number of samples in the training data.\n\nnoise_variance_ : float\n    The estimated noise covariance following the Probabilistic PCA model\n    from Tipping and Bishop 1999. See \"Pattern Recognition and\n    Machine Learning\" by C. Bishop, 12.2.1 p. 574 or\n    http://www.miketipping.com/papers/met-mppca.pdf. It is required to\n    compute the estimated data covariance and score samples.\n\n    Equal to the average of (min(n_features, n_samples) - n_components)\n    smallest eigenvalues of the covariance matrix of X.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nKernelPCA : Kernel Principal Component Analysis.\nSparsePCA : Sparse Principal Component Analysis.\nTruncatedSVD : Dimensionality reduction using truncated SVD.\nIncrementalPCA : Incremental Principal Component Analysis.\n\nReferences\n----------\nFor n_components == 'mle', this class uses the method from:\n`Minka, T. P.. \"Automatic choice of dimensionality for PCA\".\nIn NIPS, pp. 598-604 <https://tminka.github.io/papers/pca/minka-pca.pdf>`_\n\nImplements the probabilistic PCA model from:\n`Tipping, M. E., and Bishop, C. M. (1999). \"Probabilistic principal\ncomponent analysis\". Journal of the Royal Statistical Society:\nSeries B (Statistical Methodology), 61(3), 611-622.\n<http://www.miketipping.com/papers/met-mppca.pdf>`_\nvia the score and score_samples methods.\n\nFor svd_solver == 'arpack', refer to `scipy.sparse.linalg.svds`.\n\nFor svd_solver == 'randomized', see:\n:doi:`Halko, N., Martinsson, P. G., and Tropp, J. A. (2011).\n\"Finding structure with randomness: Probabilistic algorithms for\nconstructing approximate matrix decompositions\".\nSIAM review, 53(2), 217-288.\n<10.1137/090771806>`\nand also\n:doi:`Martinsson, P. G., Rokhlin, V., and Tygert, M. (2011).\n\"A randomized algorithm for the decomposition of matrices\".\nApplied and Computational Harmonic Analysis, 30(1), 47-68.\n<10.1016/j.acha.2010.02.003>`\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.decomposition import PCA\n>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n>>> pca = PCA(n_components=2)\n>>> pca.fit(X)\nPCA(n_components=2)\n>>> print(pca.explained_variance_ratio_)\n[0.9924... 0.0075...]\n>>> print(pca.singular_values_)\n[6.30061... 0.54980...]\n\n>>> pca = PCA(n_components=2, svd_solver='full')\n>>> pca.fit(X)\nPCA(n_components=2, svd_solver='full')\n>>> print(pca.explained_variance_ratio_)\n[0.9924... 0.00755...]\n>>> print(pca.singular_values_)\n[6.30061... 0.54980...]\n\n>>> pca = PCA(n_components=1, svd_solver='arpack')\n>>> pca.fit(X)\nPCA(n_components=1, svd_solver='arpack')\n>>> print(pca.explained_variance_ratio_)\n[0.99244...]\n>>> print(pca.singular_values_)\n[6.30061...]", "parameters": {"type": "object", "properties": {"n_components": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/93", "data": {"name": "sklearn.decomposition.PCA.fit_transform", "type": "callable", "signature": "(self, X)", "description": "Fit the model with X and apply the dimensionality reduction on X.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training data, where `n_samples` is the number of samples\n    and `n_features` is the number of features.\n\ny : Ignored\n    Ignored.\n\nReturns\n-------\nX_new : ndarray of shape (n_samples, n_components)\n    Transformed values.\n\nNotes\n-----\nThis method returns a Fortran-ordered array. To convert it to a\nC-ordered array, use 'np.ascontiguousarray'.", "parameters": {"type": "object", "properties": {"X": {}}}}}
{"task_id": "BigCodeBench/99", "data": {"name": "matplotlib.pyplot.rc", "type": "callable", "signature": "(group: 'str', **kwargs) -> 'None)", "description": "Set the current `.rcParams`.  *group* is the grouping for the rc, e.g.,\nfor ``lines.linewidth`` the group is ``lines``, for\n``axes.facecolor``, the group is ``axes``, and so on.  Group may\nalso be a list or tuple of group names, e.g., (*xtick*, *ytick*).\n*kwargs* is a dictionary attribute name/value pairs, e.g.,::\n\n  rc('lines', linewidth=2, color='r')\n\nsets the current `.rcParams` and is equivalent to::\n\n  rcParams['lines.linewidth'] = 2\n  rcParams['lines.color'] = 'r'\n\nThe following aliases are available to save typing for interactive users:\n\n=====   =================\nAlias   Property\n=====   =================\n'lw'    'linewidth'\n'ls'    'linestyle'\n'c'     'color'\n'fc'    'facecolor'\n'ec'    'edgecolor'\n'mew'   'markeredgewidth'\n'aa'    'antialiased'\n=====   =================\n\nThus you could abbreviate the above call as::\n\n      rc('lines', lw=2, c='r')\n\nNote you can use python's kwargs dictionary facility to store\ndictionaries of default parameters.  e.g., you can customize the\nfont rc as follows::\n\n  font = {'family' : 'monospace',\n          'weight' : 'bold',\n          'size'   : 'larger'}\n  rc('font', **font)  # pass in the font dict as kwargs\n\nThis enables you to easily switch between several configurations.  Use\n``matplotlib.style.use('default')`` or :func:`~matplotlib.rcdefaults` to\nrestore the default `.rcParams` after changes.\n\nNotes\n-----\nSimilar functionality is available by using the normal dict interface, i.e.\n``rcParams.update({\"lines.linewidth\": 2, ...})`` (but ``rcParams.update``\ndoes not support abbreviations or grouping).", "parameters": {"type": "object", "properties": {"group": {"type": "str"}}}}}
{"task_id": "BigCodeBench/99", "data": {"name": "seaborn.pairplot", "type": "callable", "signature": "(data, hue=None, vars=None)", "description": "Plot pairwise relationships in a dataset.\n\nBy default, this function will create a grid of Axes such that each numeric\nvariable in ``data`` will by shared across the y-axes across a single row and\nthe x-axes across a single column. The diagonal plots are treated\ndifferently: a univariate distribution plot is drawn to show the marginal\ndistribution of the data in each column.\n\nIt is also possible to show a subset of variables or plot different\nvariables on the rows and columns.\n\nThis is a high-level interface for :class:`PairGrid` that is intended to\nmake it easy to draw a few common styles. You should use :class:`PairGrid`\ndirectly if you need more flexibility.\n\nParameters\n----------\ndata : `pandas.DataFrame`\n    Tidy (long-form) dataframe where each column is a variable and\n    each row is an observation.\nhue : name of variable in ``data``\n    Variable in ``data`` to map plot aspects to different colors.\nhue_order : list of strings\n    Order for the levels of the hue variable in the palette\npalette : dict or seaborn color palette\n    Set of colors for mapping the ``hue`` variable. If a dict, keys\n    should be values  in the ``hue`` variable.\nvars : list of variable names\n    Variables within ``data`` to use, otherwise use every column with\n    a numeric datatype.\n{x, y}_vars : lists of variable names\n    Variables within ``data`` to use separately for the rows and\n    columns of the figure; i.e. to make a non-square plot.\nkind : {'scatter', 'kde', 'hist', 'reg'}\n    Kind of plot to make.\ndiag_kind : {'auto', 'hist', 'kde', None}\n    Kind of plot for the diagonal subplots. If 'auto', choose based on\n    whether or not ``hue`` is used.\nmarkers : single matplotlib marker code or list\n    Either the marker to use for all scatterplot points or a list of markers\n    with a length the same as the number of levels in the hue variable so that\n    differently colored points will also have different scatterplot\n    markers.\nheight : scalar\n    Height (in inches) of each facet.\naspect : scalar\n    Aspect * height gives the width (in inches) of each facet.\ncorner : bool\n    If True, don't add axes to the upper (off-diagonal) triangle of the\n    grid, making this a \"corner\" plot.\ndropna : boolean\n    Drop missing values from the data before plotting.\n{plot, diag, grid}_kws : dicts\n    Dictionaries of keyword arguments. ``plot_kws`` are passed to the\n    bivariate plotting function, ``diag_kws`` are passed to the univariate\n    plotting function, and ``grid_kws`` are passed to the :class:`PairGrid`\n    constructor.\n\nReturns\n-------\ngrid : :class:`PairGrid`\n    Returns the underlying :class:`PairGrid` instance for further tweaking.\n\nSee Also\n--------\nPairGrid : Subplot grid for more flexible plotting of pairwise relationships.\nJointGrid : Grid for plotting joint and marginal distributions of two variables.\n\nExamples\n--------\n\n.. include:: ../docstrings/pairplot.rst", "parameters": {"type": "object", "properties": {"data": {}, "hue": {"type": "NoneType", "default": null}, "vars": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/99", "data": {"name": "seaborn.pairplot.fig", "type": "callable", "signature": "(data, *, hue=None, hue_order=None, palette=None, vars=None, x_vars=None, y_vars=None, kind='scatter', diag_kind='auto', markers=None, height=2.5, aspect=1, corner=False, dropna=False, plot_kws=None, diag_kws=None, grid_kws=None, size=None)", "description": "Plot pairwise relationships in a dataset.\n\nBy default, this function will create a grid of Axes such that each numeric\nvariable in ``data`` will by shared across the y-axes across a single row and\nthe x-axes across a single column. The diagonal plots are treated\ndifferently: a univariate distribution plot is drawn to show the marginal\ndistribution of the data in each column.\n\nIt is also possible to show a subset of variables or plot different\nvariables on the rows and columns.\n\nThis is a high-level interface for :class:`PairGrid` that is intended to\nmake it easy to draw a few common styles. You should use :class:`PairGrid`\ndirectly if you need more flexibility.\n\nParameters\n----------\ndata : `pandas.DataFrame`\n    Tidy (long-form) dataframe where each column is a variable and\n    each row is an observation.\nhue : name of variable in ``data``\n    Variable in ``data`` to map plot aspects to different colors.\nhue_order : list of strings\n    Order for the levels of the hue variable in the palette\npalette : dict or seaborn color palette\n    Set of colors for mapping the ``hue`` variable. If a dict, keys\n    should be values  in the ``hue`` variable.\nvars : list of variable names\n    Variables within ``data`` to use, otherwise use every column with\n    a numeric datatype.\n{x, y}_vars : lists of variable names\n    Variables within ``data`` to use separately for the rows and\n    columns of the figure; i.e. to make a non-square plot.\nkind : {'scatter', 'kde', 'hist', 'reg'}\n    Kind of plot to make.\ndiag_kind : {'auto', 'hist', 'kde', None}\n    Kind of plot for the diagonal subplots. If 'auto', choose based on\n    whether or not ``hue`` is used.\nmarkers : single matplotlib marker code or list\n    Either the marker to use for all scatterplot points or a list of markers\n    with a length the same as the number of levels in the hue variable so that\n    differently colored points will also have different scatterplot\n    markers.\nheight : scalar\n    Height (in inches) of each facet.\naspect : scalar\n    Aspect * height gives the width (in inches) of each facet.\ncorner : bool\n    If True, don't add axes to the upper (off-diagonal) triangle of the\n    grid, making this a \"corner\" plot.\ndropna : boolean\n    Drop missing values from the data before plotting.\n{plot, diag, grid}_kws : dicts\n    Dictionaries of keyword arguments. ``plot_kws`` are passed to the\n    bivariate plotting function, ``diag_kws`` are passed to the univariate\n    plotting function, and ``grid_kws`` are passed to the :class:`PairGrid`\n    constructor.\n\nReturns\n-------\ngrid : :class:`PairGrid`\n    Returns the underlying :class:`PairGrid` instance for further tweaking.\n\nSee Also\n--------\nPairGrid : Subplot grid for more flexible plotting of pairwise relationships.\nJointGrid : Grid for plotting joint and marginal distributions of two variables.\n\nExamples\n--------\n\n.. include:: ../docstrings/pairplot.rst"}}
{"task_id": "BigCodeBench/99", "data": {"name": "seaborn.pairplot.fig.suptitle", "type": "callable", "signature": "(data)", "description": "Plot pairwise relationships in a dataset.\n\nBy default, this function will create a grid of Axes such that each numeric\nvariable in ``data`` will by shared across the y-axes across a single row and\nthe x-axes across a single column. The diagonal plots are treated\ndifferently: a univariate distribution plot is drawn to show the marginal\ndistribution of the data in each column.\n\nIt is also possible to show a subset of variables or plot different\nvariables on the rows and columns.\n\nThis is a high-level interface for :class:`PairGrid` that is intended to\nmake it easy to draw a few common styles. You should use :class:`PairGrid`\ndirectly if you need more flexibility.\n\nParameters\n----------\ndata : `pandas.DataFrame`\n    Tidy (long-form) dataframe where each column is a variable and\n    each row is an observation.\nhue : name of variable in ``data``\n    Variable in ``data`` to map plot aspects to different colors.\nhue_order : list of strings\n    Order for the levels of the hue variable in the palette\npalette : dict or seaborn color palette\n    Set of colors for mapping the ``hue`` variable. If a dict, keys\n    should be values  in the ``hue`` variable.\nvars : list of variable names\n    Variables within ``data`` to use, otherwise use every column with\n    a numeric datatype.\n{x, y}_vars : lists of variable names\n    Variables within ``data`` to use separately for the rows and\n    columns of the figure; i.e. to make a non-square plot.\nkind : {'scatter', 'kde', 'hist', 'reg'}\n    Kind of plot to make.\ndiag_kind : {'auto', 'hist', 'kde', None}\n    Kind of plot for the diagonal subplots. If 'auto', choose based on\n    whether or not ``hue`` is used.\nmarkers : single matplotlib marker code or list\n    Either the marker to use for all scatterplot points or a list of markers\n    with a length the same as the number of levels in the hue variable so that\n    differently colored points will also have different scatterplot\n    markers.\nheight : scalar\n    Height (in inches) of each facet.\naspect : scalar\n    Aspect * height gives the width (in inches) of each facet.\ncorner : bool\n    If True, don't add axes to the upper (off-diagonal) triangle of the\n    grid, making this a \"corner\" plot.\ndropna : boolean\n    Drop missing values from the data before plotting.\n{plot, diag, grid}_kws : dicts\n    Dictionaries of keyword arguments. ``plot_kws`` are passed to the\n    bivariate plotting function, ``diag_kws`` are passed to the univariate\n    plotting function, and ``grid_kws`` are passed to the :class:`PairGrid`\n    constructor.\n\nReturns\n-------\ngrid : :class:`PairGrid`\n    Returns the underlying :class:`PairGrid` instance for further tweaking.\n\nSee Also\n--------\nPairGrid : Subplot grid for more flexible plotting of pairwise relationships.\nJointGrid : Grid for plotting joint and marginal distributions of two variables.\n\nExamples\n--------\n\n.. include:: ../docstrings/pairplot.rst", "parameters": {"type": "object", "properties": {"data": {}}}}}
{"task_id": "BigCodeBench/99", "data": {"name": "sklearn.datasets.load_iris", "type": "callable", "signature": "()", "description": "Load and return the iris dataset (classification).\n\nThe iris dataset is a classic and very easy multi-class classification\ndataset.\n\n=================   ==============\nClasses                          3\nSamples per class               50\nSamples total                  150\nDimensionality                   4\nFeatures            real, positive\n=================   ==============\n\nRead more in the :ref:`User Guide <iris_dataset>`.\n\nParameters\n----------\nreturn_X_y : bool, default=False\n    If True, returns ``(data, target)`` instead of a Bunch object. See\n    below for more information about the `data` and `target` object.\n\n    .. versionadded:: 0.18\n\nas_frame : bool, default=False\n    If True, the data is a pandas DataFrame including columns with\n    appropriate dtypes (numeric). The target is\n    a pandas DataFrame or Series depending on the number of target columns.\n    If `return_X_y` is True, then (`data`, `target`) will be pandas\n    DataFrames or Series as described below.\n\n    .. versionadded:: 0.23\n\nReturns\n-------\ndata : :class:`~sklearn.utils.Bunch`\n    Dictionary-like object, with the following attributes.\n\n    data : {ndarray, dataframe} of shape (150, 4)\n        The data matrix. If `as_frame=True`, `data` will be a pandas\n        DataFrame.\n    target: {ndarray, Series} of shape (150,)\n        The classification target. If `as_frame=True`, `target` will be\n        a pandas Series.\n    feature_names: list\n        The names of the dataset columns.\n    target_names: list\n        The names of target classes.\n    frame: DataFrame of shape (150, 5)\n        Only present when `as_frame=True`. DataFrame with `data` and\n        `target`.\n\n        .. versionadded:: 0.23\n    DESCR: str\n        The full description of the dataset.\n    filename: str\n        The path to the location of the data.\n\n        .. versionadded:: 0.20\n\n(data, target) : tuple if ``return_X_y`` is True\n    A tuple of two ndarray. The first containing a 2D array of shape\n    (n_samples, n_features) with each row representing one sample and\n    each column representing the features. The second ndarray of shape\n    (n_samples,) containing the target samples.\n\n    .. versionadded:: 0.18\n\nNotes\n-----\n    .. versionchanged:: 0.20\n        Fixed two wrong data points according to Fisher's paper.\n        The new version is the same as in R, but not as in the UCI\n        Machine Learning Repository.\n\nExamples\n--------\nLet's say you are interested in the samples 10, 25, and 50, and want to\nknow their class name.\n\n>>> from sklearn.datasets import load_iris\n>>> data = load_iris()\n>>> data.target[[10, 25, 50]]\narray([0, 0, 1])\n>>> list(data.target_names)\n['setosa', 'versicolor', 'virginica']\n\nSee :ref:`sphx_glr_auto_examples_datasets_plot_iris_dataset.py` for a more\ndetailed example of how to work with the iris dataset.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/99", "data": {"name": "sklearn.datasets.load_iris.data", "type": "callable", "signature": "(*, return_X_y=False, as_frame=False)", "description": "Load and return the iris dataset (classification).\n\nThe iris dataset is a classic and very easy multi-class classification\ndataset.\n\n=================   ==============\nClasses                          3\nSamples per class               50\nSamples total                  150\nDimensionality                   4\nFeatures            real, positive\n=================   ==============\n\nRead more in the :ref:`User Guide <iris_dataset>`.\n\nParameters\n----------\nreturn_X_y : bool, default=False\n    If True, returns ``(data, target)`` instead of a Bunch object. See\n    below for more information about the `data` and `target` object.\n\n    .. versionadded:: 0.18\n\nas_frame : bool, default=False\n    If True, the data is a pandas DataFrame including columns with\n    appropriate dtypes (numeric). The target is\n    a pandas DataFrame or Series depending on the number of target columns.\n    If `return_X_y` is True, then (`data`, `target`) will be pandas\n    DataFrames or Series as described below.\n\n    .. versionadded:: 0.23\n\nReturns\n-------\ndata : :class:`~sklearn.utils.Bunch`\n    Dictionary-like object, with the following attributes.\n\n    data : {ndarray, dataframe} of shape (150, 4)\n        The data matrix. If `as_frame=True`, `data` will be a pandas\n        DataFrame.\n    target: {ndarray, Series} of shape (150,)\n        The classification target. If `as_frame=True`, `target` will be\n        a pandas Series.\n    feature_names: list\n        The names of the dataset columns.\n    target_names: list\n        The names of target classes.\n    frame: DataFrame of shape (150, 5)\n        Only present when `as_frame=True`. DataFrame with `data` and\n        `target`.\n\n        .. versionadded:: 0.23\n    DESCR: str\n        The full description of the dataset.\n    filename: str\n        The path to the location of the data.\n\n        .. versionadded:: 0.20\n\n(data, target) : tuple if ``return_X_y`` is True\n    A tuple of two ndarray. The first containing a 2D array of shape\n    (n_samples, n_features) with each row representing one sample and\n    each column representing the features. The second ndarray of shape\n    (n_samples,) containing the target samples.\n\n    .. versionadded:: 0.18\n\nNotes\n-----\n    .. versionchanged:: 0.20\n        Fixed two wrong data points according to Fisher's paper.\n        The new version is the same as in R, but not as in the UCI\n        Machine Learning Repository.\n\nExamples\n--------\nLet's say you are interested in the samples 10, 25, and 50, and want to\nknow their class name.\n\n>>> from sklearn.datasets import load_iris\n>>> data = load_iris()\n>>> data.target[[10, 25, 50]]\narray([0, 0, 1])\n>>> list(data.target_names)\n['setosa', 'versicolor', 'virginica']\n\nSee :ref:`sphx_glr_auto_examples_datasets_plot_iris_dataset.py` for a more\ndetailed example of how to work with the iris dataset.", "parameters": {"type": "object", "properties": {"return_X_y": {"type": "bool", "default": false}, "as_frame": {"type": "bool", "default": false}}}}}
{"task_id": "BigCodeBench/99", "data": {"name": "sklearn.datasets.load_iris.target", "type": "callable", "signature": "(*, return_X_y=False, as_frame=False)", "description": "Load and return the iris dataset (classification).\n\nThe iris dataset is a classic and very easy multi-class classification\ndataset.\n\n=================   ==============\nClasses                          3\nSamples per class               50\nSamples total                  150\nDimensionality                   4\nFeatures            real, positive\n=================   ==============\n\nRead more in the :ref:`User Guide <iris_dataset>`.\n\nParameters\n----------\nreturn_X_y : bool, default=False\n    If True, returns ``(data, target)`` instead of a Bunch object. See\n    below for more information about the `data` and `target` object.\n\n    .. versionadded:: 0.18\n\nas_frame : bool, default=False\n    If True, the data is a pandas DataFrame including columns with\n    appropriate dtypes (numeric). The target is\n    a pandas DataFrame or Series depending on the number of target columns.\n    If `return_X_y` is True, then (`data`, `target`) will be pandas\n    DataFrames or Series as described below.\n\n    .. versionadded:: 0.23\n\nReturns\n-------\ndata : :class:`~sklearn.utils.Bunch`\n    Dictionary-like object, with the following attributes.\n\n    data : {ndarray, dataframe} of shape (150, 4)\n        The data matrix. If `as_frame=True`, `data` will be a pandas\n        DataFrame.\n    target: {ndarray, Series} of shape (150,)\n        The classification target. If `as_frame=True`, `target` will be\n        a pandas Series.\n    feature_names: list\n        The names of the dataset columns.\n    target_names: list\n        The names of target classes.\n    frame: DataFrame of shape (150, 5)\n        Only present when `as_frame=True`. DataFrame with `data` and\n        `target`.\n\n        .. versionadded:: 0.23\n    DESCR: str\n        The full description of the dataset.\n    filename: str\n        The path to the location of the data.\n\n        .. versionadded:: 0.20\n\n(data, target) : tuple if ``return_X_y`` is True\n    A tuple of two ndarray. The first containing a 2D array of shape\n    (n_samples, n_features) with each row representing one sample and\n    each column representing the features. The second ndarray of shape\n    (n_samples,) containing the target samples.\n\n    .. versionadded:: 0.18\n\nNotes\n-----\n    .. versionchanged:: 0.20\n        Fixed two wrong data points according to Fisher's paper.\n        The new version is the same as in R, but not as in the UCI\n        Machine Learning Repository.\n\nExamples\n--------\nLet's say you are interested in the samples 10, 25, and 50, and want to\nknow their class name.\n\n>>> from sklearn.datasets import load_iris\n>>> data = load_iris()\n>>> data.target[[10, 25, 50]]\narray([0, 0, 1])\n>>> list(data.target_names)\n['setosa', 'versicolor', 'virginica']\n\nSee :ref:`sphx_glr_auto_examples_datasets_plot_iris_dataset.py` for a more\ndetailed example of how to work with the iris dataset.", "parameters": {"type": "object", "properties": {"return_X_y": {"type": "bool", "default": false}, "as_frame": {"type": "bool", "default": false}}}}}
{"task_id": "BigCodeBench/99", "data": {"name": "sklearn.datasets.load_iris.feature_names", "type": "callable", "signature": "(*, return_X_y=False, as_frame=False)", "description": "Load and return the iris dataset (classification).\n\nThe iris dataset is a classic and very easy multi-class classification\ndataset.\n\n=================   ==============\nClasses                          3\nSamples per class               50\nSamples total                  150\nDimensionality                   4\nFeatures            real, positive\n=================   ==============\n\nRead more in the :ref:`User Guide <iris_dataset>`.\n\nParameters\n----------\nreturn_X_y : bool, default=False\n    If True, returns ``(data, target)`` instead of a Bunch object. See\n    below for more information about the `data` and `target` object.\n\n    .. versionadded:: 0.18\n\nas_frame : bool, default=False\n    If True, the data is a pandas DataFrame including columns with\n    appropriate dtypes (numeric). The target is\n    a pandas DataFrame or Series depending on the number of target columns.\n    If `return_X_y` is True, then (`data`, `target`) will be pandas\n    DataFrames or Series as described below.\n\n    .. versionadded:: 0.23\n\nReturns\n-------\ndata : :class:`~sklearn.utils.Bunch`\n    Dictionary-like object, with the following attributes.\n\n    data : {ndarray, dataframe} of shape (150, 4)\n        The data matrix. If `as_frame=True`, `data` will be a pandas\n        DataFrame.\n    target: {ndarray, Series} of shape (150,)\n        The classification target. If `as_frame=True`, `target` will be\n        a pandas Series.\n    feature_names: list\n        The names of the dataset columns.\n    target_names: list\n        The names of target classes.\n    frame: DataFrame of shape (150, 5)\n        Only present when `as_frame=True`. DataFrame with `data` and\n        `target`.\n\n        .. versionadded:: 0.23\n    DESCR: str\n        The full description of the dataset.\n    filename: str\n        The path to the location of the data.\n\n        .. versionadded:: 0.20\n\n(data, target) : tuple if ``return_X_y`` is True\n    A tuple of two ndarray. The first containing a 2D array of shape\n    (n_samples, n_features) with each row representing one sample and\n    each column representing the features. The second ndarray of shape\n    (n_samples,) containing the target samples.\n\n    .. versionadded:: 0.18\n\nNotes\n-----\n    .. versionchanged:: 0.20\n        Fixed two wrong data points according to Fisher's paper.\n        The new version is the same as in R, but not as in the UCI\n        Machine Learning Repository.\n\nExamples\n--------\nLet's say you are interested in the samples 10, 25, and 50, and want to\nknow their class name.\n\n>>> from sklearn.datasets import load_iris\n>>> data = load_iris()\n>>> data.target[[10, 25, 50]]\narray([0, 0, 1])\n>>> list(data.target_names)\n['setosa', 'versicolor', 'virginica']\n\nSee :ref:`sphx_glr_auto_examples_datasets_plot_iris_dataset.py` for a more\ndetailed example of how to work with the iris dataset.", "parameters": {"type": "object", "properties": {"return_X_y": {"type": "bool", "default": false}, "as_frame": {"type": "bool", "default": false}}}}}
{"task_id": "BigCodeBench/100", "data": {"name": "datetime.datetime", "type": "class", "signature": "(self, /, *args, **kwargs)", "description": "datetime(year, month, day[, hour[, minute[, second[, microsecond[,tzinfo]]]]])\n\nThe year, month and day arguments are required. tzinfo may be None, or an\ninstance of a tzinfo subclass. The remaining arguments may be ints.", "parameters": {"type": "object", "properties": {"/": {}}}}}
{"task_id": "BigCodeBench/100", "data": {"name": "datetime.datetime.now", "type": "callable", "signature": "()", "description": "Returns new datetime object representing current time local to tz.\n\n  tz\n    Timezone object.\n\nIf no tz is specified, uses local timezone.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/100", "data": {"name": "matplotlib.pyplot.subplots", "type": "class", "signature": "()", "description": "Create a figure and a set of subplots.\n\nThis utility wrapper makes it convenient to create common layouts of\nsubplots, including the enclosing figure object, in a single call.\n\nParameters\n----------\nnrows, ncols : int, default: 1\n    Number of rows/columns of the subplot grid.\n\nsharex, sharey : bool or {'none', 'all', 'row', 'col'}, default: False\n    Controls sharing of properties among x (*sharex*) or y (*sharey*)\n    axes:\n\n    - True or 'all': x- or y-axis will be shared among all subplots.\n    - False or 'none': each subplot x- or y-axis will be independent.\n    - 'row': each subplot row will share an x- or y-axis.\n    - 'col': each subplot column will share an x- or y-axis.\n\n    When subplots have a shared x-axis along a column, only the x tick\n    labels of the bottom subplot are created. Similarly, when subplots\n    have a shared y-axis along a row, only the y tick labels of the first\n    column subplot are created. To later turn other subplots' ticklabels\n    on, use `~matplotlib.axes.Axes.tick_params`.\n\n    When subplots have a shared axis that has units, calling\n    `~matplotlib.axis.Axis.set_units` will update each axis with the\n    new units.\n\nsqueeze : bool, default: True\n    - If True, extra dimensions are squeezed out from the returned\n      array of `~matplotlib.axes.Axes`:\n\n      - if only one subplot is constructed (nrows=ncols=1), the\n        resulting single Axes object is returned as a scalar.\n      - for Nx1 or 1xM subplots, the returned object is a 1D numpy\n        object array of Axes objects.\n      - for NxM, subplots with N>1 and M>1 are returned as a 2D array.\n\n    - If False, no squeezing at all is done: the returned Axes object is\n      always a 2D array containing Axes instances, even if it ends up\n      being 1x1.\n\nwidth_ratios : array-like of length *ncols*, optional\n    Defines the relative widths of the columns. Each column gets a\n    relative width of ``width_ratios[i] / sum(width_ratios)``.\n    If not given, all columns will have the same width.  Equivalent\n    to ``gridspec_kw={'width_ratios': [...]}``.\n\nheight_ratios : array-like of length *nrows*, optional\n    Defines the relative heights of the rows. Each row gets a\n    relative height of ``height_ratios[i] / sum(height_ratios)``.\n    If not given, all rows will have the same height. Convenience\n    for ``gridspec_kw={'height_ratios': [...]}``.\n\nsubplot_kw : dict, optional\n    Dict with keywords passed to the\n    `~matplotlib.figure.Figure.add_subplot` call used to create each\n    subplot.\n\ngridspec_kw : dict, optional\n    Dict with keywords passed to the `~matplotlib.gridspec.GridSpec`\n    constructor used to create the grid the subplots are placed on.\n\n**fig_kw\n    All additional keyword arguments are passed to the\n    `.pyplot.figure` call.\n\nReturns\n-------\nfig : `.Figure`\n\nax : `~matplotlib.axes.Axes` or array of Axes\n    *ax* can be either a single `~.axes.Axes` object, or an array of Axes\n    objects if more than one subplot was created.  The dimensions of the\n    resulting array can be controlled with the squeeze keyword, see above.\n\n    Typical idioms for handling the return value are::\n\n        # using the variable ax for single a Axes\n        fig, ax = plt.subplots()\n\n        # using the variable axs for multiple Axes\n        fig, axs = plt.subplots(2, 2)\n\n        # using tuple unpacking for multiple Axes\n        fig, (ax1, ax2) = plt.subplots(1, 2)\n        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n\n    The names ``ax`` and pluralized ``axs`` are preferred over ``axes``\n    because for the latter it's not clear if it refers to a single\n    `~.axes.Axes` instance or a collection of these.\n\nSee Also\n--------\n.pyplot.figure\n.pyplot.subplot\n.pyplot.axes\n.Figure.subplots\n.Figure.add_subplot\n\nExamples\n--------\n::\n\n    # First create some toy data:\n    x = np.linspace(0, 2*np.pi, 400)\n    y = np.sin(x**2)\n\n    # Create just a figure and only one subplot\n    fig, ax = plt.subplots()\n    ax.plot(x, y)\n    ax.set_title('Simple plot')\n\n    # Create two subplots and unpack the output array immediately\n    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n    ax1.plot(x, y)\n    ax1.set_title('Sharing Y axis')\n    ax2.scatter(x, y)\n\n    # Create four polar axes and access them through the returned array\n    fig, axs = plt.subplots(2, 2, subplot_kw=dict(projection=\"polar\"))\n    axs[0, 0].plot(x, y)\n    axs[1, 1].scatter(x, y)\n\n    # Share a X axis with each column of subplots\n    plt.subplots(2, 2, sharex='col')\n\n    # Share a Y axis with each row of subplots\n    plt.subplots(2, 2, sharey='row')\n\n    # Share both X and Y axes with all subplots\n    plt.subplots(2, 2, sharex='all', sharey='all')\n\n    # Note that this is the same as\n    plt.subplots(2, 2, sharex=True, sharey=True)\n\n    # Create figure number 10 with a single subplot\n    # and clears it if it already exists.\n    fig, ax = plt.subplots(num=10, clear=True)", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/100", "data": {"name": "matplotlib.pyplot.subplots[1].legend", "type": "method", "signature": "(*args, **kwargs)", "description": "Place a legend on the Axes.\n\nCall signatures::\n\n    legend()\n    legend(handles, labels)\n    legend(handles=handles)\n    legend(labels)\n\nThe call signatures correspond to the following different ways to use\nthis method:\n\n**1. Automatic detection of elements to be shown in the legend**\n\nThe elements to be added to the legend are automatically determined,\nwhen you do not pass in any extra arguments.\n\nIn this case, the labels are taken from the artist. You can specify\nthem either at artist creation or by calling the\n:meth:`~.Artist.set_label` method on the artist::\n\n    ax.plot([1, 2, 3], label='Inline label')\n    ax.legend()\n\nor::\n\n    line, = ax.plot([1, 2, 3])\n    line.set_label('Label via method')\n    ax.legend()\n\n.. note::\n    Specific artists can be excluded from the automatic legend element\n    selection by using a label starting with an underscore, \"_\".\n    A string starting with an underscore is the default label for all\n    artists, so calling `.Axes.legend` without any arguments and\n    without setting the labels manually will result in no legend being\n    drawn.\n\n\n**2. Explicitly listing the artists and labels in the legend**\n\nFor full control of which artists have a legend entry, it is possible\nto pass an iterable of legend artists followed by an iterable of\nlegend labels respectively::\n\n    ax.legend([line1, line2, line3], ['label1', 'label2', 'label3'])\n\n\n**3. Explicitly listing the artists in the legend**\n\nThis is similar to 2, but the labels are taken from the artists'\nlabel properties. Example::\n\n    line1, = ax.plot([1, 2, 3], label='label1')\n    line2, = ax.plot([1, 2, 3], label='label2')\n    ax.legend(handles=[line1, line2])\n\n\n**4. Labeling existing plot elements**\n\n.. admonition:: Discouraged\n\n    This call signature is discouraged, because the relation between\n    plot elements and labels is only implicit by their order and can\n    easily be mixed up.\n\nTo make a legend for all artists on an Axes, call this function with\nan iterable of strings, one for each legend item. For example::\n\n    ax.plot([1, 2, 3])\n    ax.plot([5, 6, 7])\n    ax.legend(['First line', 'Second line'])\n\n\nParameters\n----------\nhandles : list of (`.Artist` or tuple of `.Artist`), optional\n    A list of Artists (lines, patches) to be added to the legend.\n    Use this together with *labels*, if you need full control on what\n    is shown in the legend and the automatic mechanism described above\n    is not sufficient.\n\n    The length of handles and labels should be the same in this\n    case. If they are not, they are truncated to the smaller length.\n\n    If an entry contains a tuple, then the legend handler for all Artists in the\n    tuple will be placed alongside a single label.\n\nlabels : list of str, optional\n    A list of labels to show next to the artists.\n    Use this together with *handles*, if you need full control on what\n    is shown in the legend and the automatic mechanism described above\n    is not sufficient.\n\nReturns\n-------\n`~matplotlib.legend.Legend`\n\nOther Parameters\n----------------\n\nloc : str or pair of floats, default: :rc:`legend.loc`\n    The location of the legend.\n\n    The strings ``'upper left'``, ``'upper right'``, ``'lower left'``,\n    ``'lower right'`` place the legend at the corresponding corner of the\n    axes.\n\n    The strings ``'upper center'``, ``'lower center'``, ``'center left'``,\n    ``'center right'`` place the legend at the center of the corresponding edge\n    of the axes.\n\n    The string ``'center'`` places the legend at the center of the axes.\n\n    The string ``'best'`` places the legend at the location, among the nine\n    locations defined so far, with the minimum overlap with other drawn\n    artists.  This option can be quite slow for plots with large amounts of\n    data; your plotting speed may benefit from providing a specific location.\n\n    The location can also be a 2-tuple giving the coordinates of the lower-left\n    corner of the legend in axes coordinates (in which case *bbox_to_anchor*\n    will be ignored).\n\n    For back-compatibility, ``'center right'`` (but no other location) can also\n    be spelled ``'right'``, and each \"string\" location can also be given as a\n    numeric value:\n\n    ==================   =============\n    Location String      Location Code\n    ==================   =============\n    'best' (Axes only)   0\n    'upper right'        1\n    'upper left'         2\n    'lower left'         3\n    'lower right'        4\n    'right'              5\n    'center left'        6\n    'center right'       7\n    'lower center'       8\n    'upper center'       9\n    'center'             10\n    ==================   =============\n    \nbbox_to_anchor : `.BboxBase`, 2-tuple, or 4-tuple of floats\n    Box that is used to position the legend in conjunction with *loc*.\n    Defaults to `axes.bbox` (if called as a method to `.Axes.legend`) or\n    `figure.bbox` (if `.Figure.legend`).  This argument allows arbitrary\n    placement of the legend.\n\n    Bbox coordinates are interpreted in the coordinate system given by\n    *bbox_transform*, with the default transform\n    Axes or Figure coordinates, depending on which ``legend`` is called.\n\n    If a 4-tuple or `.BboxBase` is given, then it specifies the bbox\n    ``(x, y, width, height)`` that the legend is placed in.\n    To put the legend in the best location in the bottom right\n    quadrant of the axes (or figure)::\n\n        loc='best', bbox_to_anchor=(0.5, 0., 0.5, 0.5)\n\n    A 2-tuple ``(x, y)`` places the corner of the legend specified by *loc* at\n    x, y.  For example, to put the legend's upper right-hand corner in the\n    center of the axes (or figure) the following keywords can be used::\n\n        loc='upper right', bbox_to_anchor=(0.5, 0.5)\n\nncols : int, default: 1\n    The number of columns that the legend has.\n\n    For backward compatibility, the spelling *ncol* is also supported\n    but it is discouraged. If both are given, *ncols* takes precedence.\n\nprop : None or `~matplotlib.font_manager.FontProperties` or dict\n    The font properties of the legend. If None (default), the current\n    :data:`matplotlib.rcParams` will be used.\n\nfontsize : int or {'xx-small', 'x-small', 'small', 'medium', 'large', 'x-large', 'xx-large'}\n    The font size of the legend. If the value is numeric the size will be the\n    absolute font size in points. String values are relative to the current\n    default font size. This argument is only used if *prop* is not specified.\n\nlabelcolor : str or list, default: :rc:`legend.labelcolor`\n    The color of the text in the legend. Either a valid color string\n    (for example, 'red'), or a list of color strings. The labelcolor can\n    also be made to match the color of the line or marker using 'linecolor',\n    'markerfacecolor' (or 'mfc'), or 'markeredgecolor' (or 'mec').\n\n    Labelcolor can be set globally using :rc:`legend.labelcolor`. If None,\n    use :rc:`text.color`.\n\nnumpoints : int, default: :rc:`legend.numpoints`\n    The number of marker points in the legend when creating a legend\n    entry for a `.Line2D` (line).\n\nscatterpoints : int, default: :rc:`legend.scatterpoints`\n    The number of marker points in the legend when creating\n    a legend entry for a `.PathCollection` (scatter plot).\n\nscatteryoffsets : iterable of floats, default: ``[0.375, 0.5, 0.3125]``\n    The vertical offset (relative to the font size) for the markers\n    created for a scatter plot legend entry. 0.0 is at the base the\n    legend text, and 1.0 is at the top. To draw all markers at the\n    same height, set to ``[0.5]``.\n\nmarkerscale : float, default: :rc:`legend.markerscale`\n    The relative size of legend markers compared to the originally drawn ones.\n\nmarkerfirst : bool, default: True\n    If *True*, legend marker is placed to the left of the legend label.\n    If *False*, legend marker is placed to the right of the legend label.\n\nreverse : bool, default: False\n    If *True*, the legend labels are displayed in reverse order from the input.\n    If *False*, the legend labels are displayed in the same order as the input.\n\n    .. versionadded:: 3.7\n\nframeon : bool, default: :rc:`legend.frameon`\n    Whether the legend should be drawn on a patch (frame).\n\nfancybox : bool, default: :rc:`legend.fancybox`\n    Whether round edges should be enabled around the `.FancyBboxPatch` which\n    makes up the legend's background.\n\nshadow : None, bool or dict, default: :rc:`legend.shadow`\n    Whether to draw a shadow behind the legend.\n    The shadow can be configured using `.Patch` keywords.\n    Customization via :rc:`legend.shadow` is currently not supported.\n\nframealpha : float, default: :rc:`legend.framealpha`\n    The alpha transparency of the legend's background.\n    If *shadow* is activated and *framealpha* is ``None``, the default value is\n    ignored.\n\nfacecolor : \"inherit\" or color, default: :rc:`legend.facecolor`\n    The legend's background color.\n    If ``\"inherit\"``, use :rc:`axes.facecolor`.\n\nedgecolor : \"inherit\" or color, default: :rc:`legend.edgecolor`\n    The legend's background patch edge color.\n    If ``\"inherit\"``, use take :rc:`axes.edgecolor`.\n\nmode : {\"expand\", None}\n    If *mode* is set to ``\"expand\"`` the legend will be horizontally\n    expanded to fill the axes area (or *bbox_to_anchor* if defines\n    the legend's size).\n\nbbox_transform : None or `~matplotlib.transforms.Transform`\n    The transform for the bounding box (*bbox_to_anchor*). For a value\n    of ``None`` (default) the Axes'\n    :data:`~matplotlib.axes.Axes.transAxes` transform will be used.\n\ntitle : str or None\n    The legend's title. Default is no title (``None``).\n\ntitle_fontproperties : None or `~matplotlib.font_manager.FontProperties` or dict\n    The font properties of the legend's title. If None (default), the\n    *title_fontsize* argument will be used if present; if *title_fontsize* is\n    also None, the current :rc:`legend.title_fontsize` will be used.\n\ntitle_fontsize : int or {'xx-small', 'x-small', 'small', 'medium', 'large', 'x-large', 'xx-large'}, default: :rc:`legend.title_fontsize`\n    The font size of the legend's title.\n    Note: This cannot be combined with *title_fontproperties*. If you want\n    to set the fontsize alongside other font properties, use the *size*\n    parameter in *title_fontproperties*.\n\nalignment : {'center', 'left', 'right'}, default: 'center'\n    The alignment of the legend title and the box of entries. The entries\n    are aligned as a single block, so that markers always lined up.\n\nborderpad : float, default: :rc:`legend.borderpad`\n    The fractional whitespace inside the legend border, in font-size units.\n\nlabelspacing : float, default: :rc:`legend.labelspacing`\n    The vertical space between the legend entries, in font-size units.\n\nhandlelength : float, default: :rc:`legend.handlelength`\n    The length of the legend handles, in font-size units.\n\nhandleheight : float, default: :rc:`legend.handleheight`\n    The height of the legend handles, in font-size units.\n\nhandletextpad : float, default: :rc:`legend.handletextpad`\n    The pad between the legend handle and text, in font-size units.\n\nborderaxespad : float, default: :rc:`legend.borderaxespad`\n    The pad between the axes and legend border, in font-size units.\n\ncolumnspacing : float, default: :rc:`legend.columnspacing`\n    The spacing between columns, in font-size units.\n\nhandler_map : dict or None\n    The custom dictionary mapping instances or types to a legend\n    handler. This *handler_map* updates the default handler map\n    found at `matplotlib.legend.Legend.get_legend_handler_map`.\n\ndraggable : bool, default: False\n    Whether the legend can be dragged with the mouse.\n\n\nSee Also\n--------\n.Figure.legend\n\nNotes\n-----\nSome artists are not supported by this function.  See\n:ref:`legend_guide` for details.\n\nExamples\n--------\n.. plot:: gallery/text_labels_and_annotations/legend.py", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/100", "data": {"name": "matplotlib.pyplot.subplots[1].set_xlabel", "type": "method", "signature": "(xlabel, fontdict=None, labelpad=None, *, loc=None, **kwargs)", "description": "Set the label for the x-axis.\n\nParameters\n----------\nxlabel : str\n    The label text.\n\nlabelpad : float, default: :rc:`axes.labelpad`\n    Spacing in points from the Axes bounding box including ticks\n    and tick labels.  If None, the previous value is left as is.\n\nloc : {'left', 'center', 'right'}, default: :rc:`xaxis.labellocation`\n    The label position. This is a high-level alternative for passing\n    parameters *x* and *horizontalalignment*.\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    `.Text` properties control the appearance of the label.\n\nSee Also\n--------\ntext : Documents the properties supported by `.Text`.", "parameters": {"type": "object", "properties": {"xlabel": {}, "fontdict": {"type": "NoneType", "default": null}, "labelpad": {"type": "NoneType", "default": null}, "loc": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/100", "data": {"name": "matplotlib.pyplot.subplots[1].set_ylabel", "type": "method", "signature": "(ylabel, fontdict=None, labelpad=None, *, loc=None, **kwargs)", "description": "Set the label for the y-axis.\n\nParameters\n----------\nylabel : str\n    The label text.\n\nlabelpad : float, default: :rc:`axes.labelpad`\n    Spacing in points from the Axes bounding box including ticks\n    and tick labels.  If None, the previous value is left as is.\n\nloc : {'bottom', 'center', 'top'}, default: :rc:`yaxis.labellocation`\n    The label position. This is a high-level alternative for passing\n    parameters *y* and *horizontalalignment*.\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    `.Text` properties control the appearance of the label.\n\nSee Also\n--------\ntext : Documents the properties supported by `.Text`.", "parameters": {"type": "object", "properties": {"ylabel": {}, "fontdict": {"type": "NoneType", "default": null}, "labelpad": {"type": "NoneType", "default": null}, "loc": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/100", "data": {"name": "matplotlib.pyplot.subplots[1].set_title", "type": "method", "signature": "(label, fontdict=None, loc=None, pad=None, *, y=None, **kwargs)", "description": "Set a title for the Axes.\n\nSet one of the three available Axes titles. The available titles\nare positioned above the Axes in the center, flush with the left\nedge, and flush with the right edge.\n\nParameters\n----------\nlabel : str\n    Text to use for the title\n\nfontdict : dict\n\n    .. admonition:: Discouraged\n\n       The use of *fontdict* is discouraged. Parameters should be passed as\n       individual keyword arguments or using dictionary-unpacking\n       ``set_title(..., **fontdict)``.\n\n    A dictionary controlling the appearance of the title text,\n    the default *fontdict* is::\n\n       {'fontsize': rcParams['axes.titlesize'],\n        'fontweight': rcParams['axes.titleweight'],\n        'color': rcParams['axes.titlecolor'],\n        'verticalalignment': 'baseline',\n        'horizontalalignment': loc}\n\nloc : {'center', 'left', 'right'}, default: :rc:`axes.titlelocation`\n    Which title to set.\n\ny : float, default: :rc:`axes.titley`\n    Vertical Axes location for the title (1.0 is the top).  If\n    None (the default) and :rc:`axes.titley` is also None, y is\n    determined automatically to avoid decorators on the Axes.\n\npad : float, default: :rc:`axes.titlepad`\n    The offset of the title from the top of the Axes, in points.\n\nReturns\n-------\n`.Text`\n    The matplotlib text instance representing the title\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    Other keyword arguments are text properties, see `.Text` for a list\n    of valid text properties.", "parameters": {"type": "object", "properties": {"label": {}, "fontdict": {"type": "NoneType", "default": null}, "loc": {"type": "NoneType", "default": null}, "pad": {"type": "NoneType", "default": null}, "y": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/100", "data": {"name": "matplotlib.pyplot.subplots[1].plot", "type": "method", "signature": "(*args, scalex=True, scaley=True, data=None, **kwargs)", "description": "Plot y versus x as lines and/or markers.\n\nCall signatures::\n\n    plot([x], y, [fmt], *, data=None, **kwargs)\n    plot([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n\nThe coordinates of the points or line nodes are given by *x*, *y*.\n\nThe optional parameter *fmt* is a convenient way for defining basic\nformatting like color, marker and linestyle. It's a shortcut string\nnotation described in the *Notes* section below.\n\n>>> plot(x, y)        # plot x and y using default line style and color\n>>> plot(x, y, 'bo')  # plot x and y using blue circle markers\n>>> plot(y)           # plot y using x as index array 0..N-1\n>>> plot(y, 'r+')     # ditto, but with red plusses\n\nYou can use `.Line2D` properties as keyword arguments for more\ncontrol on the appearance. Line properties and *fmt* can be mixed.\nThe following two calls yield identical results:\n\n>>> plot(x, y, 'go--', linewidth=2, markersize=12)\n>>> plot(x, y, color='green', marker='o', linestyle='dashed',\n...      linewidth=2, markersize=12)\n\nWhen conflicting with *fmt*, keyword arguments take precedence.\n\n\n**Plotting labelled data**\n\nThere's a convenient way for plotting objects with labelled data (i.e.\ndata that can be accessed by index ``obj['y']``). Instead of giving\nthe data in *x* and *y*, you can provide the object in the *data*\nparameter and just give the labels for *x* and *y*::\n\n>>> plot('xlabel', 'ylabel', data=obj)\n\nAll indexable objects are supported. This could e.g. be a `dict`, a\n`pandas.DataFrame` or a structured numpy array.\n\n\n**Plotting multiple sets of data**\n\nThere are various ways to plot multiple sets of data.\n\n- The most straight forward way is just to call `plot` multiple times.\n  Example:\n\n  >>> plot(x1, y1, 'bo')\n  >>> plot(x2, y2, 'go')\n\n- If *x* and/or *y* are 2D arrays a separate data set will be drawn\n  for every column. If both *x* and *y* are 2D, they must have the\n  same shape. If only one of them is 2D with shape (N, m) the other\n  must have length N and will be used for every data set m.\n\n  Example:\n\n  >>> x = [1, 2, 3]\n  >>> y = np.array([[1, 2], [3, 4], [5, 6]])\n  >>> plot(x, y)\n\n  is equivalent to:\n\n  >>> for col in range(y.shape[1]):\n  ...     plot(x, y[:, col])\n\n- The third way is to specify multiple sets of *[x]*, *y*, *[fmt]*\n  groups::\n\n  >>> plot(x1, y1, 'g^', x2, y2, 'g-')\n\n  In this case, any additional keyword argument applies to all\n  datasets. Also, this syntax cannot be combined with the *data*\n  parameter.\n\nBy default, each line is assigned a different style specified by a\n'style cycle'. The *fmt* and line property parameters are only\nnecessary if you want explicit deviations from these defaults.\nAlternatively, you can also change the style cycle using\n:rc:`axes.prop_cycle`.\n\n\nParameters\n----------\nx, y : array-like or scalar\n    The horizontal / vertical coordinates of the data points.\n    *x* values are optional and default to ``range(len(y))``.\n\n    Commonly, these parameters are 1D arrays.\n\n    They can also be scalars, or two-dimensional (in that case, the\n    columns represent separate data sets).\n\n    These arguments cannot be passed as keywords.\n\nfmt : str, optional\n    A format string, e.g. 'ro' for red circles. See the *Notes*\n    section for a full description of the format strings.\n\n    Format strings are just an abbreviation for quickly setting\n    basic line properties. All of these and more can also be\n    controlled by keyword arguments.\n\n    This argument cannot be passed as keyword.\n\ndata : indexable object, optional\n    An object with labelled data. If given, provide the label names to\n    plot in *x* and *y*.\n\n    .. note::\n        Technically there's a slight ambiguity in calls where the\n        second label is a valid *fmt*. ``plot('n', 'o', data=obj)``\n        could be ``plt(x, y)`` or ``plt(y, fmt)``. In such cases,\n        the former interpretation is chosen, but a warning is issued.\n        You may suppress the warning by adding an empty format string\n        ``plot('n', 'o', '', data=obj)``.\n\nReturns\n-------\nlist of `.Line2D`\n    A list of lines representing the plotted data.\n\nOther Parameters\n----------------\nscalex, scaley : bool, default: True\n    These parameters determine if the view limits are adapted to the\n    data limits. The values are passed on to\n    `~.axes.Axes.autoscale_view`.\n\n**kwargs : `~matplotlib.lines.Line2D` properties, optional\n    *kwargs* are used to specify properties like a line label (for\n    auto legends), linewidth, antialiasing, marker face color.\n    Example::\n\n    >>> plot([1, 2, 3], [1, 2, 3], 'go-', label='line 1', linewidth=2)\n    >>> plot([1, 2, 3], [1, 4, 9], 'rs', label='line 2')\n\n    If you specify multiple lines with one plot call, the kwargs apply\n    to all those lines. In case the label object is iterable, each\n    element is used as labels for each set of data.\n\n    Here is a list of available `.Line2D` properties:\n\n    Properties:\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: scalar or None\n    animated: bool\n    antialiased or aa: bool\n    clip_box: `~matplotlib.transforms.BboxBase` or None\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    color or c: color\n    dash_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    dash_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    dashes: sequence of floats (on/off ink in points) or (None, None)\n    data: (2, N) array or two 1D arrays\n    drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n    figure: `~matplotlib.figure.Figure`\n    fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n    gapcolor: color or None\n    gid: str\n    in_layout: bool\n    label: object\n    linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n    linewidth or lw: float\n    marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n    markeredgecolor or mec: color\n    markeredgewidth or mew: float\n    markerfacecolor or mfc: color\n    markerfacecoloralt or mfcalt: color\n    markersize or ms: float\n    markevery: None or int or (int, int) or slice or list[int] or float or (float, float) or list[bool]\n    mouseover: bool\n    path_effects: list of `.AbstractPathEffect`\n    picker: float or callable[[Artist, Event], tuple[bool, dict]]\n    pickradius: float\n    rasterized: bool\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    solid_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    solid_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    transform: unknown\n    url: str\n    visible: bool\n    xdata: 1D array\n    ydata: 1D array\n    zorder: float\n\nSee Also\n--------\nscatter : XY scatter plot with markers of varying size and/or color (\n    sometimes also called bubble chart).\n\nNotes\n-----\n**Format Strings**\n\nA format string consists of a part for color, marker and line::\n\n    fmt = '[marker][line][color]'\n\nEach of them is optional. If not provided, the value from the style\ncycle is used. Exception: If ``line`` is given, but no ``marker``,\nthe data will be a line without markers.\n\nOther combinations such as ``[color][marker][line]`` are also\nsupported, but note that their parsing may be ambiguous.\n\n**Markers**\n\n=============   ===============================\ncharacter       description\n=============   ===============================\n``'.'``         point marker\n``','``         pixel marker\n``'o'``         circle marker\n``'v'``         triangle_down marker\n``'^'``         triangle_up marker\n``'<'``         triangle_left marker\n``'>'``         triangle_right marker\n``'1'``         tri_down marker\n``'2'``         tri_up marker\n``'3'``         tri_left marker\n``'4'``         tri_right marker\n``'8'``         octagon marker\n``'s'``         square marker\n``'p'``         pentagon marker\n``'P'``         plus (filled) marker\n``'*'``         star marker\n``'h'``         hexagon1 marker\n``'H'``         hexagon2 marker\n``'+'``         plus marker\n``'x'``         x marker\n``'X'``         x (filled) marker\n``'D'``         diamond marker\n``'d'``         thin_diamond marker\n``'|'``         vline marker\n``'_'``         hline marker\n=============   ===============================\n\n**Line Styles**\n\n=============    ===============================\ncharacter        description\n=============    ===============================\n``'-'``          solid line style\n``'--'``         dashed line style\n``'-.'``         dash-dot line style\n``':'``          dotted line style\n=============    ===============================\n\nExample format strings::\n\n    'b'    # blue markers with default shape\n    'or'   # red circles\n    '-g'   # green solid line\n    '--'   # dashed line with default color\n    '^k:'  # black triangle_up markers connected by a dotted line\n\n**Colors**\n\nThe supported color abbreviations are the single letter codes\n\n=============    ===============================\ncharacter        color\n=============    ===============================\n``'b'``          blue\n``'g'``          green\n``'r'``          red\n``'c'``          cyan\n``'m'``          magenta\n``'y'``          yellow\n``'k'``          black\n``'w'``          white\n=============    ===============================\n\nand the ``'CN'`` colors that index into the default property cycle.\n\nIf the color is the only part of the format string, you can\nadditionally use any  `matplotlib.colors` spec, e.g. full names\n(``'green'``) or hex strings (``'#008000'``).", "parameters": {"type": "object", "properties": {"scalex": {"type": "bool", "default": true}, "scaley": {"type": "bool", "default": true}, "data": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/100", "data": {"name": "matplotlib.pyplot.rc", "type": "callable", "signature": "(group: 'str', **kwargs) -> 'None)", "description": "Set the current `.rcParams`.  *group* is the grouping for the rc, e.g.,\nfor ``lines.linewidth`` the group is ``lines``, for\n``axes.facecolor``, the group is ``axes``, and so on.  Group may\nalso be a list or tuple of group names, e.g., (*xtick*, *ytick*).\n*kwargs* is a dictionary attribute name/value pairs, e.g.,::\n\n  rc('lines', linewidth=2, color='r')\n\nsets the current `.rcParams` and is equivalent to::\n\n  rcParams['lines.linewidth'] = 2\n  rcParams['lines.color'] = 'r'\n\nThe following aliases are available to save typing for interactive users:\n\n=====   =================\nAlias   Property\n=====   =================\n'lw'    'linewidth'\n'ls'    'linestyle'\n'c'     'color'\n'fc'    'facecolor'\n'ec'    'edgecolor'\n'mew'   'markeredgewidth'\n'aa'    'antialiased'\n=====   =================\n\nThus you could abbreviate the above call as::\n\n      rc('lines', lw=2, c='r')\n\nNote you can use python's kwargs dictionary facility to store\ndictionaries of default parameters.  e.g., you can customize the\nfont rc as follows::\n\n  font = {'family' : 'monospace',\n          'weight' : 'bold',\n          'size'   : 'larger'}\n  rc('font', **font)  # pass in the font dict as kwargs\n\nThis enables you to easily switch between several configurations.  Use\n``matplotlib.style.use('default')`` or :func:`~matplotlib.rcdefaults` to\nrestore the default `.rcParams` after changes.\n\nNotes\n-----\nSimilar functionality is available by using the normal dict interface, i.e.\n``rcParams.update({\"lines.linewidth\": 2, ...})`` (but ``rcParams.update``\ndoes not support abbreviations or grouping).", "parameters": {"type": "object", "properties": {"group": {"type": "str"}}}}}
{"task_id": "BigCodeBench/100", "data": {"name": "pandas.date_range", "type": "callable", "signature": "(end=None, periods=None, **kwargs) -> 'DatetimeIndex)", "description": "Return a fixed frequency DatetimeIndex.\n\nReturns the range of equally spaced time points (where the difference between any\ntwo adjacent points is specified by the given frequency) such that they all\nsatisfy `start <[=] x <[=] end`, where the first one and the last one are, resp.,\nthe first and last time points in that range that fall on the boundary of ``freq``\n(if given as a frequency string) or that are valid for ``freq`` (if given as a\n:class:`pandas.tseries.offsets.DateOffset`). (If exactly one of ``start``,\n``end``, or ``freq`` is *not* specified, this missing parameter can be computed\ngiven ``periods``, the number of timesteps in the range. See the note below.)\n\nParameters\n----------\nstart : str or datetime-like, optional\n    Left bound for generating dates.\nend : str or datetime-like, optional\n    Right bound for generating dates.\nperiods : int, optional\n    Number of periods to generate.\nfreq : str, Timedelta, datetime.timedelta, or DateOffset, default 'D'\n    Frequency strings can have multiples, e.g. '5h'. See\n    :ref:`here <timeseries.offset_aliases>` for a list of\n    frequency aliases.\ntz : str or tzinfo, optional\n    Time zone name for returning localized DatetimeIndex, for example\n    'Asia/Hong_Kong'. By default, the resulting DatetimeIndex is\n    timezone-naive unless timezone-aware datetime-likes are passed.\nnormalize : bool, default False\n    Normalize start/end dates to midnight before generating date range.\nname : str, default None\n    Name of the resulting DatetimeIndex.\ninclusive : {\"both\", \"neither\", \"left\", \"right\"}, default \"both\"\n    Include boundaries; Whether to set each bound as closed or open.\n\n    .. versionadded:: 1.4.0\nunit : str, default None\n    Specify the desired resolution of the result.\n\n    .. versionadded:: 2.0.0\n**kwargs\n    For compatibility. Has no effect on the result.\n\nReturns\n-------\nDatetimeIndex\n\nSee Also\n--------\nDatetimeIndex : An immutable container for datetimes.\ntimedelta_range : Return a fixed frequency TimedeltaIndex.\nperiod_range : Return a fixed frequency PeriodIndex.\ninterval_range : Return a fixed frequency IntervalIndex.\n\nNotes\n-----\nOf the four parameters ``start``, ``end``, ``periods``, and ``freq``,\nexactly three must be specified. If ``freq`` is omitted, the resulting\n``DatetimeIndex`` will have ``periods`` linearly spaced elements between\n``start`` and ``end`` (closed on both sides).\n\nTo learn more about the frequency strings, please see `this link\n<https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases>`__.\n\nExamples\n--------\n**Specifying the values**\n\nThe next four examples generate the same `DatetimeIndex`, but vary\nthe combination of `start`, `end` and `periods`.\n\nSpecify `start` and `end`, with the default daily frequency.\n\n>>> pd.date_range(start='1/1/2018', end='1/08/2018')\nDatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03', '2018-01-04',\n               '2018-01-05', '2018-01-06', '2018-01-07', '2018-01-08'],\n              dtype='datetime64[ns]', freq='D')\n\nSpecify timezone-aware `start` and `end`, with the default daily frequency.\n\n>>> pd.date_range(\n...     start=pd.to_datetime(\"1/1/2018\").tz_localize(\"Europe/Berlin\"),\n...     end=pd.to_datetime(\"1/08/2018\").tz_localize(\"Europe/Berlin\"),\n... )\nDatetimeIndex(['2018-01-01 00:00:00+01:00', '2018-01-02 00:00:00+01:00',\n               '2018-01-03 00:00:00+01:00', '2018-01-04 00:00:00+01:00',\n               '2018-01-05 00:00:00+01:00', '2018-01-06 00:00:00+01:00',\n               '2018-01-07 00:00:00+01:00', '2018-01-08 00:00:00+01:00'],\n              dtype='datetime64[ns, Europe/Berlin]', freq='D')\n\nSpecify `start` and `periods`, the number of periods (days).\n\n>>> pd.date_range(start='1/1/2018', periods=8)\nDatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03', '2018-01-04',\n               '2018-01-05', '2018-01-06', '2018-01-07', '2018-01-08'],\n              dtype='datetime64[ns]', freq='D')\n\nSpecify `end` and `periods`, the number of periods (days).\n\n>>> pd.date_range(end='1/1/2018', periods=8)\nDatetimeIndex(['2017-12-25', '2017-12-26', '2017-12-27', '2017-12-28',\n               '2017-12-29', '2017-12-30', '2017-12-31', '2018-01-01'],\n              dtype='datetime64[ns]', freq='D')\n\nSpecify `start`, `end`, and `periods`; the frequency is generated\nautomatically (linearly spaced).\n\n>>> pd.date_range(start='2018-04-24', end='2018-04-27', periods=3)\nDatetimeIndex(['2018-04-24 00:00:00', '2018-04-25 12:00:00',\n               '2018-04-27 00:00:00'],\n              dtype='datetime64[ns]', freq=None)\n\n**Other Parameters**\n\nChanged the `freq` (frequency) to ``'ME'`` (month end frequency).\n\n>>> pd.date_range(start='1/1/2018', periods=5, freq='ME')\nDatetimeIndex(['2018-01-31', '2018-02-28', '2018-03-31', '2018-04-30',\n               '2018-05-31'],\n              dtype='datetime64[ns]', freq='ME')\n\nMultiples are allowed\n\n>>> pd.date_range(start='1/1/2018', periods=5, freq='3ME')\nDatetimeIndex(['2018-01-31', '2018-04-30', '2018-07-31', '2018-10-31',\n               '2019-01-31'],\n              dtype='datetime64[ns]', freq='3ME')\n\n`freq` can also be specified as an Offset object.\n\n>>> pd.date_range(start='1/1/2018', periods=5, freq=pd.offsets.MonthEnd(3))\nDatetimeIndex(['2018-01-31', '2018-04-30', '2018-07-31', '2018-10-31',\n               '2019-01-31'],\n              dtype='datetime64[ns]', freq='3ME')\n\nSpecify `tz` to set the timezone.\n\n>>> pd.date_range(start='1/1/2018', periods=5, tz='Asia/Tokyo')\nDatetimeIndex(['2018-01-01 00:00:00+09:00', '2018-01-02 00:00:00+09:00',\n               '2018-01-03 00:00:00+09:00', '2018-01-04 00:00:00+09:00',\n               '2018-01-05 00:00:00+09:00'],\n              dtype='datetime64[ns, Asia/Tokyo]', freq='D')\n\n`inclusive` controls whether to include `start` and `end` that are on the\nboundary. The default, \"both\", includes boundary points on either end.\n\n>>> pd.date_range(start='2017-01-01', end='2017-01-04', inclusive=\"both\")\nDatetimeIndex(['2017-01-01', '2017-01-02', '2017-01-03', '2017-01-04'],\n              dtype='datetime64[ns]', freq='D')\n\nUse ``inclusive='left'`` to exclude `end` if it falls on the boundary.\n\n>>> pd.date_range(start='2017-01-01', end='2017-01-04', inclusive='left')\nDatetimeIndex(['2017-01-01', '2017-01-02', '2017-01-03'],\n              dtype='datetime64[ns]', freq='D')\n\nUse ``inclusive='right'`` to exclude `start` if it falls on the boundary, and\nsimilarly ``inclusive='neither'`` will exclude both `start` and `end`.\n\n>>> pd.date_range(start='2017-01-01', end='2017-01-04', inclusive='right')\nDatetimeIndex(['2017-01-02', '2017-01-03', '2017-01-04'],\n              dtype='datetime64[ns]', freq='D')\n\n**Specify a unit**\n\n>>> pd.date_range(start=\"2017-01-01\", periods=10, freq=\"100YS\", unit=\"s\")\nDatetimeIndex(['2017-01-01', '2117-01-01', '2217-01-01', '2317-01-01',\n               '2417-01-01', '2517-01-01', '2617-01-01', '2717-01-01',\n               '2817-01-01', '2917-01-01'],\n              dtype='datetime64[s]', freq='100YS-JAN')", "parameters": {"type": "object", "properties": {"end": {"type": "NoneType", "default": null}, "periods": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/100", "data": {"name": "random.seed", "type": "callable", "signature": "(a=None)", "description": "Initialize internal state from a seed.\n\nThe only supported seed types are None, int, float,\nstr, bytes, and bytearray.\n\nNone or no argument seeds from current time or from an operating\nsystem specific randomness source if available.\n\nIf *a* is an int, all bits are used.\n\nFor version 2 (the default), all of the bits are used if *a* is a str,\nbytes, or bytearray.  For version 1 (provided for reproducing random\nsequences from older versions of Python), the algorithm for str and\nbytes generates a narrower range of seeds.", "parameters": {"type": "object", "properties": {"a": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/100", "data": {"name": "random.randint", "type": "callable", "signature": "(a, b)", "description": "Return random integer in range [a, b], including both end points.\n        ", "parameters": {"type": "object", "properties": {"a": {}, "b": {}}}}}
{"task_id": "BigCodeBench/101", "data": {"name": "pandas.DataFrame", "type": "class", "signature": "(data=None, index: 'Axes | None' = None, columns: 'Axes | None' = None, dtype: 'Dtype | None' = None, copy: 'bool | None' = None) -> 'None'", "description": "Two-dimensional, size-mutable, potentially heterogeneous tabular data.\n\nData structure also contains labeled axes (rows and columns).\nArithmetic operations align on both row and column labels. Can be\nthought of as a dict-like container for Series objects. The primary\npandas data structure.\n\nParameters\n----------\ndata : ndarray (structured or homogeneous), Iterable, dict, or DataFrame\n    Dict can contain Series, arrays, constants, dataclass or list-like objects. If\n    data is a dict, column order follows insertion-order. If a dict contains Series\n    which have an index defined, it is aligned by its index. This alignment also\n    occurs if data is a Series or a DataFrame itself. Alignment is done on\n    Series/DataFrame inputs.\n\n    If data is a list of dicts, column order follows insertion-order.\n\nindex : Index or array-like\n    Index to use for resulting frame. Will default to RangeIndex if\n    no indexing information part of input data and no index provided.\ncolumns : Index or array-like\n    Column labels to use for resulting frame when data does not have them,\n    defaulting to RangeIndex(0, 1, 2, ..., n). If data contains column labels,\n    will perform column selection instead.\ndtype : dtype, default None\n    Data type to force. Only a single dtype is allowed. If None, infer.\ncopy : bool or None, default None\n    Copy data from inputs.\n    For dict data, the default of None behaves like ``copy=True``.  For DataFrame\n    or 2d ndarray input, the default of None behaves like ``copy=False``.\n    If data is a dict containing one or more Series (possibly of different dtypes),\n    ``copy=False`` will ensure that these inputs are not copied.\n\n    .. versionchanged:: 1.3.0\n\nSee Also\n--------\nDataFrame.from_records : Constructor from tuples, also record arrays.\nDataFrame.from_dict : From dicts of Series, arrays, or dicts.\nread_csv : Read a comma-separated values (csv) file into DataFrame.\nread_table : Read general delimited file into DataFrame.\nread_clipboard : Read text from clipboard into DataFrame.\n\nNotes\n-----\nPlease reference the :ref:`User Guide <basics.dataframe>` for more information.\n\nExamples\n--------\nConstructing DataFrame from a dictionary.\n\n>>> d = {'col1': [1, 2], 'col2': [3, 4]}\n>>> df = pd.DataFrame(data=d)\n>>> df\n   col1  col2\n0     1     3\n1     2     4\n\nNotice that the inferred dtype is int64.\n\n>>> df.dtypes\ncol1    int64\ncol2    int64\ndtype: object\n\nTo enforce a single dtype:\n\n>>> df = pd.DataFrame(data=d, dtype=np.int8)\n>>> df.dtypes\ncol1    int8\ncol2    int8\ndtype: object\n\nConstructing DataFrame from a dictionary including Series:\n\n>>> d = {'col1': [0, 1, 2, 3], 'col2': pd.Series([2, 3], index=[2, 3])}\n>>> pd.DataFrame(data=d, index=[0, 1, 2, 3])\n   col1  col2\n0     0   NaN\n1     1   NaN\n2     2   2.0\n3     3   3.0\n\nConstructing DataFrame from numpy ndarray:\n\n>>> df2 = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),\n...                    columns=['a', 'b', 'c'])\n>>> df2\n   a  b  c\n0  1  2  3\n1  4  5  6\n2  7  8  9\n\nConstructing DataFrame from a numpy ndarray that has labeled columns:\n\n>>> data = np.array([(1, 2, 3), (4, 5, 6), (7, 8, 9)],\n...                 dtype=[(\"a\", \"i4\"), (\"b\", \"i4\"), (\"c\", \"i4\")])\n>>> df3 = pd.DataFrame(data, columns=['c', 'a'])\n...\n>>> df3\n   c  a\n0  3  1\n1  6  4\n2  9  7\n\nConstructing DataFrame from dataclass:\n\n>>> from dataclasses import make_dataclass\n>>> Point = make_dataclass(\"Point\", [(\"x\", int), (\"y\", int)])\n>>> pd.DataFrame([Point(0, 0), Point(0, 3), Point(2, 3)])\n   x  y\n0  0  0\n1  0  3\n2  2  3\n\nConstructing DataFrame from Series/DataFrame:\n\n>>> ser = pd.Series([1, 2, 3], index=[\"a\", \"b\", \"c\"])\n>>> df = pd.DataFrame(data=ser, index=[\"a\", \"c\"])\n>>> df\n   0\na  1\nc  3\n\n>>> df1 = pd.DataFrame([1, 2, 3], index=[\"a\", \"b\", \"c\"], columns=[\"x\"])\n>>> df2 = pd.DataFrame(data=df1, index=[\"a\", \"c\"])\n>>> df2\n   x\na  1\nc  3", "parameters": {"type": "object", "properties": {"data": {"type": "NoneType", "default": null}, "index": {"type": ["axes", "null"], "default": null}, "columns": {"type": ["axes", "null"], "default": null}, "dtype": {"type": ["dtype", "null"], "default": null}, "copy": {"type": ["bool", "null"], "default": null}}}}}
{"task_id": "BigCodeBench/101", "data": {"name": "pandas.DataFrame.corr", "type": "callable", "signature": "(self)", "description": "Compute pairwise correlation of columns, excluding NA/null values.\n\nParameters\n----------\nmethod : {'pearson', 'kendall', 'spearman'} or callable\n    Method of correlation:\n\n    * pearson : standard correlation coefficient\n    * kendall : Kendall Tau correlation coefficient\n    * spearman : Spearman rank correlation\n    * callable: callable with input two 1d ndarrays\n        and returning a float. Note that the returned matrix from corr\n        will have 1 along the diagonals and will be symmetric\n        regardless of the callable's behavior.\nmin_periods : int, optional\n    Minimum number of observations required per pair of columns\n    to have a valid result. Currently only available for Pearson\n    and Spearman correlation.\nnumeric_only : bool, default False\n    Include only `float`, `int` or `boolean` data.\n\n    .. versionadded:: 1.5.0\n\n    .. versionchanged:: 2.0.0\n        The default value of ``numeric_only`` is now ``False``.\n\nReturns\n-------\nDataFrame\n    Correlation matrix.\n\nSee Also\n--------\nDataFrame.corrwith : Compute pairwise correlation with another\n    DataFrame or Series.\nSeries.corr : Compute the correlation between two Series.\n\nNotes\n-----\nPearson, Kendall and Spearman correlation are currently computed using pairwise complete observations.\n\n* `Pearson correlation coefficient <https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>`_\n* `Kendall rank correlation coefficient <https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient>`_\n* `Spearman's rank correlation coefficient <https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient>`_\n\nExamples\n--------\n>>> def histogram_intersection(a, b):\n...     v = np.minimum(a, b).sum().round(decimals=1)\n...     return v\n>>> df = pd.DataFrame([(.2, .3), (.0, .6), (.6, .0), (.2, .1)],\n...                   columns=['dogs', 'cats'])\n>>> df.corr(method=histogram_intersection)\n      dogs  cats\ndogs   1.0   0.3\ncats   0.3   1.0\n\n>>> df = pd.DataFrame([(1, 1), (2, np.nan), (np.nan, 3), (4, 4)],\n...                   columns=['dogs', 'cats'])\n>>> df.corr(min_periods=3)\n      dogs  cats\ndogs   1.0   NaN\ncats   NaN   1.0", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/101", "data": {"name": "matplotlib.pyplot.rc", "type": "callable", "signature": "(group: 'str', **kwargs) -> 'None)", "description": "Set the current `.rcParams`.  *group* is the grouping for the rc, e.g.,\nfor ``lines.linewidth`` the group is ``lines``, for\n``axes.facecolor``, the group is ``axes``, and so on.  Group may\nalso be a list or tuple of group names, e.g., (*xtick*, *ytick*).\n*kwargs* is a dictionary attribute name/value pairs, e.g.,::\n\n  rc('lines', linewidth=2, color='r')\n\nsets the current `.rcParams` and is equivalent to::\n\n  rcParams['lines.linewidth'] = 2\n  rcParams['lines.color'] = 'r'\n\nThe following aliases are available to save typing for interactive users:\n\n=====   =================\nAlias   Property\n=====   =================\n'lw'    'linewidth'\n'ls'    'linestyle'\n'c'     'color'\n'fc'    'facecolor'\n'ec'    'edgecolor'\n'mew'   'markeredgewidth'\n'aa'    'antialiased'\n=====   =================\n\nThus you could abbreviate the above call as::\n\n      rc('lines', lw=2, c='r')\n\nNote you can use python's kwargs dictionary facility to store\ndictionaries of default parameters.  e.g., you can customize the\nfont rc as follows::\n\n  font = {'family' : 'monospace',\n          'weight' : 'bold',\n          'size'   : 'larger'}\n  rc('font', **font)  # pass in the font dict as kwargs\n\nThis enables you to easily switch between several configurations.  Use\n``matplotlib.style.use('default')`` or :func:`~matplotlib.rcdefaults` to\nrestore the default `.rcParams` after changes.\n\nNotes\n-----\nSimilar functionality is available by using the normal dict interface, i.e.\n``rcParams.update({\"lines.linewidth\": 2, ...})`` (but ``rcParams.update``\ndoes not support abbreviations or grouping).", "parameters": {"type": "object", "properties": {"group": {"type": "str"}}}}}
{"task_id": "BigCodeBench/101", "data": {"name": "matplotlib.pyplot.figure", "type": "callable", "signature": "(figsize: 'tuple[float, **kwargs) -> 'Figure)", "description": "Create a new figure, or activate an existing figure.\n\nParameters\n----------\nnum : int or str or `.Figure` or `.SubFigure`, optional\n    A unique identifier for the figure.\n\n    If a figure with that identifier already exists, this figure is made\n    active and returned. An integer refers to the ``Figure.number``\n    attribute, a string refers to the figure label.\n\n    If there is no figure with the identifier or *num* is not given, a new\n    figure is created, made active and returned.  If *num* is an int, it\n    will be used for the ``Figure.number`` attribute, otherwise, an\n    auto-generated integer value is used (starting at 1 and incremented\n    for each new figure). If *num* is a string, the figure label and the\n    window title is set to this value.  If num is a ``SubFigure``, its\n    parent ``Figure`` is activated.\n\nfigsize : (float, float), default: :rc:`figure.figsize`\n    Width, height in inches.\n\ndpi : float, default: :rc:`figure.dpi`\n    The resolution of the figure in dots-per-inch.\n\nfacecolor : color, default: :rc:`figure.facecolor`\n    The background color.\n\nedgecolor : color, default: :rc:`figure.edgecolor`\n    The border color.\n\nframeon : bool, default: True\n    If False, suppress drawing the figure frame.\n\nFigureClass : subclass of `~matplotlib.figure.Figure`\n    If set, an instance of this subclass will be created, rather than a\n    plain `.Figure`.\n\nclear : bool, default: False\n    If True and the figure already exists, then it is cleared.\n\nlayout : {'constrained', 'compressed', 'tight', 'none', `.LayoutEngine`, None}, default: None\n    The layout mechanism for positioning of plot elements to avoid\n    overlapping Axes decorations (labels, ticks, etc). Note that layout\n    managers can measurably slow down figure display.\n\n    - 'constrained': The constrained layout solver adjusts axes sizes\n      to avoid overlapping axes decorations.  Can handle complex plot\n      layouts and colorbars, and is thus recommended.\n\n      See :ref:`constrainedlayout_guide`\n      for examples.\n\n    - 'compressed': uses the same algorithm as 'constrained', but\n      removes extra space between fixed-aspect-ratio Axes.  Best for\n      simple grids of axes.\n\n    - 'tight': Use the tight layout mechanism. This is a relatively\n      simple algorithm that adjusts the subplot parameters so that\n      decorations do not overlap. See `.Figure.set_tight_layout` for\n      further details.\n\n    - 'none': Do not use a layout engine.\n\n    - A `.LayoutEngine` instance. Builtin layout classes are\n      `.ConstrainedLayoutEngine` and `.TightLayoutEngine`, more easily\n      accessible by 'constrained' and 'tight'.  Passing an instance\n      allows third parties to provide their own layout engine.\n\n    If not given, fall back to using the parameters *tight_layout* and\n    *constrained_layout*, including their config defaults\n    :rc:`figure.autolayout` and :rc:`figure.constrained_layout.use`.\n\n**kwargs\n    Additional keyword arguments are passed to the `.Figure` constructor.\n\nReturns\n-------\n`~matplotlib.figure.Figure`\n\nNotes\n-----\nA newly created figure is passed to the `~.FigureCanvasBase.new_manager`\nmethod or the `new_figure_manager` function provided by the current\nbackend, which install a canvas and a manager on the figure.\n\nOnce this is done, :rc:`figure.hooks` are called, one at a time, on the\nfigure; these hooks allow arbitrary customization of the figure (e.g.,\nattaching callbacks) or of associated elements (e.g., modifying the\ntoolbar).  See :doc:`/gallery/user_interfaces/mplcvd` for an example of\ntoolbar customization.\n\nIf you are creating many figures, make sure you explicitly call\n`.pyplot.close` on the figures you are not using, because this will\nenable pyplot to properly clean up the memory.\n\n`~matplotlib.rcParams` defines the default values, which can be modified\nin the matplotlibrc file.", "parameters": {"type": "object", "properties": {"figsize": {"type": "tuple[float"}}}}}
{"task_id": "BigCodeBench/101", "data": {"name": "numpy.hstack", "type": "callable", "signature": "(tup)", "description": "Stack arrays in sequence horizontally (column wise).\n\nThis is equivalent to concatenation along the second axis, except for 1-D\narrays where it concatenates along the first axis. Rebuilds arrays divided\nby `hsplit`.\n\nThis function makes most sense for arrays with up to 3 dimensions. For\ninstance, for pixel-data with a height (first axis), width (second axis),\nand r/g/b channels (third axis). The functions `concatenate`, `stack` and\n`block` provide more general stacking and concatenation operations.\n\nParameters\n----------\ntup : sequence of ndarrays\n    The arrays must have the same shape along all but the second axis,\n    except 1-D arrays which can be any length.\n\nReturns\n-------\nstacked : ndarray\n    The array formed by stacking the given arrays.\n\nSee Also\n--------\nconcatenate : Join a sequence of arrays along an existing axis.\nstack : Join a sequence of arrays along a new axis.\nblock : Assemble an nd-array from nested lists of blocks.\nvstack : Stack arrays in sequence vertically (row wise).\ndstack : Stack arrays in sequence depth wise (along third axis).\ncolumn_stack : Stack 1-D arrays as columns into a 2-D array.\nhsplit : Split an array into multiple sub-arrays horizontally (column-wise).\n\nExamples\n--------\n>>> a = np.array((1,2,3))\n>>> b = np.array((4,5,6))\n>>> np.hstack((a,b))\narray([1, 2, 3, 4, 5, 6])\n>>> a = np.array([[1],[2],[3]])\n>>> b = np.array([[4],[5],[6]])\n>>> np.hstack((a,b))\narray([[1, 4],\n       [2, 5],\n       [3, 6]])", "parameters": {"type": "object", "properties": {"tup": {}}}}}
{"task_id": "BigCodeBench/101", "data": {"name": "pandas.read_csv", "type": "callable", "signature": "(filepath_or_buffer: 'FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str]', sep: 'str | None | lib.NoDefault' = <no_default>, header: \"int | Sequence[int] | None | Literal['infer']\" = 'infer', skiprows: 'list[int] | int | Callable[[Hashable])", "description": "Read a comma-separated values (csv) file into DataFrame.\n\nAlso supports optionally iterating or breaking of the file\ninto chunks.\n\nAdditional help can be found in the online docs for\n`IO Tools <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html>`_.\n\nParameters\n----------\nfilepath_or_buffer : str, path object or file-like object\n    Any valid string path is acceptable. The string could be a URL. Valid\n    URL schemes include http, ftp, s3, gs, and file. For file URLs, a host is\n    expected. A local file could be: file://localhost/path/to/table.csv.\n\n    If you want to pass in a path object, pandas accepts any ``os.PathLike``.\n\n    By file-like object, we refer to objects with a ``read()`` method, such as\n    a file handle (e.g. via builtin ``open`` function) or ``StringIO``.\nsep : str, default ','\n    Character or regex pattern to treat as the delimiter. If ``sep=None``, the\n    C engine cannot automatically detect\n    the separator, but the Python parsing engine can, meaning the latter will\n    be used and automatically detect the separator from only the first valid\n    row of the file by Python's builtin sniffer tool, ``csv.Sniffer``.\n    In addition, separators longer than 1 character and different from\n    ``'\\s+'`` will be interpreted as regular expressions and will also force\n    the use of the Python parsing engine. Note that regex delimiters are prone\n    to ignoring quoted data. Regex example: ``'\\r\\t'``.\ndelimiter : str, optional\n    Alias for ``sep``.\nheader : int, Sequence of int, 'infer' or None, default 'infer'\n    Row number(s) containing column labels and marking the start of the\n    data (zero-indexed). Default behavior is to infer the column names: if no ``names``\n    are passed the behavior is identical to ``header=0`` and column\n    names are inferred from the first line of the file, if column\n    names are passed explicitly to ``names`` then the behavior is identical to\n    ``header=None``. Explicitly pass ``header=0`` to be able to\n    replace existing names. The header can be a list of integers that\n    specify row locations for a :class:`~pandas.MultiIndex` on the columns\n    e.g. ``[0, 1, 3]``. Intervening rows that are not specified will be\n    skipped (e.g. 2 in this example is skipped). Note that this\n    parameter ignores commented lines and empty lines if\n    ``skip_blank_lines=True``, so ``header=0`` denotes the first line of\n    data rather than the first line of the file.\nnames : Sequence of Hashable, optional\n    Sequence of column labels to apply. If the file contains a header row,\n    then you should explicitly pass ``header=0`` to override the column names.\n    Duplicates in this list are not allowed.\nindex_col : Hashable, Sequence of Hashable or False, optional\n  Column(s) to use as row label(s), denoted either by column labels or column\n  indices.  If a sequence of labels or indices is given, :class:`~pandas.MultiIndex`\n  will be formed for the row labels.\n\n  Note: ``index_col=False`` can be used to force pandas to *not* use the first\n  column as the index, e.g., when you have a malformed file with delimiters at\n  the end of each line.\nusecols : Sequence of Hashable or Callable, optional\n    Subset of columns to select, denoted either by column labels or column indices.\n    If list-like, all elements must either\n    be positional (i.e. integer indices into the document columns) or strings\n    that correspond to column names provided either by the user in ``names`` or\n    inferred from the document header row(s). If ``names`` are given, the document\n    header row(s) are not taken into account. For example, a valid list-like\n    ``usecols`` parameter would be ``[0, 1, 2]`` or ``['foo', 'bar', 'baz']``.\n    Element order is ignored, so ``usecols=[0, 1]`` is the same as ``[1, 0]``.\n    To instantiate a :class:`~pandas.DataFrame` from ``data`` with element order\n    preserved use ``pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']]``\n    for columns in ``['foo', 'bar']`` order or\n    ``pd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']]``\n    for ``['bar', 'foo']`` order.\n\n    If callable, the callable function will be evaluated against the column\n    names, returning names where the callable function evaluates to ``True``. An\n    example of a valid callable argument would be ``lambda x: x.upper() in\n    ['AAA', 'BBB', 'DDD']``. Using this parameter results in much faster\n    parsing time and lower memory usage.\ndtype : dtype or dict of {Hashable : dtype}, optional\n    Data type(s) to apply to either the whole dataset or individual columns.\n    E.g., ``{'a': np.float64, 'b': np.int32, 'c': 'Int64'}``\n    Use ``str`` or ``object`` together with suitable ``na_values`` settings\n    to preserve and not interpret ``dtype``.\n    If ``converters`` are specified, they will be applied INSTEAD\n    of ``dtype`` conversion.\n\n    .. versionadded:: 1.5.0\n\n        Support for ``defaultdict`` was added. Specify a ``defaultdict`` as input where\n        the default determines the ``dtype`` of the columns which are not explicitly\n        listed.\nengine : {'c', 'python', 'pyarrow'}, optional\n    Parser engine to use. The C and pyarrow engines are faster, while the python engine\n    is currently more feature-complete. Multithreading is currently only supported by\n    the pyarrow engine.\n\n    .. versionadded:: 1.4.0\n\n        The 'pyarrow' engine was added as an *experimental* engine, and some features\n        are unsupported, or may not work correctly, with this engine.\nconverters : dict of {Hashable : Callable}, optional\n    Functions for converting values in specified columns. Keys can either\n    be column labels or column indices.\ntrue_values : list, optional\n    Values to consider as ``True`` in addition to case-insensitive variants of 'True'.\nfalse_values : list, optional\n    Values to consider as ``False`` in addition to case-insensitive variants of 'False'.\nskipinitialspace : bool, default False\n    Skip spaces after delimiter.\nskiprows : int, list of int or Callable, optional\n    Line numbers to skip (0-indexed) or number of lines to skip (``int``)\n    at the start of the file.\n\n    If callable, the callable function will be evaluated against the row\n    indices, returning ``True`` if the row should be skipped and ``False`` otherwise.\n    An example of a valid callable argument would be ``lambda x: x in [0, 2]``.\nskipfooter : int, default 0\n    Number of lines at bottom of file to skip (Unsupported with ``engine='c'``).\nnrows : int, optional\n    Number of rows of file to read. Useful for reading pieces of large files.\nna_values : Hashable, Iterable of Hashable or dict of {Hashable : Iterable}, optional\n    Additional strings to recognize as ``NA``/``NaN``. If ``dict`` passed, specific\n    per-column ``NA`` values.  By default the following values are interpreted as\n    ``NaN``: \" \", \"#N/A\", \"#N/A N/A\", \"#NA\", \"-1.#IND\", \"-1.#QNAN\", \"-NaN\", \"-nan\",\n    \"1.#IND\", \"1.#QNAN\", \"<NA>\", \"N/A\", \"NA\", \"NULL\", \"NaN\", \"None\",\n    \"n/a\", \"nan\", \"null \".\n\nkeep_default_na : bool, default True\n    Whether or not to include the default ``NaN`` values when parsing the data.\n    Depending on whether ``na_values`` is passed in, the behavior is as follows:\n\n    * If ``keep_default_na`` is ``True``, and ``na_values`` are specified, ``na_values``\n      is appended to the default ``NaN`` values used for parsing.\n    * If ``keep_default_na`` is ``True``, and ``na_values`` are not specified, only\n      the default ``NaN`` values are used for parsing.\n    * If ``keep_default_na`` is ``False``, and ``na_values`` are specified, only\n      the ``NaN`` values specified ``na_values`` are used for parsing.\n    * If ``keep_default_na`` is ``False``, and ``na_values`` are not specified, no\n      strings will be parsed as ``NaN``.\n\n    Note that if ``na_filter`` is passed in as ``False``, the ``keep_default_na`` and\n    ``na_values`` parameters will be ignored.\nna_filter : bool, default True\n    Detect missing value markers (empty strings and the value of ``na_values``). In\n    data without any ``NA`` values, passing ``na_filter=False`` can improve the\n    performance of reading a large file.\nverbose : bool, default False\n    Indicate number of ``NA`` values placed in non-numeric columns.\n\n    .. deprecated:: 2.2.0\nskip_blank_lines : bool, default True\n    If ``True``, skip over blank lines rather than interpreting as ``NaN`` values.\nparse_dates : bool, list of Hashable, list of lists or dict of {Hashable : list}, default False\n    The behavior is as follows:\n\n    * ``bool``. If ``True`` -> try parsing the index. Note: Automatically set to\n      ``True`` if ``date_format`` or ``date_parser`` arguments have been passed.\n    * ``list`` of ``int`` or names. e.g. If ``[1, 2, 3]`` -> try parsing columns 1, 2, 3\n      each as a separate date column.\n    * ``list`` of ``list``. e.g.  If ``[[1, 3]]`` -> combine columns 1 and 3 and parse\n      as a single date column. Values are joined with a space before parsing.\n    * ``dict``, e.g. ``{'foo' : [1, 3]}`` -> parse columns 1, 3 as date and call\n      result 'foo'. Values are joined with a space before parsing.\n\n    If a column or index cannot be represented as an array of ``datetime``,\n    say because of an unparsable value or a mixture of timezones, the column\n    or index will be returned unaltered as an ``object`` data type. For\n    non-standard ``datetime`` parsing, use :func:`~pandas.to_datetime` after\n    :func:`~pandas.read_csv`.\n\n    Note: A fast-path exists for iso8601-formatted dates.\ninfer_datetime_format : bool, default False\n    If ``True`` and ``parse_dates`` is enabled, pandas will attempt to infer the\n    format of the ``datetime`` strings in the columns, and if it can be inferred,\n    switch to a faster method of parsing them. In some cases this can increase\n    the parsing speed by 5-10x.\n\n    .. deprecated:: 2.0.0\n        A strict version of this argument is now the default, passing it has no effect.\n\nkeep_date_col : bool, default False\n    If ``True`` and ``parse_dates`` specifies combining multiple columns then\n    keep the original columns.\ndate_parser : Callable, optional\n    Function to use for converting a sequence of string columns to an array of\n    ``datetime`` instances. The default uses ``dateutil.parser.parser`` to do the\n    conversion. pandas will try to call ``date_parser`` in three different ways,\n    advancing to the next if an exception occurs: 1) Pass one or more arrays\n    (as defined by ``parse_dates``) as arguments; 2) concatenate (row-wise) the\n    string values from the columns defined by ``parse_dates`` into a single array\n    and pass that; and 3) call ``date_parser`` once for each row using one or\n    more strings (corresponding to the columns defined by ``parse_dates``) as\n    arguments.\n\n    .. deprecated:: 2.0.0\n       Use ``date_format`` instead, or read in as ``object`` and then apply\n       :func:`~pandas.to_datetime` as-needed.\ndate_format : str or dict of column -> format, optional\n    Format to use for parsing dates when used in conjunction with ``parse_dates``.\n    The strftime to parse time, e.g. :const:`\"%d/%m/%Y\"`. See\n    `strftime documentation\n    <https://docs.python.org/3/library/datetime.html\n    #strftime-and-strptime-behavior>`_ for more information on choices, though\n    note that :const:`\"%f\"` will parse all the way up to nanoseconds.\n    You can also pass:\n\n    - \"ISO8601\", to parse any `ISO8601 <https://en.wikipedia.org/wiki/ISO_8601>`_\n        time string (not necessarily in exactly the same format);\n    - \"mixed\", to infer the format for each element individually. This is risky,\n        and you should probably use it along with `dayfirst`.\n\n    .. versionadded:: 2.0.0\ndayfirst : bool, default False\n    DD/MM format dates, international and European format.\ncache_dates : bool, default True\n    If ``True``, use a cache of unique, converted dates to apply the ``datetime``\n    conversion. May produce significant speed-up when parsing duplicate\n    date strings, especially ones with timezone offsets.\n\niterator : bool, default False\n    Return ``TextFileReader`` object for iteration or getting chunks with\n    ``get_chunk()``.\nchunksize : int, optional\n    Number of lines to read from the file per chunk. Passing a value will cause the\n    function to return a ``TextFileReader`` object for iteration.\n    See the `IO Tools docs\n    <https://pandas.pydata.org/pandas-docs/stable/io.html#io-chunking>`_\n    for more information on ``iterator`` and ``chunksize``.\n\ncompression : str or dict, default 'infer'\n    For on-the-fly decompression of on-disk data. If 'infer' and 'filepath_or_buffer' is\n    path-like, then detect compression from the following extensions: '.gz',\n    '.bz2', '.zip', '.xz', '.zst', '.tar', '.tar.gz', '.tar.xz' or '.tar.bz2'\n    (otherwise no compression).\n    If using 'zip' or 'tar', the ZIP file must contain only one data file to be read in.\n    Set to ``None`` for no decompression.\n    Can also be a dict with key ``'method'`` set\n    to one of {``'zip'``, ``'gzip'``, ``'bz2'``, ``'zstd'``, ``'xz'``, ``'tar'``} and\n    other key-value pairs are forwarded to\n    ``zipfile.ZipFile``, ``gzip.GzipFile``,\n    ``bz2.BZ2File``, ``zstandard.ZstdDecompressor``, ``lzma.LZMAFile`` or\n    ``tarfile.TarFile``, respectively.\n    As an example, the following could be passed for Zstandard decompression using a\n    custom compression dictionary:\n    ``compression={'method': 'zstd', 'dict_data': my_compression_dict}``.\n\n    .. versionadded:: 1.5.0\n        Added support for `.tar` files.\n\n    .. versionchanged:: 1.4.0 Zstandard support.\n\nthousands : str (length 1), optional\n    Character acting as the thousands separator in numerical values.\ndecimal : str (length 1), default '.'\n    Character to recognize as decimal point (e.g., use ',' for European data).\nlineterminator : str (length 1), optional\n    Character used to denote a line break. Only valid with C parser.\nquotechar : str (length 1), optional\n    Character used to denote the start and end of a quoted item. Quoted\n    items can include the ``delimiter`` and it will be ignored.\nquoting : {0 or csv.QUOTE_MINIMAL, 1 or csv.QUOTE_ALL, 2 or csv.QUOTE_NONNUMERIC, 3 or csv.QUOTE_NONE}, default csv.QUOTE_MINIMAL\n    Control field quoting behavior per ``csv.QUOTE_*`` constants. Default is\n    ``csv.QUOTE_MINIMAL`` (i.e., 0) which implies that only fields containing special\n    characters are quoted (e.g., characters defined in ``quotechar``, ``delimiter``,\n    or ``lineterminator``.\ndoublequote : bool, default True\n   When ``quotechar`` is specified and ``quoting`` is not ``QUOTE_NONE``, indicate\n   whether or not to interpret two consecutive ``quotechar`` elements INSIDE a\n   field as a single ``quotechar`` element.\nescapechar : str (length 1), optional\n    Character used to escape other characters.\ncomment : str (length 1), optional\n    Character indicating that the remainder of line should not be parsed.\n    If found at the beginning\n    of a line, the line will be ignored altogether. This parameter must be a\n    single character. Like empty lines (as long as ``skip_blank_lines=True``),\n    fully commented lines are ignored by the parameter ``header`` but not by\n    ``skiprows``. For example, if ``comment='#'``, parsing\n    ``#empty\\na,b,c\\n1,2,3`` with ``header=0`` will result in ``'a,b,c'`` being\n    treated as the header.\nencoding : str, optional, default 'utf-8'\n    Encoding to use for UTF when reading/writing (ex. ``'utf-8'``). `List of Python\n    standard encodings\n    <https://docs.python.org/3/library/codecs.html#standard-encodings>`_ .\n\nencoding_errors : str, optional, default 'strict'\n    How encoding errors are treated. `List of possible values\n    <https://docs.python.org/3/library/codecs.html#error-handlers>`_ .\n\n    .. versionadded:: 1.3.0\n\ndialect : str or csv.Dialect, optional\n    If provided, this parameter will override values (default or not) for the\n    following parameters: ``delimiter``, ``doublequote``, ``escapechar``,\n    ``skipinitialspace``, ``quotechar``, and ``quoting``. If it is necessary to\n    override values, a ``ParserWarning`` will be issued. See ``csv.Dialect``\n    documentation for more details.\non_bad_lines : {'error', 'warn', 'skip'} or Callable, default 'error'\n    Specifies what to do upon encountering a bad line (a line with too many fields).\n    Allowed values are :\n\n    - ``'error'``, raise an Exception when a bad line is encountered.\n    - ``'warn'``, raise a warning when a bad line is encountered and skip that line.\n    - ``'skip'``, skip bad lines without raising or warning when they are encountered.\n\n    .. versionadded:: 1.3.0\n\n    .. versionadded:: 1.4.0\n\n        - Callable, function with signature\n          ``(bad_line: list[str]) -> list[str] | None`` that will process a single\n          bad line. ``bad_line`` is a list of strings split by the ``sep``.\n          If the function returns ``None``, the bad line will be ignored.\n          If the function returns a new ``list`` of strings with more elements than\n          expected, a ``ParserWarning`` will be emitted while dropping extra elements.\n          Only supported when ``engine='python'``\n\n    .. versionchanged:: 2.2.0\n\n        - Callable, function with signature\n          as described in `pyarrow documentation\n          <https://arrow.apache.org/docs/python/generated/pyarrow.csv.ParseOptions.html\n          #pyarrow.csv.ParseOptions.invalid_row_handler>`_ when ``engine='pyarrow'``\n\ndelim_whitespace : bool, default False\n    Specifies whether or not whitespace (e.g. ``' '`` or ``'\\t'``) will be\n    used as the ``sep`` delimiter. Equivalent to setting ``sep='\\s+'``. If this option\n    is set to ``True``, nothing should be passed in for the ``delimiter``\n    parameter.\n\n    .. deprecated:: 2.2.0\n        Use ``sep=\"\\s+\"`` instead.\nlow_memory : bool, default True\n    Internally process the file in chunks, resulting in lower memory use\n    while parsing, but possibly mixed type inference.  To ensure no mixed\n    types either set ``False``, or specify the type with the ``dtype`` parameter.\n    Note that the entire file is read into a single :class:`~pandas.DataFrame`\n    regardless, use the ``chunksize`` or ``iterator`` parameter to return the data in\n    chunks. (Only valid with C parser).\nmemory_map : bool, default False\n    If a filepath is provided for ``filepath_or_buffer``, map the file object\n    directly onto memory and access the data directly from there. Using this\n    option can improve performance because there is no longer any I/O overhead.\nfloat_precision : {'high', 'legacy', 'round_trip'}, optional\n    Specifies which converter the C engine should use for floating-point\n    values. The options are ``None`` or ``'high'`` for the ordinary converter,\n    ``'legacy'`` for the original lower precision pandas converter, and\n    ``'round_trip'`` for the round-trip converter.\n\nstorage_options : dict, optional\n    Extra options that make sense for a particular storage connection, e.g.\n    host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n    are forwarded to ``urllib.request.Request`` as header options. For other\n    URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n    forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n    details, and for more examples on storage options refer `here\n    <https://pandas.pydata.org/docs/user_guide/io.html?\n    highlight=storage_options#reading-writing-remote-files>`_.\n\ndtype_backend : {'numpy_nullable', 'pyarrow'}, default 'numpy_nullable'\n    Back-end data type applied to the resultant :class:`DataFrame`\n    (still experimental). Behaviour is as follows:\n\n    * ``\"numpy_nullable\"``: returns nullable-dtype-backed :class:`DataFrame`\n      (default).\n    * ``\"pyarrow\"``: returns pyarrow-backed nullable :class:`ArrowDtype`\n      DataFrame.\n\n    .. versionadded:: 2.0\n\nReturns\n-------\nDataFrame or TextFileReader\n    A comma-separated values (csv) file is returned as two-dimensional\n    data structure with labeled axes.\n\nSee Also\n--------\nDataFrame.to_csv : Write DataFrame to a comma-separated values (csv) file.\nread_table : Read general delimited file into DataFrame.\nread_fwf : Read a table of fixed-width formatted lines into DataFrame.\n\nExamples\n--------\n>>> pd.read_csv('data.csv')  # doctest: +SKIP", "parameters": {"type": "object", "properties": {"filepath_or_buffer": {"type": ["readcsvbuffer[bytes]", "readcsvbuffer[str]", "filepath"]}, "sep": {"type": ["lib.nodefault", "str", "null"], "default": "<no_default>"}, "header": {"type": ["\"int", "literal['infer']\"", "sequence[int]", "null"], "default": "infer"}, "skiprows": {"type": ["list[int]", "callable[[hashable]", "integer"]}}}}}
{"task_id": "BigCodeBench/101", "data": {"name": "pandas.read_csv.values", "type": "callable", "signature": "(filepath_or_buffer: 'FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str]', *, sep: 'str | None | lib.NoDefault' = <no_default>, delimiter: 'str | None | lib.NoDefault' = None, header: \"int | Sequence[int] | None | Literal['infer']\" = 'infer', names: 'Sequence[Hashable] | None | lib.NoDefault' = <no_default>, index_col: 'IndexLabel | Literal[False] | None' = None, usecols: 'UsecolsArgType' = None, dtype: 'DtypeArg | None' = None, engine: 'CSVEngine | None' = None, converters: 'Mapping[Hashable, Callable] | None' = None, true_values: 'list | None' = None, false_values: 'list | None' = None, skipinitialspace: 'bool' = False, skiprows: 'list[int] | int | Callable[[Hashable], bool] | None' = None, skipfooter: 'int' = 0, nrows: 'int | None' = None, na_values: 'Hashable | Iterable[Hashable] | Mapping[Hashable, Iterable[Hashable]] | None' = None, keep_default_na: 'bool' = True, na_filter: 'bool' = True, verbose: 'bool | lib.NoDefault' = <no_default>, skip_blank_lines: 'bool' = True, parse_dates: 'bool | Sequence[Hashable] | None' = None, infer_datetime_format: 'bool | lib.NoDefault' = <no_default>, keep_date_col: 'bool | lib.NoDefault' = <no_default>, date_parser: 'Callable | lib.NoDefault' = <no_default>, date_format: 'str | dict[Hashable, str] | None' = None, dayfirst: 'bool' = False, cache_dates: 'bool' = True, iterator: 'bool' = False, chunksize: 'int | None' = None, compression: 'CompressionOptions' = 'infer', thousands: 'str | None' = None, decimal: 'str' = '.', lineterminator: 'str | None' = None, quotechar: 'str' = '\"', quoting: 'int' = 0, doublequote: 'bool' = True, escapechar: 'str | None' = None, comment: 'str | None' = None, encoding: 'str | None' = None, encoding_errors: 'str | None' = 'strict', dialect: 'str | csv.Dialect | None' = None, on_bad_lines: 'str' = 'error', delim_whitespace: 'bool | lib.NoDefault' = <no_default>, low_memory: 'bool' = True, memory_map: 'bool' = False, float_precision: \"Literal['high', 'legacy'] | None\" = None, storage_options: 'StorageOptions | None' = None, dtype_backend: 'DtypeBackend | lib.NoDefault' = <no_default>) -> 'DataFrame | TextFileReader'", "description": "Read a comma-separated values (csv) file into DataFrame.\n\nAlso supports optionally iterating or breaking of the file\ninto chunks.\n\nAdditional help can be found in the online docs for\n`IO Tools <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html>`_.\n\nParameters\n----------\nfilepath_or_buffer : str, path object or file-like object\n    Any valid string path is acceptable. The string could be a URL. Valid\n    URL schemes include http, ftp, s3, gs, and file. For file URLs, a host is\n    expected. A local file could be: file://localhost/path/to/table.csv.\n\n    If you want to pass in a path object, pandas accepts any ``os.PathLike``.\n\n    By file-like object, we refer to objects with a ``read()`` method, such as\n    a file handle (e.g. via builtin ``open`` function) or ``StringIO``.\nsep : str, default ','\n    Character or regex pattern to treat as the delimiter. If ``sep=None``, the\n    C engine cannot automatically detect\n    the separator, but the Python parsing engine can, meaning the latter will\n    be used and automatically detect the separator from only the first valid\n    row of the file by Python's builtin sniffer tool, ``csv.Sniffer``.\n    In addition, separators longer than 1 character and different from\n    ``'\\s+'`` will be interpreted as regular expressions and will also force\n    the use of the Python parsing engine. Note that regex delimiters are prone\n    to ignoring quoted data. Regex example: ``'\\r\\t'``.\ndelimiter : str, optional\n    Alias for ``sep``.\nheader : int, Sequence of int, 'infer' or None, default 'infer'\n    Row number(s) containing column labels and marking the start of the\n    data (zero-indexed). Default behavior is to infer the column names: if no ``names``\n    are passed the behavior is identical to ``header=0`` and column\n    names are inferred from the first line of the file, if column\n    names are passed explicitly to ``names`` then the behavior is identical to\n    ``header=None``. Explicitly pass ``header=0`` to be able to\n    replace existing names. The header can be a list of integers that\n    specify row locations for a :class:`~pandas.MultiIndex` on the columns\n    e.g. ``[0, 1, 3]``. Intervening rows that are not specified will be\n    skipped (e.g. 2 in this example is skipped). Note that this\n    parameter ignores commented lines and empty lines if\n    ``skip_blank_lines=True``, so ``header=0`` denotes the first line of\n    data rather than the first line of the file.\nnames : Sequence of Hashable, optional\n    Sequence of column labels to apply. If the file contains a header row,\n    then you should explicitly pass ``header=0`` to override the column names.\n    Duplicates in this list are not allowed.\nindex_col : Hashable, Sequence of Hashable or False, optional\n  Column(s) to use as row label(s), denoted either by column labels or column\n  indices.  If a sequence of labels or indices is given, :class:`~pandas.MultiIndex`\n  will be formed for the row labels.\n\n  Note: ``index_col=False`` can be used to force pandas to *not* use the first\n  column as the index, e.g., when you have a malformed file with delimiters at\n  the end of each line.\nusecols : Sequence of Hashable or Callable, optional\n    Subset of columns to select, denoted either by column labels or column indices.\n    If list-like, all elements must either\n    be positional (i.e. integer indices into the document columns) or strings\n    that correspond to column names provided either by the user in ``names`` or\n    inferred from the document header row(s). If ``names`` are given, the document\n    header row(s) are not taken into account. For example, a valid list-like\n    ``usecols`` parameter would be ``[0, 1, 2]`` or ``['foo', 'bar', 'baz']``.\n    Element order is ignored, so ``usecols=[0, 1]`` is the same as ``[1, 0]``.\n    To instantiate a :class:`~pandas.DataFrame` from ``data`` with element order\n    preserved use ``pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']]``\n    for columns in ``['foo', 'bar']`` order or\n    ``pd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']]``\n    for ``['bar', 'foo']`` order.\n\n    If callable, the callable function will be evaluated against the column\n    names, returning names where the callable function evaluates to ``True``. An\n    example of a valid callable argument would be ``lambda x: x.upper() in\n    ['AAA', 'BBB', 'DDD']``. Using this parameter results in much faster\n    parsing time and lower memory usage.\ndtype : dtype or dict of {Hashable : dtype}, optional\n    Data type(s) to apply to either the whole dataset or individual columns.\n    E.g., ``{'a': np.float64, 'b': np.int32, 'c': 'Int64'}``\n    Use ``str`` or ``object`` together with suitable ``na_values`` settings\n    to preserve and not interpret ``dtype``.\n    If ``converters`` are specified, they will be applied INSTEAD\n    of ``dtype`` conversion.\n\n    .. versionadded:: 1.5.0\n\n        Support for ``defaultdict`` was added. Specify a ``defaultdict`` as input where\n        the default determines the ``dtype`` of the columns which are not explicitly\n        listed.\nengine : {'c', 'python', 'pyarrow'}, optional\n    Parser engine to use. The C and pyarrow engines are faster, while the python engine\n    is currently more feature-complete. Multithreading is currently only supported by\n    the pyarrow engine.\n\n    .. versionadded:: 1.4.0\n\n        The 'pyarrow' engine was added as an *experimental* engine, and some features\n        are unsupported, or may not work correctly, with this engine.\nconverters : dict of {Hashable : Callable}, optional\n    Functions for converting values in specified columns. Keys can either\n    be column labels or column indices.\ntrue_values : list, optional\n    Values to consider as ``True`` in addition to case-insensitive variants of 'True'.\nfalse_values : list, optional\n    Values to consider as ``False`` in addition to case-insensitive variants of 'False'.\nskipinitialspace : bool, default False\n    Skip spaces after delimiter.\nskiprows : int, list of int or Callable, optional\n    Line numbers to skip (0-indexed) or number of lines to skip (``int``)\n    at the start of the file.\n\n    If callable, the callable function will be evaluated against the row\n    indices, returning ``True`` if the row should be skipped and ``False`` otherwise.\n    An example of a valid callable argument would be ``lambda x: x in [0, 2]``.\nskipfooter : int, default 0\n    Number of lines at bottom of file to skip (Unsupported with ``engine='c'``).\nnrows : int, optional\n    Number of rows of file to read. Useful for reading pieces of large files.\nna_values : Hashable, Iterable of Hashable or dict of {Hashable : Iterable}, optional\n    Additional strings to recognize as ``NA``/``NaN``. If ``dict`` passed, specific\n    per-column ``NA`` values.  By default the following values are interpreted as\n    ``NaN``: \" \", \"#N/A\", \"#N/A N/A\", \"#NA\", \"-1.#IND\", \"-1.#QNAN\", \"-NaN\", \"-nan\",\n    \"1.#IND\", \"1.#QNAN\", \"<NA>\", \"N/A\", \"NA\", \"NULL\", \"NaN\", \"None\",\n    \"n/a\", \"nan\", \"null \".\n\nkeep_default_na : bool, default True\n    Whether or not to include the default ``NaN`` values when parsing the data.\n    Depending on whether ``na_values`` is passed in, the behavior is as follows:\n\n    * If ``keep_default_na`` is ``True``, and ``na_values`` are specified, ``na_values``\n      is appended to the default ``NaN`` values used for parsing.\n    * If ``keep_default_na`` is ``True``, and ``na_values`` are not specified, only\n      the default ``NaN`` values are used for parsing.\n    * If ``keep_default_na`` is ``False``, and ``na_values`` are specified, only\n      the ``NaN`` values specified ``na_values`` are used for parsing.\n    * If ``keep_default_na`` is ``False``, and ``na_values`` are not specified, no\n      strings will be parsed as ``NaN``.\n\n    Note that if ``na_filter`` is passed in as ``False``, the ``keep_default_na`` and\n    ``na_values`` parameters will be ignored.\nna_filter : bool, default True\n    Detect missing value markers (empty strings and the value of ``na_values``). In\n    data without any ``NA`` values, passing ``na_filter=False`` can improve the\n    performance of reading a large file.\nverbose : bool, default False\n    Indicate number of ``NA`` values placed in non-numeric columns.\n\n    .. deprecated:: 2.2.0\nskip_blank_lines : bool, default True\n    If ``True``, skip over blank lines rather than interpreting as ``NaN`` values.\nparse_dates : bool, list of Hashable, list of lists or dict of {Hashable : list}, default False\n    The behavior is as follows:\n\n    * ``bool``. If ``True`` -> try parsing the index. Note: Automatically set to\n      ``True`` if ``date_format`` or ``date_parser`` arguments have been passed.\n    * ``list`` of ``int`` or names. e.g. If ``[1, 2, 3]`` -> try parsing columns 1, 2, 3\n      each as a separate date column.\n    * ``list`` of ``list``. e.g.  If ``[[1, 3]]`` -> combine columns 1 and 3 and parse\n      as a single date column. Values are joined with a space before parsing.\n    * ``dict``, e.g. ``{'foo' : [1, 3]}`` -> parse columns 1, 3 as date and call\n      result 'foo'. Values are joined with a space before parsing.\n\n    If a column or index cannot be represented as an array of ``datetime``,\n    say because of an unparsable value or a mixture of timezones, the column\n    or index will be returned unaltered as an ``object`` data type. For\n    non-standard ``datetime`` parsing, use :func:`~pandas.to_datetime` after\n    :func:`~pandas.read_csv`.\n\n    Note: A fast-path exists for iso8601-formatted dates.\ninfer_datetime_format : bool, default False\n    If ``True`` and ``parse_dates`` is enabled, pandas will attempt to infer the\n    format of the ``datetime`` strings in the columns, and if it can be inferred,\n    switch to a faster method of parsing them. In some cases this can increase\n    the parsing speed by 5-10x.\n\n    .. deprecated:: 2.0.0\n        A strict version of this argument is now the default, passing it has no effect.\n\nkeep_date_col : bool, default False\n    If ``True`` and ``parse_dates`` specifies combining multiple columns then\n    keep the original columns.\ndate_parser : Callable, optional\n    Function to use for converting a sequence of string columns to an array of\n    ``datetime`` instances. The default uses ``dateutil.parser.parser`` to do the\n    conversion. pandas will try to call ``date_parser`` in three different ways,\n    advancing to the next if an exception occurs: 1) Pass one or more arrays\n    (as defined by ``parse_dates``) as arguments; 2) concatenate (row-wise) the\n    string values from the columns defined by ``parse_dates`` into a single array\n    and pass that; and 3) call ``date_parser`` once for each row using one or\n    more strings (corresponding to the columns defined by ``parse_dates``) as\n    arguments.\n\n    .. deprecated:: 2.0.0\n       Use ``date_format`` instead, or read in as ``object`` and then apply\n       :func:`~pandas.to_datetime` as-needed.\ndate_format : str or dict of column -> format, optional\n    Format to use for parsing dates when used in conjunction with ``parse_dates``.\n    The strftime to parse time, e.g. :const:`\"%d/%m/%Y\"`. See\n    `strftime documentation\n    <https://docs.python.org/3/library/datetime.html\n    #strftime-and-strptime-behavior>`_ for more information on choices, though\n    note that :const:`\"%f\"` will parse all the way up to nanoseconds.\n    You can also pass:\n\n    - \"ISO8601\", to parse any `ISO8601 <https://en.wikipedia.org/wiki/ISO_8601>`_\n        time string (not necessarily in exactly the same format);\n    - \"mixed\", to infer the format for each element individually. This is risky,\n        and you should probably use it along with `dayfirst`.\n\n    .. versionadded:: 2.0.0\ndayfirst : bool, default False\n    DD/MM format dates, international and European format.\ncache_dates : bool, default True\n    If ``True``, use a cache of unique, converted dates to apply the ``datetime``\n    conversion. May produce significant speed-up when parsing duplicate\n    date strings, especially ones with timezone offsets.\n\niterator : bool, default False\n    Return ``TextFileReader`` object for iteration or getting chunks with\n    ``get_chunk()``.\nchunksize : int, optional\n    Number of lines to read from the file per chunk. Passing a value will cause the\n    function to return a ``TextFileReader`` object for iteration.\n    See the `IO Tools docs\n    <https://pandas.pydata.org/pandas-docs/stable/io.html#io-chunking>`_\n    for more information on ``iterator`` and ``chunksize``.\n\ncompression : str or dict, default 'infer'\n    For on-the-fly decompression of on-disk data. If 'infer' and 'filepath_or_buffer' is\n    path-like, then detect compression from the following extensions: '.gz',\n    '.bz2', '.zip', '.xz', '.zst', '.tar', '.tar.gz', '.tar.xz' or '.tar.bz2'\n    (otherwise no compression).\n    If using 'zip' or 'tar', the ZIP file must contain only one data file to be read in.\n    Set to ``None`` for no decompression.\n    Can also be a dict with key ``'method'`` set\n    to one of {``'zip'``, ``'gzip'``, ``'bz2'``, ``'zstd'``, ``'xz'``, ``'tar'``} and\n    other key-value pairs are forwarded to\n    ``zipfile.ZipFile``, ``gzip.GzipFile``,\n    ``bz2.BZ2File``, ``zstandard.ZstdDecompressor``, ``lzma.LZMAFile`` or\n    ``tarfile.TarFile``, respectively.\n    As an example, the following could be passed for Zstandard decompression using a\n    custom compression dictionary:\n    ``compression={'method': 'zstd', 'dict_data': my_compression_dict}``.\n\n    .. versionadded:: 1.5.0\n        Added support for `.tar` files.\n\n    .. versionchanged:: 1.4.0 Zstandard support.\n\nthousands : str (length 1), optional\n    Character acting as the thousands separator in numerical values.\ndecimal : str (length 1), default '.'\n    Character to recognize as decimal point (e.g., use ',' for European data).\nlineterminator : str (length 1), optional\n    Character used to denote a line break. Only valid with C parser.\nquotechar : str (length 1), optional\n    Character used to denote the start and end of a quoted item. Quoted\n    items can include the ``delimiter`` and it will be ignored.\nquoting : {0 or csv.QUOTE_MINIMAL, 1 or csv.QUOTE_ALL, 2 or csv.QUOTE_NONNUMERIC, 3 or csv.QUOTE_NONE}, default csv.QUOTE_MINIMAL\n    Control field quoting behavior per ``csv.QUOTE_*`` constants. Default is\n    ``csv.QUOTE_MINIMAL`` (i.e., 0) which implies that only fields containing special\n    characters are quoted (e.g., characters defined in ``quotechar``, ``delimiter``,\n    or ``lineterminator``.\ndoublequote : bool, default True\n   When ``quotechar`` is specified and ``quoting`` is not ``QUOTE_NONE``, indicate\n   whether or not to interpret two consecutive ``quotechar`` elements INSIDE a\n   field as a single ``quotechar`` element.\nescapechar : str (length 1), optional\n    Character used to escape other characters.\ncomment : str (length 1), optional\n    Character indicating that the remainder of line should not be parsed.\n    If found at the beginning\n    of a line, the line will be ignored altogether. This parameter must be a\n    single character. Like empty lines (as long as ``skip_blank_lines=True``),\n    fully commented lines are ignored by the parameter ``header`` but not by\n    ``skiprows``. For example, if ``comment='#'``, parsing\n    ``#empty\\na,b,c\\n1,2,3`` with ``header=0`` will result in ``'a,b,c'`` being\n    treated as the header.\nencoding : str, optional, default 'utf-8'\n    Encoding to use for UTF when reading/writing (ex. ``'utf-8'``). `List of Python\n    standard encodings\n    <https://docs.python.org/3/library/codecs.html#standard-encodings>`_ .\n\nencoding_errors : str, optional, default 'strict'\n    How encoding errors are treated. `List of possible values\n    <https://docs.python.org/3/library/codecs.html#error-handlers>`_ .\n\n    .. versionadded:: 1.3.0\n\ndialect : str or csv.Dialect, optional\n    If provided, this parameter will override values (default or not) for the\n    following parameters: ``delimiter``, ``doublequote``, ``escapechar``,\n    ``skipinitialspace``, ``quotechar``, and ``quoting``. If it is necessary to\n    override values, a ``ParserWarning`` will be issued. See ``csv.Dialect``\n    documentation for more details.\non_bad_lines : {'error', 'warn', 'skip'} or Callable, default 'error'\n    Specifies what to do upon encountering a bad line (a line with too many fields).\n    Allowed values are :\n\n    - ``'error'``, raise an Exception when a bad line is encountered.\n    - ``'warn'``, raise a warning when a bad line is encountered and skip that line.\n    - ``'skip'``, skip bad lines without raising or warning when they are encountered.\n\n    .. versionadded:: 1.3.0\n\n    .. versionadded:: 1.4.0\n\n        - Callable, function with signature\n          ``(bad_line: list[str]) -> list[str] | None`` that will process a single\n          bad line. ``bad_line`` is a list of strings split by the ``sep``.\n          If the function returns ``None``, the bad line will be ignored.\n          If the function returns a new ``list`` of strings with more elements than\n          expected, a ``ParserWarning`` will be emitted while dropping extra elements.\n          Only supported when ``engine='python'``\n\n    .. versionchanged:: 2.2.0\n\n        - Callable, function with signature\n          as described in `pyarrow documentation\n          <https://arrow.apache.org/docs/python/generated/pyarrow.csv.ParseOptions.html\n          #pyarrow.csv.ParseOptions.invalid_row_handler>`_ when ``engine='pyarrow'``\n\ndelim_whitespace : bool, default False\n    Specifies whether or not whitespace (e.g. ``' '`` or ``'\\t'``) will be\n    used as the ``sep`` delimiter. Equivalent to setting ``sep='\\s+'``. If this option\n    is set to ``True``, nothing should be passed in for the ``delimiter``\n    parameter.\n\n    .. deprecated:: 2.2.0\n        Use ``sep=\"\\s+\"`` instead.\nlow_memory : bool, default True\n    Internally process the file in chunks, resulting in lower memory use\n    while parsing, but possibly mixed type inference.  To ensure no mixed\n    types either set ``False``, or specify the type with the ``dtype`` parameter.\n    Note that the entire file is read into a single :class:`~pandas.DataFrame`\n    regardless, use the ``chunksize`` or ``iterator`` parameter to return the data in\n    chunks. (Only valid with C parser).\nmemory_map : bool, default False\n    If a filepath is provided for ``filepath_or_buffer``, map the file object\n    directly onto memory and access the data directly from there. Using this\n    option can improve performance because there is no longer any I/O overhead.\nfloat_precision : {'high', 'legacy', 'round_trip'}, optional\n    Specifies which converter the C engine should use for floating-point\n    values. The options are ``None`` or ``'high'`` for the ordinary converter,\n    ``'legacy'`` for the original lower precision pandas converter, and\n    ``'round_trip'`` for the round-trip converter.\n\nstorage_options : dict, optional\n    Extra options that make sense for a particular storage connection, e.g.\n    host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n    are forwarded to ``urllib.request.Request`` as header options. For other\n    URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n    forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n    details, and for more examples on storage options refer `here\n    <https://pandas.pydata.org/docs/user_guide/io.html?\n    highlight=storage_options#reading-writing-remote-files>`_.\n\ndtype_backend : {'numpy_nullable', 'pyarrow'}, default 'numpy_nullable'\n    Back-end data type applied to the resultant :class:`DataFrame`\n    (still experimental). Behaviour is as follows:\n\n    * ``\"numpy_nullable\"``: returns nullable-dtype-backed :class:`DataFrame`\n      (default).\n    * ``\"pyarrow\"``: returns pyarrow-backed nullable :class:`ArrowDtype`\n      DataFrame.\n\n    .. versionadded:: 2.0\n\nReturns\n-------\nDataFrame or TextFileReader\n    A comma-separated values (csv) file is returned as two-dimensional\n    data structure with labeled axes.\n\nSee Also\n--------\nDataFrame.to_csv : Write DataFrame to a comma-separated values (csv) file.\nread_table : Read general delimited file into DataFrame.\nread_fwf : Read a table of fixed-width formatted lines into DataFrame.\n\nExamples\n--------\n>>> pd.read_csv('data.csv')  # doctest: +SKIP", "parameters": {"type": "object", "properties": {"filepath_or_buffer": {"type": ["readcsvbuffer[bytes]", "readcsvbuffer[str]", "filepath"]}, "sep": {"type": ["lib.nodefault", "str", "null"], "default": "<no_default>"}, "delimiter": {"type": ["lib.nodefault", "str", "null"], "default": null}, "header": {"type": ["\"int", "literal['infer']\"", "sequence[int]", "null"], "default": "infer"}, "names": {"type": ["lib.nodefault", "sequence[hashable]", "null"], "default": "<no_default>"}, "index_col": {"type": ["indexlabel", "literal[false]", "null"], "default": null}, "usecols": {"type": "usecolsargtype", "default": null}, "dtype": {"type": ["dtypearg", "null"], "default": null}, "engine": {"type": ["csvengine", "null"], "default": null}, "converters": {"type": ["mapping[hashable, callable]", "null"], "default": null}, "true_values": {"type": ["list", "null"], "default": null}, "false_values": {"type": ["list", "null"], "default": null}, "skipinitialspace": {"type": "bool", "default": false}, "skiprows": {"type": ["null", "list[int]", "callable[[hashable], bool]", "integer"], "default": null}, "skipfooter": {"type": "integer", "default": 0}, "nrows": {"type": ["integer", "null"], "default": null}, "na_values": {"type": ["hashable", "iterable[hashable]", "mapping[hashable"]}, "Iterable[Hashable]] | None'": {"type": "NoneType", "default": null}, "keep_default_na": {"type": "bool", "default": true}, "na_filter": {"type": "bool", "default": true}, "verbose": {"type": ["bool", "lib.nodefault"], "default": "<no_default>"}, "skip_blank_lines": {"type": "bool", "default": true}, "parse_dates": {"type": ["bool", "sequence[hashable]", "null"], "default": null}, "infer_datetime_format": {"type": ["bool", "lib.nodefault"], "default": "<no_default>"}, "keep_date_col": {"type": ["bool", "lib.nodefault"], "default": "<no_default>"}, "date_parser": {"type": ["callable", "lib.nodefault"], "default": "<no_default>"}, "date_format": {"type": ["dict[hashable, str]", "str", "null"], "default": null}, "dayfirst": {"type": "bool", "default": false}, "cache_dates": {"type": "bool", "default": true}, "iterator": {"type": "bool", "default": false}, "chunksize": {"type": ["integer", "null"], "default": null}, "compression": {"type": "compressionoptions", "default": "infer"}, "thousands": {"type": ["str", "null"], "default": null}, "decimal": {"type": "str", "default": "."}, "lineterminator": {"type": ["str", "null"], "default": null}, "quotechar": {"type": "str", "default": ""}, "quoting": {"type": "integer", "default": 0}, "doublequote": {"type": "bool", "default": true}, "escapechar": {"type": ["str", "null"], "default": null}, "comment": {"type": ["str", "null"], "default": null}, "encoding": {"type": ["str", "null"], "default": null}, "encoding_errors": {"type": ["str", "null"], "default": "strict"}, "dialect": {"type": ["csv.dialect", "str", "null"], "default": null}, "on_bad_lines": {"type": "str", "default": "error"}, "delim_whitespace": {"type": ["bool", "lib.nodefault"], "default": "<no_default>"}, "low_memory": {"type": "bool", "default": true}, "memory_map": {"type": "bool", "default": false}, "float_precision": {"type": ["none\"", "\"literal['high', 'legacy']"], "default": null}, "storage_options": {"type": ["storageoptions", "null"], "default": null}, "dtype_backend": {"type": ["dtypebackend", "lib.nodefault"], "default": "<no_default>"}}}}}
{"task_id": "BigCodeBench/101", "data": {"name": "seaborn.set_theme", "type": "callable", "signature": "(style='darkgrid')", "description": "Set aspects of the visual theme for all matplotlib and seaborn plots.\n\nThis function changes the global defaults for all plots using the\nmatplotlib rcParams system. The themeing is decomposed into several distinct\nsets of parameter values.\n\nThe options are illustrated in the :doc:`aesthetics <../tutorial/aesthetics>`\nand :doc:`color palette <../tutorial/color_palettes>` tutorials.\n\nParameters\n----------\ncontext : string or dict\n    Scaling parameters, see :func:`plotting_context`.\nstyle : string or dict\n    Axes style parameters, see :func:`axes_style`.\npalette : string or sequence\n    Color palette, see :func:`color_palette`.\nfont : string\n    Font family, see matplotlib font manager.\nfont_scale : float, optional\n    Separate scaling factor to independently scale the size of the\n    font elements.\ncolor_codes : bool\n    If ``True`` and ``palette`` is a seaborn palette, remap the shorthand\n    color codes (e.g. \"b\", \"g\", \"r\", etc.) to the colors from this palette.\nrc : dict or None\n    Dictionary of rc parameter mappings to override the above.\n\nExamples\n--------\n\n.. include:: ../docstrings/set_theme.rst", "parameters": {"type": "object", "properties": {"style": {"type": "str", "default": "darkgrid"}}}}}
{"task_id": "BigCodeBench/101", "data": {"name": "seaborn.heatmap", "type": "callable", "signature": "(data, annot=None, **kwargs)", "description": "Plot rectangular data as a color-encoded matrix.\n\nThis is an Axes-level function and will draw the heatmap into the\ncurrently-active Axes if none is provided to the ``ax`` argument.  Part of\nthis Axes space will be taken and used to plot a colormap, unless ``cbar``\nis False or a separate Axes is provided to ``cbar_ax``.\n\nParameters\n----------\ndata : rectangular dataset\n    2D dataset that can be coerced into an ndarray. If a Pandas DataFrame\n    is provided, the index/column information will be used to label the\n    columns and rows.\nvmin, vmax : floats, optional\n    Values to anchor the colormap, otherwise they are inferred from the\n    data and other keyword arguments.\ncmap : matplotlib colormap name or object, or list of colors, optional\n    The mapping from data values to color space. If not provided, the\n    default will depend on whether ``center`` is set.\ncenter : float, optional\n    The value at which to center the colormap when plotting divergent data.\n    Using this parameter will change the default ``cmap`` if none is\n    specified.\nrobust : bool, optional\n    If True and ``vmin`` or ``vmax`` are absent, the colormap range is\n    computed with robust quantiles instead of the extreme values.\nannot : bool or rectangular dataset, optional\n    If True, write the data value in each cell. If an array-like with the\n    same shape as ``data``, then use this to annotate the heatmap instead\n    of the data. Note that DataFrames will match on position, not index.\nfmt : str, optional\n    String formatting code to use when adding annotations.\nannot_kws : dict of key, value mappings, optional\n    Keyword arguments for :meth:`matplotlib.axes.Axes.text` when ``annot``\n    is True.\nlinewidths : float, optional\n    Width of the lines that will divide each cell.\nlinecolor : color, optional\n    Color of the lines that will divide each cell.\ncbar : bool, optional\n    Whether to draw a colorbar.\ncbar_kws : dict of key, value mappings, optional\n    Keyword arguments for :meth:`matplotlib.figure.Figure.colorbar`.\ncbar_ax : matplotlib Axes, optional\n    Axes in which to draw the colorbar, otherwise take space from the\n    main Axes.\nsquare : bool, optional\n    If True, set the Axes aspect to \"equal\" so each cell will be\n    square-shaped.\nxticklabels, yticklabels : \"auto\", bool, list-like, or int, optional\n    If True, plot the column names of the dataframe. If False, don't plot\n    the column names. If list-like, plot these alternate labels as the\n    xticklabels. If an integer, use the column names but plot only every\n    n label. If \"auto\", try to densely plot non-overlapping labels.\nmask : bool array or DataFrame, optional\n    If passed, data will not be shown in cells where ``mask`` is True.\n    Cells with missing values are automatically masked.\nax : matplotlib Axes, optional\n    Axes in which to draw the plot, otherwise use the currently-active\n    Axes.\nkwargs : other keyword arguments\n    All other keyword arguments are passed to\n    :meth:`matplotlib.axes.Axes.pcolormesh`.\n\nReturns\n-------\nax : matplotlib Axes\n    Axes object with the heatmap.\n\nSee Also\n--------\nclustermap : Plot a matrix using hierarchical clustering to arrange the\n             rows and columns.\n\nExamples\n--------\n\n.. include:: ../docstrings/heatmap.rst", "parameters": {"type": "object", "properties": {"data": {}, "annot": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/108", "data": {"name": "df.columns"}}
{"task_id": "BigCodeBench/108", "data": {"name": "df.set_index('date').set_index('date')"}}
{"task_id": "BigCodeBench/108", "data": {"name": "df.asfreq(freq, method='pad').plot(y='value')"}}
{"task_id": "BigCodeBench/108", "data": {"name": "df.asfreq(freq, method='pad').asfreq(freq, method='pad')"}}
{"task_id": "BigCodeBench/108", "data": {"name": "matplotlib.pyplot.ylabel", "type": "callable", "signature": "(ylabel: 'str')", "description": "Set the label for the y-axis.\n\nParameters\n----------\nylabel : str\n    The label text.\n\nlabelpad : float, default: :rc:`axes.labelpad`\n    Spacing in points from the Axes bounding box including ticks\n    and tick labels.  If None, the previous value is left as is.\n\nloc : {'bottom', 'center', 'top'}, default: :rc:`yaxis.labellocation`\n    The label position. This is a high-level alternative for passing\n    parameters *y* and *horizontalalignment*.\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    `.Text` properties control the appearance of the label.\n\nSee Also\n--------\ntext : Documents the properties supported by `.Text`.", "parameters": {"type": "object", "properties": {"ylabel": {"type": "str"}}}}}
{"task_id": "BigCodeBench/108", "data": {"name": "matplotlib.pyplot.title", "type": "callable", "signature": "(label: 'str')", "description": "Set a title for the Axes.\n\nSet one of the three available Axes titles. The available titles\nare positioned above the Axes in the center, flush with the left\nedge, and flush with the right edge.\n\nParameters\n----------\nlabel : str\n    Text to use for the title\n\nfontdict : dict\n\n    .. admonition:: Discouraged\n\n       The use of *fontdict* is discouraged. Parameters should be passed as\n       individual keyword arguments or using dictionary-unpacking\n       ``set_title(..., **fontdict)``.\n\n    A dictionary controlling the appearance of the title text,\n    the default *fontdict* is::\n\n       {'fontsize': rcParams['axes.titlesize'],\n        'fontweight': rcParams['axes.titleweight'],\n        'color': rcParams['axes.titlecolor'],\n        'verticalalignment': 'baseline',\n        'horizontalalignment': loc}\n\nloc : {'center', 'left', 'right'}, default: :rc:`axes.titlelocation`\n    Which title to set.\n\ny : float, default: :rc:`axes.titley`\n    Vertical Axes location for the title (1.0 is the top).  If\n    None (the default) and :rc:`axes.titley` is also None, y is\n    determined automatically to avoid decorators on the Axes.\n\npad : float, default: :rc:`axes.titlepad`\n    The offset of the title from the top of the Axes, in points.\n\nReturns\n-------\n`.Text`\n    The matplotlib text instance representing the title\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    Other keyword arguments are text properties, see `.Text` for a list\n    of valid text properties.", "parameters": {"type": "object", "properties": {"label": {"type": "str"}}}}}
{"task_id": "BigCodeBench/108", "data": {"name": "pandas.to_numeric", "type": "callable", "signature": "(arg, errors: 'DateTimeErrorChoices' = 'raise')", "description": "Convert argument to a numeric type.\n\nThe default return dtype is `float64` or `int64`\ndepending on the data supplied. Use the `downcast` parameter\nto obtain other dtypes.\n\nPlease note that precision loss may occur if really large numbers\nare passed in. Due to the internal limitations of `ndarray`, if\nnumbers smaller than `-9223372036854775808` (np.iinfo(np.int64).min)\nor larger than `18446744073709551615` (np.iinfo(np.uint64).max) are\npassed in, it is very likely they will be converted to float so that\nthey can be stored in an `ndarray`. These warnings apply similarly to\n`Series` since it internally leverages `ndarray`.\n\nParameters\n----------\narg : scalar, list, tuple, 1-d array, or Series\n    Argument to be converted.\nerrors : {'ignore', 'raise', 'coerce'}, default 'raise'\n    - If 'raise', then invalid parsing will raise an exception.\n    - If 'coerce', then invalid parsing will be set as NaN.\n    - If 'ignore', then invalid parsing will return the input.\n\n    .. versionchanged:: 2.2\n\n    \"ignore\" is deprecated. Catch exceptions explicitly instead.\n\ndowncast : str, default None\n    Can be 'integer', 'signed', 'unsigned', or 'float'.\n    If not None, and if the data has been successfully cast to a\n    numerical dtype (or if the data was numeric to begin with),\n    downcast that resulting data to the smallest numerical dtype\n    possible according to the following rules:\n\n    - 'integer' or 'signed': smallest signed int dtype (min.: np.int8)\n    - 'unsigned': smallest unsigned int dtype (min.: np.uint8)\n    - 'float': smallest float dtype (min.: np.float32)\n\n    As this behaviour is separate from the core conversion to\n    numeric values, any errors raised during the downcasting\n    will be surfaced regardless of the value of the 'errors' input.\n\n    In addition, downcasting will only occur if the size\n    of the resulting data's dtype is strictly larger than\n    the dtype it is to be cast to, so if none of the dtypes\n    checked satisfy that specification, no downcasting will be\n    performed on the data.\ndtype_backend : {'numpy_nullable', 'pyarrow'}, default 'numpy_nullable'\n    Back-end data type applied to the resultant :class:`DataFrame`\n    (still experimental). Behaviour is as follows:\n\n    * ``\"numpy_nullable\"``: returns nullable-dtype-backed :class:`DataFrame`\n      (default).\n    * ``\"pyarrow\"``: returns pyarrow-backed nullable :class:`ArrowDtype`\n      DataFrame.\n\n    .. versionadded:: 2.0\n\nReturns\n-------\nret\n    Numeric if parsing succeeded.\n    Return type depends on input.  Series if Series, otherwise ndarray.\n\nSee Also\n--------\nDataFrame.astype : Cast argument to a specified dtype.\nto_datetime : Convert argument to datetime.\nto_timedelta : Convert argument to timedelta.\nnumpy.ndarray.astype : Cast a numpy array to a specified type.\nDataFrame.convert_dtypes : Convert dtypes.\n\nExamples\n--------\nTake separate series and convert to numeric, coercing when told to\n\n>>> s = pd.Series(['1.0', '2', -3])\n>>> pd.to_numeric(s)\n0    1.0\n1    2.0\n2   -3.0\ndtype: float64\n>>> pd.to_numeric(s, downcast='float')\n0    1.0\n1    2.0\n2   -3.0\ndtype: float32\n>>> pd.to_numeric(s, downcast='signed')\n0    1\n1    2\n2   -3\ndtype: int8\n>>> s = pd.Series(['apple', '1.0', '2', -3])\n>>> pd.to_numeric(s, errors='coerce')\n0    NaN\n1    1.0\n2    2.0\n3   -3.0\ndtype: float64\n\nDowncasting of nullable integer and floating dtypes is supported:\n\n>>> s = pd.Series([1, 2, 3], dtype=\"Int64\")\n>>> pd.to_numeric(s, downcast=\"integer\")\n0    1\n1    2\n2    3\ndtype: Int8\n>>> s = pd.Series([1.0, 2.1, 3.0], dtype=\"Float64\")\n>>> pd.to_numeric(s, downcast=\"float\")\n0    1.0\n1    2.1\n2    3.0\ndtype: Float32", "parameters": {"type": "object", "properties": {"arg": {}, "errors": {"type": "datetimeerrorchoices", "default": "raise"}}}}}
{"task_id": "BigCodeBench/108", "data": {"name": "statsmodels.tsa.seasonal.seasonal_decompose", "type": "callable", "signature": "(x, model='additive')", "description": "Seasonal decomposition using moving averages.\n\nParameters\n----------\nx : array_like\n    Time series. If 2d, individual series are in columns. x must contain 2\n    complete cycles.\nmodel : {\"additive\", \"multiplicative\"}, optional\n    Type of seasonal component. Abbreviations are accepted.\nfilt : array_like, optional\n    The filter coefficients for filtering out the seasonal component.\n    The concrete moving average method used in filtering is determined by\n    two_sided.\nperiod : int, optional\n    Period of the series. Must be used if x is not a pandas object or if\n    the index of x does not have  a frequency. Overrides default\n    periodicity of x if x is a pandas object with a timeseries index.\ntwo_sided : bool, optional\n    The moving average method used in filtering.\n    If True (default), a centered moving average is computed using the\n    filt. If False, the filter coefficients are for past values only.\nextrapolate_trend : int or 'freq', optional\n    If set to > 0, the trend resulting from the convolution is\n    linear least-squares extrapolated on both ends (or the single one\n    if two_sided is False) considering this many (+1) closest points.\n    If set to 'freq', use `freq` closest points. Setting this parameter\n    results in no NaN values in trend or resid components.\n\nReturns\n-------\nDecomposeResult\n    A object with seasonal, trend, and resid attributes.\n\nSee Also\n--------\nstatsmodels.tsa.filters.bk_filter.bkfilter\n    Baxter-King filter.\nstatsmodels.tsa.filters.cf_filter.cffilter\n    Christiano-Fitzgerald asymmetric, random walk filter.\nstatsmodels.tsa.filters.hp_filter.hpfilter\n    Hodrick-Prescott filter.\nstatsmodels.tsa.filters.convolution_filter\n    Linear filtering via convolution.\nstatsmodels.tsa.seasonal.STL\n    Season-Trend decomposition using LOESS.\n\nNotes\n-----\nThis is a naive decomposition. More sophisticated methods should\nbe preferred.\n\nThe additive model is Y[t] = T[t] + S[t] + e[t]\n\nThe multiplicative model is Y[t] = T[t] * S[t] * e[t]\n\nThe results are obtained by first estimating the trend by applying\na convolution filter to the data. The trend is then removed from the\nseries and the average of this de-trended series for each period is\nthe returned seasonal component.", "parameters": {"type": "object", "properties": {"x": {}, "model": {"type": "str", "default": "additive"}}}}}
{"task_id": "BigCodeBench/120", "data": {"name": "random.seed", "type": "callable", "signature": "(a=None)", "description": "Initialize internal state from a seed.\n\nThe only supported seed types are None, int, float,\nstr, bytes, and bytearray.\n\nNone or no argument seeds from current time or from an operating\nsystem specific randomness source if available.\n\nIf *a* is an int, all bits are used.\n\nFor version 2 (the default), all of the bits are used if *a* is a str,\nbytes, or bytearray.  For version 1 (provided for reproducing random\nsequences from older versions of Python), the algorithm for str and\nbytes generates a narrower range of seeds.", "parameters": {"type": "object", "properties": {"a": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/120", "data": {"name": "random.randint", "type": "callable", "signature": "(a, b)", "description": "Return random integer in range [a, b], including both end points.\n        ", "parameters": {"type": "object", "properties": {"a": {}, "b": {}}}}}
{"task_id": "BigCodeBench/123", "data": {"name": "glob.glob", "type": "callable", "signature": "(pathname)", "description": "Return a list of paths matching a pathname pattern.\n\nThe pattern may contain simple shell-style wildcards a la\nfnmatch. However, unlike fnmatch, filenames starting with a\ndot are special cases that are not matched by '*' and '?'\npatterns.\n\nIf recursive is true, the pattern '**' will match any files and\nzero or more directories and subdirectories.", "parameters": {"type": "object", "properties": {"pathname": {}}}}}
{"task_id": "BigCodeBench/123", "data": {"name": "my_list.append(12)"}}
{"task_id": "BigCodeBench/123", "data": {"name": "os.path.join", "type": "callable", "signature": "(a, *p)", "description": "Join two or more pathname components, inserting '/' as needed.\nIf any component is an absolute path, all previous path components\nwill be discarded.  An empty last part will result in a path that\nends with a separator.", "parameters": {"type": "object", "properties": {"a": {}}}}}
{"task_id": "BigCodeBench/123", "data": {"name": "pandas.read_csv", "type": "callable", "signature": "(filepath_or_buffer: 'FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str]')", "description": "Read a comma-separated values (csv) file into DataFrame.\n\nAlso supports optionally iterating or breaking of the file\ninto chunks.\n\nAdditional help can be found in the online docs for\n`IO Tools <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html>`_.\n\nParameters\n----------\nfilepath_or_buffer : str, path object or file-like object\n    Any valid string path is acceptable. The string could be a URL. Valid\n    URL schemes include http, ftp, s3, gs, and file. For file URLs, a host is\n    expected. A local file could be: file://localhost/path/to/table.csv.\n\n    If you want to pass in a path object, pandas accepts any ``os.PathLike``.\n\n    By file-like object, we refer to objects with a ``read()`` method, such as\n    a file handle (e.g. via builtin ``open`` function) or ``StringIO``.\nsep : str, default ','\n    Character or regex pattern to treat as the delimiter. If ``sep=None``, the\n    C engine cannot automatically detect\n    the separator, but the Python parsing engine can, meaning the latter will\n    be used and automatically detect the separator from only the first valid\n    row of the file by Python's builtin sniffer tool, ``csv.Sniffer``.\n    In addition, separators longer than 1 character and different from\n    ``'\\s+'`` will be interpreted as regular expressions and will also force\n    the use of the Python parsing engine. Note that regex delimiters are prone\n    to ignoring quoted data. Regex example: ``'\\r\\t'``.\ndelimiter : str, optional\n    Alias for ``sep``.\nheader : int, Sequence of int, 'infer' or None, default 'infer'\n    Row number(s) containing column labels and marking the start of the\n    data (zero-indexed). Default behavior is to infer the column names: if no ``names``\n    are passed the behavior is identical to ``header=0`` and column\n    names are inferred from the first line of the file, if column\n    names are passed explicitly to ``names`` then the behavior is identical to\n    ``header=None``. Explicitly pass ``header=0`` to be able to\n    replace existing names. The header can be a list of integers that\n    specify row locations for a :class:`~pandas.MultiIndex` on the columns\n    e.g. ``[0, 1, 3]``. Intervening rows that are not specified will be\n    skipped (e.g. 2 in this example is skipped). Note that this\n    parameter ignores commented lines and empty lines if\n    ``skip_blank_lines=True``, so ``header=0`` denotes the first line of\n    data rather than the first line of the file.\nnames : Sequence of Hashable, optional\n    Sequence of column labels to apply. If the file contains a header row,\n    then you should explicitly pass ``header=0`` to override the column names.\n    Duplicates in this list are not allowed.\nindex_col : Hashable, Sequence of Hashable or False, optional\n  Column(s) to use as row label(s), denoted either by column labels or column\n  indices.  If a sequence of labels or indices is given, :class:`~pandas.MultiIndex`\n  will be formed for the row labels.\n\n  Note: ``index_col=False`` can be used to force pandas to *not* use the first\n  column as the index, e.g., when you have a malformed file with delimiters at\n  the end of each line.\nusecols : Sequence of Hashable or Callable, optional\n    Subset of columns to select, denoted either by column labels or column indices.\n    If list-like, all elements must either\n    be positional (i.e. integer indices into the document columns) or strings\n    that correspond to column names provided either by the user in ``names`` or\n    inferred from the document header row(s). If ``names`` are given, the document\n    header row(s) are not taken into account. For example, a valid list-like\n    ``usecols`` parameter would be ``[0, 1, 2]`` or ``['foo', 'bar', 'baz']``.\n    Element order is ignored, so ``usecols=[0, 1]`` is the same as ``[1, 0]``.\n    To instantiate a :class:`~pandas.DataFrame` from ``data`` with element order\n    preserved use ``pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']]``\n    for columns in ``['foo', 'bar']`` order or\n    ``pd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']]``\n    for ``['bar', 'foo']`` order.\n\n    If callable, the callable function will be evaluated against the column\n    names, returning names where the callable function evaluates to ``True``. An\n    example of a valid callable argument would be ``lambda x: x.upper() in\n    ['AAA', 'BBB', 'DDD']``. Using this parameter results in much faster\n    parsing time and lower memory usage.\ndtype : dtype or dict of {Hashable : dtype}, optional\n    Data type(s) to apply to either the whole dataset or individual columns.\n    E.g., ``{'a': np.float64, 'b': np.int32, 'c': 'Int64'}``\n    Use ``str`` or ``object`` together with suitable ``na_values`` settings\n    to preserve and not interpret ``dtype``.\n    If ``converters`` are specified, they will be applied INSTEAD\n    of ``dtype`` conversion.\n\n    .. versionadded:: 1.5.0\n\n        Support for ``defaultdict`` was added. Specify a ``defaultdict`` as input where\n        the default determines the ``dtype`` of the columns which are not explicitly\n        listed.\nengine : {'c', 'python', 'pyarrow'}, optional\n    Parser engine to use. The C and pyarrow engines are faster, while the python engine\n    is currently more feature-complete. Multithreading is currently only supported by\n    the pyarrow engine.\n\n    .. versionadded:: 1.4.0\n\n        The 'pyarrow' engine was added as an *experimental* engine, and some features\n        are unsupported, or may not work correctly, with this engine.\nconverters : dict of {Hashable : Callable}, optional\n    Functions for converting values in specified columns. Keys can either\n    be column labels or column indices.\ntrue_values : list, optional\n    Values to consider as ``True`` in addition to case-insensitive variants of 'True'.\nfalse_values : list, optional\n    Values to consider as ``False`` in addition to case-insensitive variants of 'False'.\nskipinitialspace : bool, default False\n    Skip spaces after delimiter.\nskiprows : int, list of int or Callable, optional\n    Line numbers to skip (0-indexed) or number of lines to skip (``int``)\n    at the start of the file.\n\n    If callable, the callable function will be evaluated against the row\n    indices, returning ``True`` if the row should be skipped and ``False`` otherwise.\n    An example of a valid callable argument would be ``lambda x: x in [0, 2]``.\nskipfooter : int, default 0\n    Number of lines at bottom of file to skip (Unsupported with ``engine='c'``).\nnrows : int, optional\n    Number of rows of file to read. Useful for reading pieces of large files.\nna_values : Hashable, Iterable of Hashable or dict of {Hashable : Iterable}, optional\n    Additional strings to recognize as ``NA``/``NaN``. If ``dict`` passed, specific\n    per-column ``NA`` values.  By default the following values are interpreted as\n    ``NaN``: \" \", \"#N/A\", \"#N/A N/A\", \"#NA\", \"-1.#IND\", \"-1.#QNAN\", \"-NaN\", \"-nan\",\n    \"1.#IND\", \"1.#QNAN\", \"<NA>\", \"N/A\", \"NA\", \"NULL\", \"NaN\", \"None\",\n    \"n/a\", \"nan\", \"null \".\n\nkeep_default_na : bool, default True\n    Whether or not to include the default ``NaN`` values when parsing the data.\n    Depending on whether ``na_values`` is passed in, the behavior is as follows:\n\n    * If ``keep_default_na`` is ``True``, and ``na_values`` are specified, ``na_values``\n      is appended to the default ``NaN`` values used for parsing.\n    * If ``keep_default_na`` is ``True``, and ``na_values`` are not specified, only\n      the default ``NaN`` values are used for parsing.\n    * If ``keep_default_na`` is ``False``, and ``na_values`` are specified, only\n      the ``NaN`` values specified ``na_values`` are used for parsing.\n    * If ``keep_default_na`` is ``False``, and ``na_values`` are not specified, no\n      strings will be parsed as ``NaN``.\n\n    Note that if ``na_filter`` is passed in as ``False``, the ``keep_default_na`` and\n    ``na_values`` parameters will be ignored.\nna_filter : bool, default True\n    Detect missing value markers (empty strings and the value of ``na_values``). In\n    data without any ``NA`` values, passing ``na_filter=False`` can improve the\n    performance of reading a large file.\nverbose : bool, default False\n    Indicate number of ``NA`` values placed in non-numeric columns.\n\n    .. deprecated:: 2.2.0\nskip_blank_lines : bool, default True\n    If ``True``, skip over blank lines rather than interpreting as ``NaN`` values.\nparse_dates : bool, list of Hashable, list of lists or dict of {Hashable : list}, default False\n    The behavior is as follows:\n\n    * ``bool``. If ``True`` -> try parsing the index. Note: Automatically set to\n      ``True`` if ``date_format`` or ``date_parser`` arguments have been passed.\n    * ``list`` of ``int`` or names. e.g. If ``[1, 2, 3]`` -> try parsing columns 1, 2, 3\n      each as a separate date column.\n    * ``list`` of ``list``. e.g.  If ``[[1, 3]]`` -> combine columns 1 and 3 and parse\n      as a single date column. Values are joined with a space before parsing.\n    * ``dict``, e.g. ``{'foo' : [1, 3]}`` -> parse columns 1, 3 as date and call\n      result 'foo'. Values are joined with a space before parsing.\n\n    If a column or index cannot be represented as an array of ``datetime``,\n    say because of an unparsable value or a mixture of timezones, the column\n    or index will be returned unaltered as an ``object`` data type. For\n    non-standard ``datetime`` parsing, use :func:`~pandas.to_datetime` after\n    :func:`~pandas.read_csv`.\n\n    Note: A fast-path exists for iso8601-formatted dates.\ninfer_datetime_format : bool, default False\n    If ``True`` and ``parse_dates`` is enabled, pandas will attempt to infer the\n    format of the ``datetime`` strings in the columns, and if it can be inferred,\n    switch to a faster method of parsing them. In some cases this can increase\n    the parsing speed by 5-10x.\n\n    .. deprecated:: 2.0.0\n        A strict version of this argument is now the default, passing it has no effect.\n\nkeep_date_col : bool, default False\n    If ``True`` and ``parse_dates`` specifies combining multiple columns then\n    keep the original columns.\ndate_parser : Callable, optional\n    Function to use for converting a sequence of string columns to an array of\n    ``datetime`` instances. The default uses ``dateutil.parser.parser`` to do the\n    conversion. pandas will try to call ``date_parser`` in three different ways,\n    advancing to the next if an exception occurs: 1) Pass one or more arrays\n    (as defined by ``parse_dates``) as arguments; 2) concatenate (row-wise) the\n    string values from the columns defined by ``parse_dates`` into a single array\n    and pass that; and 3) call ``date_parser`` once for each row using one or\n    more strings (corresponding to the columns defined by ``parse_dates``) as\n    arguments.\n\n    .. deprecated:: 2.0.0\n       Use ``date_format`` instead, or read in as ``object`` and then apply\n       :func:`~pandas.to_datetime` as-needed.\ndate_format : str or dict of column -> format, optional\n    Format to use for parsing dates when used in conjunction with ``parse_dates``.\n    The strftime to parse time, e.g. :const:`\"%d/%m/%Y\"`. See\n    `strftime documentation\n    <https://docs.python.org/3/library/datetime.html\n    #strftime-and-strptime-behavior>`_ for more information on choices, though\n    note that :const:`\"%f\"` will parse all the way up to nanoseconds.\n    You can also pass:\n\n    - \"ISO8601\", to parse any `ISO8601 <https://en.wikipedia.org/wiki/ISO_8601>`_\n        time string (not necessarily in exactly the same format);\n    - \"mixed\", to infer the format for each element individually. This is risky,\n        and you should probably use it along with `dayfirst`.\n\n    .. versionadded:: 2.0.0\ndayfirst : bool, default False\n    DD/MM format dates, international and European format.\ncache_dates : bool, default True\n    If ``True``, use a cache of unique, converted dates to apply the ``datetime``\n    conversion. May produce significant speed-up when parsing duplicate\n    date strings, especially ones with timezone offsets.\n\niterator : bool, default False\n    Return ``TextFileReader`` object for iteration or getting chunks with\n    ``get_chunk()``.\nchunksize : int, optional\n    Number of lines to read from the file per chunk. Passing a value will cause the\n    function to return a ``TextFileReader`` object for iteration.\n    See the `IO Tools docs\n    <https://pandas.pydata.org/pandas-docs/stable/io.html#io-chunking>`_\n    for more information on ``iterator`` and ``chunksize``.\n\ncompression : str or dict, default 'infer'\n    For on-the-fly decompression of on-disk data. If 'infer' and 'filepath_or_buffer' is\n    path-like, then detect compression from the following extensions: '.gz',\n    '.bz2', '.zip', '.xz', '.zst', '.tar', '.tar.gz', '.tar.xz' or '.tar.bz2'\n    (otherwise no compression).\n    If using 'zip' or 'tar', the ZIP file must contain only one data file to be read in.\n    Set to ``None`` for no decompression.\n    Can also be a dict with key ``'method'`` set\n    to one of {``'zip'``, ``'gzip'``, ``'bz2'``, ``'zstd'``, ``'xz'``, ``'tar'``} and\n    other key-value pairs are forwarded to\n    ``zipfile.ZipFile``, ``gzip.GzipFile``,\n    ``bz2.BZ2File``, ``zstandard.ZstdDecompressor``, ``lzma.LZMAFile`` or\n    ``tarfile.TarFile``, respectively.\n    As an example, the following could be passed for Zstandard decompression using a\n    custom compression dictionary:\n    ``compression={'method': 'zstd', 'dict_data': my_compression_dict}``.\n\n    .. versionadded:: 1.5.0\n        Added support for `.tar` files.\n\n    .. versionchanged:: 1.4.0 Zstandard support.\n\nthousands : str (length 1), optional\n    Character acting as the thousands separator in numerical values.\ndecimal : str (length 1), default '.'\n    Character to recognize as decimal point (e.g., use ',' for European data).\nlineterminator : str (length 1), optional\n    Character used to denote a line break. Only valid with C parser.\nquotechar : str (length 1), optional\n    Character used to denote the start and end of a quoted item. Quoted\n    items can include the ``delimiter`` and it will be ignored.\nquoting : {0 or csv.QUOTE_MINIMAL, 1 or csv.QUOTE_ALL, 2 or csv.QUOTE_NONNUMERIC, 3 or csv.QUOTE_NONE}, default csv.QUOTE_MINIMAL\n    Control field quoting behavior per ``csv.QUOTE_*`` constants. Default is\n    ``csv.QUOTE_MINIMAL`` (i.e., 0) which implies that only fields containing special\n    characters are quoted (e.g., characters defined in ``quotechar``, ``delimiter``,\n    or ``lineterminator``.\ndoublequote : bool, default True\n   When ``quotechar`` is specified and ``quoting`` is not ``QUOTE_NONE``, indicate\n   whether or not to interpret two consecutive ``quotechar`` elements INSIDE a\n   field as a single ``quotechar`` element.\nescapechar : str (length 1), optional\n    Character used to escape other characters.\ncomment : str (length 1), optional\n    Character indicating that the remainder of line should not be parsed.\n    If found at the beginning\n    of a line, the line will be ignored altogether. This parameter must be a\n    single character. Like empty lines (as long as ``skip_blank_lines=True``),\n    fully commented lines are ignored by the parameter ``header`` but not by\n    ``skiprows``. For example, if ``comment='#'``, parsing\n    ``#empty\\na,b,c\\n1,2,3`` with ``header=0`` will result in ``'a,b,c'`` being\n    treated as the header.\nencoding : str, optional, default 'utf-8'\n    Encoding to use for UTF when reading/writing (ex. ``'utf-8'``). `List of Python\n    standard encodings\n    <https://docs.python.org/3/library/codecs.html#standard-encodings>`_ .\n\nencoding_errors : str, optional, default 'strict'\n    How encoding errors are treated. `List of possible values\n    <https://docs.python.org/3/library/codecs.html#error-handlers>`_ .\n\n    .. versionadded:: 1.3.0\n\ndialect : str or csv.Dialect, optional\n    If provided, this parameter will override values (default or not) for the\n    following parameters: ``delimiter``, ``doublequote``, ``escapechar``,\n    ``skipinitialspace``, ``quotechar``, and ``quoting``. If it is necessary to\n    override values, a ``ParserWarning`` will be issued. See ``csv.Dialect``\n    documentation for more details.\non_bad_lines : {'error', 'warn', 'skip'} or Callable, default 'error'\n    Specifies what to do upon encountering a bad line (a line with too many fields).\n    Allowed values are :\n\n    - ``'error'``, raise an Exception when a bad line is encountered.\n    - ``'warn'``, raise a warning when a bad line is encountered and skip that line.\n    - ``'skip'``, skip bad lines without raising or warning when they are encountered.\n\n    .. versionadded:: 1.3.0\n\n    .. versionadded:: 1.4.0\n\n        - Callable, function with signature\n          ``(bad_line: list[str]) -> list[str] | None`` that will process a single\n          bad line. ``bad_line`` is a list of strings split by the ``sep``.\n          If the function returns ``None``, the bad line will be ignored.\n          If the function returns a new ``list`` of strings with more elements than\n          expected, a ``ParserWarning`` will be emitted while dropping extra elements.\n          Only supported when ``engine='python'``\n\n    .. versionchanged:: 2.2.0\n\n        - Callable, function with signature\n          as described in `pyarrow documentation\n          <https://arrow.apache.org/docs/python/generated/pyarrow.csv.ParseOptions.html\n          #pyarrow.csv.ParseOptions.invalid_row_handler>`_ when ``engine='pyarrow'``\n\ndelim_whitespace : bool, default False\n    Specifies whether or not whitespace (e.g. ``' '`` or ``'\\t'``) will be\n    used as the ``sep`` delimiter. Equivalent to setting ``sep='\\s+'``. If this option\n    is set to ``True``, nothing should be passed in for the ``delimiter``\n    parameter.\n\n    .. deprecated:: 2.2.0\n        Use ``sep=\"\\s+\"`` instead.\nlow_memory : bool, default True\n    Internally process the file in chunks, resulting in lower memory use\n    while parsing, but possibly mixed type inference.  To ensure no mixed\n    types either set ``False``, or specify the type with the ``dtype`` parameter.\n    Note that the entire file is read into a single :class:`~pandas.DataFrame`\n    regardless, use the ``chunksize`` or ``iterator`` parameter to return the data in\n    chunks. (Only valid with C parser).\nmemory_map : bool, default False\n    If a filepath is provided for ``filepath_or_buffer``, map the file object\n    directly onto memory and access the data directly from there. Using this\n    option can improve performance because there is no longer any I/O overhead.\nfloat_precision : {'high', 'legacy', 'round_trip'}, optional\n    Specifies which converter the C engine should use for floating-point\n    values. The options are ``None`` or ``'high'`` for the ordinary converter,\n    ``'legacy'`` for the original lower precision pandas converter, and\n    ``'round_trip'`` for the round-trip converter.\n\nstorage_options : dict, optional\n    Extra options that make sense for a particular storage connection, e.g.\n    host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n    are forwarded to ``urllib.request.Request`` as header options. For other\n    URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n    forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n    details, and for more examples on storage options refer `here\n    <https://pandas.pydata.org/docs/user_guide/io.html?\n    highlight=storage_options#reading-writing-remote-files>`_.\n\ndtype_backend : {'numpy_nullable', 'pyarrow'}, default 'numpy_nullable'\n    Back-end data type applied to the resultant :class:`DataFrame`\n    (still experimental). Behaviour is as follows:\n\n    * ``\"numpy_nullable\"``: returns nullable-dtype-backed :class:`DataFrame`\n      (default).\n    * ``\"pyarrow\"``: returns pyarrow-backed nullable :class:`ArrowDtype`\n      DataFrame.\n\n    .. versionadded:: 2.0\n\nReturns\n-------\nDataFrame or TextFileReader\n    A comma-separated values (csv) file is returned as two-dimensional\n    data structure with labeled axes.\n\nSee Also\n--------\nDataFrame.to_csv : Write DataFrame to a comma-separated values (csv) file.\nread_table : Read general delimited file into DataFrame.\nread_fwf : Read a table of fixed-width formatted lines into DataFrame.\n\nExamples\n--------\n>>> pd.read_csv('data.csv')  # doctest: +SKIP", "parameters": {"type": "object", "properties": {"filepath_or_buffer": {"type": ["readcsvbuffer[bytes]", "readcsvbuffer[str]", "filepath"]}}}}}
{"task_id": "BigCodeBench/123", "data": {"name": "pandas.concat", "type": "callable", "signature": "(objs: 'Iterable[Series | DataFrame] | Mapping[HashableT, ignore_index: 'bool' = False)", "description": "Concatenate pandas objects along a particular axis.\n\nAllows optional set logic along the other axes.\n\nCan also add a layer of hierarchical indexing on the concatenation axis,\nwhich may be useful if the labels are the same (or overlapping) on\nthe passed axis number.\n\nParameters\n----------\nobjs : a sequence or mapping of Series or DataFrame objects\n    If a mapping is passed, the sorted keys will be used as the `keys`\n    argument, unless it is passed, in which case the values will be\n    selected (see below). Any None objects will be dropped silently unless\n    they are all None in which case a ValueError will be raised.\naxis : {0/'index', 1/'columns'}, default 0\n    The axis to concatenate along.\njoin : {'inner', 'outer'}, default 'outer'\n    How to handle indexes on other axis (or axes).\nignore_index : bool, default False\n    If True, do not use the index values along the concatenation axis. The\n    resulting axis will be labeled 0, ..., n - 1. This is useful if you are\n    concatenating objects where the concatenation axis does not have\n    meaningful indexing information. Note the index values on the other\n    axes are still respected in the join.\nkeys : sequence, default None\n    If multiple levels passed, should contain tuples. Construct\n    hierarchical index using the passed keys as the outermost level.\nlevels : list of sequences, default None\n    Specific levels (unique values) to use for constructing a\n    MultiIndex. Otherwise they will be inferred from the keys.\nnames : list, default None\n    Names for the levels in the resulting hierarchical index.\nverify_integrity : bool, default False\n    Check whether the new concatenated axis contains duplicates. This can\n    be very expensive relative to the actual data concatenation.\nsort : bool, default False\n    Sort non-concatenation axis if it is not already aligned. One exception to\n    this is when the non-concatentation axis is a DatetimeIndex and join='outer'\n    and the axis is not already aligned. In that case, the non-concatenation\n    axis is always sorted lexicographically.\ncopy : bool, default True\n    If False, do not copy data unnecessarily.\n\nReturns\n-------\nobject, type of objs\n    When concatenating all ``Series`` along the index (axis=0), a\n    ``Series`` is returned. When ``objs`` contains at least one\n    ``DataFrame``, a ``DataFrame`` is returned. When concatenating along\n    the columns (axis=1), a ``DataFrame`` is returned.\n\nSee Also\n--------\nDataFrame.join : Join DataFrames using indexes.\nDataFrame.merge : Merge DataFrames by indexes or columns.\n\nNotes\n-----\nThe keys, levels, and names arguments are all optional.\n\nA walkthrough of how this method fits in with other tools for combining\npandas objects can be found `here\n<https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html>`__.\n\nIt is not recommended to build DataFrames by adding single rows in a\nfor loop. Build a list of rows and make a DataFrame in a single concat.\n\nExamples\n--------\nCombine two ``Series``.\n\n>>> s1 = pd.Series(['a', 'b'])\n>>> s2 = pd.Series(['c', 'd'])\n>>> pd.concat([s1, s2])\n0    a\n1    b\n0    c\n1    d\ndtype: object\n\nClear the existing index and reset it in the result\nby setting the ``ignore_index`` option to ``True``.\n\n>>> pd.concat([s1, s2], ignore_index=True)\n0    a\n1    b\n2    c\n3    d\ndtype: object\n\nAdd a hierarchical index at the outermost level of\nthe data with the ``keys`` option.\n\n>>> pd.concat([s1, s2], keys=['s1', 's2'])\ns1  0    a\n    1    b\ns2  0    c\n    1    d\ndtype: object\n\nLabel the index keys you create with the ``names`` option.\n\n>>> pd.concat([s1, s2], keys=['s1', 's2'],\n...           names=['Series name', 'Row ID'])\nSeries name  Row ID\ns1           0         a\n             1         b\ns2           0         c\n             1         d\ndtype: object\n\nCombine two ``DataFrame`` objects with identical columns.\n\n>>> df1 = pd.DataFrame([['a', 1], ['b', 2]],\n...                    columns=['letter', 'number'])\n>>> df1\n  letter  number\n0      a       1\n1      b       2\n>>> df2 = pd.DataFrame([['c', 3], ['d', 4]],\n...                    columns=['letter', 'number'])\n>>> df2\n  letter  number\n0      c       3\n1      d       4\n>>> pd.concat([df1, df2])\n  letter  number\n0      a       1\n1      b       2\n0      c       3\n1      d       4\n\nCombine ``DataFrame`` objects with overlapping columns\nand return everything. Columns outside the intersection will\nbe filled with ``NaN`` values.\n\n>>> df3 = pd.DataFrame([['c', 3, 'cat'], ['d', 4, 'dog']],\n...                    columns=['letter', 'number', 'animal'])\n>>> df3\n  letter  number animal\n0      c       3    cat\n1      d       4    dog\n>>> pd.concat([df1, df3], sort=False)\n  letter  number animal\n0      a       1    NaN\n1      b       2    NaN\n0      c       3    cat\n1      d       4    dog\n\nCombine ``DataFrame`` objects with overlapping columns\nand return only those that are shared by passing ``inner`` to\nthe ``join`` keyword argument.\n\n>>> pd.concat([df1, df3], join=\"inner\")\n  letter  number\n0      a       1\n1      b       2\n0      c       3\n1      d       4\n\nCombine ``DataFrame`` objects horizontally along the x axis by\npassing in ``axis=1``.\n\n>>> df4 = pd.DataFrame([['bird', 'polly'], ['monkey', 'george']],\n...                    columns=['animal', 'name'])\n>>> pd.concat([df1, df4], axis=1)\n  letter  number  animal    name\n0      a       1    bird   polly\n1      b       2  monkey  george\n\nPrevent the result from including duplicate index values with the\n``verify_integrity`` option.\n\n>>> df5 = pd.DataFrame([1], index=['a'])\n>>> df5\n   0\na  1\n>>> df6 = pd.DataFrame([2], index=['a'])\n>>> df6\n   0\na  2\n>>> pd.concat([df5, df6], verify_integrity=True)\nTraceback (most recent call last):\n    ...\nValueError: Indexes have overlapping values: ['a']\n\nAppend a single row to the end of a ``DataFrame`` object.\n\n>>> df7 = pd.DataFrame({'a': 1, 'b': 2}, index=[0])\n>>> df7\n    a   b\n0   1   2\n>>> new_row = pd.Series({'a': 3, 'b': 4})\n>>> new_row\na    3\nb    4\ndtype: int64\n>>> pd.concat([df7, new_row.to_frame().T], ignore_index=True)\n    a   b\n0   1   2\n1   3   4", "parameters": {"type": "object", "properties": {"objs": {"type": ["dataframe]", "iterable[series", "mapping[hashablet"]}, "ignore_index": {"type": "bool", "default": false}}}}}
{"task_id": "BigCodeBench/124", "data": {"name": "matplotlib.pyplot.subplots", "type": "class", "signature": "()", "description": "Create a figure and a set of subplots.\n\nThis utility wrapper makes it convenient to create common layouts of\nsubplots, including the enclosing figure object, in a single call.\n\nParameters\n----------\nnrows, ncols : int, default: 1\n    Number of rows/columns of the subplot grid.\n\nsharex, sharey : bool or {'none', 'all', 'row', 'col'}, default: False\n    Controls sharing of properties among x (*sharex*) or y (*sharey*)\n    axes:\n\n    - True or 'all': x- or y-axis will be shared among all subplots.\n    - False or 'none': each subplot x- or y-axis will be independent.\n    - 'row': each subplot row will share an x- or y-axis.\n    - 'col': each subplot column will share an x- or y-axis.\n\n    When subplots have a shared x-axis along a column, only the x tick\n    labels of the bottom subplot are created. Similarly, when subplots\n    have a shared y-axis along a row, only the y tick labels of the first\n    column subplot are created. To later turn other subplots' ticklabels\n    on, use `~matplotlib.axes.Axes.tick_params`.\n\n    When subplots have a shared axis that has units, calling\n    `~matplotlib.axis.Axis.set_units` will update each axis with the\n    new units.\n\nsqueeze : bool, default: True\n    - If True, extra dimensions are squeezed out from the returned\n      array of `~matplotlib.axes.Axes`:\n\n      - if only one subplot is constructed (nrows=ncols=1), the\n        resulting single Axes object is returned as a scalar.\n      - for Nx1 or 1xM subplots, the returned object is a 1D numpy\n        object array of Axes objects.\n      - for NxM, subplots with N>1 and M>1 are returned as a 2D array.\n\n    - If False, no squeezing at all is done: the returned Axes object is\n      always a 2D array containing Axes instances, even if it ends up\n      being 1x1.\n\nwidth_ratios : array-like of length *ncols*, optional\n    Defines the relative widths of the columns. Each column gets a\n    relative width of ``width_ratios[i] / sum(width_ratios)``.\n    If not given, all columns will have the same width.  Equivalent\n    to ``gridspec_kw={'width_ratios': [...]}``.\n\nheight_ratios : array-like of length *nrows*, optional\n    Defines the relative heights of the rows. Each row gets a\n    relative height of ``height_ratios[i] / sum(height_ratios)``.\n    If not given, all rows will have the same height. Convenience\n    for ``gridspec_kw={'height_ratios': [...]}``.\n\nsubplot_kw : dict, optional\n    Dict with keywords passed to the\n    `~matplotlib.figure.Figure.add_subplot` call used to create each\n    subplot.\n\ngridspec_kw : dict, optional\n    Dict with keywords passed to the `~matplotlib.gridspec.GridSpec`\n    constructor used to create the grid the subplots are placed on.\n\n**fig_kw\n    All additional keyword arguments are passed to the\n    `.pyplot.figure` call.\n\nReturns\n-------\nfig : `.Figure`\n\nax : `~matplotlib.axes.Axes` or array of Axes\n    *ax* can be either a single `~.axes.Axes` object, or an array of Axes\n    objects if more than one subplot was created.  The dimensions of the\n    resulting array can be controlled with the squeeze keyword, see above.\n\n    Typical idioms for handling the return value are::\n\n        # using the variable ax for single a Axes\n        fig, ax = plt.subplots()\n\n        # using the variable axs for multiple Axes\n        fig, axs = plt.subplots(2, 2)\n\n        # using tuple unpacking for multiple Axes\n        fig, (ax1, ax2) = plt.subplots(1, 2)\n        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n\n    The names ``ax`` and pluralized ``axs`` are preferred over ``axes``\n    because for the latter it's not clear if it refers to a single\n    `~.axes.Axes` instance or a collection of these.\n\nSee Also\n--------\n.pyplot.figure\n.pyplot.subplot\n.pyplot.axes\n.Figure.subplots\n.Figure.add_subplot\n\nExamples\n--------\n::\n\n    # First create some toy data:\n    x = np.linspace(0, 2*np.pi, 400)\n    y = np.sin(x**2)\n\n    # Create just a figure and only one subplot\n    fig, ax = plt.subplots()\n    ax.plot(x, y)\n    ax.set_title('Simple plot')\n\n    # Create two subplots and unpack the output array immediately\n    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n    ax1.plot(x, y)\n    ax1.set_title('Sharing Y axis')\n    ax2.scatter(x, y)\n\n    # Create four polar axes and access them through the returned array\n    fig, axs = plt.subplots(2, 2, subplot_kw=dict(projection=\"polar\"))\n    axs[0, 0].plot(x, y)\n    axs[1, 1].scatter(x, y)\n\n    # Share a X axis with each column of subplots\n    plt.subplots(2, 2, sharex='col')\n\n    # Share a Y axis with each row of subplots\n    plt.subplots(2, 2, sharey='row')\n\n    # Share both X and Y axes with all subplots\n    plt.subplots(2, 2, sharex='all', sharey='all')\n\n    # Note that this is the same as\n    plt.subplots(2, 2, sharex=True, sharey=True)\n\n    # Create figure number 10 with a single subplot\n    # and clears it if it already exists.\n    fig, ax = plt.subplots(num=10, clear=True)", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/124", "data": {"name": "matplotlib.pyplot.subplots[1].set_xlabel", "type": "method", "signature": "(xlabel, fontdict=None, labelpad=None, *, loc=None, **kwargs)", "description": "Set the label for the x-axis.\n\nParameters\n----------\nxlabel : str\n    The label text.\n\nlabelpad : float, default: :rc:`axes.labelpad`\n    Spacing in points from the Axes bounding box including ticks\n    and tick labels.  If None, the previous value is left as is.\n\nloc : {'left', 'center', 'right'}, default: :rc:`xaxis.labellocation`\n    The label position. This is a high-level alternative for passing\n    parameters *x* and *horizontalalignment*.\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    `.Text` properties control the appearance of the label.\n\nSee Also\n--------\ntext : Documents the properties supported by `.Text`.", "parameters": {"type": "object", "properties": {"xlabel": {}, "fontdict": {"type": "NoneType", "default": null}, "labelpad": {"type": "NoneType", "default": null}, "loc": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/124", "data": {"name": "matplotlib.pyplot.subplots[1].set_ylabel", "type": "method", "signature": "(ylabel, fontdict=None, labelpad=None, *, loc=None, **kwargs)", "description": "Set the label for the y-axis.\n\nParameters\n----------\nylabel : str\n    The label text.\n\nlabelpad : float, default: :rc:`axes.labelpad`\n    Spacing in points from the Axes bounding box including ticks\n    and tick labels.  If None, the previous value is left as is.\n\nloc : {'bottom', 'center', 'top'}, default: :rc:`yaxis.labellocation`\n    The label position. This is a high-level alternative for passing\n    parameters *y* and *horizontalalignment*.\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    `.Text` properties control the appearance of the label.\n\nSee Also\n--------\ntext : Documents the properties supported by `.Text`.", "parameters": {"type": "object", "properties": {"ylabel": {}, "fontdict": {"type": "NoneType", "default": null}, "labelpad": {"type": "NoneType", "default": null}, "loc": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/124", "data": {"name": "matplotlib.pyplot.subplots[1].hist", "type": "method", "signature": "(x, bins=None, range=None, density=False, weights=None, cumulative=False, bottom=None, histtype='bar', align='mid', orientation='vertical', rwidth=None, log=False, color=None, label=None, stacked=False, *, data=None, **kwargs)", "description": "Compute and plot a histogram.\n\nThis method uses `numpy.histogram` to bin the data in *x* and count the\nnumber of values in each bin, then draws the distribution either as a\n`.BarContainer` or `.Polygon`. The *bins*, *range*, *density*, and\n*weights* parameters are forwarded to `numpy.histogram`.\n\nIf the data has already been binned and counted, use `~.bar` or\n`~.stairs` to plot the distribution::\n\n    counts, bins = np.histogram(x)\n    plt.stairs(counts, bins)\n\nAlternatively, plot pre-computed bins and counts using ``hist()`` by\ntreating each bin as a single point with a weight equal to its count::\n\n    plt.hist(bins[:-1], bins, weights=counts)\n\nThe data input *x* can be a singular array, a list of datasets of\npotentially different lengths ([*x0*, *x1*, ...]), or a 2D ndarray in\nwhich each column is a dataset. Note that the ndarray form is\ntransposed relative to the list form. If the input is an array, then\nthe return value is a tuple (*n*, *bins*, *patches*); if the input is a\nsequence of arrays, then the return value is a tuple\n([*n0*, *n1*, ...], *bins*, [*patches0*, *patches1*, ...]).\n\nMasked arrays are not supported.\n\nParameters\n----------\nx : (n,) array or sequence of (n,) arrays\n    Input values, this takes either a single array or a sequence of\n    arrays which are not required to be of the same length.\n\nbins : int or sequence or str, default: :rc:`hist.bins`\n    If *bins* is an integer, it defines the number of equal-width bins\n    in the range.\n\n    If *bins* is a sequence, it defines the bin edges, including the\n    left edge of the first bin and the right edge of the last bin;\n    in this case, bins may be unequally spaced.  All but the last\n    (righthand-most) bin is half-open.  In other words, if *bins* is::\n\n        [1, 2, 3, 4]\n\n    then the first bin is ``[1, 2)`` (including 1, but excluding 2) and\n    the second ``[2, 3)``.  The last bin, however, is ``[3, 4]``, which\n    *includes* 4.\n\n    If *bins* is a string, it is one of the binning strategies\n    supported by `numpy.histogram_bin_edges`: 'auto', 'fd', 'doane',\n    'scott', 'stone', 'rice', 'sturges', or 'sqrt'.\n\nrange : tuple or None, default: None\n    The lower and upper range of the bins. Lower and upper outliers\n    are ignored. If not provided, *range* is ``(x.min(), x.max())``.\n    Range has no effect if *bins* is a sequence.\n\n    If *bins* is a sequence or *range* is specified, autoscaling\n    is based on the specified bin range instead of the\n    range of x.\n\ndensity : bool, default: False\n    If ``True``, draw and return a probability density: each bin\n    will display the bin's raw count divided by the total number of\n    counts *and the bin width*\n    (``density = counts / (sum(counts) * np.diff(bins))``),\n    so that the area under the histogram integrates to 1\n    (``np.sum(density * np.diff(bins)) == 1``).\n\n    If *stacked* is also ``True``, the sum of the histograms is\n    normalized to 1.\n\nweights : (n,) array-like or None, default: None\n    An array of weights, of the same shape as *x*.  Each value in\n    *x* only contributes its associated weight towards the bin count\n    (instead of 1).  If *density* is ``True``, the weights are\n    normalized, so that the integral of the density over the range\n    remains 1.\n\ncumulative : bool or -1, default: False\n    If ``True``, then a histogram is computed where each bin gives the\n    counts in that bin plus all bins for smaller values. The last bin\n    gives the total number of datapoints.\n\n    If *density* is also ``True`` then the histogram is normalized such\n    that the last bin equals 1.\n\n    If *cumulative* is a number less than 0 (e.g., -1), the direction\n    of accumulation is reversed.  In this case, if *density* is also\n    ``True``, then the histogram is normalized such that the first bin\n    equals 1.\n\nbottom : array-like, scalar, or None, default: None\n    Location of the bottom of each bin, i.e. bins are drawn from\n    ``bottom`` to ``bottom + hist(x, bins)`` If a scalar, the bottom\n    of each bin is shifted by the same amount. If an array, each bin\n    is shifted independently and the length of bottom must match the\n    number of bins. If None, defaults to 0.\n\nhisttype : {'bar', 'barstacked', 'step', 'stepfilled'}, default: 'bar'\n    The type of histogram to draw.\n\n    - 'bar' is a traditional bar-type histogram.  If multiple data\n      are given the bars are arranged side by side.\n    - 'barstacked' is a bar-type histogram where multiple\n      data are stacked on top of each other.\n    - 'step' generates a lineplot that is by default unfilled.\n    - 'stepfilled' generates a lineplot that is by default filled.\n\nalign : {'left', 'mid', 'right'}, default: 'mid'\n    The horizontal alignment of the histogram bars.\n\n    - 'left': bars are centered on the left bin edges.\n    - 'mid': bars are centered between the bin edges.\n    - 'right': bars are centered on the right bin edges.\n\norientation : {'vertical', 'horizontal'}, default: 'vertical'\n    If 'horizontal', `~.Axes.barh` will be used for bar-type histograms\n    and the *bottom* kwarg will be the left edges.\n\nrwidth : float or None, default: None\n    The relative width of the bars as a fraction of the bin width.  If\n    ``None``, automatically compute the width.\n\n    Ignored if *histtype* is 'step' or 'stepfilled'.\n\nlog : bool, default: False\n    If ``True``, the histogram axis will be set to a log scale.\n\ncolor : color or array-like of colors or None, default: None\n    Color or sequence of colors, one per dataset.  Default (``None``)\n    uses the standard line color sequence.\n\nlabel : str or None, default: None\n    String, or sequence of strings to match multiple datasets.  Bar\n    charts yield multiple patches per dataset, but only the first gets\n    the label, so that `~.Axes.legend` will work as expected.\n\nstacked : bool, default: False\n    If ``True``, multiple data are stacked on top of each other If\n    ``False`` multiple data are arranged side by side if histtype is\n    'bar' or on top of each other if histtype is 'step'\n\nReturns\n-------\nn : array or list of arrays\n    The values of the histogram bins. See *density* and *weights* for a\n    description of the possible semantics.  If input *x* is an array,\n    then this is an array of length *nbins*. If input is a sequence of\n    arrays ``[data1, data2, ...]``, then this is a list of arrays with\n    the values of the histograms for each of the arrays in the same\n    order.  The dtype of the array *n* (or of its element arrays) will\n    always be float even if no weighting or normalization is used.\n\nbins : array\n    The edges of the bins. Length nbins + 1 (nbins left edges and right\n    edge of last bin).  Always a single array even when multiple data\n    sets are passed in.\n\npatches : `.BarContainer` or list of a single `.Polygon` or list of such objects\n    Container of individual artists used to create the histogram\n    or list of such containers if there are multiple input datasets.\n\nOther Parameters\n----------------\ndata : indexable object, optional\n    If given, the following parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` (unless this raises an exception):\n\n    *x*, *weights*\n\n**kwargs\n    `~matplotlib.patches.Patch` properties\n\nSee Also\n--------\nhist2d : 2D histogram with rectangular bins\nhexbin : 2D histogram with hexagonal bins\nstairs : Plot a pre-computed histogram\nbar : Plot a pre-computed histogram\n\nNotes\n-----\nFor large numbers of bins (>1000), plotting can be significantly\naccelerated by using `~.Axes.stairs` to plot a pre-computed histogram\n(``plt.stairs(*np.histogram(data))``), or by setting *histtype* to\n'step' or 'stepfilled' rather than 'bar' or 'barstacked'.", "parameters": {"type": "object", "properties": {"x": {}, "bins": {"type": "NoneType", "default": null}, "range": {"type": "NoneType", "default": null}, "density": {"type": "bool", "default": false}, "weights": {"type": "NoneType", "default": null}, "cumulative": {"type": "bool", "default": false}, "bottom": {"type": "NoneType", "default": null}, "histtype": {"type": "str", "default": "bar"}, "align": {"type": "str", "default": "mid"}, "orientation": {"type": "str", "default": "vertical"}, "rwidth": {"type": "NoneType", "default": null}, "log": {"type": "bool", "default": false}, "color": {"type": "NoneType", "default": null}, "label": {"type": "NoneType", "default": null}, "stacked": {"type": "bool", "default": false}, "data": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/124", "data": {"name": "matplotlib.pyplot.subplots[1].set_title", "type": "method", "signature": "(label, fontdict=None, loc=None, pad=None, *, y=None, **kwargs)", "description": "Set a title for the Axes.\n\nSet one of the three available Axes titles. The available titles\nare positioned above the Axes in the center, flush with the left\nedge, and flush with the right edge.\n\nParameters\n----------\nlabel : str\n    Text to use for the title\n\nfontdict : dict\n\n    .. admonition:: Discouraged\n\n       The use of *fontdict* is discouraged. Parameters should be passed as\n       individual keyword arguments or using dictionary-unpacking\n       ``set_title(..., **fontdict)``.\n\n    A dictionary controlling the appearance of the title text,\n    the default *fontdict* is::\n\n       {'fontsize': rcParams['axes.titlesize'],\n        'fontweight': rcParams['axes.titleweight'],\n        'color': rcParams['axes.titlecolor'],\n        'verticalalignment': 'baseline',\n        'horizontalalignment': loc}\n\nloc : {'center', 'left', 'right'}, default: :rc:`axes.titlelocation`\n    Which title to set.\n\ny : float, default: :rc:`axes.titley`\n    Vertical Axes location for the title (1.0 is the top).  If\n    None (the default) and :rc:`axes.titley` is also None, y is\n    determined automatically to avoid decorators on the Axes.\n\npad : float, default: :rc:`axes.titlepad`\n    The offset of the title from the top of the Axes, in points.\n\nReturns\n-------\n`.Text`\n    The matplotlib text instance representing the title\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    Other keyword arguments are text properties, see `.Text` for a list\n    of valid text properties.", "parameters": {"type": "object", "properties": {"label": {}, "fontdict": {"type": "NoneType", "default": null}, "loc": {"type": "NoneType", "default": null}, "pad": {"type": "NoneType", "default": null}, "y": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/124", "data": {"name": "my_list.append(12)"}}
{"task_id": "BigCodeBench/124", "data": {"name": "random.seed", "type": "callable", "signature": "(a=None)", "description": "Initialize internal state from a seed.\n\nThe only supported seed types are None, int, float,\nstr, bytes, and bytearray.\n\nNone or no argument seeds from current time or from an operating\nsystem specific randomness source if available.\n\nIf *a* is an int, all bits are used.\n\nFor version 2 (the default), all of the bits are used if *a* is a str,\nbytes, or bytearray.  For version 1 (provided for reproducing random\nsequences from older versions of Python), the algorithm for str and\nbytes generates a narrower range of seeds.", "parameters": {"type": "object", "properties": {"a": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/124", "data": {"name": "random.randint", "type": "callable", "signature": "(a, b)", "description": "Return random integer in range [a, b], including both end points.\n        ", "parameters": {"type": "object", "properties": {"a": {}, "b": {}}}}}
{"task_id": "BigCodeBench/124", "data": {"name": "time.time", "type": "callable", "signature": "()", "description": "time() -> floating point number\n\nReturn the current time in seconds since the Epoch.\nFractions of a second may be present if the system clock provides them.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/129", "data": {"name": "bs4.BeautifulSoup", "type": "class", "signature": "(markup='', features=None, builder=None, parse_only=None, from_encoding=None, exclude_encodings=None, element_classes=None, **kwargs)", "description": "A data structure representing a parsed HTML or XML document.\n\nMost of the methods you'll call on a BeautifulSoup object are inherited from\nPageElement or Tag.\n\nInternally, this class defines the basic interface called by the\ntree builders when converting an HTML/XML document into a data\nstructure. The interface abstracts away the differences between\nparsers. To write a new tree builder, you'll need to understand\nthese methods as a whole.\n\nThese methods will be called by the BeautifulSoup constructor:\n  * reset()\n  * feed(markup)\n\nThe tree builder may call these methods from its feed() implementation:\n  * handle_starttag(name, attrs) # See note about return value\n  * handle_endtag(name)\n  * handle_data(data) # Appends to the current data node\n  * endData(containerClass) # Ends the current data node\n\nNo matter how complicated the underlying parser is, you should be\nable to build a tree using 'start tag' events, 'end tag' events,\n'data' events, and \"done with data\" events.\n\nIf you encounter an empty-element tag (aka a self-closing tag,\nlike HTML's <br> tag), call handle_starttag and then\nhandle_endtag.", "parameters": {"type": "object", "properties": {"markup": {"type": "str", "default": ""}, "features": {"type": "NoneType", "default": null}, "builder": {"type": "NoneType", "default": null}, "parse_only": {"type": "NoneType", "default": null}, "from_encoding": {"type": "NoneType", "default": null}, "exclude_encodings": {"type": "NoneType", "default": null}, "element_classes": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/129", "data": {"name": "bs4.BeautifulSoup.find", "type": "callable", "signature": "(self, name=None)", "description": "Look in the children of this PageElement and find the first\nPageElement that matches the given criteria.\n\nAll find_* methods take a common set of arguments. See the online\ndocumentation for detailed explanations.\n\n:param name: A filter on tag name.\n:param attrs: A dictionary of filters on attribute values.\n:param recursive: If this is True, find() will perform a\n    recursive search of this PageElement's children. Otherwise,\n    only the direct children will be considered.\n:param limit: Stop looking after finding this many results.\n:kwargs: A dictionary of filters on attribute values.\n:return: A PageElement.\n:rtype: bs4.element.PageElement", "parameters": {"type": "object", "properties": {"name": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/129", "data": {"name": "data.append(cols)"}}
{"task_id": "BigCodeBench/129", "data": {"name": "ele.text"}}
{"task_id": "BigCodeBench/129", "data": {"name": "ele.text.strip()"}}
{"task_id": "BigCodeBench/129", "data": {"name": "requests.get", "type": "callable", "signature": "(url)", "description": "Sends a GET request.\n\n:param url: URL for the new :class:`Request` object.\n:param params: (optional) Dictionary, list of tuples or bytes to send\n    in the query string for the :class:`Request`.\n:param \\*\\*kwargs: Optional arguments that ``request`` takes.\n:return: :class:`Response <Response>` object\n:rtype: requests.Response", "parameters": {"type": "object", "properties": {"url": {}}}}}
{"task_id": "BigCodeBench/129", "data": {"name": "requests.get.text", "type": "callable", "signature": "(url, params=None, **kwargs)", "description": "Sends a GET request.\n\n:param url: URL for the new :class:`Request` object.\n:param params: (optional) Dictionary, list of tuples or bytes to send\n    in the query string for the :class:`Request`.\n:param \\*\\*kwargs: Optional arguments that ``request`` takes.\n:return: :class:`Response <Response>` object\n:rtype: requests.Response", "parameters": {"type": "object", "properties": {"url": {}, "params": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/129", "data": {"name": "requests.get.raise_for_status", "type": "callable", "signature": "()", "description": "Sends a GET request.\n\n:param url: URL for the new :class:`Request` object.\n:param params: (optional) Dictionary, list of tuples or bytes to send\n    in the query string for the :class:`Request`.\n:param \\*\\*kwargs: Optional arguments that ``request`` takes.\n:return: :class:`Response <Response>` object\n:rtype: requests.Response", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/129", "data": {"name": "row.find_all('td')"}}
{"task_id": "BigCodeBench/129", "data": {"name": "soup.find('table').find_all('tr')"}}
{"task_id": "BigCodeBench/129", "data": {"name": "soup.find('table').find_all('th')"}}
{"task_id": "BigCodeBench/129", "data": {"name": "th.text"}}
{"task_id": "BigCodeBench/129", "data": {"name": "th.text.strip()"}}
{"task_id": "BigCodeBench/139", "data": {"name": "axes.append(ax)"}}
{"task_id": "BigCodeBench/139", "data": {"name": "df.empty"}}
{"task_id": "BigCodeBench/139", "data": {"name": "df.select_dtypes(include=np.number)"}}
{"task_id": "BigCodeBench/139", "data": {"name": "matplotlib.pyplot.subplots", "type": "class", "signature": "()", "description": "Create a figure and a set of subplots.\n\nThis utility wrapper makes it convenient to create common layouts of\nsubplots, including the enclosing figure object, in a single call.\n\nParameters\n----------\nnrows, ncols : int, default: 1\n    Number of rows/columns of the subplot grid.\n\nsharex, sharey : bool or {'none', 'all', 'row', 'col'}, default: False\n    Controls sharing of properties among x (*sharex*) or y (*sharey*)\n    axes:\n\n    - True or 'all': x- or y-axis will be shared among all subplots.\n    - False or 'none': each subplot x- or y-axis will be independent.\n    - 'row': each subplot row will share an x- or y-axis.\n    - 'col': each subplot column will share an x- or y-axis.\n\n    When subplots have a shared x-axis along a column, only the x tick\n    labels of the bottom subplot are created. Similarly, when subplots\n    have a shared y-axis along a row, only the y tick labels of the first\n    column subplot are created. To later turn other subplots' ticklabels\n    on, use `~matplotlib.axes.Axes.tick_params`.\n\n    When subplots have a shared axis that has units, calling\n    `~matplotlib.axis.Axis.set_units` will update each axis with the\n    new units.\n\nsqueeze : bool, default: True\n    - If True, extra dimensions are squeezed out from the returned\n      array of `~matplotlib.axes.Axes`:\n\n      - if only one subplot is constructed (nrows=ncols=1), the\n        resulting single Axes object is returned as a scalar.\n      - for Nx1 or 1xM subplots, the returned object is a 1D numpy\n        object array of Axes objects.\n      - for NxM, subplots with N>1 and M>1 are returned as a 2D array.\n\n    - If False, no squeezing at all is done: the returned Axes object is\n      always a 2D array containing Axes instances, even if it ends up\n      being 1x1.\n\nwidth_ratios : array-like of length *ncols*, optional\n    Defines the relative widths of the columns. Each column gets a\n    relative width of ``width_ratios[i] / sum(width_ratios)``.\n    If not given, all columns will have the same width.  Equivalent\n    to ``gridspec_kw={'width_ratios': [...]}``.\n\nheight_ratios : array-like of length *nrows*, optional\n    Defines the relative heights of the rows. Each row gets a\n    relative height of ``height_ratios[i] / sum(height_ratios)``.\n    If not given, all rows will have the same height. Convenience\n    for ``gridspec_kw={'height_ratios': [...]}``.\n\nsubplot_kw : dict, optional\n    Dict with keywords passed to the\n    `~matplotlib.figure.Figure.add_subplot` call used to create each\n    subplot.\n\ngridspec_kw : dict, optional\n    Dict with keywords passed to the `~matplotlib.gridspec.GridSpec`\n    constructor used to create the grid the subplots are placed on.\n\n**fig_kw\n    All additional keyword arguments are passed to the\n    `.pyplot.figure` call.\n\nReturns\n-------\nfig : `.Figure`\n\nax : `~matplotlib.axes.Axes` or array of Axes\n    *ax* can be either a single `~.axes.Axes` object, or an array of Axes\n    objects if more than one subplot was created.  The dimensions of the\n    resulting array can be controlled with the squeeze keyword, see above.\n\n    Typical idioms for handling the return value are::\n\n        # using the variable ax for single a Axes\n        fig, ax = plt.subplots()\n\n        # using the variable axs for multiple Axes\n        fig, axs = plt.subplots(2, 2)\n\n        # using tuple unpacking for multiple Axes\n        fig, (ax1, ax2) = plt.subplots(1, 2)\n        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n\n    The names ``ax`` and pluralized ``axs`` are preferred over ``axes``\n    because for the latter it's not clear if it refers to a single\n    `~.axes.Axes` instance or a collection of these.\n\nSee Also\n--------\n.pyplot.figure\n.pyplot.subplot\n.pyplot.axes\n.Figure.subplots\n.Figure.add_subplot\n\nExamples\n--------\n::\n\n    # First create some toy data:\n    x = np.linspace(0, 2*np.pi, 400)\n    y = np.sin(x**2)\n\n    # Create just a figure and only one subplot\n    fig, ax = plt.subplots()\n    ax.plot(x, y)\n    ax.set_title('Simple plot')\n\n    # Create two subplots and unpack the output array immediately\n    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n    ax1.plot(x, y)\n    ax1.set_title('Sharing Y axis')\n    ax2.scatter(x, y)\n\n    # Create four polar axes and access them through the returned array\n    fig, axs = plt.subplots(2, 2, subplot_kw=dict(projection=\"polar\"))\n    axs[0, 0].plot(x, y)\n    axs[1, 1].scatter(x, y)\n\n    # Share a X axis with each column of subplots\n    plt.subplots(2, 2, sharex='col')\n\n    # Share a Y axis with each row of subplots\n    plt.subplots(2, 2, sharey='row')\n\n    # Share both X and Y axes with all subplots\n    plt.subplots(2, 2, sharex='all', sharey='all')\n\n    # Note that this is the same as\n    plt.subplots(2, 2, sharex=True, sharey=True)\n\n    # Create figure number 10 with a single subplot\n    # and clears it if it already exists.\n    fig, ax = plt.subplots(num=10, clear=True)", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/139", "data": {"name": "matplotlib.pyplot.subplots[1].set_xlabel", "type": "method", "signature": "(xlabel, fontdict=None, labelpad=None, *, loc=None, **kwargs)", "description": "Set the label for the x-axis.\n\nParameters\n----------\nxlabel : str\n    The label text.\n\nlabelpad : float, default: :rc:`axes.labelpad`\n    Spacing in points from the Axes bounding box including ticks\n    and tick labels.  If None, the previous value is left as is.\n\nloc : {'left', 'center', 'right'}, default: :rc:`xaxis.labellocation`\n    The label position. This is a high-level alternative for passing\n    parameters *x* and *horizontalalignment*.\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    `.Text` properties control the appearance of the label.\n\nSee Also\n--------\ntext : Documents the properties supported by `.Text`.", "parameters": {"type": "object", "properties": {"xlabel": {}, "fontdict": {"type": "NoneType", "default": null}, "labelpad": {"type": "NoneType", "default": null}, "loc": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/139", "data": {"name": "matplotlib.pyplot.subplots[1].set_ylabel", "type": "method", "signature": "(ylabel, fontdict=None, labelpad=None, *, loc=None, **kwargs)", "description": "Set the label for the y-axis.\n\nParameters\n----------\nylabel : str\n    The label text.\n\nlabelpad : float, default: :rc:`axes.labelpad`\n    Spacing in points from the Axes bounding box including ticks\n    and tick labels.  If None, the previous value is left as is.\n\nloc : {'bottom', 'center', 'top'}, default: :rc:`yaxis.labellocation`\n    The label position. This is a high-level alternative for passing\n    parameters *y* and *horizontalalignment*.\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    `.Text` properties control the appearance of the label.\n\nSee Also\n--------\ntext : Documents the properties supported by `.Text`.", "parameters": {"type": "object", "properties": {"ylabel": {}, "fontdict": {"type": "NoneType", "default": null}, "labelpad": {"type": "NoneType", "default": null}, "loc": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/139", "data": {"name": "numeric_cols.size"}}
{"task_id": "BigCodeBench/147", "data": {"name": "socket.socket", "type": "class", "signature": "(family=-1, type=-1, proto=-1, fileno=None)", "description": "A subclass of _socket.socket adding the makefile() method.", "parameters": {"type": "object", "properties": {"family": {"type": "integer", "default": -1}, "type": {"type": "integer", "default": -1}, "proto": {"type": "integer", "default": -1}, "fileno": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/147", "data": {"name": "socket.socket.close", "type": "callable", "signature": "(self)", "description": "close()\n\nClose the socket.  It cannot be used after this call.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/147", "data": {"name": "socket.socket.settimeout", "type": "callable", "signature": "(*args, **kwargs)", "description": "settimeout(timeout)\n\nSet a timeout on socket operations.  'timeout' can be a float,\ngiving in seconds, or None.  Setting a timeout of None disables\nthe timeout feature and is equivalent to setblocking(1).\nSetting a timeout of zero is the same as setblocking(0).", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/147", "data": {"name": "socket.socket.connect", "type": "callable", "signature": "(*args, **kwargs)", "description": "connect(address)\n\nConnect the socket to a remote address.  For IP sockets, the address\nis a pair (host, port).", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/147", "data": {"name": "threading.Thread", "type": "class", "signature": "(group=None, target=None, name=None, args=(), kwargs=None, *, daemon=None)", "description": "A class that represents a thread of control.\n\nThis class can be safely subclassed in a limited fashion. There are two ways\nto specify the activity: by passing a callable object to the constructor, or\nby overriding the run() method in a subclass.", "parameters": {"type": "object", "properties": {"group": {"type": "NoneType", "default": null}, "target": {"type": "NoneType", "default": null}, "name": {"type": "NoneType", "default": null}, "args": {"type": "str", "default": "()"}, "kwargs": {"type": "NoneType", "default": null}, "daemon": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/147", "data": {"name": "threading.Thread.join", "type": "callable", "signature": "(self)", "description": "Wait until the thread terminates.\n\nThis blocks the calling thread until the thread whose join() method is\ncalled terminates -- either normally or through an unhandled exception\nor until the optional timeout occurs.\n\nWhen the timeout argument is present and not None, it should be a\nfloating point number specifying a timeout for the operation in seconds\n(or fractions thereof). As join() always returns None, you must call\nis_alive() after join() to decide whether a timeout happened -- if the\nthread is still alive, the join() call timed out.\n\nWhen the timeout argument is not present or None, the operation will\nblock until the thread terminates.\n\nA thread can be join()ed many times.\n\njoin() raises a RuntimeError if an attempt is made to join the current\nthread as that would cause a deadlock. It is also an error to join() a\nthread before it has been started and attempts to do so raises the same\nexception.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/147", "data": {"name": "threading.Thread.start", "type": "callable", "signature": "(self)", "description": "Start the thread's activity.\n\nIt must be called at most once per thread object. It arranges for the\nobject's run() method to be invoked in a separate thread of control.\n\nThis method will raise a RuntimeError if called more than once on the\nsame thread object.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/147", "data": {"name": "socket.AF_INET", "type": "constant", "signature": null, "description": "An enumeration.", "value": "AddressFamily.AF_INET", "parameters": {"type": "constant"}}}
{"task_id": "BigCodeBench/147", "data": {"name": "socket.SOCK_STREAM", "type": "constant", "signature": null, "description": "An enumeration.", "value": "SocketKind.SOCK_STREAM", "parameters": {"type": "constant"}}}
{"task_id": "BigCodeBench/147", "data": {"name": "threads.append(thread)"}}
{"task_id": "BigCodeBench/161", "data": {"name": "datetime.datetime", "type": "class", "signature": "(self, /, *args, **kwargs)", "description": "datetime(year, month, day[, hour[, minute[, second[, microsecond[,tzinfo]]]]])\n\nThe year, month and day arguments are required. tzinfo may be None, or an\ninstance of a tzinfo subclass. The remaining arguments may be ints.", "parameters": {"type": "object", "properties": {"/": {}}}}}
{"task_id": "BigCodeBench/161", "data": {"name": "datetime.datetime.strptime", "type": "callable", "signature": "(*args, **kwargs)", "description": "string, format -> new datetime parsed from a string (like time.strptime()).", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/161", "data": {"name": "pandas.DataFrame", "type": "class", "signature": "(data=None, index: 'Axes | None' = None, columns: 'Axes | None' = None, dtype: 'Dtype | None' = None, copy: 'bool | None' = None) -> 'None'", "description": "Two-dimensional, size-mutable, potentially heterogeneous tabular data.\n\nData structure also contains labeled axes (rows and columns).\nArithmetic operations align on both row and column labels. Can be\nthought of as a dict-like container for Series objects. The primary\npandas data structure.\n\nParameters\n----------\ndata : ndarray (structured or homogeneous), Iterable, dict, or DataFrame\n    Dict can contain Series, arrays, constants, dataclass or list-like objects. If\n    data is a dict, column order follows insertion-order. If a dict contains Series\n    which have an index defined, it is aligned by its index. This alignment also\n    occurs if data is a Series or a DataFrame itself. Alignment is done on\n    Series/DataFrame inputs.\n\n    If data is a list of dicts, column order follows insertion-order.\n\nindex : Index or array-like\n    Index to use for resulting frame. Will default to RangeIndex if\n    no indexing information part of input data and no index provided.\ncolumns : Index or array-like\n    Column labels to use for resulting frame when data does not have them,\n    defaulting to RangeIndex(0, 1, 2, ..., n). If data contains column labels,\n    will perform column selection instead.\ndtype : dtype, default None\n    Data type to force. Only a single dtype is allowed. If None, infer.\ncopy : bool or None, default None\n    Copy data from inputs.\n    For dict data, the default of None behaves like ``copy=True``.  For DataFrame\n    or 2d ndarray input, the default of None behaves like ``copy=False``.\n    If data is a dict containing one or more Series (possibly of different dtypes),\n    ``copy=False`` will ensure that these inputs are not copied.\n\n    .. versionchanged:: 1.3.0\n\nSee Also\n--------\nDataFrame.from_records : Constructor from tuples, also record arrays.\nDataFrame.from_dict : From dicts of Series, arrays, or dicts.\nread_csv : Read a comma-separated values (csv) file into DataFrame.\nread_table : Read general delimited file into DataFrame.\nread_clipboard : Read text from clipboard into DataFrame.\n\nNotes\n-----\nPlease reference the :ref:`User Guide <basics.dataframe>` for more information.\n\nExamples\n--------\nConstructing DataFrame from a dictionary.\n\n>>> d = {'col1': [1, 2], 'col2': [3, 4]}\n>>> df = pd.DataFrame(data=d)\n>>> df\n   col1  col2\n0     1     3\n1     2     4\n\nNotice that the inferred dtype is int64.\n\n>>> df.dtypes\ncol1    int64\ncol2    int64\ndtype: object\n\nTo enforce a single dtype:\n\n>>> df = pd.DataFrame(data=d, dtype=np.int8)\n>>> df.dtypes\ncol1    int8\ncol2    int8\ndtype: object\n\nConstructing DataFrame from a dictionary including Series:\n\n>>> d = {'col1': [0, 1, 2, 3], 'col2': pd.Series([2, 3], index=[2, 3])}\n>>> pd.DataFrame(data=d, index=[0, 1, 2, 3])\n   col1  col2\n0     0   NaN\n1     1   NaN\n2     2   2.0\n3     3   3.0\n\nConstructing DataFrame from numpy ndarray:\n\n>>> df2 = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),\n...                    columns=['a', 'b', 'c'])\n>>> df2\n   a  b  c\n0  1  2  3\n1  4  5  6\n2  7  8  9\n\nConstructing DataFrame from a numpy ndarray that has labeled columns:\n\n>>> data = np.array([(1, 2, 3), (4, 5, 6), (7, 8, 9)],\n...                 dtype=[(\"a\", \"i4\"), (\"b\", \"i4\"), (\"c\", \"i4\")])\n>>> df3 = pd.DataFrame(data, columns=['c', 'a'])\n...\n>>> df3\n   c  a\n0  3  1\n1  6  4\n2  9  7\n\nConstructing DataFrame from dataclass:\n\n>>> from dataclasses import make_dataclass\n>>> Point = make_dataclass(\"Point\", [(\"x\", int), (\"y\", int)])\n>>> pd.DataFrame([Point(0, 0), Point(0, 3), Point(2, 3)])\n   x  y\n0  0  0\n1  0  3\n2  2  3\n\nConstructing DataFrame from Series/DataFrame:\n\n>>> ser = pd.Series([1, 2, 3], index=[\"a\", \"b\", \"c\"])\n>>> df = pd.DataFrame(data=ser, index=[\"a\", \"c\"])\n>>> df\n   0\na  1\nc  3\n\n>>> df1 = pd.DataFrame([1, 2, 3], index=[\"a\", \"b\", \"c\"], columns=[\"x\"])\n>>> df2 = pd.DataFrame(data=df1, index=[\"a\", \"c\"])\n>>> df2\n   x\na  1\nc  3", "parameters": {"type": "object", "properties": {"data": {"type": "NoneType", "default": null}, "index": {"type": ["axes", "null"], "default": null}, "columns": {"type": ["axes", "null"], "default": null}, "dtype": {"type": ["dtype", "null"], "default": null}, "copy": {"type": ["bool", "null"], "default": null}}}}}
{"task_id": "BigCodeBench/161", "data": {"name": "pandas.DataFrame.to_csv", "type": "callable", "signature": "(self, path_or_buf: 'FilePath | WriteBuffer[bytes] | WriteBuffer[str] | None' = None, index: 'bool_t' = True)", "description": "Write object to a comma-separated values (csv) file.\n\nParameters\n----------\npath_or_buf : str, path object, file-like object, or None, default None\n    String, path object (implementing os.PathLike[str]), or file-like\n    object implementing a write() function. If None, the result is\n    returned as a string. If a non-binary file object is passed, it should\n    be opened with `newline=''`, disabling universal newlines. If a binary\n    file object is passed, `mode` might need to contain a `'b'`.\nsep : str, default ','\n    String of length 1. Field delimiter for the output file.\nna_rep : str, default ''\n    Missing data representation.\nfloat_format : str, Callable, default None\n    Format string for floating point numbers. If a Callable is given, it takes\n    precedence over other numeric formatting parameters, like decimal.\ncolumns : sequence, optional\n    Columns to write.\nheader : bool or list of str, default True\n    Write out the column names. If a list of strings is given it is\n    assumed to be aliases for the column names.\nindex : bool, default True\n    Write row names (index).\nindex_label : str or sequence, or False, default None\n    Column label for index column(s) if desired. If None is given, and\n    `header` and `index` are True, then the index names are used. A\n    sequence should be given if the object uses MultiIndex. If\n    False do not print fields for index names. Use index_label=False\n    for easier importing in R.\nmode : {'w', 'x', 'a'}, default 'w'\n    Forwarded to either `open(mode=)` or `fsspec.open(mode=)` to control\n    the file opening. Typical values include:\n\n    - 'w', truncate the file first.\n    - 'x', exclusive creation, failing if the file already exists.\n    - 'a', append to the end of file if it exists.\n\nencoding : str, optional\n    A string representing the encoding to use in the output file,\n    defaults to 'utf-8'. `encoding` is not supported if `path_or_buf`\n    is a non-binary file object.\ncompression : str or dict, default 'infer'\n    For on-the-fly compression of the output data. If 'infer' and 'path_or_buf' is\n    path-like, then detect compression from the following extensions: '.gz',\n    '.bz2', '.zip', '.xz', '.zst', '.tar', '.tar.gz', '.tar.xz' or '.tar.bz2'\n    (otherwise no compression).\n    Set to ``None`` for no compression.\n    Can also be a dict with key ``'method'`` set\n    to one of {``'zip'``, ``'gzip'``, ``'bz2'``, ``'zstd'``, ``'xz'``, ``'tar'``} and\n    other key-value pairs are forwarded to\n    ``zipfile.ZipFile``, ``gzip.GzipFile``,\n    ``bz2.BZ2File``, ``zstandard.ZstdCompressor``, ``lzma.LZMAFile`` or\n    ``tarfile.TarFile``, respectively.\n    As an example, the following could be passed for faster compression and to create\n    a reproducible gzip archive:\n    ``compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}``.\n\n    .. versionadded:: 1.5.0\n        Added support for `.tar` files.\n\n       May be a dict with key 'method' as compression mode\n       and other entries as additional compression options if\n       compression mode is 'zip'.\n\n       Passing compression options as keys in dict is\n       supported for compression modes 'gzip', 'bz2', 'zstd', and 'zip'.\nquoting : optional constant from csv module\n    Defaults to csv.QUOTE_MINIMAL. If you have set a `float_format`\n    then floats are converted to strings and thus csv.QUOTE_NONNUMERIC\n    will treat them as non-numeric.\nquotechar : str, default '\\\"'\n    String of length 1. Character used to quote fields.\nlineterminator : str, optional\n    The newline character or character sequence to use in the output\n    file. Defaults to `os.linesep`, which depends on the OS in which\n    this method is called ('\\\\n' for linux, '\\\\r\\\\n' for Windows, i.e.).\n\n    .. versionchanged:: 1.5.0\n\n        Previously was line_terminator, changed for consistency with\n        read_csv and the standard library 'csv' module.\n\nchunksize : int or None\n    Rows to write at a time.\ndate_format : str, default None\n    Format string for datetime objects.\ndoublequote : bool, default True\n    Control quoting of `quotechar` inside a field.\nescapechar : str, default None\n    String of length 1. Character used to escape `sep` and `quotechar`\n    when appropriate.\ndecimal : str, default '.'\n    Character recognized as decimal separator. E.g. use ',' for\n    European data.\nerrors : str, default 'strict'\n    Specifies how encoding and decoding errors are to be handled.\n    See the errors argument for :func:`open` for a full list\n    of options.\n\nstorage_options : dict, optional\n    Extra options that make sense for a particular storage connection, e.g.\n    host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n    are forwarded to ``urllib.request.Request`` as header options. For other\n    URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n    forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n    details, and for more examples on storage options refer `here\n    <https://pandas.pydata.org/docs/user_guide/io.html?\n    highlight=storage_options#reading-writing-remote-files>`_.\n\nReturns\n-------\nNone or str\n    If path_or_buf is None, returns the resulting csv format as a\n    string. Otherwise returns None.\n\nSee Also\n--------\nread_csv : Load a CSV file into a DataFrame.\nto_excel : Write DataFrame to an Excel file.\n\nExamples\n--------\nCreate 'out.csv' containing 'df' without indices\n\n>>> df = pd.DataFrame({'name': ['Raphael', 'Donatello'],\n...                    'mask': ['red', 'purple'],\n...                    'weapon': ['sai', 'bo staff']})\n>>> df.to_csv('out.csv', index=False)  # doctest: +SKIP\n\nCreate 'out.zip' containing 'out.csv'\n\n>>> df.to_csv(index=False)\n'name,mask,weapon\\nRaphael,red,sai\\nDonatello,purple,bo staff\\n'\n>>> compression_opts = dict(method='zip',\n...                         archive_name='out.csv')  # doctest: +SKIP\n>>> df.to_csv('out.zip', index=False,\n...           compression=compression_opts)  # doctest: +SKIP\n\nTo write a csv file to a new folder or nested folder you will first\nneed to create it using either Pathlib or os:\n\n>>> from pathlib import Path  # doctest: +SKIP\n>>> filepath = Path('folder/subfolder/out.csv')  # doctest: +SKIP\n>>> filepath.parent.mkdir(parents=True, exist_ok=True)  # doctest: +SKIP\n>>> df.to_csv(filepath)  # doctest: +SKIP\n\n>>> import os  # doctest: +SKIP\n>>> os.makedirs('folder/subfolder', exist_ok=True)  # doctest: +SKIP\n>>> df.to_csv('folder/subfolder/out.csv')  # doctest: +SKIP", "parameters": {"type": "object", "properties": {"path_or_buf": {"type": ["writebuffer[bytes]", "null", "writebuffer[str]", "filepath"], "default": null}, "index": {"type": "bool_t", "default": true}}}}}
{"task_id": "BigCodeBench/161", "data": {"name": "line.strip().strip()"}}
{"task_id": "BigCodeBench/161", "data": {"name": "match.groups[2].strip"}}
{"task_id": "BigCodeBench/161", "data": {"name": "parsed_data.append([log_type, timestamp, message.strip()])"}}
{"task_id": "BigCodeBench/161", "data": {"name": "re.match", "type": "callable", "signature": "(pattern, string)", "description": "Try to apply the pattern at the start of the string, returning\na Match object, or None if no match was found.", "parameters": {"type": "object", "properties": {"pattern": {}, "string": {}}}}}
{"task_id": "BigCodeBench/161", "data": {"name": "re.match.groups", "type": "callable", "signature": "()", "description": "Try to apply the pattern at the start of the string, returning\na Match object, or None if no match was found.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/162", "data": {"name": "matplotlib.pyplot.subplots", "type": "class", "signature": "()", "description": "Create a figure and a set of subplots.\n\nThis utility wrapper makes it convenient to create common layouts of\nsubplots, including the enclosing figure object, in a single call.\n\nParameters\n----------\nnrows, ncols : int, default: 1\n    Number of rows/columns of the subplot grid.\n\nsharex, sharey : bool or {'none', 'all', 'row', 'col'}, default: False\n    Controls sharing of properties among x (*sharex*) or y (*sharey*)\n    axes:\n\n    - True or 'all': x- or y-axis will be shared among all subplots.\n    - False or 'none': each subplot x- or y-axis will be independent.\n    - 'row': each subplot row will share an x- or y-axis.\n    - 'col': each subplot column will share an x- or y-axis.\n\n    When subplots have a shared x-axis along a column, only the x tick\n    labels of the bottom subplot are created. Similarly, when subplots\n    have a shared y-axis along a row, only the y tick labels of the first\n    column subplot are created. To later turn other subplots' ticklabels\n    on, use `~matplotlib.axes.Axes.tick_params`.\n\n    When subplots have a shared axis that has units, calling\n    `~matplotlib.axis.Axis.set_units` will update each axis with the\n    new units.\n\nsqueeze : bool, default: True\n    - If True, extra dimensions are squeezed out from the returned\n      array of `~matplotlib.axes.Axes`:\n\n      - if only one subplot is constructed (nrows=ncols=1), the\n        resulting single Axes object is returned as a scalar.\n      - for Nx1 or 1xM subplots, the returned object is a 1D numpy\n        object array of Axes objects.\n      - for NxM, subplots with N>1 and M>1 are returned as a 2D array.\n\n    - If False, no squeezing at all is done: the returned Axes object is\n      always a 2D array containing Axes instances, even if it ends up\n      being 1x1.\n\nwidth_ratios : array-like of length *ncols*, optional\n    Defines the relative widths of the columns. Each column gets a\n    relative width of ``width_ratios[i] / sum(width_ratios)``.\n    If not given, all columns will have the same width.  Equivalent\n    to ``gridspec_kw={'width_ratios': [...]}``.\n\nheight_ratios : array-like of length *nrows*, optional\n    Defines the relative heights of the rows. Each row gets a\n    relative height of ``height_ratios[i] / sum(height_ratios)``.\n    If not given, all rows will have the same height. Convenience\n    for ``gridspec_kw={'height_ratios': [...]}``.\n\nsubplot_kw : dict, optional\n    Dict with keywords passed to the\n    `~matplotlib.figure.Figure.add_subplot` call used to create each\n    subplot.\n\ngridspec_kw : dict, optional\n    Dict with keywords passed to the `~matplotlib.gridspec.GridSpec`\n    constructor used to create the grid the subplots are placed on.\n\n**fig_kw\n    All additional keyword arguments are passed to the\n    `.pyplot.figure` call.\n\nReturns\n-------\nfig : `.Figure`\n\nax : `~matplotlib.axes.Axes` or array of Axes\n    *ax* can be either a single `~.axes.Axes` object, or an array of Axes\n    objects if more than one subplot was created.  The dimensions of the\n    resulting array can be controlled with the squeeze keyword, see above.\n\n    Typical idioms for handling the return value are::\n\n        # using the variable ax for single a Axes\n        fig, ax = plt.subplots()\n\n        # using the variable axs for multiple Axes\n        fig, axs = plt.subplots(2, 2)\n\n        # using tuple unpacking for multiple Axes\n        fig, (ax1, ax2) = plt.subplots(1, 2)\n        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n\n    The names ``ax`` and pluralized ``axs`` are preferred over ``axes``\n    because for the latter it's not clear if it refers to a single\n    `~.axes.Axes` instance or a collection of these.\n\nSee Also\n--------\n.pyplot.figure\n.pyplot.subplot\n.pyplot.axes\n.Figure.subplots\n.Figure.add_subplot\n\nExamples\n--------\n::\n\n    # First create some toy data:\n    x = np.linspace(0, 2*np.pi, 400)\n    y = np.sin(x**2)\n\n    # Create just a figure and only one subplot\n    fig, ax = plt.subplots()\n    ax.plot(x, y)\n    ax.set_title('Simple plot')\n\n    # Create two subplots and unpack the output array immediately\n    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n    ax1.plot(x, y)\n    ax1.set_title('Sharing Y axis')\n    ax2.scatter(x, y)\n\n    # Create four polar axes and access them through the returned array\n    fig, axs = plt.subplots(2, 2, subplot_kw=dict(projection=\"polar\"))\n    axs[0, 0].plot(x, y)\n    axs[1, 1].scatter(x, y)\n\n    # Share a X axis with each column of subplots\n    plt.subplots(2, 2, sharex='col')\n\n    # Share a Y axis with each row of subplots\n    plt.subplots(2, 2, sharey='row')\n\n    # Share both X and Y axes with all subplots\n    plt.subplots(2, 2, sharex='all', sharey='all')\n\n    # Note that this is the same as\n    plt.subplots(2, 2, sharex=True, sharey=True)\n\n    # Create figure number 10 with a single subplot\n    # and clears it if it already exists.\n    fig, ax = plt.subplots(num=10, clear=True)", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/162", "data": {"name": "matplotlib.pyplot.subplots[1].set_ylabel", "type": "method", "signature": "(ylabel, fontdict=None, labelpad=None, *, loc=None, **kwargs)", "description": "Set the label for the y-axis.\n\nParameters\n----------\nylabel : str\n    The label text.\n\nlabelpad : float, default: :rc:`axes.labelpad`\n    Spacing in points from the Axes bounding box including ticks\n    and tick labels.  If None, the previous value is left as is.\n\nloc : {'bottom', 'center', 'top'}, default: :rc:`yaxis.labellocation`\n    The label position. This is a high-level alternative for passing\n    parameters *y* and *horizontalalignment*.\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    `.Text` properties control the appearance of the label.\n\nSee Also\n--------\ntext : Documents the properties supported by `.Text`.", "parameters": {"type": "object", "properties": {"ylabel": {}, "fontdict": {"type": "NoneType", "default": null}, "labelpad": {"type": "NoneType", "default": null}, "loc": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/162", "data": {"name": "matplotlib.pyplot.subplots[1].set_xlabel", "type": "method", "signature": "(xlabel, fontdict=None, labelpad=None, *, loc=None, **kwargs)", "description": "Set the label for the x-axis.\n\nParameters\n----------\nxlabel : str\n    The label text.\n\nlabelpad : float, default: :rc:`axes.labelpad`\n    Spacing in points from the Axes bounding box including ticks\n    and tick labels.  If None, the previous value is left as is.\n\nloc : {'left', 'center', 'right'}, default: :rc:`xaxis.labellocation`\n    The label position. This is a high-level alternative for passing\n    parameters *x* and *horizontalalignment*.\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    `.Text` properties control the appearance of the label.\n\nSee Also\n--------\ntext : Documents the properties supported by `.Text`.", "parameters": {"type": "object", "properties": {"xlabel": {}, "fontdict": {"type": "NoneType", "default": null}, "labelpad": {"type": "NoneType", "default": null}, "loc": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/162", "data": {"name": "matplotlib.pyplot.subplots[1].set_title", "type": "method", "signature": "(label, fontdict=None, loc=None, pad=None, *, y=None, **kwargs)", "description": "Set a title for the Axes.\n\nSet one of the three available Axes titles. The available titles\nare positioned above the Axes in the center, flush with the left\nedge, and flush with the right edge.\n\nParameters\n----------\nlabel : str\n    Text to use for the title\n\nfontdict : dict\n\n    .. admonition:: Discouraged\n\n       The use of *fontdict* is discouraged. Parameters should be passed as\n       individual keyword arguments or using dictionary-unpacking\n       ``set_title(..., **fontdict)``.\n\n    A dictionary controlling the appearance of the title text,\n    the default *fontdict* is::\n\n       {'fontsize': rcParams['axes.titlesize'],\n        'fontweight': rcParams['axes.titleweight'],\n        'color': rcParams['axes.titlecolor'],\n        'verticalalignment': 'baseline',\n        'horizontalalignment': loc}\n\nloc : {'center', 'left', 'right'}, default: :rc:`axes.titlelocation`\n    Which title to set.\n\ny : float, default: :rc:`axes.titley`\n    Vertical Axes location for the title (1.0 is the top).  If\n    None (the default) and :rc:`axes.titley` is also None, y is\n    determined automatically to avoid decorators on the Axes.\n\npad : float, default: :rc:`axes.titlepad`\n    The offset of the title from the top of the Axes, in points.\n\nReturns\n-------\n`.Text`\n    The matplotlib text instance representing the title\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    Other keyword arguments are text properties, see `.Text` for a list\n    of valid text properties.", "parameters": {"type": "object", "properties": {"label": {}, "fontdict": {"type": "NoneType", "default": null}, "loc": {"type": "NoneType", "default": null}, "pad": {"type": "NoneType", "default": null}, "y": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/162", "data": {"name": "matplotlib.pyplot.subplots[1].hist", "type": "method", "signature": "(x, bins=None, range=None, density=False, weights=None, cumulative=False, bottom=None, histtype='bar', align='mid', orientation='vertical', rwidth=None, log=False, color=None, label=None, stacked=False, *, data=None, **kwargs)", "description": "Compute and plot a histogram.\n\nThis method uses `numpy.histogram` to bin the data in *x* and count the\nnumber of values in each bin, then draws the distribution either as a\n`.BarContainer` or `.Polygon`. The *bins*, *range*, *density*, and\n*weights* parameters are forwarded to `numpy.histogram`.\n\nIf the data has already been binned and counted, use `~.bar` or\n`~.stairs` to plot the distribution::\n\n    counts, bins = np.histogram(x)\n    plt.stairs(counts, bins)\n\nAlternatively, plot pre-computed bins and counts using ``hist()`` by\ntreating each bin as a single point with a weight equal to its count::\n\n    plt.hist(bins[:-1], bins, weights=counts)\n\nThe data input *x* can be a singular array, a list of datasets of\npotentially different lengths ([*x0*, *x1*, ...]), or a 2D ndarray in\nwhich each column is a dataset. Note that the ndarray form is\ntransposed relative to the list form. If the input is an array, then\nthe return value is a tuple (*n*, *bins*, *patches*); if the input is a\nsequence of arrays, then the return value is a tuple\n([*n0*, *n1*, ...], *bins*, [*patches0*, *patches1*, ...]).\n\nMasked arrays are not supported.\n\nParameters\n----------\nx : (n,) array or sequence of (n,) arrays\n    Input values, this takes either a single array or a sequence of\n    arrays which are not required to be of the same length.\n\nbins : int or sequence or str, default: :rc:`hist.bins`\n    If *bins* is an integer, it defines the number of equal-width bins\n    in the range.\n\n    If *bins* is a sequence, it defines the bin edges, including the\n    left edge of the first bin and the right edge of the last bin;\n    in this case, bins may be unequally spaced.  All but the last\n    (righthand-most) bin is half-open.  In other words, if *bins* is::\n\n        [1, 2, 3, 4]\n\n    then the first bin is ``[1, 2)`` (including 1, but excluding 2) and\n    the second ``[2, 3)``.  The last bin, however, is ``[3, 4]``, which\n    *includes* 4.\n\n    If *bins* is a string, it is one of the binning strategies\n    supported by `numpy.histogram_bin_edges`: 'auto', 'fd', 'doane',\n    'scott', 'stone', 'rice', 'sturges', or 'sqrt'.\n\nrange : tuple or None, default: None\n    The lower and upper range of the bins. Lower and upper outliers\n    are ignored. If not provided, *range* is ``(x.min(), x.max())``.\n    Range has no effect if *bins* is a sequence.\n\n    If *bins* is a sequence or *range* is specified, autoscaling\n    is based on the specified bin range instead of the\n    range of x.\n\ndensity : bool, default: False\n    If ``True``, draw and return a probability density: each bin\n    will display the bin's raw count divided by the total number of\n    counts *and the bin width*\n    (``density = counts / (sum(counts) * np.diff(bins))``),\n    so that the area under the histogram integrates to 1\n    (``np.sum(density * np.diff(bins)) == 1``).\n\n    If *stacked* is also ``True``, the sum of the histograms is\n    normalized to 1.\n\nweights : (n,) array-like or None, default: None\n    An array of weights, of the same shape as *x*.  Each value in\n    *x* only contributes its associated weight towards the bin count\n    (instead of 1).  If *density* is ``True``, the weights are\n    normalized, so that the integral of the density over the range\n    remains 1.\n\ncumulative : bool or -1, default: False\n    If ``True``, then a histogram is computed where each bin gives the\n    counts in that bin plus all bins for smaller values. The last bin\n    gives the total number of datapoints.\n\n    If *density* is also ``True`` then the histogram is normalized such\n    that the last bin equals 1.\n\n    If *cumulative* is a number less than 0 (e.g., -1), the direction\n    of accumulation is reversed.  In this case, if *density* is also\n    ``True``, then the histogram is normalized such that the first bin\n    equals 1.\n\nbottom : array-like, scalar, or None, default: None\n    Location of the bottom of each bin, i.e. bins are drawn from\n    ``bottom`` to ``bottom + hist(x, bins)`` If a scalar, the bottom\n    of each bin is shifted by the same amount. If an array, each bin\n    is shifted independently and the length of bottom must match the\n    number of bins. If None, defaults to 0.\n\nhisttype : {'bar', 'barstacked', 'step', 'stepfilled'}, default: 'bar'\n    The type of histogram to draw.\n\n    - 'bar' is a traditional bar-type histogram.  If multiple data\n      are given the bars are arranged side by side.\n    - 'barstacked' is a bar-type histogram where multiple\n      data are stacked on top of each other.\n    - 'step' generates a lineplot that is by default unfilled.\n    - 'stepfilled' generates a lineplot that is by default filled.\n\nalign : {'left', 'mid', 'right'}, default: 'mid'\n    The horizontal alignment of the histogram bars.\n\n    - 'left': bars are centered on the left bin edges.\n    - 'mid': bars are centered between the bin edges.\n    - 'right': bars are centered on the right bin edges.\n\norientation : {'vertical', 'horizontal'}, default: 'vertical'\n    If 'horizontal', `~.Axes.barh` will be used for bar-type histograms\n    and the *bottom* kwarg will be the left edges.\n\nrwidth : float or None, default: None\n    The relative width of the bars as a fraction of the bin width.  If\n    ``None``, automatically compute the width.\n\n    Ignored if *histtype* is 'step' or 'stepfilled'.\n\nlog : bool, default: False\n    If ``True``, the histogram axis will be set to a log scale.\n\ncolor : color or array-like of colors or None, default: None\n    Color or sequence of colors, one per dataset.  Default (``None``)\n    uses the standard line color sequence.\n\nlabel : str or None, default: None\n    String, or sequence of strings to match multiple datasets.  Bar\n    charts yield multiple patches per dataset, but only the first gets\n    the label, so that `~.Axes.legend` will work as expected.\n\nstacked : bool, default: False\n    If ``True``, multiple data are stacked on top of each other If\n    ``False`` multiple data are arranged side by side if histtype is\n    'bar' or on top of each other if histtype is 'step'\n\nReturns\n-------\nn : array or list of arrays\n    The values of the histogram bins. See *density* and *weights* for a\n    description of the possible semantics.  If input *x* is an array,\n    then this is an array of length *nbins*. If input is a sequence of\n    arrays ``[data1, data2, ...]``, then this is a list of arrays with\n    the values of the histograms for each of the arrays in the same\n    order.  The dtype of the array *n* (or of its element arrays) will\n    always be float even if no weighting or normalization is used.\n\nbins : array\n    The edges of the bins. Length nbins + 1 (nbins left edges and right\n    edge of last bin).  Always a single array even when multiple data\n    sets are passed in.\n\npatches : `.BarContainer` or list of a single `.Polygon` or list of such objects\n    Container of individual artists used to create the histogram\n    or list of such containers if there are multiple input datasets.\n\nOther Parameters\n----------------\ndata : indexable object, optional\n    If given, the following parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` (unless this raises an exception):\n\n    *x*, *weights*\n\n**kwargs\n    `~matplotlib.patches.Patch` properties\n\nSee Also\n--------\nhist2d : 2D histogram with rectangular bins\nhexbin : 2D histogram with hexagonal bins\nstairs : Plot a pre-computed histogram\nbar : Plot a pre-computed histogram\n\nNotes\n-----\nFor large numbers of bins (>1000), plotting can be significantly\naccelerated by using `~.Axes.stairs` to plot a pre-computed histogram\n(``plt.stairs(*np.histogram(data))``), or by setting *histtype* to\n'step' or 'stepfilled' rather than 'bar' or 'barstacked'.", "parameters": {"type": "object", "properties": {"x": {}, "bins": {"type": "NoneType", "default": null}, "range": {"type": "NoneType", "default": null}, "density": {"type": "bool", "default": false}, "weights": {"type": "NoneType", "default": null}, "cumulative": {"type": "bool", "default": false}, "bottom": {"type": "NoneType", "default": null}, "histtype": {"type": "str", "default": "bar"}, "align": {"type": "str", "default": "mid"}, "orientation": {"type": "str", "default": "vertical"}, "rwidth": {"type": "NoneType", "default": null}, "log": {"type": "bool", "default": false}, "color": {"type": "NoneType", "default": null}, "label": {"type": "NoneType", "default": null}, "stacked": {"type": "bool", "default": false}, "data": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/162", "data": {"name": "numpy.arange", "type": "callable", "signature": "(*args, **kwargs)", "description": "arange([start,] stop[, step,], dtype=None, *, like=None)\n\nReturn evenly spaced values within a given interval.\n\n``arange`` can be called with a varying number of positional arguments:\n\n* ``arange(stop)``: Values are generated within the half-open interval\n  ``[0, stop)`` (in other words, the interval including `start` but\n  excluding `stop`).\n* ``arange(start, stop)``: Values are generated within the half-open\n  interval ``[start, stop)``.\n* ``arange(start, stop, step)`` Values are generated within the half-open\n  interval ``[start, stop)``, with spacing between values given by\n  ``step``.\n\nFor integer arguments the function is roughly equivalent to the Python\nbuilt-in :py:class:`range`, but returns an ndarray rather than a ``range``\ninstance.\n\nWhen using a non-integer step, such as 0.1, it is often better to use\n`numpy.linspace`.\n\nSee the Warning sections below for more information.\n\nParameters\n----------\nstart : integer or real, optional\n    Start of interval.  The interval includes this value.  The default\n    start value is 0.\nstop : integer or real\n    End of interval.  The interval does not include this value, except\n    in some cases where `step` is not an integer and floating point\n    round-off affects the length of `out`.\nstep : integer or real, optional\n    Spacing between values.  For any output `out`, this is the distance\n    between two adjacent values, ``out[i+1] - out[i]``.  The default\n    step size is 1.  If `step` is specified as a position argument,\n    `start` must also be given.\ndtype : dtype, optional\n    The type of the output array.  If `dtype` is not given, infer the data\n    type from the other input arguments.\nlike : array_like, optional\n    Reference object to allow the creation of arrays which are not\n    NumPy arrays. If an array-like passed in as ``like`` supports\n    the ``__array_function__`` protocol, the result will be defined\n    by it. In this case, it ensures the creation of an array object\n    compatible with that passed in via this argument.\n\n    .. versionadded:: 1.20.0\n\nReturns\n-------\narange : ndarray\n    Array of evenly spaced values.\n\n    For floating point arguments, the length of the result is\n    ``ceil((stop - start)/step)``.  Because of floating point overflow,\n    this rule may result in the last element of `out` being greater\n    than `stop`.\n\nWarnings\n--------\nThe length of the output might not be numerically stable.\n\nAnother stability issue is due to the internal implementation of\n`numpy.arange`.\nThe actual step value used to populate the array is\n``dtype(start + step) - dtype(start)`` and not `step`. Precision loss\ncan occur here, due to casting or due to using floating points when\n`start` is much larger than `step`. This can lead to unexpected\nbehaviour. For example::\n\n  >>> np.arange(0, 5, 0.5, dtype=int)\n  array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n  >>> np.arange(-3, 3, 0.5, dtype=int)\n  array([-3, -2, -1,  0,  1,  2,  3,  4,  5,  6,  7,  8])\n\nIn such cases, the use of `numpy.linspace` should be preferred.\n\nThe built-in :py:class:`range` generates :std:doc:`Python built-in integers\nthat have arbitrary size <c-api/long>`, while `numpy.arange` produces\n`numpy.int32` or `numpy.int64` numbers. This may result in incorrect\nresults for large integer values::\n\n  >>> power = 40\n  >>> modulo = 10000\n  >>> x1 = [(n ** power) % modulo for n in range(8)]\n  >>> x2 = [(n ** power) % modulo for n in np.arange(8)]\n  >>> print(x1)\n  [0, 1, 7776, 8801, 6176, 625, 6576, 4001]  # correct\n  >>> print(x2)\n  [0, 1, 7776, 7185, 0, 5969, 4816, 3361]  # incorrect\n\nSee Also\n--------\nnumpy.linspace : Evenly spaced numbers with careful handling of endpoints.\nnumpy.ogrid: Arrays of evenly spaced numbers in N-dimensions.\nnumpy.mgrid: Grid-shaped arrays of evenly spaced numbers in N-dimensions.\n\nExamples\n--------\n>>> np.arange(3)\narray([0, 1, 2])\n>>> np.arange(3.0)\narray([ 0.,  1.,  2.])\n>>> np.arange(3,7)\narray([3, 4, 5, 6])\n>>> np.arange(3,7,2)\narray([3, 5])", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/162", "data": {"name": "re.split", "type": "callable", "signature": "(pattern, string)", "description": "Split the source string by the occurrences of the pattern,\nreturning a list containing the resulting substrings.  If\ncapturing parentheses are used in pattern, then the text of all\ngroups in the pattern are also returned as part of the resulting\nlist.  If maxsplit is nonzero, at most maxsplit splits occur,\nand the remainder of the string is returned as the final element\nof the list.", "parameters": {"type": "object", "properties": {"pattern": {}, "string": {}}}}}
{"task_id": "BigCodeBench/177", "data": {"name": "df.columns"}}
{"task_id": "BigCodeBench/177", "data": {"name": "interesting_articles.empty"}}
{"task_id": "BigCodeBench/177", "data": {"name": "nltk.word_tokenize", "type": "callable", "signature": "(text)", "description": "Return a tokenized copy of *text*,\nusing NLTK's recommended word tokenizer\n(currently an improved :class:`.TreebankWordTokenizer`\nalong with :class:`.PunktSentenceTokenizer`\nfor the specified language).\n\n:param text: text to split into words\n:type text: str\n:param language: the model name in the Punkt corpus\n:type language: str\n:param preserve_line: A flag to decide whether to sentence tokenize the text or not.\n:type preserve_line: bool", "parameters": {"type": "object", "properties": {"text": {}}}}}
{"task_id": "BigCodeBench/177", "data": {"name": "re.IGNORECASE", "type": "constant", "signature": null, "description": "An enumeration.", "value": "re.IGNORECASE", "parameters": {"type": "constant"}}}
{"task_id": "BigCodeBench/177", "data": {"name": "re.compile", "type": "callable", "signature": "(pattern, flags=0)", "description": "Compile a regular expression pattern, returning a Pattern object.", "parameters": {"type": "object", "properties": {"pattern": {}, "flags": {"type": "integer", "default": 0}}}}}
{"task_id": "BigCodeBench/177", "data": {"name": "re.compile.search", "type": "callable", "signature": "(pattern)", "description": "Compile a regular expression pattern, returning a Pattern object.", "parameters": {"type": "object", "properties": {"pattern": {}}}}}
{"task_id": "BigCodeBench/184", "data": {"name": "' '.join((word for word in text.split() if word not in STOPWORDS)).split()"}}
{"task_id": "BigCodeBench/184", "data": {"name": "re.sub", "type": "callable", "signature": "(pattern, repl, string)", "description": "Return the string obtained by replacing the leftmost\nnon-overlapping occurrences of the pattern in string by the\nreplacement repl.  repl can be either a string or a callable;\nif a string, backslash escapes in it are processed.  If it is\na callable, it's passed the Match object and must return\na replacement string to be used.", "parameters": {"type": "object", "properties": {"pattern": {}, "repl": {}, "string": {}}}}}
{"task_id": "BigCodeBench/184", "data": {"name": "sklearn.feature_extraction.text.CountVectorizer", "type": "class", "signature": "()", "description": "Convert a collection of text documents to a matrix of token counts.\n\nThis implementation produces a sparse representation of the counts using\nscipy.sparse.csr_matrix.\n\nIf you do not provide an a-priori dictionary and you do not use an analyzer\nthat does some kind of feature selection then the number of features will\nbe equal to the vocabulary size found by analyzing the data.\n\nFor an efficiency comparison of the different feature extractors, see\n:ref:`sphx_glr_auto_examples_text_plot_hashing_vs_dict_vectorizer.py`.\n\nRead more in the :ref:`User Guide <text_feature_extraction>`.\n\nParameters\n----------\ninput : {'filename', 'file', 'content'}, default='content'\n    - If `'filename'`, the sequence passed as an argument to fit is\n      expected to be a list of filenames that need reading to fetch\n      the raw content to analyze.\n\n    - If `'file'`, the sequence items must have a 'read' method (file-like\n      object) that is called to fetch the bytes in memory.\n\n    - If `'content'`, the input is expected to be a sequence of items that\n      can be of type string or byte.\n\nencoding : str, default='utf-8'\n    If bytes or files are given to analyze, this encoding is used to\n    decode.\n\ndecode_error : {'strict', 'ignore', 'replace'}, default='strict'\n    Instruction on what to do if a byte sequence is given to analyze that\n    contains characters not of the given `encoding`. By default, it is\n    'strict', meaning that a UnicodeDecodeError will be raised. Other\n    values are 'ignore' and 'replace'.\n\nstrip_accents : {'ascii', 'unicode'} or callable, default=None\n    Remove accents and perform other character normalization\n    during the preprocessing step.\n    'ascii' is a fast method that only works on characters that have\n    a direct ASCII mapping.\n    'unicode' is a slightly slower method that works on any characters.\n    None (default) means no character normalization is performed.\n\n    Both 'ascii' and 'unicode' use NFKD normalization from\n    :func:`unicodedata.normalize`.\n\nlowercase : bool, default=True\n    Convert all characters to lowercase before tokenizing.\n\npreprocessor : callable, default=None\n    Override the preprocessing (strip_accents and lowercase) stage while\n    preserving the tokenizing and n-grams generation steps.\n    Only applies if ``analyzer`` is not callable.\n\ntokenizer : callable, default=None\n    Override the string tokenization step while preserving the\n    preprocessing and n-grams generation steps.\n    Only applies if ``analyzer == 'word'``.\n\nstop_words : {'english'}, list, default=None\n    If 'english', a built-in stop word list for English is used.\n    There are several known issues with 'english' and you should\n    consider an alternative (see :ref:`stop_words`).\n\n    If a list, that list is assumed to contain stop words, all of which\n    will be removed from the resulting tokens.\n    Only applies if ``analyzer == 'word'``.\n\n    If None, no stop words will be used. In this case, setting `max_df`\n    to a higher value, such as in the range (0.7, 1.0), can automatically detect\n    and filter stop words based on intra corpus document frequency of terms.\n\ntoken_pattern : str or None, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n    Regular expression denoting what constitutes a \"token\", only used\n    if ``analyzer == 'word'``. The default regexp select tokens of 2\n    or more alphanumeric characters (punctuation is completely ignored\n    and always treated as a token separator).\n\n    If there is a capturing group in token_pattern then the\n    captured group content, not the entire match, becomes the token.\n    At most one capturing group is permitted.\n\nngram_range : tuple (min_n, max_n), default=(1, 1)\n    The lower and upper boundary of the range of n-values for different\n    word n-grams or char n-grams to be extracted. All values of n such\n    such that min_n <= n <= max_n will be used. For example an\n    ``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means\n    unigrams and bigrams, and ``(2, 2)`` means only bigrams.\n    Only applies if ``analyzer`` is not callable.\n\nanalyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n    Whether the feature should be made of word n-gram or character\n    n-grams.\n    Option 'char_wb' creates character n-grams only from text inside\n    word boundaries; n-grams at the edges of words are padded with space.\n\n    If a callable is passed it is used to extract the sequence of features\n    out of the raw, unprocessed input.\n\n    .. versionchanged:: 0.21\n\n    Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n    first read from the file and then passed to the given callable\n    analyzer.\n\nmax_df : float in range [0.0, 1.0] or int, default=1.0\n    When building the vocabulary ignore terms that have a document\n    frequency strictly higher than the given threshold (corpus-specific\n    stop words).\n    If float, the parameter represents a proportion of documents, integer\n    absolute counts.\n    This parameter is ignored if vocabulary is not None.\n\nmin_df : float in range [0.0, 1.0] or int, default=1\n    When building the vocabulary ignore terms that have a document\n    frequency strictly lower than the given threshold. This value is also\n    called cut-off in the literature.\n    If float, the parameter represents a proportion of documents, integer\n    absolute counts.\n    This parameter is ignored if vocabulary is not None.\n\nmax_features : int, default=None\n    If not None, build a vocabulary that only consider the top\n    `max_features` ordered by term frequency across the corpus.\n    Otherwise, all features are used.\n\n    This parameter is ignored if vocabulary is not None.\n\nvocabulary : Mapping or iterable, default=None\n    Either a Mapping (e.g., a dict) where keys are terms and values are\n    indices in the feature matrix, or an iterable over terms. If not\n    given, a vocabulary is determined from the input documents. Indices\n    in the mapping should not be repeated and should not have any gap\n    between 0 and the largest index.\n\nbinary : bool, default=False\n    If True, all non zero counts are set to 1. This is useful for discrete\n    probabilistic models that model binary events rather than integer\n    counts.\n\ndtype : dtype, default=np.int64\n    Type of the matrix returned by fit_transform() or transform().\n\nAttributes\n----------\nvocabulary_ : dict\n    A mapping of terms to feature indices.\n\nfixed_vocabulary_ : bool\n    True if a fixed vocabulary of term to indices mapping\n    is provided by the user.\n\nstop_words_ : set\n    Terms that were ignored because they either:\n\n      - occurred in too many documents (`max_df`)\n      - occurred in too few documents (`min_df`)\n      - were cut off by feature selection (`max_features`).\n\n    This is only available if no vocabulary was given.\n\nSee Also\n--------\nHashingVectorizer : Convert a collection of text documents to a\n    matrix of token counts.\n\nTfidfVectorizer : Convert a collection of raw documents to a matrix\n    of TF-IDF features.\n\nNotes\n-----\nThe ``stop_words_`` attribute can get large and increase the model size\nwhen pickling. This attribute is provided only for introspection and can\nbe safely removed using delattr or set to None before pickling.\n\nExamples\n--------\n>>> from sklearn.feature_extraction.text import CountVectorizer\n>>> corpus = [\n...     'This is the first document.',\n...     'This document is the second document.',\n...     'And this is the third one.',\n...     'Is this the first document?',\n... ]\n>>> vectorizer = CountVectorizer()\n>>> X = vectorizer.fit_transform(corpus)\n>>> vectorizer.get_feature_names_out()\narray(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n       'this'], ...)\n>>> print(X.toarray())\n[[0 1 1 1 0 0 1 0 1]\n [0 2 0 1 0 1 1 0 1]\n [1 0 0 1 1 0 1 1 1]\n [0 1 1 1 0 0 1 0 1]]\n>>> vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n>>> X2 = vectorizer2.fit_transform(corpus)\n>>> vectorizer2.get_feature_names_out()\narray(['and this', 'document is', 'first document', 'is the', 'is this',\n       'second document', 'the first', 'the second', 'the third', 'third one',\n       'this document', 'this is', 'this the'], ...)\n >>> print(X2.toarray())\n [[0 0 1 1 0 0 1 0 0 0 0 1 0]\n [0 1 0 1 0 1 0 1 0 0 1 0 0]\n [1 0 0 1 0 0 0 0 1 1 0 1 0]\n [0 0 1 0 1 0 1 0 0 0 0 0 1]]", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/184", "data": {"name": "sklearn.feature_extraction.text.CountVectorizer.get_feature_names_out", "type": "callable", "signature": "(self)", "description": "Get output feature names for transformation.\n\nParameters\n----------\ninput_features : array-like of str or None, default=None\n    Not used, present here for API consistency by convention.\n\nReturns\n-------\nfeature_names_out : ndarray of str objects\n    Transformed feature names.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/184", "data": {"name": "sklearn.feature_extraction.text.CountVectorizer.fit_transform", "type": "callable", "signature": "(self, raw_documents)", "description": "Learn the vocabulary dictionary and return document-term matrix.\n\nThis is equivalent to fit followed by transform, but more efficiently\nimplemented.\n\nParameters\n----------\nraw_documents : iterable\n    An iterable which generates either str, unicode or file objects.\n\ny : None\n    This parameter is ignored.\n\nReturns\n-------\nX : array of shape (n_samples, n_features)\n    Document-term matrix.", "parameters": {"type": "object", "properties": {"raw_documents": {}}}}}
{"task_id": "BigCodeBench/184", "data": {"name": "text.lower().lower()"}}
{"task_id": "BigCodeBench/184", "data": {"name": "vectorizer.fit_transform(dataframe[text_column]).toarray()"}}
{"task_id": "BigCodeBench/187", "data": {"name": "numpy.random.uniform", "type": "callable", "signature": "(*args, **kwargs)", "description": "uniform(low=0.0, high=1.0, size=None)\n\nDraw samples from a uniform distribution.\n\nSamples are uniformly distributed over the half-open interval\n``[low, high)`` (includes low, but excludes high).  In other words,\nany value within the given interval is equally likely to be drawn\nby `uniform`.\n\n.. note::\n    New code should use the ``uniform`` method of a ``default_rng()``\n    instance instead; please see the :ref:`random-quick-start`.\n\nParameters\n----------\nlow : float or array_like of floats, optional\n    Lower boundary of the output interval.  All values generated will be\n    greater than or equal to low.  The default value is 0.\nhigh : float or array_like of floats\n    Upper boundary of the output interval.  All values generated will be\n    less than or equal to high.  The high limit may be included in the \n    returned array of floats due to floating-point rounding in the \n    equation ``low + (high-low) * random_sample()``.  The default value \n    is 1.0.\nsize : int or tuple of ints, optional\n    Output shape.  If the given shape is, e.g., ``(m, n, k)``, then\n    ``m * n * k`` samples are drawn.  If size is ``None`` (default),\n    a single value is returned if ``low`` and ``high`` are both scalars.\n    Otherwise, ``np.broadcast(low, high).size`` samples are drawn.\n\nReturns\n-------\nout : ndarray or scalar\n    Drawn samples from the parameterized uniform distribution.\n\nSee Also\n--------\nrandint : Discrete uniform distribution, yielding integers.\nrandom_integers : Discrete uniform distribution over the closed\n                  interval ``[low, high]``.\nrandom_sample : Floats uniformly distributed over ``[0, 1)``.\nrandom : Alias for `random_sample`.\nrand : Convenience function that accepts dimensions as input, e.g.,\n       ``rand(2,2)`` would generate a 2-by-2 array of floats,\n       uniformly distributed over ``[0, 1)``.\nrandom.Generator.uniform: which should be used for new code.\n\nNotes\n-----\nThe probability density function of the uniform distribution is\n\n.. math:: p(x) = \\frac{1}{b - a}\n\nanywhere within the interval ``[a, b)``, and zero elsewhere.\n\nWhen ``high`` == ``low``, values of ``low`` will be returned.\nIf ``high`` < ``low``, the results are officially undefined\nand may eventually raise an error, i.e. do not rely on this\nfunction to behave when passed arguments satisfying that\ninequality condition. The ``high`` limit may be included in the\nreturned array of floats due to floating-point rounding in the\nequation ``low + (high-low) * random_sample()``. For example:\n\n>>> x = np.float32(5*0.99999999)\n>>> x\n5.0\n\n\nExamples\n--------\nDraw samples from the distribution:\n\n>>> s = np.random.uniform(-1,0,1000)\n\nAll values are within the given interval:\n\n>>> np.all(s >= -1)\nTrue\n>>> np.all(s < 0)\nTrue\n\nDisplay the histogram of the samples, along with the\nprobability density function:\n\n>>> import matplotlib.pyplot as plt\n>>> count, bins, ignored = plt.hist(s, 15, density=True)\n>>> plt.plot(bins, np.ones_like(bins), linewidth=2, color='r')\n>>> plt.show()", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/187", "data": {"name": "shapely.geometry.Point", "type": "class", "signature": "(*args)", "description": "A geometry type that represents a single coordinate with\nx,y and possibly z values.\n\nA point is a zero-dimensional feature and has zero length and zero area.\n\nParameters\n----------\nargs : float, or sequence of floats\n    The coordinates can either be passed as a single parameter, or as\n    individual float values using multiple parameters:\n\n    1) 1 parameter: a sequence or array-like of with 2 or 3 values.\n    2) 2 or 3 parameters (float): x, y, and possibly z.\n\nAttributes\n----------\nx, y, z : float\n    Coordinate values\n\nExamples\n--------\nConstructing the Point using separate parameters for x and y:\n\n>>> p = Point(1.0, -1.0)\n\nConstructing the Point using a list of x, y coordinates:\n\n>>> p = Point([1.0, -1.0])\n>>> print(p)\nPOINT (1 -1)\n>>> p.y\n-1.0\n>>> p.x\n1.0", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/199", "data": {"name": "pytz.timezone", "type": "callable", "signature": "(zone)", "description": "Return a datetime.tzinfo implementation for the given timezone\n\n>>> from datetime import datetime, timedelta\n>>> utc = timezone('UTC')\n>>> eastern = timezone('US/Eastern')\n>>> eastern.zone\n'US/Eastern'\n>>> timezone(unicode('US/Eastern')) is eastern\nTrue\n>>> utc_dt = datetime(2002, 10, 27, 6, 0, 0, tzinfo=utc)\n>>> loc_dt = utc_dt.astimezone(eastern)\n>>> fmt = '%Y-%m-%d %H:%M:%S %Z (%z)'\n>>> loc_dt.strftime(fmt)\n'2002-10-27 01:00:00 EST (-0500)'\n>>> (loc_dt - timedelta(minutes=10)).strftime(fmt)\n'2002-10-27 00:50:00 EST (-0500)'\n>>> eastern.normalize(loc_dt - timedelta(minutes=10)).strftime(fmt)\n'2002-10-27 01:50:00 EDT (-0400)'\n>>> (loc_dt + timedelta(minutes=10)).strftime(fmt)\n'2002-10-27 01:10:00 EST (-0500)'\n\nRaises UnknownTimeZoneError if passed an unknown zone.\n\n>>> try:\n...     timezone('Asia/Shangri-La')\n... except UnknownTimeZoneError:\n...     print('Unknown')\nUnknown\n\n>>> try:\n...     timezone(unicode('\\N{TRADE MARK SIGN}'))\n... except UnknownTimeZoneError:\n...     print('Unknown')\nUnknown", "parameters": {"type": "object", "properties": {"zone": {}}}}}
{"task_id": "BigCodeBench/199", "data": {"name": "random.seed", "type": "callable", "signature": "(a=None)", "description": "Initialize internal state from a seed.\n\nThe only supported seed types are None, int, float,\nstr, bytes, and bytearray.\n\nNone or no argument seeds from current time or from an operating\nsystem specific randomness source if available.\n\nIf *a* is an int, all bits are used.\n\nFor version 2 (the default), all of the bits are used if *a* is a str,\nbytes, or bytearray.  For version 1 (provided for reproducing random\nsequences from older versions of Python), the algorithm for str and\nbytes generates a narrower range of seeds.", "parameters": {"type": "object", "properties": {"a": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/199", "data": {"name": "random.randint", "type": "callable", "signature": "(a, b)", "description": "Return random integer in range [a, b], including both end points.\n        ", "parameters": {"type": "object", "properties": {"a": {}, "b": {}}}}}
{"task_id": "BigCodeBench/199", "data": {"name": "report_data.append([city, city_time.strftime('%Y-%m-%d %H:%M:%S %Z'), weather])"}}
{"task_id": "BigCodeBench/199", "data": {"name": "utc_datetime.astimezone(city_tz)"}}
{"task_id": "BigCodeBench/199", "data": {"name": "utc_datetime.astimezone.strftime('%Y-%m-%d %H:%M:%S %Z')"}}
{"task_id": "BigCodeBench/208", "data": {"name": "matplotlib.pyplot.gca", "type": "callable", "signature": "()", "description": "Get the current Axes.\n\nIf there is currently no Axes on this Figure, a new one is created\nusing `.Figure.add_subplot`.  (To test whether there is currently an\nAxes on a Figure, check whether ``figure.axes`` is empty.  To test\nwhether there is currently a Figure on the pyplot figure stack, check\nwhether `.pyplot.get_fignums()` is empty.)", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/208", "data": {"name": "matplotlib.pyplot.plot", "type": "callable", "signature": "(*args: 'float | ArrayLike | str')", "description": "Plot y versus x as lines and/or markers.\n\nCall signatures::\n\n    plot([x], y, [fmt], *, data=None, **kwargs)\n    plot([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n\nThe coordinates of the points or line nodes are given by *x*, *y*.\n\nThe optional parameter *fmt* is a convenient way for defining basic\nformatting like color, marker and linestyle. It's a shortcut string\nnotation described in the *Notes* section below.\n\n>>> plot(x, y)        # plot x and y using default line style and color\n>>> plot(x, y, 'bo')  # plot x and y using blue circle markers\n>>> plot(y)           # plot y using x as index array 0..N-1\n>>> plot(y, 'r+')     # ditto, but with red plusses\n\nYou can use `.Line2D` properties as keyword arguments for more\ncontrol on the appearance. Line properties and *fmt* can be mixed.\nThe following two calls yield identical results:\n\n>>> plot(x, y, 'go--', linewidth=2, markersize=12)\n>>> plot(x, y, color='green', marker='o', linestyle='dashed',\n...      linewidth=2, markersize=12)\n\nWhen conflicting with *fmt*, keyword arguments take precedence.\n\n\n**Plotting labelled data**\n\nThere's a convenient way for plotting objects with labelled data (i.e.\ndata that can be accessed by index ``obj['y']``). Instead of giving\nthe data in *x* and *y*, you can provide the object in the *data*\nparameter and just give the labels for *x* and *y*::\n\n>>> plot('xlabel', 'ylabel', data=obj)\n\nAll indexable objects are supported. This could e.g. be a `dict`, a\n`pandas.DataFrame` or a structured numpy array.\n\n\n**Plotting multiple sets of data**\n\nThere are various ways to plot multiple sets of data.\n\n- The most straight forward way is just to call `plot` multiple times.\n  Example:\n\n  >>> plot(x1, y1, 'bo')\n  >>> plot(x2, y2, 'go')\n\n- If *x* and/or *y* are 2D arrays a separate data set will be drawn\n  for every column. If both *x* and *y* are 2D, they must have the\n  same shape. If only one of them is 2D with shape (N, m) the other\n  must have length N and will be used for every data set m.\n\n  Example:\n\n  >>> x = [1, 2, 3]\n  >>> y = np.array([[1, 2], [3, 4], [5, 6]])\n  >>> plot(x, y)\n\n  is equivalent to:\n\n  >>> for col in range(y.shape[1]):\n  ...     plot(x, y[:, col])\n\n- The third way is to specify multiple sets of *[x]*, *y*, *[fmt]*\n  groups::\n\n  >>> plot(x1, y1, 'g^', x2, y2, 'g-')\n\n  In this case, any additional keyword argument applies to all\n  datasets. Also, this syntax cannot be combined with the *data*\n  parameter.\n\nBy default, each line is assigned a different style specified by a\n'style cycle'. The *fmt* and line property parameters are only\nnecessary if you want explicit deviations from these defaults.\nAlternatively, you can also change the style cycle using\n:rc:`axes.prop_cycle`.\n\n\nParameters\n----------\nx, y : array-like or scalar\n    The horizontal / vertical coordinates of the data points.\n    *x* values are optional and default to ``range(len(y))``.\n\n    Commonly, these parameters are 1D arrays.\n\n    They can also be scalars, or two-dimensional (in that case, the\n    columns represent separate data sets).\n\n    These arguments cannot be passed as keywords.\n\nfmt : str, optional\n    A format string, e.g. 'ro' for red circles. See the *Notes*\n    section for a full description of the format strings.\n\n    Format strings are just an abbreviation for quickly setting\n    basic line properties. All of these and more can also be\n    controlled by keyword arguments.\n\n    This argument cannot be passed as keyword.\n\ndata : indexable object, optional\n    An object with labelled data. If given, provide the label names to\n    plot in *x* and *y*.\n\n    .. note::\n        Technically there's a slight ambiguity in calls where the\n        second label is a valid *fmt*. ``plot('n', 'o', data=obj)``\n        could be ``plt(x, y)`` or ``plt(y, fmt)``. In such cases,\n        the former interpretation is chosen, but a warning is issued.\n        You may suppress the warning by adding an empty format string\n        ``plot('n', 'o', '', data=obj)``.\n\nReturns\n-------\nlist of `.Line2D`\n    A list of lines representing the plotted data.\n\nOther Parameters\n----------------\nscalex, scaley : bool, default: True\n    These parameters determine if the view limits are adapted to the\n    data limits. The values are passed on to\n    `~.axes.Axes.autoscale_view`.\n\n**kwargs : `~matplotlib.lines.Line2D` properties, optional\n    *kwargs* are used to specify properties like a line label (for\n    auto legends), linewidth, antialiasing, marker face color.\n    Example::\n\n    >>> plot([1, 2, 3], [1, 2, 3], 'go-', label='line 1', linewidth=2)\n    >>> plot([1, 2, 3], [1, 4, 9], 'rs', label='line 2')\n\n    If you specify multiple lines with one plot call, the kwargs apply\n    to all those lines. In case the label object is iterable, each\n    element is used as labels for each set of data.\n\n    Here is a list of available `.Line2D` properties:\n\n    Properties:\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: scalar or None\n    animated: bool\n    antialiased or aa: bool\n    clip_box: `~matplotlib.transforms.BboxBase` or None\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    color or c: color\n    dash_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    dash_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    dashes: sequence of floats (on/off ink in points) or (None, None)\n    data: (2, N) array or two 1D arrays\n    drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n    figure: `~matplotlib.figure.Figure`\n    fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n    gapcolor: color or None\n    gid: str\n    in_layout: bool\n    label: object\n    linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n    linewidth or lw: float\n    marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n    markeredgecolor or mec: color\n    markeredgewidth or mew: float\n    markerfacecolor or mfc: color\n    markerfacecoloralt or mfcalt: color\n    markersize or ms: float\n    markevery: None or int or (int, int) or slice or list[int] or float or (float, float) or list[bool]\n    mouseover: bool\n    path_effects: list of `.AbstractPathEffect`\n    picker: float or callable[[Artist, Event], tuple[bool, dict]]\n    pickradius: float\n    rasterized: bool\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    solid_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    solid_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    transform: unknown\n    url: str\n    visible: bool\n    xdata: 1D array\n    ydata: 1D array\n    zorder: float\n\nSee Also\n--------\nscatter : XY scatter plot with markers of varying size and/or color (\n    sometimes also called bubble chart).\n\nNotes\n-----\n**Format Strings**\n\nA format string consists of a part for color, marker and line::\n\n    fmt = '[marker][line][color]'\n\nEach of them is optional. If not provided, the value from the style\ncycle is used. Exception: If ``line`` is given, but no ``marker``,\nthe data will be a line without markers.\n\nOther combinations such as ``[color][marker][line]`` are also\nsupported, but note that their parsing may be ambiguous.\n\n**Markers**\n\n=============   ===============================\ncharacter       description\n=============   ===============================\n``'.'``         point marker\n``','``         pixel marker\n``'o'``         circle marker\n``'v'``         triangle_down marker\n``'^'``         triangle_up marker\n``'<'``         triangle_left marker\n``'>'``         triangle_right marker\n``'1'``         tri_down marker\n``'2'``         tri_up marker\n``'3'``         tri_left marker\n``'4'``         tri_right marker\n``'8'``         octagon marker\n``'s'``         square marker\n``'p'``         pentagon marker\n``'P'``         plus (filled) marker\n``'*'``         star marker\n``'h'``         hexagon1 marker\n``'H'``         hexagon2 marker\n``'+'``         plus marker\n``'x'``         x marker\n``'X'``         x (filled) marker\n``'D'``         diamond marker\n``'d'``         thin_diamond marker\n``'|'``         vline marker\n``'_'``         hline marker\n=============   ===============================\n\n**Line Styles**\n\n=============    ===============================\ncharacter        description\n=============    ===============================\n``'-'``          solid line style\n``'--'``         dashed line style\n``'-.'``         dash-dot line style\n``':'``          dotted line style\n=============    ===============================\n\nExample format strings::\n\n    'b'    # blue markers with default shape\n    'or'   # red circles\n    '-g'   # green solid line\n    '--'   # dashed line with default color\n    '^k:'  # black triangle_up markers connected by a dotted line\n\n**Colors**\n\nThe supported color abbreviations are the single letter codes\n\n=============    ===============================\ncharacter        color\n=============    ===============================\n``'b'``          blue\n``'g'``          green\n``'r'``          red\n``'c'``          cyan\n``'m'``          magenta\n``'y'``          yellow\n``'k'``          black\n``'w'``          white\n=============    ===============================\n\nand the ``'CN'`` colors that index into the default property cycle.\n\nIf the color is the only part of the format string, you can\nadditionally use any  `matplotlib.colors` spec, e.g. full names\n(``'green'``) or hex strings (``'#008000'``).", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/208", "data": {"name": "matplotlib.pyplot.title", "type": "callable", "signature": "(label: 'str')", "description": "Set a title for the Axes.\n\nSet one of the three available Axes titles. The available titles\nare positioned above the Axes in the center, flush with the left\nedge, and flush with the right edge.\n\nParameters\n----------\nlabel : str\n    Text to use for the title\n\nfontdict : dict\n\n    .. admonition:: Discouraged\n\n       The use of *fontdict* is discouraged. Parameters should be passed as\n       individual keyword arguments or using dictionary-unpacking\n       ``set_title(..., **fontdict)``.\n\n    A dictionary controlling the appearance of the title text,\n    the default *fontdict* is::\n\n       {'fontsize': rcParams['axes.titlesize'],\n        'fontweight': rcParams['axes.titleweight'],\n        'color': rcParams['axes.titlecolor'],\n        'verticalalignment': 'baseline',\n        'horizontalalignment': loc}\n\nloc : {'center', 'left', 'right'}, default: :rc:`axes.titlelocation`\n    Which title to set.\n\ny : float, default: :rc:`axes.titley`\n    Vertical Axes location for the title (1.0 is the top).  If\n    None (the default) and :rc:`axes.titley` is also None, y is\n    determined automatically to avoid decorators on the Axes.\n\npad : float, default: :rc:`axes.titlepad`\n    The offset of the title from the top of the Axes, in points.\n\nReturns\n-------\n`.Text`\n    The matplotlib text instance representing the title\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    Other keyword arguments are text properties, see `.Text` for a list\n    of valid text properties.", "parameters": {"type": "object", "properties": {"label": {"type": "str"}}}}}
{"task_id": "BigCodeBench/208", "data": {"name": "matplotlib.pyplot.figure", "type": "callable", "signature": "(figsize: 'tuple[float, **kwargs) -> 'Figure)", "description": "Create a new figure, or activate an existing figure.\n\nParameters\n----------\nnum : int or str or `.Figure` or `.SubFigure`, optional\n    A unique identifier for the figure.\n\n    If a figure with that identifier already exists, this figure is made\n    active and returned. An integer refers to the ``Figure.number``\n    attribute, a string refers to the figure label.\n\n    If there is no figure with the identifier or *num* is not given, a new\n    figure is created, made active and returned.  If *num* is an int, it\n    will be used for the ``Figure.number`` attribute, otherwise, an\n    auto-generated integer value is used (starting at 1 and incremented\n    for each new figure). If *num* is a string, the figure label and the\n    window title is set to this value.  If num is a ``SubFigure``, its\n    parent ``Figure`` is activated.\n\nfigsize : (float, float), default: :rc:`figure.figsize`\n    Width, height in inches.\n\ndpi : float, default: :rc:`figure.dpi`\n    The resolution of the figure in dots-per-inch.\n\nfacecolor : color, default: :rc:`figure.facecolor`\n    The background color.\n\nedgecolor : color, default: :rc:`figure.edgecolor`\n    The border color.\n\nframeon : bool, default: True\n    If False, suppress drawing the figure frame.\n\nFigureClass : subclass of `~matplotlib.figure.Figure`\n    If set, an instance of this subclass will be created, rather than a\n    plain `.Figure`.\n\nclear : bool, default: False\n    If True and the figure already exists, then it is cleared.\n\nlayout : {'constrained', 'compressed', 'tight', 'none', `.LayoutEngine`, None}, default: None\n    The layout mechanism for positioning of plot elements to avoid\n    overlapping Axes decorations (labels, ticks, etc). Note that layout\n    managers can measurably slow down figure display.\n\n    - 'constrained': The constrained layout solver adjusts axes sizes\n      to avoid overlapping axes decorations.  Can handle complex plot\n      layouts and colorbars, and is thus recommended.\n\n      See :ref:`constrainedlayout_guide`\n      for examples.\n\n    - 'compressed': uses the same algorithm as 'constrained', but\n      removes extra space between fixed-aspect-ratio Axes.  Best for\n      simple grids of axes.\n\n    - 'tight': Use the tight layout mechanism. This is a relatively\n      simple algorithm that adjusts the subplot parameters so that\n      decorations do not overlap. See `.Figure.set_tight_layout` for\n      further details.\n\n    - 'none': Do not use a layout engine.\n\n    - A `.LayoutEngine` instance. Builtin layout classes are\n      `.ConstrainedLayoutEngine` and `.TightLayoutEngine`, more easily\n      accessible by 'constrained' and 'tight'.  Passing an instance\n      allows third parties to provide their own layout engine.\n\n    If not given, fall back to using the parameters *tight_layout* and\n    *constrained_layout*, including their config defaults\n    :rc:`figure.autolayout` and :rc:`figure.constrained_layout.use`.\n\n**kwargs\n    Additional keyword arguments are passed to the `.Figure` constructor.\n\nReturns\n-------\n`~matplotlib.figure.Figure`\n\nNotes\n-----\nA newly created figure is passed to the `~.FigureCanvasBase.new_manager`\nmethod or the `new_figure_manager` function provided by the current\nbackend, which install a canvas and a manager on the figure.\n\nOnce this is done, :rc:`figure.hooks` are called, one at a time, on the\nfigure; these hooks allow arbitrary customization of the figure (e.g.,\nattaching callbacks) or of associated elements (e.g., modifying the\ntoolbar).  See :doc:`/gallery/user_interfaces/mplcvd` for an example of\ntoolbar customization.\n\nIf you are creating many figures, make sure you explicitly call\n`.pyplot.close` on the figures you are not using, because this will\nenable pyplot to properly clean up the memory.\n\n`~matplotlib.rcParams` defines the default values, which can be modified\nin the matplotlibrc file.", "parameters": {"type": "object", "properties": {"figsize": {"type": "tuple[float"}}}}}
{"task_id": "BigCodeBench/208", "data": {"name": "numpy.cumsum", "type": "callable", "signature": "(a)", "description": "Return the cumulative sum of the elements along a given axis.\n\nParameters\n----------\na : array_like\n    Input array.\naxis : int, optional\n    Axis along which the cumulative sum is computed. The default\n    (None) is to compute the cumsum over the flattened array.\ndtype : dtype, optional\n    Type of the returned array and of the accumulator in which the\n    elements are summed.  If `dtype` is not specified, it defaults\n    to the dtype of `a`, unless `a` has an integer dtype with a\n    precision less than that of the default platform integer.  In\n    that case, the default platform integer is used.\nout : ndarray, optional\n    Alternative output array in which to place the result. It must\n    have the same shape and buffer length as the expected output\n    but the type will be cast if necessary. See :ref:`ufuncs-output-type` for\n    more details.\n\nReturns\n-------\ncumsum_along_axis : ndarray.\n    A new array holding the result is returned unless `out` is\n    specified, in which case a reference to `out` is returned. The\n    result has the same size as `a`, and the same shape as `a` if\n    `axis` is not None or `a` is a 1-d array.\n\nSee Also\n--------\nsum : Sum array elements.\ntrapz : Integration of array values using the composite trapezoidal rule.\ndiff : Calculate the n-th discrete difference along given axis.\n\nNotes\n-----\nArithmetic is modular when using integer types, and no error is\nraised on overflow.\n\n``cumsum(a)[-1]`` may not be equal to ``sum(a)`` for floating-point\nvalues since ``sum`` may use a pairwise summation routine, reducing\nthe roundoff-error. See `sum` for more information.\n\nExamples\n--------\n>>> a = np.array([[1,2,3], [4,5,6]])\n>>> a\narray([[1, 2, 3],\n       [4, 5, 6]])\n>>> np.cumsum(a)\narray([ 1,  3,  6, 10, 15, 21])\n>>> np.cumsum(a, dtype=float)     # specifies type of output value(s)\narray([  1.,   3.,   6.,  10.,  15.,  21.])\n\n>>> np.cumsum(a,axis=0)      # sum over rows for each of the 3 columns\narray([[1, 2, 3],\n       [5, 7, 9]])\n>>> np.cumsum(a,axis=1)      # sum over columns for each of the 2 rows\narray([[ 1,  3,  6],\n       [ 4,  9, 15]])\n\n``cumsum(b)[-1]`` may not be equal to ``sum(b)``\n\n>>> b = np.array([1, 2e-9, 3e-9] * 1000000)\n>>> b.cumsum()[-1]\n1000000.0050045159\n>>> b.sum()\n1000000.0050000029", "parameters": {"type": "object", "properties": {"a": {}}}}}
{"task_id": "BigCodeBench/208", "data": {"name": "numpy.random.seed", "type": "callable", "signature": "(*args, **kwargs)", "description": "seed(self, seed=None)\n\nReseed a legacy MT19937 BitGenerator\n\nNotes\n-----\nThis is a convenience, legacy function.\n\nThe best practice is to **not** reseed a BitGenerator, rather to\nrecreate a new one. This method is here for legacy reasons.\nThis example demonstrates best practice.\n\n>>> from numpy.random import MT19937\n>>> from numpy.random import RandomState, SeedSequence\n>>> rs = RandomState(MT19937(SeedSequence(123456789)))\n# Later, you want to restart the stream\n>>> rs = RandomState(MT19937(SeedSequence(987654321)))", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/208", "data": {"name": "numpy.random.choice", "type": "callable", "signature": "(*args, **kwargs)", "description": "choice(a, size=None, replace=True, p=None)\n\nGenerates a random sample from a given 1-D array\n\n.. versionadded:: 1.7.0\n\n.. note::\n    New code should use the ``choice`` method of a ``default_rng()``\n    instance instead; please see the :ref:`random-quick-start`.\n\nParameters\n----------\na : 1-D array-like or int\n    If an ndarray, a random sample is generated from its elements.\n    If an int, the random sample is generated as if it were ``np.arange(a)``\nsize : int or tuple of ints, optional\n    Output shape.  If the given shape is, e.g., ``(m, n, k)``, then\n    ``m * n * k`` samples are drawn.  Default is None, in which case a\n    single value is returned.\nreplace : boolean, optional\n    Whether the sample is with or without replacement. Default is True,\n    meaning that a value of ``a`` can be selected multiple times.\np : 1-D array-like, optional\n    The probabilities associated with each entry in a.\n    If not given, the sample assumes a uniform distribution over all\n    entries in ``a``.\n\nReturns\n-------\nsamples : single item or ndarray\n    The generated random samples\n\nRaises\n------\nValueError\n    If a is an int and less than zero, if a or p are not 1-dimensional,\n    if a is an array-like of size 0, if p is not a vector of\n    probabilities, if a and p have different lengths, or if\n    replace=False and the sample size is greater than the population\n    size\n\nSee Also\n--------\nrandint, shuffle, permutation\nrandom.Generator.choice: which should be used in new code\n\nNotes\n-----\nSetting user-specified probabilities through ``p`` uses a more general but less\nefficient sampler than the default. The general sampler produces a different sample\nthan the optimized sampler even if each element of ``p`` is 1 / len(a).\n\nSampling random rows from a 2-D array is not possible with this function,\nbut is possible with `Generator.choice` through its ``axis`` keyword.\n\nExamples\n--------\nGenerate a uniform random sample from np.arange(5) of size 3:\n\n>>> np.random.choice(5, 3)\narray([0, 3, 4]) # random\n>>> #This is equivalent to np.random.randint(0,5,3)\n\nGenerate a non-uniform random sample from np.arange(5) of size 3:\n\n>>> np.random.choice(5, 3, p=[0.1, 0, 0.3, 0.6, 0])\narray([3, 3, 0]) # random\n\nGenerate a uniform random sample from np.arange(5) of size 3 without\nreplacement:\n\n>>> np.random.choice(5, 3, replace=False)\narray([3,1,0]) # random\n>>> #This is equivalent to np.random.permutation(np.arange(5))[:3]\n\nGenerate a non-uniform random sample from np.arange(5) of size\n3 without replacement:\n\n>>> np.random.choice(5, 3, replace=False, p=[0.1, 0, 0.3, 0.6, 0])\narray([2, 3, 0]) # random\n\nAny of the above can be repeated with an arbitrary array-like\ninstead of just integers. For instance:\n\n>>> aa_milne_arr = ['pooh', 'rabbit', 'piglet', 'Christopher']\n>>> np.random.choice(aa_milne_arr, 5, p=[0.5, 0.1, 0.1, 0.3])\narray(['pooh', 'pooh', 'pooh', 'Christopher', 'piglet'], # random\n      dtype='<U11')", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/211", "data": {"name": "zipfile.ZipFile", "type": "class", "signature": "(file, mode='r', compression=0, allowZip64=True, compresslevel=None, *, strict_timestamps=True)", "description": "Class with methods to open, read, write, close, list zip files.\n\nz = ZipFile(file, mode=\"r\", compression=ZIP_STORED, allowZip64=True,\n            compresslevel=None)\n\nfile: Either the path to the file, or a file-like object.\n      If it is a path, the file will be opened and closed by ZipFile.\nmode: The mode can be either read 'r', write 'w', exclusive create 'x',\n      or append 'a'.\ncompression: ZIP_STORED (no compression), ZIP_DEFLATED (requires zlib),\n             ZIP_BZIP2 (requires bz2) or ZIP_LZMA (requires lzma).\nallowZip64: if True ZipFile will create files with ZIP64 extensions when\n            needed, otherwise it will raise an exception when this would\n            be necessary.\ncompresslevel: None (default for the given compression type) or an integer\n               specifying the level to pass to the compressor.\n               When using ZIP_STORED or ZIP_LZMA this keyword has no effect.\n               When using ZIP_DEFLATED integers 0 through 9 are accepted.\n               When using ZIP_BZIP2 integers 1 through 9 are accepted.", "parameters": {"type": "object", "properties": {"file": {}, "mode": {"type": "str", "default": "r"}, "compression": {"type": "integer", "default": 0}, "allowZip64": {"type": "bool", "default": true}, "compresslevel": {"type": "NoneType", "default": null}, "strict_timestamps": {"type": "bool", "default": true}}}}}
{"task_id": "BigCodeBench/211", "data": {"name": "zipfile.ZipFile.extractall", "type": "callable", "signature": "(self, path=None)", "description": "Extract all members from the archive to the current working\ndirectory. `path' specifies a different directory to extract to.\n`members' is optional and must be a subset of the list returned\nby namelist().", "parameters": {"type": "object", "properties": {"path": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/211", "data": {"name": "open.write(response.content)"}}
{"task_id": "BigCodeBench/211", "data": {"name": "os.path.basename", "type": "callable", "signature": "(p)", "description": "Returns the final component of a pathname", "parameters": {"type": "object", "properties": {"p": {}}}}}
{"task_id": "BigCodeBench/211", "data": {"name": "os.listdir", "type": "callable", "signature": "(path=None)", "description": "Return a list containing the names of the files in the directory.\n\npath can be specified as either str, bytes, or a path-like object.  If path is bytes,\n  the filenames returned will also be bytes; in all other circumstances\n  the filenames returned will be str.\nIf path is None, uses the path='.'.\nOn some platforms, path may also be specified as an open file descriptor;\\\n  the file descriptor must refer to a directory.\n  If this functionality is unavailable, using it raises NotImplementedError.\n\nThe list is in arbitrary order.  It does not include the special\nentries '.' and '..' even if they are present in the directory.", "parameters": {"type": "object", "properties": {"path": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/211", "data": {"name": "os.path.join", "type": "callable", "signature": "(a, *p)", "description": "Join two or more pathname components, inserting '/' as needed.\nIf any component is an absolute path, all previous path components\nwill be discarded.  An empty last part will result in a path that\nends with a separator.", "parameters": {"type": "object", "properties": {"a": {}}}}}
{"task_id": "BigCodeBench/211", "data": {"name": "requests.get", "type": "callable", "signature": "(url, **kwargs)", "description": "Sends a GET request.\n\n:param url: URL for the new :class:`Request` object.\n:param params: (optional) Dictionary, list of tuples or bytes to send\n    in the query string for the :class:`Request`.\n:param \\*\\*kwargs: Optional arguments that ``request`` takes.\n:return: :class:`Response <Response>` object\n:rtype: requests.Response", "parameters": {"type": "object", "properties": {"url": {}}}}}
{"task_id": "BigCodeBench/211", "data": {"name": "requests.get.content", "type": "callable", "signature": "(url, params=None, **kwargs)", "description": "Sends a GET request.\n\n:param url: URL for the new :class:`Request` object.\n:param params: (optional) Dictionary, list of tuples or bytes to send\n    in the query string for the :class:`Request`.\n:param \\*\\*kwargs: Optional arguments that ``request`` takes.\n:return: :class:`Response <Response>` object\n:rtype: requests.Response", "parameters": {"type": "object", "properties": {"url": {}, "params": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/214", "data": {"name": "cv2.COLOR_BGR2RGB", "type": "constant", "signature": null, "description": "int([x]) -> integer\nint(x, base=10) -> integer\n\nConvert a number or string to an integer, or return 0 if no arguments\nare given.  If x is a number, return x.__int__().  For floating point\nnumbers, this truncates towards zero.\n\nIf x is not a number or if base is given, then x must be a string,\nbytes, or bytearray instance representing an integer literal in the\ngiven base.  The literal can be preceded by '+' or '-' and be surrounded\nby whitespace.  The base defaults to 10.  Valid bases are 0 and 2-36.\nBase 0 means to interpret the base from the string as an integer literal.\n>>> int('0b100', base=0)\n4", "value": "4", "parameters": {"type": "constant"}}}
{"task_id": "BigCodeBench/214", "data": {"name": "cv2.cvtColor", "type": "callable", "signature": "(*args, **kwargs)", "description": "cvtColor(src, code[, dst[, dstCn]]) -> dst\n.   @brief Converts an image from one color space to another.\n.   \n.   The function converts an input image from one color space to another. In case of a transformation\n.   to-from RGB color space, the order of the channels should be specified explicitly (RGB or BGR). Note\n.   that the default color format in OpenCV is often referred to as RGB but it is actually BGR (the\n.   bytes are reversed). So the first byte in a standard (24-bit) color image will be an 8-bit Blue\n.   component, the second byte will be Green, and the third byte will be Red. The fourth, fifth, and\n.   sixth bytes would then be the second pixel (Blue, then Green, then Red), and so on.\n.   \n.   The conventional ranges for R, G, and B channel values are:\n.   -   0 to 255 for CV_8U images\n.   -   0 to 65535 for CV_16U images\n.   -   0 to 1 for CV_32F images\n.   \n.   In case of linear transformations, the range does not matter. But in case of a non-linear\n.   transformation, an input RGB image should be normalized to the proper value range to get the correct\n.   results, for example, for RGB \\f$\\rightarrow\\f$ L\\*u\\*v\\* transformation. For example, if you have a\n.   32-bit floating-point image directly converted from an 8-bit image without any scaling, then it will\n.   have the 0..255 value range instead of 0..1 assumed by the function. So, before calling #cvtColor ,\n.   you need first to scale the image down:\n.   @code\n.       img *= 1./255;\n.       cvtColor(img, img, COLOR_BGR2Luv);\n.   @endcode\n.   If you use #cvtColor with 8-bit images, the conversion will have some information lost. For many\n.   applications, this will not be noticeable but it is recommended to use 32-bit images in applications\n.   that need the full range of colors or that convert an image before an operation and then convert\n.   back.\n.   \n.   If conversion adds the alpha channel, its value will set to the maximum of corresponding channel\n.   range: 255 for CV_8U, 65535 for CV_16U, 1 for CV_32F.\n.   \n.   @param src input image: 8-bit unsigned, 16-bit unsigned ( CV_16UC... ), or single-precision\n.   floating-point.\n.   @param dst output image of the same size and depth as src.\n.   @param code color space conversion code (see #ColorConversionCodes).\n.   @param dstCn number of channels in the destination image; if the parameter is 0, the number of the\n.   channels is derived automatically from src and code.\n.   \n.   @see @ref imgproc_color_conversions", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/214", "data": {"name": "matplotlib.pyplot.subplots", "type": "class", "signature": "()", "description": "Create a figure and a set of subplots.\n\nThis utility wrapper makes it convenient to create common layouts of\nsubplots, including the enclosing figure object, in a single call.\n\nParameters\n----------\nnrows, ncols : int, default: 1\n    Number of rows/columns of the subplot grid.\n\nsharex, sharey : bool or {'none', 'all', 'row', 'col'}, default: False\n    Controls sharing of properties among x (*sharex*) or y (*sharey*)\n    axes:\n\n    - True or 'all': x- or y-axis will be shared among all subplots.\n    - False or 'none': each subplot x- or y-axis will be independent.\n    - 'row': each subplot row will share an x- or y-axis.\n    - 'col': each subplot column will share an x- or y-axis.\n\n    When subplots have a shared x-axis along a column, only the x tick\n    labels of the bottom subplot are created. Similarly, when subplots\n    have a shared y-axis along a row, only the y tick labels of the first\n    column subplot are created. To later turn other subplots' ticklabels\n    on, use `~matplotlib.axes.Axes.tick_params`.\n\n    When subplots have a shared axis that has units, calling\n    `~matplotlib.axis.Axis.set_units` will update each axis with the\n    new units.\n\nsqueeze : bool, default: True\n    - If True, extra dimensions are squeezed out from the returned\n      array of `~matplotlib.axes.Axes`:\n\n      - if only one subplot is constructed (nrows=ncols=1), the\n        resulting single Axes object is returned as a scalar.\n      - for Nx1 or 1xM subplots, the returned object is a 1D numpy\n        object array of Axes objects.\n      - for NxM, subplots with N>1 and M>1 are returned as a 2D array.\n\n    - If False, no squeezing at all is done: the returned Axes object is\n      always a 2D array containing Axes instances, even if it ends up\n      being 1x1.\n\nwidth_ratios : array-like of length *ncols*, optional\n    Defines the relative widths of the columns. Each column gets a\n    relative width of ``width_ratios[i] / sum(width_ratios)``.\n    If not given, all columns will have the same width.  Equivalent\n    to ``gridspec_kw={'width_ratios': [...]}``.\n\nheight_ratios : array-like of length *nrows*, optional\n    Defines the relative heights of the rows. Each row gets a\n    relative height of ``height_ratios[i] / sum(height_ratios)``.\n    If not given, all rows will have the same height. Convenience\n    for ``gridspec_kw={'height_ratios': [...]}``.\n\nsubplot_kw : dict, optional\n    Dict with keywords passed to the\n    `~matplotlib.figure.Figure.add_subplot` call used to create each\n    subplot.\n\ngridspec_kw : dict, optional\n    Dict with keywords passed to the `~matplotlib.gridspec.GridSpec`\n    constructor used to create the grid the subplots are placed on.\n\n**fig_kw\n    All additional keyword arguments are passed to the\n    `.pyplot.figure` call.\n\nReturns\n-------\nfig : `.Figure`\n\nax : `~matplotlib.axes.Axes` or array of Axes\n    *ax* can be either a single `~.axes.Axes` object, or an array of Axes\n    objects if more than one subplot was created.  The dimensions of the\n    resulting array can be controlled with the squeeze keyword, see above.\n\n    Typical idioms for handling the return value are::\n\n        # using the variable ax for single a Axes\n        fig, ax = plt.subplots()\n\n        # using the variable axs for multiple Axes\n        fig, axs = plt.subplots(2, 2)\n\n        # using tuple unpacking for multiple Axes\n        fig, (ax1, ax2) = plt.subplots(1, 2)\n        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n\n    The names ``ax`` and pluralized ``axs`` are preferred over ``axes``\n    because for the latter it's not clear if it refers to a single\n    `~.axes.Axes` instance or a collection of these.\n\nSee Also\n--------\n.pyplot.figure\n.pyplot.subplot\n.pyplot.axes\n.Figure.subplots\n.Figure.add_subplot\n\nExamples\n--------\n::\n\n    # First create some toy data:\n    x = np.linspace(0, 2*np.pi, 400)\n    y = np.sin(x**2)\n\n    # Create just a figure and only one subplot\n    fig, ax = plt.subplots()\n    ax.plot(x, y)\n    ax.set_title('Simple plot')\n\n    # Create two subplots and unpack the output array immediately\n    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n    ax1.plot(x, y)\n    ax1.set_title('Sharing Y axis')\n    ax2.scatter(x, y)\n\n    # Create four polar axes and access them through the returned array\n    fig, axs = plt.subplots(2, 2, subplot_kw=dict(projection=\"polar\"))\n    axs[0, 0].plot(x, y)\n    axs[1, 1].scatter(x, y)\n\n    # Share a X axis with each column of subplots\n    plt.subplots(2, 2, sharex='col')\n\n    # Share a Y axis with each row of subplots\n    plt.subplots(2, 2, sharey='row')\n\n    # Share both X and Y axes with all subplots\n    plt.subplots(2, 2, sharex='all', sharey='all')\n\n    # Note that this is the same as\n    plt.subplots(2, 2, sharex=True, sharey=True)\n\n    # Create figure number 10 with a single subplot\n    # and clears it if it already exists.\n    fig, ax = plt.subplots(num=10, clear=True)", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/214", "data": {"name": "matplotlib.pyplot.subplots[1].set_title", "type": "method", "signature": "(label, fontdict=None, loc=None, pad=None, *, y=None, **kwargs)", "description": "Set a title for the Axes.\n\nSet one of the three available Axes titles. The available titles\nare positioned above the Axes in the center, flush with the left\nedge, and flush with the right edge.\n\nParameters\n----------\nlabel : str\n    Text to use for the title\n\nfontdict : dict\n\n    .. admonition:: Discouraged\n\n       The use of *fontdict* is discouraged. Parameters should be passed as\n       individual keyword arguments or using dictionary-unpacking\n       ``set_title(..., **fontdict)``.\n\n    A dictionary controlling the appearance of the title text,\n    the default *fontdict* is::\n\n       {'fontsize': rcParams['axes.titlesize'],\n        'fontweight': rcParams['axes.titleweight'],\n        'color': rcParams['axes.titlecolor'],\n        'verticalalignment': 'baseline',\n        'horizontalalignment': loc}\n\nloc : {'center', 'left', 'right'}, default: :rc:`axes.titlelocation`\n    Which title to set.\n\ny : float, default: :rc:`axes.titley`\n    Vertical Axes location for the title (1.0 is the top).  If\n    None (the default) and :rc:`axes.titley` is also None, y is\n    determined automatically to avoid decorators on the Axes.\n\npad : float, default: :rc:`axes.titlepad`\n    The offset of the title from the top of the Axes, in points.\n\nReturns\n-------\n`.Text`\n    The matplotlib text instance representing the title\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    Other keyword arguments are text properties, see `.Text` for a list\n    of valid text properties.", "parameters": {"type": "object", "properties": {"label": {}, "fontdict": {"type": "NoneType", "default": null}, "loc": {"type": "NoneType", "default": null}, "pad": {"type": "NoneType", "default": null}, "y": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/214", "data": {"name": "matplotlib.pyplot.subplots[1].imshow", "type": "method", "signature": "(X, cmap=None, norm=None, *, aspect=None, interpolation=None, alpha=None, vmin=None, vmax=None, origin=None, extent=None, interpolation_stage=None, filternorm=True, filterrad=4.0, resample=None, url=None, data=None, **kwargs)", "description": "Display data as an image, i.e., on a 2D regular raster.\n\nThe input may either be actual RGB(A) data, or 2D scalar data, which\nwill be rendered as a pseudocolor image. For displaying a grayscale\nimage, set up the colormapping using the parameters\n``cmap='gray', vmin=0, vmax=255``.\n\nThe number of pixels used to render an image is set by the Axes size\nand the figure *dpi*. This can lead to aliasing artifacts when\nthe image is resampled, because the displayed image size will usually\nnot match the size of *X* (see\n:doc:`/gallery/images_contours_and_fields/image_antialiasing`).\nThe resampling can be controlled via the *interpolation* parameter\nand/or :rc:`image.interpolation`.\n\nParameters\n----------\nX : array-like or PIL image\n    The image data. Supported array shapes are:\n\n    - (M, N): an image with scalar data. The values are mapped to\n      colors using normalization and a colormap. See parameters *norm*,\n      *cmap*, *vmin*, *vmax*.\n    - (M, N, 3): an image with RGB values (0-1 float or 0-255 int).\n    - (M, N, 4): an image with RGBA values (0-1 float or 0-255 int),\n      i.e. including transparency.\n\n    The first two dimensions (M, N) define the rows and columns of\n    the image.\n\n    Out-of-range RGB(A) values are clipped.\n\ncmap : str or `~matplotlib.colors.Colormap`, default: :rc:`image.cmap`\n    The Colormap instance or registered colormap name used to map scalar data\n    to colors.\n\n    This parameter is ignored if *X* is RGB(A).\n\nnorm : str or `~matplotlib.colors.Normalize`, optional\n    The normalization method used to scale scalar data to the [0, 1] range\n    before mapping to colors using *cmap*. By default, a linear scaling is\n    used, mapping the lowest value to 0 and the highest to 1.\n\n    If given, this can be one of the following:\n\n    - An instance of `.Normalize` or one of its subclasses\n      (see :ref:`colormapnorms`).\n    - A scale name, i.e. one of \"linear\", \"log\", \"symlog\", \"logit\", etc.  For a\n      list of available scales, call `matplotlib.scale.get_scale_names()`.\n      In that case, a suitable `.Normalize` subclass is dynamically generated\n      and instantiated.\n\n    This parameter is ignored if *X* is RGB(A).\n\nvmin, vmax : float, optional\n    When using scalar data and no explicit *norm*, *vmin* and *vmax* define\n    the data range that the colormap covers. By default, the colormap covers\n    the complete value range of the supplied data. It is an error to use\n    *vmin*/*vmax* when a *norm* instance is given (but using a `str` *norm*\n    name together with *vmin*/*vmax* is acceptable).\n\n    This parameter is ignored if *X* is RGB(A).\n\naspect : {'equal', 'auto'} or float or None, default: None\n    The aspect ratio of the Axes.  This parameter is particularly\n    relevant for images since it determines whether data pixels are\n    square.\n\n    This parameter is a shortcut for explicitly calling\n    `.Axes.set_aspect`. See there for further details.\n\n    - 'equal': Ensures an aspect ratio of 1. Pixels will be square\n      (unless pixel sizes are explicitly made non-square in data\n      coordinates using *extent*).\n    - 'auto': The Axes is kept fixed and the aspect is adjusted so\n      that the data fit in the Axes. In general, this will result in\n      non-square pixels.\n\n    Normally, None (the default) means to use :rc:`image.aspect`.  However, if\n    the image uses a transform that does not contain the axes data transform,\n    then None means to not modify the axes aspect at all (in that case, directly\n    call `.Axes.set_aspect` if desired).\n\ninterpolation : str, default: :rc:`image.interpolation`\n    The interpolation method used.\n\n    Supported values are 'none', 'antialiased', 'nearest', 'bilinear',\n    'bicubic', 'spline16', 'spline36', 'hanning', 'hamming', 'hermite',\n    'kaiser', 'quadric', 'catrom', 'gaussian', 'bessel', 'mitchell',\n    'sinc', 'lanczos', 'blackman'.\n\n    The data *X* is resampled to the pixel size of the image on the\n    figure canvas, using the interpolation method to either up- or\n    downsample the data.\n\n    If *interpolation* is 'none', then for the ps, pdf, and svg\n    backends no down- or upsampling occurs, and the image data is\n    passed to the backend as a native image.  Note that different ps,\n    pdf, and svg viewers may display these raw pixels differently. On\n    other backends, 'none' is the same as 'nearest'.\n\n    If *interpolation* is the default 'antialiased', then 'nearest'\n    interpolation is used if the image is upsampled by more than a\n    factor of three (i.e. the number of display pixels is at least\n    three times the size of the data array).  If the upsampling rate is\n    smaller than 3, or the image is downsampled, then 'hanning'\n    interpolation is used to act as an anti-aliasing filter, unless the\n    image happens to be upsampled by exactly a factor of two or one.\n\n    See\n    :doc:`/gallery/images_contours_and_fields/interpolation_methods`\n    for an overview of the supported interpolation methods, and\n    :doc:`/gallery/images_contours_and_fields/image_antialiasing` for\n    a discussion of image antialiasing.\n\n    Some interpolation methods require an additional radius parameter,\n    which can be set by *filterrad*. Additionally, the antigrain image\n    resize filter is controlled by the parameter *filternorm*.\n\ninterpolation_stage : {'data', 'rgba'}, default: 'data'\n    If 'data', interpolation\n    is carried out on the data provided by the user.  If 'rgba', the\n    interpolation is carried out after the colormapping has been\n    applied (visual interpolation).\n\nalpha : float or array-like, optional\n    The alpha blending value, between 0 (transparent) and 1 (opaque).\n    If *alpha* is an array, the alpha blending values are applied pixel\n    by pixel, and *alpha* must have the same shape as *X*.\n\norigin : {'upper', 'lower'}, default: :rc:`image.origin`\n    Place the [0, 0] index of the array in the upper left or lower\n    left corner of the Axes. The convention (the default) 'upper' is\n    typically used for matrices and images.\n\n    Note that the vertical axis points upward for 'lower'\n    but downward for 'upper'.\n\n    See the :ref:`imshow_extent` tutorial for\n    examples and a more detailed description.\n\nextent : floats (left, right, bottom, top), optional\n    The bounding box in data coordinates that the image will fill.\n    These values may be unitful and match the units of the Axes.\n    The image is stretched individually along x and y to fill the box.\n\n    The default extent is determined by the following conditions.\n    Pixels have unit size in data coordinates. Their centers are on\n    integer coordinates, and their center coordinates range from 0 to\n    columns-1 horizontally and from 0 to rows-1 vertically.\n\n    Note that the direction of the vertical axis and thus the default\n    values for top and bottom depend on *origin*:\n\n    - For ``origin == 'upper'`` the default is\n      ``(-0.5, numcols-0.5, numrows-0.5, -0.5)``.\n    - For ``origin == 'lower'`` the default is\n      ``(-0.5, numcols-0.5, -0.5, numrows-0.5)``.\n\n    See the :ref:`imshow_extent` tutorial for\n    examples and a more detailed description.\n\nfilternorm : bool, default: True\n    A parameter for the antigrain image resize filter (see the\n    antigrain documentation).  If *filternorm* is set, the filter\n    normalizes integer values and corrects the rounding errors. It\n    doesn't do anything with the source floating point values, it\n    corrects only integers according to the rule of 1.0 which means\n    that any sum of pixel weights must be equal to 1.0.  So, the\n    filter function must produce a graph of the proper shape.\n\nfilterrad : float > 0, default: 4.0\n    The filter radius for filters that have a radius parameter, i.e.\n    when interpolation is one of: 'sinc', 'lanczos' or 'blackman'.\n\nresample : bool, default: :rc:`image.resample`\n    When *True*, use a full resampling method.  When *False*, only\n    resample when the output image is larger than the input image.\n\nurl : str, optional\n    Set the url of the created `.AxesImage`. See `.Artist.set_url`.\n\nReturns\n-------\n`~matplotlib.image.AxesImage`\n\nOther Parameters\n----------------\ndata : indexable object, optional\n    If given, all parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` (unless this raises an exception).\n\n**kwargs : `~matplotlib.artist.Artist` properties\n    These parameters are passed on to the constructor of the\n    `.AxesImage` artist.\n\nSee Also\n--------\nmatshow : Plot a matrix or an array as an image.\n\nNotes\n-----\nUnless *extent* is used, pixel centers will be located at integer\ncoordinates. In other words: the origin will coincide with the center\nof pixel (0, 0).\n\nThere are two common representations for RGB images with an alpha\nchannel:\n\n-   Straight (unassociated) alpha: R, G, and B channels represent the\n    color of the pixel, disregarding its opacity.\n-   Premultiplied (associated) alpha: R, G, and B channels represent\n    the color of the pixel, adjusted for its opacity by multiplication.\n\n`~matplotlib.pyplot.imshow` expects RGB images adopting the straight\n(unassociated) alpha representation.", "parameters": {"type": "object", "properties": {"X": {}, "cmap": {"type": "NoneType", "default": null}, "norm": {"type": "NoneType", "default": null}, "aspect": {"type": "NoneType", "default": null}, "interpolation": {"type": "NoneType", "default": null}, "alpha": {"type": "NoneType", "default": null}, "vmin": {"type": "NoneType", "default": null}, "vmax": {"type": "NoneType", "default": null}, "origin": {"type": "NoneType", "default": null}, "extent": {"type": "NoneType", "default": null}, "interpolation_stage": {"type": "NoneType", "default": null}, "filternorm": {"type": "bool", "default": true}, "filterrad": {"type": "float", "default": 4.0}, "resample": {"type": "NoneType", "default": null}, "url": {"type": "NoneType", "default": null}, "data": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/214", "data": {"name": "numpy.random.seed", "type": "callable", "signature": "(*args, **kwargs)", "description": "seed(self, seed=None)\n\nReseed a legacy MT19937 BitGenerator\n\nNotes\n-----\nThis is a convenience, legacy function.\n\nThe best practice is to **not** reseed a BitGenerator, rather to\nrecreate a new one. This method is here for legacy reasons.\nThis example demonstrates best practice.\n\n>>> from numpy.random import MT19937\n>>> from numpy.random import RandomState, SeedSequence\n>>> rs = RandomState(MT19937(SeedSequence(123456789)))\n# Later, you want to restart the stream\n>>> rs = RandomState(MT19937(SeedSequence(987654321)))", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/214", "data": {"name": "numpy.zeros", "type": "callable", "signature": "(*args, **kwargs)", "description": "zeros(shape, dtype=float, order='C', *, like=None)\n\nReturn a new array of given shape and type, filled with zeros.\n\nParameters\n----------\nshape : int or tuple of ints\n    Shape of the new array, e.g., ``(2, 3)`` or ``2``.\ndtype : data-type, optional\n    The desired data-type for the array, e.g., `numpy.int8`.  Default is\n    `numpy.float64`.\norder : {'C', 'F'}, optional, default: 'C'\n    Whether to store multi-dimensional data in row-major\n    (C-style) or column-major (Fortran-style) order in\n    memory.\nlike : array_like, optional\n    Reference object to allow the creation of arrays which are not\n    NumPy arrays. If an array-like passed in as ``like`` supports\n    the ``__array_function__`` protocol, the result will be defined\n    by it. In this case, it ensures the creation of an array object\n    compatible with that passed in via this argument.\n\n    .. versionadded:: 1.20.0\n\nReturns\n-------\nout : ndarray\n    Array of zeros with the given shape, dtype, and order.\n\nSee Also\n--------\nzeros_like : Return an array of zeros with shape and type of input.\nempty : Return a new uninitialized array.\nones : Return a new array setting values to one.\nfull : Return a new array of given shape filled with value.\n\nExamples\n--------\n>>> np.zeros(5)\narray([ 0.,  0.,  0.,  0.,  0.])\n\n>>> np.zeros((5,), dtype=int)\narray([0, 0, 0, 0, 0])\n\n>>> np.zeros((2, 1))\narray([[ 0.],\n       [ 0.]])\n\n>>> s = (2,2)\n>>> np.zeros(s)\narray([[ 0.,  0.],\n       [ 0.,  0.]])\n\n>>> np.zeros((2,), dtype=[('x', 'i4'), ('y', 'i4')]) # custom dtype\narray([(0, 0), (0, 0)],\n      dtype=[('x', '<i4'), ('y', '<i4')])", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/214", "data": {"name": "random.seed", "type": "callable", "signature": "(a=None)", "description": "Initialize internal state from a seed.\n\nThe only supported seed types are None, int, float,\nstr, bytes, and bytearray.\n\nNone or no argument seeds from current time or from an operating\nsystem specific randomness source if available.\n\nIf *a* is an int, all bits are used.\n\nFor version 2 (the default), all of the bits are used if *a* is a str,\nbytes, or bytearray.  For version 1 (provided for reproducing random\nsequences from older versions of Python), the algorithm for str and\nbytes generates a narrower range of seeds.", "parameters": {"type": "object", "properties": {"a": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/214", "data": {"name": "random.randint", "type": "callable", "signature": "(a, b)", "description": "Return random integer in range [a, b], including both end points.\n        ", "parameters": {"type": "object", "properties": {"a": {}, "b": {}}}}}
{"task_id": "BigCodeBench/227", "data": {"name": "librosa.stft", "type": "callable", "signature": "(y: 'np.ndarray')", "description": "Short-time Fourier transform (STFT).\n\nThe STFT represents a signal in the time-frequency domain by\ncomputing discrete Fourier transforms (DFT) over short overlapping\nwindows.\n\nThis function returns a complex-valued matrix D such that\n\n- ``np.abs(D[..., f, t])`` is the magnitude of frequency bin ``f``\n  at frame ``t``, and\n\n- ``np.angle(D[..., f, t])`` is the phase of frequency bin ``f``\n  at frame ``t``.\n\nThe integers ``t`` and ``f`` can be converted to physical units by means\nof the utility functions `frames_to_samples` and `fft_frequencies`.\n\nParameters\n----------\ny : np.ndarray [shape=(..., n)], real-valued\n    input signal. Multi-channel is supported.\n\nn_fft : int > 0 [scalar]\n    length of the windowed signal after padding with zeros.\n    The number of rows in the STFT matrix ``D`` is ``(1 + n_fft/2)``.\n    The default value, ``n_fft=2048`` samples, corresponds to a physical\n    duration of 93 milliseconds at a sample rate of 22050 Hz, i.e. the\n    default sample rate in librosa. This value is well adapted for music\n    signals. However, in speech processing, the recommended value is 512,\n    corresponding to 23 milliseconds at a sample rate of 22050 Hz.\n    In any case, we recommend setting ``n_fft`` to a power of two for\n    optimizing the speed of the fast Fourier transform (FFT) algorithm.\n\nhop_length : int > 0 [scalar]\n    number of audio samples between adjacent STFT columns.\n\n    Smaller values increase the number of columns in ``D`` without\n    affecting the frequency resolution of the STFT.\n\n    If unspecified, defaults to ``win_length // 4`` (see below).\n\nwin_length : int <= n_fft [scalar]\n    Each frame of audio is windowed by ``window`` of length ``win_length``\n    and then padded with zeros to match ``n_fft``.  Padding is added on\n    both the left- and the right-side of the window so that the window\n    is centered within the frame.\n\n    Smaller values improve the temporal resolution of the STFT (i.e. the\n    ability to discriminate impulses that are closely spaced in time)\n    at the expense of frequency resolution (i.e. the ability to discriminate\n    pure tones that are closely spaced in frequency). This effect is known\n    as the time-frequency localization trade-off and needs to be adjusted\n    according to the properties of the input signal ``y``.\n\n    If unspecified, defaults to ``win_length = n_fft``.\n\nwindow : string, tuple, number, function, or np.ndarray [shape=(n_fft,)]\n    Either:\n\n    - a window specification (string, tuple, or number);\n      see `scipy.signal.get_window`\n    - a window function, such as `scipy.signal.windows.hann`\n    - a vector or array of length ``n_fft``\n\n    Defaults to a raised cosine window (`'hann'`), which is adequate for\n    most applications in audio signal processing.\n\n    .. see also:: `filters.get_window`\n\ncenter : boolean\n    If ``True``, the signal ``y`` is padded so that frame\n    ``D[:, t]`` is centered at ``y[t * hop_length]``.\n\n    If ``False``, then ``D[:, t]`` begins at ``y[t * hop_length]``.\n\n    Defaults to ``True``,  which simplifies the alignment of ``D`` onto a\n    time grid by means of `librosa.frames_to_samples`.\n    Note, however, that ``center`` must be set to `False` when analyzing\n    signals with `librosa.stream`.\n\n    .. see also:: `librosa.stream`\n\ndtype : np.dtype, optional\n    Complex numeric type for ``D``.  Default is inferred to match the\n    precision of the input signal.\n\npad_mode : string or function\n    If ``center=True``, this argument is passed to `np.pad` for padding\n    the edges of the signal ``y``. By default (``pad_mode=\"constant\"``),\n    ``y`` is padded on both sides with zeros.\n\n    .. note:: Not all padding modes supported by `numpy.pad` are supported here.\n        `wrap`, `mean`, `maximum`, `median`, and `minimum` are not supported.\n\n        Other modes that depend at most on input values at the edges of the\n        signal (e.g., `constant`, `edge`, `linear_ramp`) are supported.\n\n    If ``center=False``,  this argument is ignored.\n\n    .. see also:: `numpy.pad`\n\nout : np.ndarray or None\n    A pre-allocated, complex-valued array to store the STFT results.\n    This must be of compatible shape and dtype for the given input parameters.\n\n    If `out` is larger than necessary for the provided input signal, then only\n    a prefix slice of `out` will be used.\n\n    If not provided, a new array is allocated and returned.\n\nReturns\n-------\nD : np.ndarray [shape=(..., 1 + n_fft/2, n_frames), dtype=dtype]\n    Complex-valued matrix of short-term Fourier transform\n    coefficients.\n\n    If a pre-allocated `out` array is provided, then `D` will be\n    a reference to `out`.\n\n    If `out` is larger than necessary, then `D` will be a sliced\n    view: `D = out[..., :n_frames]`.\n\nSee Also\n--------\nistft : Inverse STFT\nreassigned_spectrogram : Time-frequency reassigned spectrogram\n\nNotes\n-----\nThis function caches at level 20.\n\nExamples\n--------\n>>> y, sr = librosa.load(librosa.ex('trumpet'))\n>>> S = np.abs(librosa.stft(y))\n>>> S\narray([[5.395e-03, 3.332e-03, ..., 9.862e-07, 1.201e-05],\n       [3.244e-03, 2.690e-03, ..., 9.536e-07, 1.201e-05],\n       ...,\n       [7.523e-05, 3.722e-05, ..., 1.188e-04, 1.031e-03],\n       [7.640e-05, 3.944e-05, ..., 5.180e-04, 1.346e-03]],\n      dtype=float32)\n\nUse left-aligned frames, instead of centered frames\n\n>>> S_left = librosa.stft(y, center=False)\n\nUse a shorter hop length\n\n>>> D_short = librosa.stft(y, hop_length=64)\n\nDisplay a spectrogram\n\n>>> import matplotlib.pyplot as plt\n>>> fig, ax = plt.subplots()\n>>> img = librosa.display.specshow(librosa.amplitude_to_db(S,\n...                                                        ref=np.max),\n...                                y_axis='log', x_axis='time', ax=ax)\n>>> ax.set_title('Power spectrogram')\n>>> fig.colorbar(img, ax=ax, format=\"%+2.0f dB\")", "parameters": {"type": "object", "properties": {"y": {"type": "np.ndarray"}}}}}
{"task_id": "BigCodeBench/227", "data": {"name": "librosa.amplitude_to_db", "type": "callable", "signature": "(S: '_ScalarOrSequence[_ComplexLike_co]', ref: 'Union[float)", "description": "Convert an amplitude spectrogram to dB-scaled spectrogram.\n\nThis is equivalent to ``power_to_db(S**2, ref=ref**2, amin=amin**2, top_db=top_db)``,\nbut is provided for convenience.\n\nParameters\n----------\nS : np.ndarray\n    input amplitude\n\nref : scalar or callable\n    If scalar, the amplitude ``abs(S)`` is scaled relative to ``ref``:\n    ``20 * log10(S / ref)``.\n    Zeros in the output correspond to positions where ``S == ref``.\n\n    If callable, the reference value is computed as ``ref(S)``.\n\namin : float > 0 [scalar]\n    minimum threshold for ``S`` and ``ref``\n\ntop_db : float >= 0 [scalar]\n    threshold the output at ``top_db`` below the peak:\n    ``max(20 * log10(S/ref)) - top_db``\n\nReturns\n-------\nS_db : np.ndarray\n    ``S`` measured in dB\n\nSee Also\n--------\npower_to_db, db_to_amplitude\n\nNotes\n-----\nThis function caches at level 30.", "parameters": {"type": "object", "properties": {"S": {"type": "_scalarorsequence[_complexlike_co]"}, "ref": {"type": "union[float"}}}}}
{"task_id": "BigCodeBench/227", "data": {"name": "librosa.display.specshow", "type": "callable", "signature": "(data: 'np.ndarray', x_axis: 'Optional[str]' = None, y_axis: 'Optional[str]' = None, sr: 'float' = 22050)", "description": "Display a spectrogram/chromagram/cqt/etc.\n\nFor a detailed overview of this function, see :ref:`sphx_glr_auto_examples_plot_display.py`\n\nParameters\n----------\ndata : np.ndarray [shape=(d, n)]\n    Matrix to display (e.g., spectrogram)\n\nsr : number > 0 [scalar]\n    Sample rate used to determine time scale in x-axis.\n\nhop_length : int > 0 [scalar]\n    Hop length, also used to determine time scale in x-axis\n\nn_fft : int > 0 or None\n    Number of samples per frame in STFT/spectrogram displays.\n    By default, this will be inferred from the shape of ``data``\n    as ``2 * (d - 1)``.\n    If ``data`` was generated using an odd frame length, the correct\n    value can be specified here.\n\nwin_length : int > 0 or None\n    The number of samples per window.\n    By default, this will be inferred to match ``n_fft``.\n    This is primarily useful for specifying odd window lengths in\n    Fourier tempogram displays.\n\nx_axis, y_axis : None or str\n    Range for the x- and y-axes.\n\n    Valid types are:\n\n    - None, 'none', or 'off' : no axis decoration is displayed.\n\n    Frequency types:\n\n    - 'linear', 'fft', 'hz' : frequency range is determined by\n      the FFT window and sampling rate.\n    - 'log' : the spectrum is displayed on a log scale.\n    - 'fft_note': the spectrum is displayed on a log scale with pitches marked.\n    - 'fft_svara': the spectrum is displayed on a log scale with svara marked.\n    - 'mel' : frequencies are determined by the mel scale.\n    - 'cqt_hz' : frequencies are determined by the CQT scale.\n    - 'cqt_note' : pitches are determined by the CQT scale.\n    - 'cqt_svara' : like `cqt_note` but using Hindustani or Carnatic svara\n    - 'vqt_fjs' : like `cqt_note` but using Functional Just System (FJS)\n      notation.  This requires a just intonation-based variable-Q\n      transform representation.\n\n    All frequency types are plotted in units of Hz.\n\n    Any spectrogram parameters (hop_length, sr, bins_per_octave, etc.)\n    used to generate the input data should also be provided when\n    calling `specshow`.\n\n    Categorical types:\n\n    - 'chroma' : pitches are determined by the chroma filters.\n      Pitch classes are arranged at integer locations (0-11) according to\n      a given key.\n\n    - `chroma_h`, `chroma_c`: pitches are determined by chroma filters,\n      and labeled as svara in the Hindustani (`chroma_h`) or Carnatic (`chroma_c`)\n      according to a given thaat (Hindustani) or melakarta raga (Carnatic).\n\n    - 'chroma_fjs': pitches are determined by chroma filters using just\n      intonation.  All pitch classes are annotated.\n\n    - 'tonnetz' : axes are labeled by Tonnetz dimensions (0-5)\n    - 'frames' : markers are shown as frame counts.\n\n    Time types:\n\n    - 'time' : markers are shown as milliseconds, seconds, minutes, or hours.\n            Values are plotted in units of seconds.\n    - 'h' : markers are shown as hours, minutes, and seconds.\n    - 'm' : markers are shown as minutes and seconds.\n    - 's' : markers are shown as seconds.\n    - 'ms' : markers are shown as milliseconds.\n    - 'lag' : like time, but past the halfway point counts as negative values.\n    - 'lag_h' : same as lag, but in hours, minutes and seconds.\n    - 'lag_m' : same as lag, but in minutes and seconds.\n    - 'lag_s' : same as lag, but in seconds.\n    - 'lag_ms' : same as lag, but in milliseconds.\n\n    Rhythm:\n\n    - 'tempo' : markers are shown as beats-per-minute (BPM)\n        using a logarithmic scale.  This is useful for\n        visualizing the outputs of `feature.tempogram`.\n\n    - 'fourier_tempo' : same as `'tempo'`, but used when\n        tempograms are calculated in the Frequency domain\n        using `feature.fourier_tempogram`.\n\nx_coords, y_coords : np.ndarray [shape=data.shape[0 or 1]]\n    Optional positioning coordinates of the input data.\n    These can be use to explicitly set the location of each\n    element ``data[i, j]``, e.g., for displaying beat-synchronous\n    features in natural time coordinates.\n\n    If not provided, they are inferred from ``x_axis`` and ``y_axis``.\n\nfmin : float > 0 [scalar] or None\n    Frequency of the lowest spectrogram bin.  Used for Mel, CQT, and VQT\n    scales.\n\n    If ``y_axis`` is `cqt_hz` or `cqt_note` and ``fmin`` is not given,\n    it is set by default to ``note_to_hz('C1')``.\n\nfmax : float > 0 [scalar] or None\n    Used for setting the Mel frequency scales\n\ntempo_min : float > 0 [scalar]\n    Lowest tempo (in beats per minute) for tempogram display.\n\ntempo_max : float > 0 [scalar]\n    Highest tempo (in beats per minute) for tempogram display.\n\ntuning : float\n    Tuning deviation from A440, in fractions of a bin.\n\n    This is used for CQT frequency scales, so that ``fmin`` is adjusted\n    to ``fmin * 2**(tuning / bins_per_octave)``.\n\nbins_per_octave : int > 0 [scalar]\n    Number of bins per octave.  Used for CQT frequency scale.\n\nkey : str\n    The reference key to use when using note axes (`cqt_note`, `chroma`).\n\nSa : float or int\n    If using Hindustani or Carnatic svara axis decorations, specify Sa.\n\n    For `cqt_svara`, ``Sa`` should be specified as a frequency in Hz.\n\n    For `chroma_c` or `chroma_h`, ``Sa`` should correspond to the position\n    of Sa within the chromagram.\n    If not provided, Sa will default to 0 (equivalent to `C`)\n\nmela : str or int, optional\n    If using `chroma_c` or `cqt_svara` display mode, specify the melakarta raga.\n\nthaat : str, optional\n    If using `chroma_h` display mode, specify the parent thaat.\n\nintervals : str or array of floats in [1, 2), optional\n    If using an FJS notation (`chroma_fjs`, `vqt_fjs`), the interval specification.\n\n    See `core.interval_frequencies` for a description of supported values.\n\nunison : str, optional\n    If using an FJS notation (`chroma_fjs`, `vqt_fjs`), the pitch name of the unison\n    interval.  If not provided, it will be inferred from `fmin` (for VQT display) or\n    assumed as `'C'` (for chroma display).\n\nauto_aspect : bool\n    Axes will have 'equal' aspect if the horizontal and vertical dimensions\n    cover the same extent and their types match.\n\n    To override, set to `False`.\n\nhtk : bool\n    If plotting on a mel frequency axis, specify which version of the mel\n    scale to use.\n\n        - `False`: use Slaney formula (default)\n        - `True`: use HTK formula\n\n    See `core.mel_frequencies` for more information.\n\nunicode : bool\n    If using note or svara decorations, setting `unicode=True`\n    will use unicode glyphs for accidentals and octave encoding.\n\n    Setting `unicode=False` will use ASCII glyphs.  This can be helpful\n    if your font does not support musical notation symbols.\n\nax : matplotlib.axes.Axes or None\n    Axes to plot on instead of the default `plt.gca()`.\n\n**kwargs : additional keyword arguments\n    Arguments passed through to `matplotlib.pyplot.pcolormesh`.\n\n    By default, the following options are set:\n\n        - ``rasterized=True``\n        - ``shading='auto'``\n        - ``edgecolors='None'``\n\n    The ``cmap`` option if not provided, is inferred from data automatically.\n    Set ``cmap=None`` to use matplotlib's default colormap.\n\nReturns\n-------\ncolormesh : `matplotlib.collections.QuadMesh`\n    The color mesh object produced by `matplotlib.pyplot.pcolormesh`\n\nSee Also\n--------\ncmap : Automatic colormap detection\nmatplotlib.pyplot.pcolormesh\n\nExamples\n--------\nVisualize an STFT power spectrum using default parameters\n\n>>> import matplotlib.pyplot as plt\n>>> y, sr = librosa.load(librosa.ex('choice'), duration=15)\n>>> fig, ax = plt.subplots(nrows=2, ncols=1, sharex=True)\n>>> D = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)\n>>> img = librosa.display.specshow(D, y_axis='linear', x_axis='time',\n...                                sr=sr, ax=ax[0])\n>>> ax[0].set(title='Linear-frequency power spectrogram')\n>>> ax[0].label_outer()\n\nOr on a logarithmic scale, and using a larger hop\n\n>>> hop_length = 1024\n>>> D = librosa.amplitude_to_db(np.abs(librosa.stft(y, hop_length=hop_length)),\n...                             ref=np.max)\n>>> librosa.display.specshow(D, y_axis='log', sr=sr, hop_length=hop_length,\n...                          x_axis='time', ax=ax[1])\n>>> ax[1].set(title='Log-frequency power spectrogram')\n>>> ax[1].label_outer()\n>>> fig.colorbar(img, ax=ax, format=\"%+2.f dB\")", "parameters": {"type": "object", "properties": {"data": {"type": "np.ndarray"}, "x_axis": {"type": "optional[str]", "default": null}, "y_axis": {"type": "optional[str]", "default": null}, "sr": {"type": "float", "default": 22050}}}}}
{"task_id": "BigCodeBench/227", "data": {"name": "matplotlib.pyplot.gcf", "type": "callable", "signature": "()", "description": "Get the current figure.\n\nIf there is currently no figure on the pyplot figure stack, a new one is\ncreated using `~.pyplot.figure()`.  (To test whether there is currently a\nfigure on the pyplot figure stack, check whether `~.pyplot.get_fignums()`\nis empty.)", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/227", "data": {"name": "matplotlib.pyplot.title", "type": "callable", "signature": "(label: 'str')", "description": "Set a title for the Axes.\n\nSet one of the three available Axes titles. The available titles\nare positioned above the Axes in the center, flush with the left\nedge, and flush with the right edge.\n\nParameters\n----------\nlabel : str\n    Text to use for the title\n\nfontdict : dict\n\n    .. admonition:: Discouraged\n\n       The use of *fontdict* is discouraged. Parameters should be passed as\n       individual keyword arguments or using dictionary-unpacking\n       ``set_title(..., **fontdict)``.\n\n    A dictionary controlling the appearance of the title text,\n    the default *fontdict* is::\n\n       {'fontsize': rcParams['axes.titlesize'],\n        'fontweight': rcParams['axes.titleweight'],\n        'color': rcParams['axes.titlecolor'],\n        'verticalalignment': 'baseline',\n        'horizontalalignment': loc}\n\nloc : {'center', 'left', 'right'}, default: :rc:`axes.titlelocation`\n    Which title to set.\n\ny : float, default: :rc:`axes.titley`\n    Vertical Axes location for the title (1.0 is the top).  If\n    None (the default) and :rc:`axes.titley` is also None, y is\n    determined automatically to avoid decorators on the Axes.\n\npad : float, default: :rc:`axes.titlepad`\n    The offset of the title from the top of the Axes, in points.\n\nReturns\n-------\n`.Text`\n    The matplotlib text instance representing the title\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    Other keyword arguments are text properties, see `.Text` for a list\n    of valid text properties.", "parameters": {"type": "object", "properties": {"label": {"type": "str"}}}}}
{"task_id": "BigCodeBench/227", "data": {"name": "matplotlib.pyplot.colorbar", "type": "callable", "signature": "(**kwargs) -> 'Colorbar)", "description": "Add a colorbar to a plot.\n\nParameters\n----------\nmappable\n    The `matplotlib.cm.ScalarMappable` (i.e., `.AxesImage`,\n    `.ContourSet`, etc.) described by this colorbar.  This argument is\n    mandatory for the `.Figure.colorbar` method but optional for the\n    `.pyplot.colorbar` function, which sets the default to the current\n    image.\n\n    Note that one can create a `.ScalarMappable` \"on-the-fly\" to\n    generate colorbars not attached to a previously drawn artist, e.g.\n    ::\n\n        fig.colorbar(cm.ScalarMappable(norm=norm, cmap=cmap), ax=ax)\n\ncax : `~matplotlib.axes.Axes`, optional\n    Axes into which the colorbar will be drawn.  If `None`, then a new\n    Axes is created and the space for it will be stolen from the Axes(s)\n    specified in *ax*.\n\nax : `~matplotlib.axes.Axes` or iterable or `numpy.ndarray` of Axes, optional\n    The one or more parent Axes from which space for a new colorbar Axes\n    will be stolen. This parameter is only used if *cax* is not set.\n\n    Defaults to the Axes that contains the mappable used to create the\n    colorbar.\n\nuse_gridspec : bool, optional\n    If *cax* is ``None``, a new *cax* is created as an instance of\n    Axes.  If *ax* is positioned with a subplotspec and *use_gridspec*\n    is ``True``, then *cax* is also positioned with a subplotspec.\n\nReturns\n-------\ncolorbar : `~matplotlib.colorbar.Colorbar`\n\nOther Parameters\n----------------\n\nlocation : None or {'left', 'right', 'top', 'bottom'}\n    The location, relative to the parent axes, where the colorbar axes\n    is created.  It also determines the *orientation* of the colorbar\n    (colorbars on the left and right are vertical, colorbars at the top\n    and bottom are horizontal).  If None, the location will come from the\n    *orientation* if it is set (vertical colorbars on the right, horizontal\n    ones at the bottom), or default to 'right' if *orientation* is unset.\n\norientation : None or {'vertical', 'horizontal'}\n    The orientation of the colorbar.  It is preferable to set the *location*\n    of the colorbar, as that also determines the *orientation*; passing\n    incompatible values for *location* and *orientation* raises an exception.\n\nfraction : float, default: 0.15\n    Fraction of original axes to use for colorbar.\n\nshrink : float, default: 1.0\n    Fraction by which to multiply the size of the colorbar.\n\naspect : float, default: 20\n    Ratio of long to short dimensions.\n\npad : float, default: 0.05 if vertical, 0.15 if horizontal\n    Fraction of original axes between colorbar and new image axes.\n\nanchor : (float, float), optional\n    The anchor point of the colorbar axes.\n    Defaults to (0.0, 0.5) if vertical; (0.5, 1.0) if horizontal.\n\npanchor : (float, float), or *False*, optional\n    The anchor point of the colorbar parent axes. If *False*, the parent\n    axes' anchor will be unchanged.\n    Defaults to (1.0, 0.5) if vertical; (0.5, 0.0) if horizontal.\n\nextend : {'neither', 'both', 'min', 'max'}\n    Make pointed end(s) for out-of-range values (unless 'neither').  These are\n    set for a given colormap using the colormap set_under and set_over methods.\n\nextendfrac : {*None*, 'auto', length, lengths}\n    If set to *None*, both the minimum and maximum triangular colorbar\n    extensions will have a length of 5% of the interior colorbar length (this\n    is the default setting).\n\n    If set to 'auto', makes the triangular colorbar extensions the same lengths\n    as the interior boxes (when *spacing* is set to 'uniform') or the same\n    lengths as the respective adjacent interior boxes (when *spacing* is set to\n    'proportional').\n\n    If a scalar, indicates the length of both the minimum and maximum\n    triangular colorbar extensions as a fraction of the interior colorbar\n    length.  A two-element sequence of fractions may also be given, indicating\n    the lengths of the minimum and maximum colorbar extensions respectively as\n    a fraction of the interior colorbar length.\n\nextendrect : bool\n    If *False* the minimum and maximum colorbar extensions will be triangular\n    (the default).  If *True* the extensions will be rectangular.\n\nspacing : {'uniform', 'proportional'}\n    For discrete colorbars (`.BoundaryNorm` or contours), 'uniform' gives each\n    color the same space; 'proportional' makes the space proportional to the\n    data interval.\n\nticks : None or list of ticks or Locator\n    If None, ticks are determined automatically from the input.\n\nformat : None or str or Formatter\n    If None, `~.ticker.ScalarFormatter` is used.\n    Format strings, e.g., ``\"%4.2e\"`` or ``\"{x:.2e}\"``, are supported.\n    An alternative `~.ticker.Formatter` may be given instead.\n\ndrawedges : bool\n    Whether to draw lines at color boundaries.\n\nlabel : str\n    The label on the colorbar's long axis.\n\nboundaries, values : None or a sequence\n    If unset, the colormap will be displayed on a 0-1 scale.\n    If sequences, *values* must have a length 1 less than *boundaries*.  For\n    each region delimited by adjacent entries in *boundaries*, the color mapped\n    to the corresponding value in values will be used.\n    Normally only useful for indexed colors (i.e. ``norm=NoNorm()``) or other\n    unusual circumstances.\n\nNotes\n-----\nIf *mappable* is a `~.contour.ContourSet`, its *extend* kwarg is\nincluded automatically.\n\nThe *shrink* kwarg provides a simple way to scale the colorbar with\nrespect to the axes. Note that if *cax* is specified, it determines the\nsize of the colorbar, and *shrink* and *aspect* are ignored.\n\nFor more precise control, you can manually specify the positions of the\naxes objects in which the mappable and the colorbar are drawn.  In this\ncase, do not use any of the axes properties kwargs.\n\nIt is known that some vector graphics viewers (svg and pdf) render\nwhite gaps between segments of the colorbar.  This is due to bugs in\nthe viewers, not Matplotlib.  As a workaround, the colorbar can be\nrendered with overlapping segments::\n\n    cbar = colorbar()\n    cbar.solids.set_edgecolor(\"face\")\n    draw()\n\nHowever, this has negative consequences in other circumstances, e.g.\nwith semi-transparent images (alpha < 1) and colorbar extensions;\ntherefore, this workaround is not used by default (see issue #1188).", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/227", "data": {"name": "numpy.max", "type": "callable", "signature": "(a)", "description": "Return the maximum of an array or maximum along an axis.\n\nParameters\n----------\na : array_like\n    Input data.\naxis : None or int or tuple of ints, optional\n    Axis or axes along which to operate.  By default, flattened input is\n    used.\n\n    .. versionadded:: 1.7.0\n\n    If this is a tuple of ints, the maximum is selected over multiple axes,\n    instead of a single axis or all the axes as before.\nout : ndarray, optional\n    Alternative output array in which to place the result.  Must\n    be of the same shape and buffer length as the expected output.\n    See :ref:`ufuncs-output-type` for more details.\n\nkeepdims : bool, optional\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\n    If the default value is passed, then `keepdims` will not be\n    passed through to the `amax` method of sub-classes of\n    `ndarray`, however any non-default value will be.  If the\n    sub-class' method does not implement `keepdims` any\n    exceptions will be raised.\n\ninitial : scalar, optional\n    The minimum value of an output element. Must be present to allow\n    computation on empty slice. See `~numpy.ufunc.reduce` for details.\n\n    .. versionadded:: 1.15.0\n\nwhere : array_like of bool, optional\n    Elements to compare for the maximum. See `~numpy.ufunc.reduce`\n    for details.\n\n    .. versionadded:: 1.17.0\n\nReturns\n-------\namax : ndarray or scalar\n    Maximum of `a`. If `axis` is None, the result is a scalar value.\n    If `axis` is given, the result is an array of dimension\n    ``a.ndim - 1``.\n\nSee Also\n--------\namin :\n    The minimum value of an array along a given axis, propagating any NaNs.\nnanmax :\n    The maximum value of an array along a given axis, ignoring any NaNs.\nmaximum :\n    Element-wise maximum of two arrays, propagating any NaNs.\nfmax :\n    Element-wise maximum of two arrays, ignoring any NaNs.\nargmax :\n    Return the indices of the maximum values.\n\nnanmin, minimum, fmin\n\nNotes\n-----\nNaN values are propagated, that is if at least one item is NaN, the\ncorresponding max value will be NaN as well. To ignore NaN values\n(MATLAB behavior), please use nanmax.\n\nDon't use `amax` for element-wise comparison of 2 arrays; when\n``a.shape[0]`` is 2, ``maximum(a[0], a[1])`` is faster than\n``amax(a, axis=0)``.\n\nExamples\n--------\n>>> a = np.arange(4).reshape((2,2))\n>>> a\narray([[0, 1],\n       [2, 3]])\n>>> np.amax(a)           # Maximum of the flattened array\n3\n>>> np.amax(a, axis=0)   # Maxima along the first axis\narray([2, 3])\n>>> np.amax(a, axis=1)   # Maxima along the second axis\narray([1, 3])\n>>> np.amax(a, where=[False, True], initial=-1, axis=0)\narray([-1,  3])\n>>> b = np.arange(5, dtype=float)\n>>> b[2] = np.NaN\n>>> np.amax(b)\nnan\n>>> np.amax(b, where=~np.isnan(b), initial=-1)\n4.0\n>>> np.nanmax(b)\n4.0\n\nYou can use an initial value to compute the maximum of an empty slice, or\nto initialize it to a different value:\n\n>>> np.amax([[-50], [10]], axis=-1, initial=0)\narray([ 0, 10])\n\nNotice that the initial value is used as one of the elements for which the\nmaximum is determined, unlike for the default argument Python's max\nfunction, which is only used for empty iterables.\n\n>>> np.amax([5], initial=6)\n6\n>>> max([5], default=6)\n5", "parameters": {"type": "object", "properties": {"a": {}}}}}
{"task_id": "BigCodeBench/227", "data": {"name": "numpy.array", "type": "callable", "signature": "(*args, **kwargs)", "description": "array(object, dtype=None, *, copy=True, order='K', subok=False, ndmin=0,\n      like=None)\n\nCreate an array.\n\nParameters\n----------\nobject : array_like\n    An array, any object exposing the array interface, an object whose\n    __array__ method returns an array, or any (nested) sequence.\n    If object is a scalar, a 0-dimensional array containing object is\n    returned.\ndtype : data-type, optional\n    The desired data-type for the array.  If not given, then the type will\n    be determined as the minimum type required to hold the objects in the\n    sequence.\ncopy : bool, optional\n    If true (default), then the object is copied.  Otherwise, a copy will\n    only be made if __array__ returns a copy, if obj is a nested sequence,\n    or if a copy is needed to satisfy any of the other requirements\n    (`dtype`, `order`, etc.).\norder : {'K', 'A', 'C', 'F'}, optional\n    Specify the memory layout of the array. If object is not an array, the\n    newly created array will be in C order (row major) unless 'F' is\n    specified, in which case it will be in Fortran order (column major).\n    If object is an array the following holds.\n\n    ===== ========= ===================================================\n    order  no copy                     copy=True\n    ===== ========= ===================================================\n    'K'   unchanged F & C order preserved, otherwise most similar order\n    'A'   unchanged F order if input is F and not C, otherwise C order\n    'C'   C order   C order\n    'F'   F order   F order\n    ===== ========= ===================================================\n\n    When ``copy=False`` and a copy is made for other reasons, the result is\n    the same as if ``copy=True``, with some exceptions for 'A', see the\n    Notes section. The default order is 'K'.\nsubok : bool, optional\n    If True, then sub-classes will be passed-through, otherwise\n    the returned array will be forced to be a base-class array (default).\nndmin : int, optional\n    Specifies the minimum number of dimensions that the resulting\n    array should have.  Ones will be prepended to the shape as\n    needed to meet this requirement.\nlike : array_like, optional\n    Reference object to allow the creation of arrays which are not\n    NumPy arrays. If an array-like passed in as ``like`` supports\n    the ``__array_function__`` protocol, the result will be defined\n    by it. In this case, it ensures the creation of an array object\n    compatible with that passed in via this argument.\n\n    .. versionadded:: 1.20.0\n\nReturns\n-------\nout : ndarray\n    An array object satisfying the specified requirements.\n\nSee Also\n--------\nempty_like : Return an empty array with shape and type of input.\nones_like : Return an array of ones with shape and type of input.\nzeros_like : Return an array of zeros with shape and type of input.\nfull_like : Return a new array with shape of input filled with value.\nempty : Return a new uninitialized array.\nones : Return a new array setting values to one.\nzeros : Return a new array setting values to zero.\nfull : Return a new array of given shape filled with value.\n\n\nNotes\n-----\nWhen order is 'A' and `object` is an array in neither 'C' nor 'F' order,\nand a copy is forced by a change in dtype, then the order of the result is\nnot necessarily 'C' as expected. This is likely a bug.\n\nExamples\n--------\n>>> np.array([1, 2, 3])\narray([1, 2, 3])\n\nUpcasting:\n\n>>> np.array([1, 2, 3.0])\narray([ 1.,  2.,  3.])\n\nMore than one dimension:\n\n>>> np.array([[1, 2], [3, 4]])\narray([[1, 2],\n       [3, 4]])\n\nMinimum dimensions 2:\n\n>>> np.array([1, 2, 3], ndmin=2)\narray([[1, 2, 3]])\n\nType provided:\n\n>>> np.array([1, 2, 3], dtype=complex)\narray([ 1.+0.j,  2.+0.j,  3.+0.j])\n\nData-type consisting of more than one element:\n\n>>> x = np.array([(1,2),(3,4)],dtype=[('a','<i4'),('b','<i4')])\n>>> x['a']\narray([1, 3])\n\nCreating an array from sub-classes:\n\n>>> np.array(np.mat('1 2; 3 4'))\narray([[1, 2],\n       [3, 4]])\n\n>>> np.array(np.mat('1 2; 3 4'), subok=True)\nmatrix([[1, 2],\n        [3, 4]])", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/227", "data": {"name": "numpy.mean", "type": "callable", "signature": "(a)", "description": "Compute the arithmetic mean along the specified axis.\n\nReturns the average of the array elements.  The average is taken over\nthe flattened array by default, otherwise over the specified axis.\n`float64` intermediate and return values are used for integer inputs.\n\nParameters\n----------\na : array_like\n    Array containing numbers whose mean is desired. If `a` is not an\n    array, a conversion is attempted.\naxis : None or int or tuple of ints, optional\n    Axis or axes along which the means are computed. The default is to\n    compute the mean of the flattened array.\n\n    .. versionadded:: 1.7.0\n\n    If this is a tuple of ints, a mean is performed over multiple axes,\n    instead of a single axis or all the axes as before.\ndtype : data-type, optional\n    Type to use in computing the mean.  For integer inputs, the default\n    is `float64`; for floating point inputs, it is the same as the\n    input dtype.\nout : ndarray, optional\n    Alternate output array in which to place the result.  The default\n    is ``None``; if provided, it must have the same shape as the\n    expected output, but the type will be cast if necessary.\n    See :ref:`ufuncs-output-type` for more details.\n\nkeepdims : bool, optional\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\n    If the default value is passed, then `keepdims` will not be\n    passed through to the `mean` method of sub-classes of\n    `ndarray`, however any non-default value will be.  If the\n    sub-class' method does not implement `keepdims` any\n    exceptions will be raised.\n\nwhere : array_like of bool, optional\n    Elements to include in the mean. See `~numpy.ufunc.reduce` for details.\n\n    .. versionadded:: 1.20.0\n\nReturns\n-------\nm : ndarray, see dtype parameter above\n    If `out=None`, returns a new array containing the mean values,\n    otherwise a reference to the output array is returned.\n\nSee Also\n--------\naverage : Weighted average\nstd, var, nanmean, nanstd, nanvar\n\nNotes\n-----\nThe arithmetic mean is the sum of the elements along the axis divided\nby the number of elements.\n\nNote that for floating-point input, the mean is computed using the\nsame precision the input has.  Depending on the input data, this can\ncause the results to be inaccurate, especially for `float32` (see\nexample below).  Specifying a higher-precision accumulator using the\n`dtype` keyword can alleviate this issue.\n\nBy default, `float16` results are computed using `float32` intermediates\nfor extra precision.\n\nExamples\n--------\n>>> a = np.array([[1, 2], [3, 4]])\n>>> np.mean(a)\n2.5\n>>> np.mean(a, axis=0)\narray([2., 3.])\n>>> np.mean(a, axis=1)\narray([1.5, 3.5])\n\nIn single precision, `mean` can be inaccurate:\n\n>>> a = np.zeros((2, 512*512), dtype=np.float32)\n>>> a[0, :] = 1.0\n>>> a[1, :] = 0.1\n>>> np.mean(a)\n0.54999924\n\nComputing the mean in float64 is more accurate:\n\n>>> np.mean(a, dtype=np.float64)\n0.55000000074505806 # may vary\n\nSpecifying a where argument:\n\n>>> a = np.array([[5, 9, 13], [14, 10, 12], [11, 15, 19]])\n>>> np.mean(a)\n12.0\n>>> np.mean(a, where=[[True], [False], [False]])\n9.0", "parameters": {"type": "object", "properties": {"a": {}}}}}
{"task_id": "BigCodeBench/227", "data": {"name": "numpy.sqrt", "type": "callable", "signature": "(*args, **kwargs)", "description": "sqrt(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n\nReturn the non-negative square-root of an array, element-wise.\n\nParameters\n----------\nx : array_like\n    The values whose square-roots are required.\nout : ndarray, None, or tuple of ndarray and None, optional\n    A location into which the result is stored. If provided, it must have\n    a shape that the inputs broadcast to. If not provided or None,\n    a freshly-allocated array is returned. A tuple (possible only as a\n    keyword argument) must have length equal to the number of outputs.\nwhere : array_like, optional\n    This condition is broadcast over the input. At locations where the\n    condition is True, the `out` array will be set to the ufunc result.\n    Elsewhere, the `out` array will retain its original value.\n    Note that if an uninitialized `out` array is created via the default\n    ``out=None``, locations within it where the condition is False will\n    remain uninitialized.\n**kwargs\n    For other keyword-only arguments, see the\n    :ref:`ufunc docs <ufuncs.kwargs>`.\n\nReturns\n-------\ny : ndarray\n    An array of the same shape as `x`, containing the positive\n    square-root of each element in `x`.  If any element in `x` is\n    complex, a complex array is returned (and the square-roots of\n    negative reals are calculated).  If all of the elements in `x`\n    are real, so is `y`, with negative elements returning ``nan``.\n    If `out` was provided, `y` is a reference to it.\n    This is a scalar if `x` is a scalar.\n\nSee Also\n--------\nemath.sqrt\n    A version which returns complex numbers when given negative reals.\n    Note: 0.0 and -0.0 are handled differently for complex inputs.\n\nNotes\n-----\n*sqrt* has--consistent with common convention--as its branch cut the\nreal \"interval\" [`-inf`, 0), and is continuous from above on it.\nA branch cut is a curve in the complex plane across which a given\ncomplex function fails to be continuous.\n\nExamples\n--------\n>>> np.sqrt([1,4,9])\narray([ 1.,  2.,  3.])\n\n>>> np.sqrt([4, -1, -3+4J])\narray([ 2.+0.j,  0.+1.j,  1.+2.j])\n\n>>> np.sqrt([4, -1, np.inf])\narray([ 2., nan, inf])", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/227", "data": {"name": "numpy.abs", "type": "callable", "signature": "(*args, **kwargs)", "description": "absolute(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n\nCalculate the absolute value element-wise.\n\n``np.abs`` is a shorthand for this function.\n\nParameters\n----------\nx : array_like\n    Input array.\nout : ndarray, None, or tuple of ndarray and None, optional\n    A location into which the result is stored. If provided, it must have\n    a shape that the inputs broadcast to. If not provided or None,\n    a freshly-allocated array is returned. A tuple (possible only as a\n    keyword argument) must have length equal to the number of outputs.\nwhere : array_like, optional\n    This condition is broadcast over the input. At locations where the\n    condition is True, the `out` array will be set to the ufunc result.\n    Elsewhere, the `out` array will retain its original value.\n    Note that if an uninitialized `out` array is created via the default\n    ``out=None``, locations within it where the condition is False will\n    remain uninitialized.\n**kwargs\n    For other keyword-only arguments, see the\n    :ref:`ufunc docs <ufuncs.kwargs>`.\n\nReturns\n-------\nabsolute : ndarray\n    An ndarray containing the absolute value of\n    each element in `x`.  For complex input, ``a + ib``, the\n    absolute value is :math:`\\sqrt{ a^2 + b^2 }`.\n    This is a scalar if `x` is a scalar.\n\nExamples\n--------\n>>> x = np.array([-1.2, 1.2])\n>>> np.absolute(x)\narray([ 1.2,  1.2])\n>>> np.absolute(1.2 + 1j)\n1.5620499351813308\n\nPlot the function over ``[-10, 10]``:\n\n>>> import matplotlib.pyplot as plt\n\n>>> x = np.linspace(start=-10, stop=10, num=101)\n>>> plt.plot(x, np.absolute(x))\n>>> plt.show()\n\nPlot the function over the complex plane:\n\n>>> xx = x + 1j * x[:, np.newaxis]\n>>> plt.imshow(np.abs(xx), extent=[-10, 10, -10, 10], cmap='gray')\n>>> plt.show()\n\nThe `abs` function can be used as a shorthand for ``np.absolute`` on\nndarrays.\n\n>>> x = np.array([-1.2, 1.2])\n>>> abs(x)\narray([1.2, 1.2])", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/227", "data": {"name": "numpy.log10", "type": "callable", "signature": "(*args, **kwargs)", "description": "log10(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n\nReturn the base 10 logarithm of the input array, element-wise.\n\nParameters\n----------\nx : array_like\n    Input values.\nout : ndarray, None, or tuple of ndarray and None, optional\n    A location into which the result is stored. If provided, it must have\n    a shape that the inputs broadcast to. If not provided or None,\n    a freshly-allocated array is returned. A tuple (possible only as a\n    keyword argument) must have length equal to the number of outputs.\nwhere : array_like, optional\n    This condition is broadcast over the input. At locations where the\n    condition is True, the `out` array will be set to the ufunc result.\n    Elsewhere, the `out` array will retain its original value.\n    Note that if an uninitialized `out` array is created via the default\n    ``out=None``, locations within it where the condition is False will\n    remain uninitialized.\n**kwargs\n    For other keyword-only arguments, see the\n    :ref:`ufunc docs <ufuncs.kwargs>`.\n\nReturns\n-------\ny : ndarray\n    The logarithm to the base 10 of `x`, element-wise. NaNs are\n    returned where x is negative.\n    This is a scalar if `x` is a scalar.\n\nSee Also\n--------\nemath.log10\n\nNotes\n-----\nLogarithm is a multivalued function: for each `x` there is an infinite\nnumber of `z` such that `10**z = x`. The convention is to return the\n`z` whose imaginary part lies in `[-pi, pi]`.\n\nFor real-valued input data types, `log10` always returns real output.\nFor each value that cannot be expressed as a real number or infinity,\nit yields ``nan`` and sets the `invalid` floating point error flag.\n\nFor complex-valued input, `log10` is a complex analytical function that\nhas a branch cut `[-inf, 0]` and is continuous from above on it.\n`log10` handles the floating-point negative zero as an infinitesimal\nnegative number, conforming to the C99 standard.\n\nReferences\n----------\n.. [1] M. Abramowitz and I.A. Stegun, \"Handbook of Mathematical Functions\",\n       10th printing, 1964, pp. 67.\n       https://personal.math.ubc.ca/~cbm/aands/page_67.htm\n.. [2] Wikipedia, \"Logarithm\". https://en.wikipedia.org/wiki/Logarithm\n\nExamples\n--------\n>>> np.log10([1e-15, -3.])\narray([-15.,  nan])", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/227", "data": {"name": "os.path.isfile", "type": "callable", "signature": "(path)", "description": "Test whether a path is a regular file", "parameters": {"type": "object", "properties": {"path": {}}}}}
{"task_id": "BigCodeBench/227", "data": {"name": "soundfile.read", "type": "callable", "signature": "(file)", "description": "Provide audio data from a sound file as NumPy array.\n\nBy default, the whole file is read from the beginning, but the\nposition to start reading can be specified with *start* and the\nnumber of frames to read can be specified with *frames*.\nAlternatively, a range can be specified with *start* and *stop*.\n\nIf there is less data left in the file than requested, the rest of\nthe frames are filled with *fill_value*.\nIf no *fill_value* is specified, a smaller array is returned.\n\nParameters\n----------\nfile : str or int or file-like object\n    The file to read from.  See `SoundFile` for details.\nframes : int, optional\n    The number of frames to read. If *frames* is negative, the whole\n    rest of the file is read.  Not allowed if *stop* is given.\nstart : int, optional\n    Where to start reading.  A negative value counts from the end.\nstop : int, optional\n    The index after the last frame to be read.  A negative value\n    counts from the end.  Not allowed if *frames* is given.\ndtype : {'float64', 'float32', 'int32', 'int16'}, optional\n    Data type of the returned array, by default ``'float64'``.\n    Floating point audio data is typically in the range from\n    ``-1.0`` to ``1.0``.  Integer data is in the range from\n    ``-2**15`` to ``2**15-1`` for ``'int16'`` and from ``-2**31`` to\n    ``2**31-1`` for ``'int32'``.\n\n    .. note:: Reading int values from a float file will *not*\n        scale the data to [-1.0, 1.0). If the file contains\n        ``np.array([42.6], dtype='float32')``, you will read\n        ``np.array([43], dtype='int32')`` for ``dtype='int32'``.\n\nReturns\n-------\naudiodata : `numpy.ndarray` or type(out)\n    A two-dimensional (frames x channels) NumPy array is returned.\n    If the sound file has only one channel, a one-dimensional array\n    is returned.  Use ``always_2d=True`` to return a two-dimensional\n    array anyway.\n\n    If *out* was specified, it is returned.  If *out* has more\n    frames than available in the file (or if *frames* is smaller\n    than the length of *out*) and no *fill_value* is given, then\n    only a part of *out* is overwritten and a view containing all\n    valid frames is returned.\nsamplerate : int\n    The sample rate of the audio file.\n\nOther Parameters\n----------------\nalways_2d : bool, optional\n    By default, reading a mono sound file will return a\n    one-dimensional array.  With ``always_2d=True``, audio data is\n    always returned as a two-dimensional array, even if the audio\n    file has only one channel.\nfill_value : float, optional\n    If more frames are requested than available in the file, the\n    rest of the output is be filled with *fill_value*.  If\n    *fill_value* is not specified, a smaller array is returned.\nout : `numpy.ndarray` or subclass, optional\n    If *out* is specified, the data is written into the given array\n    instead of creating a new array.  In this case, the arguments\n    *dtype* and *always_2d* are silently ignored!  If *frames* is\n    not given, it is obtained from the length of *out*.\nsamplerate, channels, format, subtype, endian, closefd\n    See `SoundFile`.\n\nExamples\n--------\n>>> import soundfile as sf\n>>> data, samplerate = sf.read('stereo_file.wav')\n>>> data\narray([[ 0.71329652,  0.06294799],\n       [-0.26450912, -0.38874483],\n       ...\n       [ 0.67398441, -0.11516333]])\n>>> samplerate\n44100", "parameters": {"type": "object", "properties": {"file": {}}}}}
{"task_id": "BigCodeBench/239", "data": {"name": "matplotlib.pyplot.close", "type": "callable", "signature": "(fig: \"None | int | str | Figure | Literal['all']\" = None) -> 'None)", "description": "Close a figure window.\n\nParameters\n----------\nfig : None or int or str or `.Figure`\n    The figure to close. There are a number of ways to specify this:\n\n    - *None*: the current figure\n    - `.Figure`: the given `.Figure` instance\n    - ``int``: a figure number\n    - ``str``: a figure name\n    - 'all': all figures", "parameters": {"type": "object", "properties": {"fig": {"type": ["\"none", "figure", "str", "literal['all']\"", "integer"], "default": null}}}}}
{"task_id": "BigCodeBench/239", "data": {"name": "matplotlib.pyplot.subplots", "type": "class", "signature": "()", "description": "Create a figure and a set of subplots.\n\nThis utility wrapper makes it convenient to create common layouts of\nsubplots, including the enclosing figure object, in a single call.\n\nParameters\n----------\nnrows, ncols : int, default: 1\n    Number of rows/columns of the subplot grid.\n\nsharex, sharey : bool or {'none', 'all', 'row', 'col'}, default: False\n    Controls sharing of properties among x (*sharex*) or y (*sharey*)\n    axes:\n\n    - True or 'all': x- or y-axis will be shared among all subplots.\n    - False or 'none': each subplot x- or y-axis will be independent.\n    - 'row': each subplot row will share an x- or y-axis.\n    - 'col': each subplot column will share an x- or y-axis.\n\n    When subplots have a shared x-axis along a column, only the x tick\n    labels of the bottom subplot are created. Similarly, when subplots\n    have a shared y-axis along a row, only the y tick labels of the first\n    column subplot are created. To later turn other subplots' ticklabels\n    on, use `~matplotlib.axes.Axes.tick_params`.\n\n    When subplots have a shared axis that has units, calling\n    `~matplotlib.axis.Axis.set_units` will update each axis with the\n    new units.\n\nsqueeze : bool, default: True\n    - If True, extra dimensions are squeezed out from the returned\n      array of `~matplotlib.axes.Axes`:\n\n      - if only one subplot is constructed (nrows=ncols=1), the\n        resulting single Axes object is returned as a scalar.\n      - for Nx1 or 1xM subplots, the returned object is a 1D numpy\n        object array of Axes objects.\n      - for NxM, subplots with N>1 and M>1 are returned as a 2D array.\n\n    - If False, no squeezing at all is done: the returned Axes object is\n      always a 2D array containing Axes instances, even if it ends up\n      being 1x1.\n\nwidth_ratios : array-like of length *ncols*, optional\n    Defines the relative widths of the columns. Each column gets a\n    relative width of ``width_ratios[i] / sum(width_ratios)``.\n    If not given, all columns will have the same width.  Equivalent\n    to ``gridspec_kw={'width_ratios': [...]}``.\n\nheight_ratios : array-like of length *nrows*, optional\n    Defines the relative heights of the rows. Each row gets a\n    relative height of ``height_ratios[i] / sum(height_ratios)``.\n    If not given, all rows will have the same height. Convenience\n    for ``gridspec_kw={'height_ratios': [...]}``.\n\nsubplot_kw : dict, optional\n    Dict with keywords passed to the\n    `~matplotlib.figure.Figure.add_subplot` call used to create each\n    subplot.\n\ngridspec_kw : dict, optional\n    Dict with keywords passed to the `~matplotlib.gridspec.GridSpec`\n    constructor used to create the grid the subplots are placed on.\n\n**fig_kw\n    All additional keyword arguments are passed to the\n    `.pyplot.figure` call.\n\nReturns\n-------\nfig : `.Figure`\n\nax : `~matplotlib.axes.Axes` or array of Axes\n    *ax* can be either a single `~.axes.Axes` object, or an array of Axes\n    objects if more than one subplot was created.  The dimensions of the\n    resulting array can be controlled with the squeeze keyword, see above.\n\n    Typical idioms for handling the return value are::\n\n        # using the variable ax for single a Axes\n        fig, ax = plt.subplots()\n\n        # using the variable axs for multiple Axes\n        fig, axs = plt.subplots(2, 2)\n\n        # using tuple unpacking for multiple Axes\n        fig, (ax1, ax2) = plt.subplots(1, 2)\n        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n\n    The names ``ax`` and pluralized ``axs`` are preferred over ``axes``\n    because for the latter it's not clear if it refers to a single\n    `~.axes.Axes` instance or a collection of these.\n\nSee Also\n--------\n.pyplot.figure\n.pyplot.subplot\n.pyplot.axes\n.Figure.subplots\n.Figure.add_subplot\n\nExamples\n--------\n::\n\n    # First create some toy data:\n    x = np.linspace(0, 2*np.pi, 400)\n    y = np.sin(x**2)\n\n    # Create just a figure and only one subplot\n    fig, ax = plt.subplots()\n    ax.plot(x, y)\n    ax.set_title('Simple plot')\n\n    # Create two subplots and unpack the output array immediately\n    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n    ax1.plot(x, y)\n    ax1.set_title('Sharing Y axis')\n    ax2.scatter(x, y)\n\n    # Create four polar axes and access them through the returned array\n    fig, axs = plt.subplots(2, 2, subplot_kw=dict(projection=\"polar\"))\n    axs[0, 0].plot(x, y)\n    axs[1, 1].scatter(x, y)\n\n    # Share a X axis with each column of subplots\n    plt.subplots(2, 2, sharex='col')\n\n    # Share a Y axis with each row of subplots\n    plt.subplots(2, 2, sharey='row')\n\n    # Share both X and Y axes with all subplots\n    plt.subplots(2, 2, sharex='all', sharey='all')\n\n    # Note that this is the same as\n    plt.subplots(2, 2, sharex=True, sharey=True)\n\n    # Create figure number 10 with a single subplot\n    # and clears it if it already exists.\n    fig, ax = plt.subplots(num=10, clear=True)", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/239", "data": {"name": "matplotlib.pyplot.subplots[1].legend", "type": "method", "signature": "(*args, **kwargs)", "description": "Place a legend on the Axes.\n\nCall signatures::\n\n    legend()\n    legend(handles, labels)\n    legend(handles=handles)\n    legend(labels)\n\nThe call signatures correspond to the following different ways to use\nthis method:\n\n**1. Automatic detection of elements to be shown in the legend**\n\nThe elements to be added to the legend are automatically determined,\nwhen you do not pass in any extra arguments.\n\nIn this case, the labels are taken from the artist. You can specify\nthem either at artist creation or by calling the\n:meth:`~.Artist.set_label` method on the artist::\n\n    ax.plot([1, 2, 3], label='Inline label')\n    ax.legend()\n\nor::\n\n    line, = ax.plot([1, 2, 3])\n    line.set_label('Label via method')\n    ax.legend()\n\n.. note::\n    Specific artists can be excluded from the automatic legend element\n    selection by using a label starting with an underscore, \"_\".\n    A string starting with an underscore is the default label for all\n    artists, so calling `.Axes.legend` without any arguments and\n    without setting the labels manually will result in no legend being\n    drawn.\n\n\n**2. Explicitly listing the artists and labels in the legend**\n\nFor full control of which artists have a legend entry, it is possible\nto pass an iterable of legend artists followed by an iterable of\nlegend labels respectively::\n\n    ax.legend([line1, line2, line3], ['label1', 'label2', 'label3'])\n\n\n**3. Explicitly listing the artists in the legend**\n\nThis is similar to 2, but the labels are taken from the artists'\nlabel properties. Example::\n\n    line1, = ax.plot([1, 2, 3], label='label1')\n    line2, = ax.plot([1, 2, 3], label='label2')\n    ax.legend(handles=[line1, line2])\n\n\n**4. Labeling existing plot elements**\n\n.. admonition:: Discouraged\n\n    This call signature is discouraged, because the relation between\n    plot elements and labels is only implicit by their order and can\n    easily be mixed up.\n\nTo make a legend for all artists on an Axes, call this function with\nan iterable of strings, one for each legend item. For example::\n\n    ax.plot([1, 2, 3])\n    ax.plot([5, 6, 7])\n    ax.legend(['First line', 'Second line'])\n\n\nParameters\n----------\nhandles : list of (`.Artist` or tuple of `.Artist`), optional\n    A list of Artists (lines, patches) to be added to the legend.\n    Use this together with *labels*, if you need full control on what\n    is shown in the legend and the automatic mechanism described above\n    is not sufficient.\n\n    The length of handles and labels should be the same in this\n    case. If they are not, they are truncated to the smaller length.\n\n    If an entry contains a tuple, then the legend handler for all Artists in the\n    tuple will be placed alongside a single label.\n\nlabels : list of str, optional\n    A list of labels to show next to the artists.\n    Use this together with *handles*, if you need full control on what\n    is shown in the legend and the automatic mechanism described above\n    is not sufficient.\n\nReturns\n-------\n`~matplotlib.legend.Legend`\n\nOther Parameters\n----------------\n\nloc : str or pair of floats, default: :rc:`legend.loc`\n    The location of the legend.\n\n    The strings ``'upper left'``, ``'upper right'``, ``'lower left'``,\n    ``'lower right'`` place the legend at the corresponding corner of the\n    axes.\n\n    The strings ``'upper center'``, ``'lower center'``, ``'center left'``,\n    ``'center right'`` place the legend at the center of the corresponding edge\n    of the axes.\n\n    The string ``'center'`` places the legend at the center of the axes.\n\n    The string ``'best'`` places the legend at the location, among the nine\n    locations defined so far, with the minimum overlap with other drawn\n    artists.  This option can be quite slow for plots with large amounts of\n    data; your plotting speed may benefit from providing a specific location.\n\n    The location can also be a 2-tuple giving the coordinates of the lower-left\n    corner of the legend in axes coordinates (in which case *bbox_to_anchor*\n    will be ignored).\n\n    For back-compatibility, ``'center right'`` (but no other location) can also\n    be spelled ``'right'``, and each \"string\" location can also be given as a\n    numeric value:\n\n    ==================   =============\n    Location String      Location Code\n    ==================   =============\n    'best' (Axes only)   0\n    'upper right'        1\n    'upper left'         2\n    'lower left'         3\n    'lower right'        4\n    'right'              5\n    'center left'        6\n    'center right'       7\n    'lower center'       8\n    'upper center'       9\n    'center'             10\n    ==================   =============\n    \nbbox_to_anchor : `.BboxBase`, 2-tuple, or 4-tuple of floats\n    Box that is used to position the legend in conjunction with *loc*.\n    Defaults to `axes.bbox` (if called as a method to `.Axes.legend`) or\n    `figure.bbox` (if `.Figure.legend`).  This argument allows arbitrary\n    placement of the legend.\n\n    Bbox coordinates are interpreted in the coordinate system given by\n    *bbox_transform*, with the default transform\n    Axes or Figure coordinates, depending on which ``legend`` is called.\n\n    If a 4-tuple or `.BboxBase` is given, then it specifies the bbox\n    ``(x, y, width, height)`` that the legend is placed in.\n    To put the legend in the best location in the bottom right\n    quadrant of the axes (or figure)::\n\n        loc='best', bbox_to_anchor=(0.5, 0., 0.5, 0.5)\n\n    A 2-tuple ``(x, y)`` places the corner of the legend specified by *loc* at\n    x, y.  For example, to put the legend's upper right-hand corner in the\n    center of the axes (or figure) the following keywords can be used::\n\n        loc='upper right', bbox_to_anchor=(0.5, 0.5)\n\nncols : int, default: 1\n    The number of columns that the legend has.\n\n    For backward compatibility, the spelling *ncol* is also supported\n    but it is discouraged. If both are given, *ncols* takes precedence.\n\nprop : None or `~matplotlib.font_manager.FontProperties` or dict\n    The font properties of the legend. If None (default), the current\n    :data:`matplotlib.rcParams` will be used.\n\nfontsize : int or {'xx-small', 'x-small', 'small', 'medium', 'large', 'x-large', 'xx-large'}\n    The font size of the legend. If the value is numeric the size will be the\n    absolute font size in points. String values are relative to the current\n    default font size. This argument is only used if *prop* is not specified.\n\nlabelcolor : str or list, default: :rc:`legend.labelcolor`\n    The color of the text in the legend. Either a valid color string\n    (for example, 'red'), or a list of color strings. The labelcolor can\n    also be made to match the color of the line or marker using 'linecolor',\n    'markerfacecolor' (or 'mfc'), or 'markeredgecolor' (or 'mec').\n\n    Labelcolor can be set globally using :rc:`legend.labelcolor`. If None,\n    use :rc:`text.color`.\n\nnumpoints : int, default: :rc:`legend.numpoints`\n    The number of marker points in the legend when creating a legend\n    entry for a `.Line2D` (line).\n\nscatterpoints : int, default: :rc:`legend.scatterpoints`\n    The number of marker points in the legend when creating\n    a legend entry for a `.PathCollection` (scatter plot).\n\nscatteryoffsets : iterable of floats, default: ``[0.375, 0.5, 0.3125]``\n    The vertical offset (relative to the font size) for the markers\n    created for a scatter plot legend entry. 0.0 is at the base the\n    legend text, and 1.0 is at the top. To draw all markers at the\n    same height, set to ``[0.5]``.\n\nmarkerscale : float, default: :rc:`legend.markerscale`\n    The relative size of legend markers compared to the originally drawn ones.\n\nmarkerfirst : bool, default: True\n    If *True*, legend marker is placed to the left of the legend label.\n    If *False*, legend marker is placed to the right of the legend label.\n\nreverse : bool, default: False\n    If *True*, the legend labels are displayed in reverse order from the input.\n    If *False*, the legend labels are displayed in the same order as the input.\n\n    .. versionadded:: 3.7\n\nframeon : bool, default: :rc:`legend.frameon`\n    Whether the legend should be drawn on a patch (frame).\n\nfancybox : bool, default: :rc:`legend.fancybox`\n    Whether round edges should be enabled around the `.FancyBboxPatch` which\n    makes up the legend's background.\n\nshadow : None, bool or dict, default: :rc:`legend.shadow`\n    Whether to draw a shadow behind the legend.\n    The shadow can be configured using `.Patch` keywords.\n    Customization via :rc:`legend.shadow` is currently not supported.\n\nframealpha : float, default: :rc:`legend.framealpha`\n    The alpha transparency of the legend's background.\n    If *shadow* is activated and *framealpha* is ``None``, the default value is\n    ignored.\n\nfacecolor : \"inherit\" or color, default: :rc:`legend.facecolor`\n    The legend's background color.\n    If ``\"inherit\"``, use :rc:`axes.facecolor`.\n\nedgecolor : \"inherit\" or color, default: :rc:`legend.edgecolor`\n    The legend's background patch edge color.\n    If ``\"inherit\"``, use take :rc:`axes.edgecolor`.\n\nmode : {\"expand\", None}\n    If *mode* is set to ``\"expand\"`` the legend will be horizontally\n    expanded to fill the axes area (or *bbox_to_anchor* if defines\n    the legend's size).\n\nbbox_transform : None or `~matplotlib.transforms.Transform`\n    The transform for the bounding box (*bbox_to_anchor*). For a value\n    of ``None`` (default) the Axes'\n    :data:`~matplotlib.axes.Axes.transAxes` transform will be used.\n\ntitle : str or None\n    The legend's title. Default is no title (``None``).\n\ntitle_fontproperties : None or `~matplotlib.font_manager.FontProperties` or dict\n    The font properties of the legend's title. If None (default), the\n    *title_fontsize* argument will be used if present; if *title_fontsize* is\n    also None, the current :rc:`legend.title_fontsize` will be used.\n\ntitle_fontsize : int or {'xx-small', 'x-small', 'small', 'medium', 'large', 'x-large', 'xx-large'}, default: :rc:`legend.title_fontsize`\n    The font size of the legend's title.\n    Note: This cannot be combined with *title_fontproperties*. If you want\n    to set the fontsize alongside other font properties, use the *size*\n    parameter in *title_fontproperties*.\n\nalignment : {'center', 'left', 'right'}, default: 'center'\n    The alignment of the legend title and the box of entries. The entries\n    are aligned as a single block, so that markers always lined up.\n\nborderpad : float, default: :rc:`legend.borderpad`\n    The fractional whitespace inside the legend border, in font-size units.\n\nlabelspacing : float, default: :rc:`legend.labelspacing`\n    The vertical space between the legend entries, in font-size units.\n\nhandlelength : float, default: :rc:`legend.handlelength`\n    The length of the legend handles, in font-size units.\n\nhandleheight : float, default: :rc:`legend.handleheight`\n    The height of the legend handles, in font-size units.\n\nhandletextpad : float, default: :rc:`legend.handletextpad`\n    The pad between the legend handle and text, in font-size units.\n\nborderaxespad : float, default: :rc:`legend.borderaxespad`\n    The pad between the axes and legend border, in font-size units.\n\ncolumnspacing : float, default: :rc:`legend.columnspacing`\n    The spacing between columns, in font-size units.\n\nhandler_map : dict or None\n    The custom dictionary mapping instances or types to a legend\n    handler. This *handler_map* updates the default handler map\n    found at `matplotlib.legend.Legend.get_legend_handler_map`.\n\ndraggable : bool, default: False\n    Whether the legend can be dragged with the mouse.\n\n\nSee Also\n--------\n.Figure.legend\n\nNotes\n-----\nSome artists are not supported by this function.  See\n:ref:`legend_guide` for details.\n\nExamples\n--------\n.. plot:: gallery/text_labels_and_annotations/legend.py", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/239", "data": {"name": "matplotlib.pyplot.subplots[1].get_xlim", "type": "method", "signature": "()", "description": "Return the x-axis view limits.\n\nReturns\n-------\nleft, right : (float, float)\n    The current x-axis limits in data coordinates.\n\nSee Also\n--------\n.Axes.set_xlim\n.Axes.set_xbound, .Axes.get_xbound\n.Axes.invert_xaxis, .Axes.xaxis_inverted\n\nNotes\n-----\nThe x-axis may be inverted, in which case the *left* value will\nbe greater than the *right* value.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/239", "data": {"name": "matplotlib.pyplot.subplots[1].set_title", "type": "method", "signature": "(label, fontdict=None, loc=None, pad=None, *, y=None, **kwargs)", "description": "Set a title for the Axes.\n\nSet one of the three available Axes titles. The available titles\nare positioned above the Axes in the center, flush with the left\nedge, and flush with the right edge.\n\nParameters\n----------\nlabel : str\n    Text to use for the title\n\nfontdict : dict\n\n    .. admonition:: Discouraged\n\n       The use of *fontdict* is discouraged. Parameters should be passed as\n       individual keyword arguments or using dictionary-unpacking\n       ``set_title(..., **fontdict)``.\n\n    A dictionary controlling the appearance of the title text,\n    the default *fontdict* is::\n\n       {'fontsize': rcParams['axes.titlesize'],\n        'fontweight': rcParams['axes.titleweight'],\n        'color': rcParams['axes.titlecolor'],\n        'verticalalignment': 'baseline',\n        'horizontalalignment': loc}\n\nloc : {'center', 'left', 'right'}, default: :rc:`axes.titlelocation`\n    Which title to set.\n\ny : float, default: :rc:`axes.titley`\n    Vertical Axes location for the title (1.0 is the top).  If\n    None (the default) and :rc:`axes.titley` is also None, y is\n    determined automatically to avoid decorators on the Axes.\n\npad : float, default: :rc:`axes.titlepad`\n    The offset of the title from the top of the Axes, in points.\n\nReturns\n-------\n`.Text`\n    The matplotlib text instance representing the title\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    Other keyword arguments are text properties, see `.Text` for a list\n    of valid text properties.", "parameters": {"type": "object", "properties": {"label": {}, "fontdict": {"type": "NoneType", "default": null}, "loc": {"type": "NoneType", "default": null}, "pad": {"type": "NoneType", "default": null}, "y": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/239", "data": {"name": "matplotlib.pyplot.subplots[1].plot", "type": "method", "signature": "(*args, scalex=True, scaley=True, data=None, **kwargs)", "description": "Plot y versus x as lines and/or markers.\n\nCall signatures::\n\n    plot([x], y, [fmt], *, data=None, **kwargs)\n    plot([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n\nThe coordinates of the points or line nodes are given by *x*, *y*.\n\nThe optional parameter *fmt* is a convenient way for defining basic\nformatting like color, marker and linestyle. It's a shortcut string\nnotation described in the *Notes* section below.\n\n>>> plot(x, y)        # plot x and y using default line style and color\n>>> plot(x, y, 'bo')  # plot x and y using blue circle markers\n>>> plot(y)           # plot y using x as index array 0..N-1\n>>> plot(y, 'r+')     # ditto, but with red plusses\n\nYou can use `.Line2D` properties as keyword arguments for more\ncontrol on the appearance. Line properties and *fmt* can be mixed.\nThe following two calls yield identical results:\n\n>>> plot(x, y, 'go--', linewidth=2, markersize=12)\n>>> plot(x, y, color='green', marker='o', linestyle='dashed',\n...      linewidth=2, markersize=12)\n\nWhen conflicting with *fmt*, keyword arguments take precedence.\n\n\n**Plotting labelled data**\n\nThere's a convenient way for plotting objects with labelled data (i.e.\ndata that can be accessed by index ``obj['y']``). Instead of giving\nthe data in *x* and *y*, you can provide the object in the *data*\nparameter and just give the labels for *x* and *y*::\n\n>>> plot('xlabel', 'ylabel', data=obj)\n\nAll indexable objects are supported. This could e.g. be a `dict`, a\n`pandas.DataFrame` or a structured numpy array.\n\n\n**Plotting multiple sets of data**\n\nThere are various ways to plot multiple sets of data.\n\n- The most straight forward way is just to call `plot` multiple times.\n  Example:\n\n  >>> plot(x1, y1, 'bo')\n  >>> plot(x2, y2, 'go')\n\n- If *x* and/or *y* are 2D arrays a separate data set will be drawn\n  for every column. If both *x* and *y* are 2D, they must have the\n  same shape. If only one of them is 2D with shape (N, m) the other\n  must have length N and will be used for every data set m.\n\n  Example:\n\n  >>> x = [1, 2, 3]\n  >>> y = np.array([[1, 2], [3, 4], [5, 6]])\n  >>> plot(x, y)\n\n  is equivalent to:\n\n  >>> for col in range(y.shape[1]):\n  ...     plot(x, y[:, col])\n\n- The third way is to specify multiple sets of *[x]*, *y*, *[fmt]*\n  groups::\n\n  >>> plot(x1, y1, 'g^', x2, y2, 'g-')\n\n  In this case, any additional keyword argument applies to all\n  datasets. Also, this syntax cannot be combined with the *data*\n  parameter.\n\nBy default, each line is assigned a different style specified by a\n'style cycle'. The *fmt* and line property parameters are only\nnecessary if you want explicit deviations from these defaults.\nAlternatively, you can also change the style cycle using\n:rc:`axes.prop_cycle`.\n\n\nParameters\n----------\nx, y : array-like or scalar\n    The horizontal / vertical coordinates of the data points.\n    *x* values are optional and default to ``range(len(y))``.\n\n    Commonly, these parameters are 1D arrays.\n\n    They can also be scalars, or two-dimensional (in that case, the\n    columns represent separate data sets).\n\n    These arguments cannot be passed as keywords.\n\nfmt : str, optional\n    A format string, e.g. 'ro' for red circles. See the *Notes*\n    section for a full description of the format strings.\n\n    Format strings are just an abbreviation for quickly setting\n    basic line properties. All of these and more can also be\n    controlled by keyword arguments.\n\n    This argument cannot be passed as keyword.\n\ndata : indexable object, optional\n    An object with labelled data. If given, provide the label names to\n    plot in *x* and *y*.\n\n    .. note::\n        Technically there's a slight ambiguity in calls where the\n        second label is a valid *fmt*. ``plot('n', 'o', data=obj)``\n        could be ``plt(x, y)`` or ``plt(y, fmt)``. In such cases,\n        the former interpretation is chosen, but a warning is issued.\n        You may suppress the warning by adding an empty format string\n        ``plot('n', 'o', '', data=obj)``.\n\nReturns\n-------\nlist of `.Line2D`\n    A list of lines representing the plotted data.\n\nOther Parameters\n----------------\nscalex, scaley : bool, default: True\n    These parameters determine if the view limits are adapted to the\n    data limits. The values are passed on to\n    `~.axes.Axes.autoscale_view`.\n\n**kwargs : `~matplotlib.lines.Line2D` properties, optional\n    *kwargs* are used to specify properties like a line label (for\n    auto legends), linewidth, antialiasing, marker face color.\n    Example::\n\n    >>> plot([1, 2, 3], [1, 2, 3], 'go-', label='line 1', linewidth=2)\n    >>> plot([1, 2, 3], [1, 4, 9], 'rs', label='line 2')\n\n    If you specify multiple lines with one plot call, the kwargs apply\n    to all those lines. In case the label object is iterable, each\n    element is used as labels for each set of data.\n\n    Here is a list of available `.Line2D` properties:\n\n    Properties:\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: scalar or None\n    animated: bool\n    antialiased or aa: bool\n    clip_box: `~matplotlib.transforms.BboxBase` or None\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    color or c: color\n    dash_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    dash_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    dashes: sequence of floats (on/off ink in points) or (None, None)\n    data: (2, N) array or two 1D arrays\n    drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n    figure: `~matplotlib.figure.Figure`\n    fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n    gapcolor: color or None\n    gid: str\n    in_layout: bool\n    label: object\n    linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n    linewidth or lw: float\n    marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n    markeredgecolor or mec: color\n    markeredgewidth or mew: float\n    markerfacecolor or mfc: color\n    markerfacecoloralt or mfcalt: color\n    markersize or ms: float\n    markevery: None or int or (int, int) or slice or list[int] or float or (float, float) or list[bool]\n    mouseover: bool\n    path_effects: list of `.AbstractPathEffect`\n    picker: float or callable[[Artist, Event], tuple[bool, dict]]\n    pickradius: float\n    rasterized: bool\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    solid_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    solid_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    transform: unknown\n    url: str\n    visible: bool\n    xdata: 1D array\n    ydata: 1D array\n    zorder: float\n\nSee Also\n--------\nscatter : XY scatter plot with markers of varying size and/or color (\n    sometimes also called bubble chart).\n\nNotes\n-----\n**Format Strings**\n\nA format string consists of a part for color, marker and line::\n\n    fmt = '[marker][line][color]'\n\nEach of them is optional. If not provided, the value from the style\ncycle is used. Exception: If ``line`` is given, but no ``marker``,\nthe data will be a line without markers.\n\nOther combinations such as ``[color][marker][line]`` are also\nsupported, but note that their parsing may be ambiguous.\n\n**Markers**\n\n=============   ===============================\ncharacter       description\n=============   ===============================\n``'.'``         point marker\n``','``         pixel marker\n``'o'``         circle marker\n``'v'``         triangle_down marker\n``'^'``         triangle_up marker\n``'<'``         triangle_left marker\n``'>'``         triangle_right marker\n``'1'``         tri_down marker\n``'2'``         tri_up marker\n``'3'``         tri_left marker\n``'4'``         tri_right marker\n``'8'``         octagon marker\n``'s'``         square marker\n``'p'``         pentagon marker\n``'P'``         plus (filled) marker\n``'*'``         star marker\n``'h'``         hexagon1 marker\n``'H'``         hexagon2 marker\n``'+'``         plus marker\n``'x'``         x marker\n``'X'``         x (filled) marker\n``'D'``         diamond marker\n``'d'``         thin_diamond marker\n``'|'``         vline marker\n``'_'``         hline marker\n=============   ===============================\n\n**Line Styles**\n\n=============    ===============================\ncharacter        description\n=============    ===============================\n``'-'``          solid line style\n``'--'``         dashed line style\n``'-.'``         dash-dot line style\n``':'``          dotted line style\n=============    ===============================\n\nExample format strings::\n\n    'b'    # blue markers with default shape\n    'or'   # red circles\n    '-g'   # green solid line\n    '--'   # dashed line with default color\n    '^k:'  # black triangle_up markers connected by a dotted line\n\n**Colors**\n\nThe supported color abbreviations are the single letter codes\n\n=============    ===============================\ncharacter        color\n=============    ===============================\n``'b'``          blue\n``'g'``          green\n``'r'``          red\n``'c'``          cyan\n``'m'``          magenta\n``'y'``          yellow\n``'k'``          black\n``'w'``          white\n=============    ===============================\n\nand the ``'CN'`` colors that index into the default property cycle.\n\nIf the color is the only part of the format string, you can\nadditionally use any  `matplotlib.colors` spec, e.g. full names\n(``'green'``) or hex strings (``'#008000'``).", "parameters": {"type": "object", "properties": {"scalex": {"type": "bool", "default": true}, "scaley": {"type": "bool", "default": true}, "data": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/239", "data": {"name": "matplotlib.pyplot.subplots[1].hist", "type": "method", "signature": "(x, bins=None, range=None, density=False, weights=None, cumulative=False, bottom=None, histtype='bar', align='mid', orientation='vertical', rwidth=None, log=False, color=None, label=None, stacked=False, *, data=None, **kwargs)", "description": "Compute and plot a histogram.\n\nThis method uses `numpy.histogram` to bin the data in *x* and count the\nnumber of values in each bin, then draws the distribution either as a\n`.BarContainer` or `.Polygon`. The *bins*, *range*, *density*, and\n*weights* parameters are forwarded to `numpy.histogram`.\n\nIf the data has already been binned and counted, use `~.bar` or\n`~.stairs` to plot the distribution::\n\n    counts, bins = np.histogram(x)\n    plt.stairs(counts, bins)\n\nAlternatively, plot pre-computed bins and counts using ``hist()`` by\ntreating each bin as a single point with a weight equal to its count::\n\n    plt.hist(bins[:-1], bins, weights=counts)\n\nThe data input *x* can be a singular array, a list of datasets of\npotentially different lengths ([*x0*, *x1*, ...]), or a 2D ndarray in\nwhich each column is a dataset. Note that the ndarray form is\ntransposed relative to the list form. If the input is an array, then\nthe return value is a tuple (*n*, *bins*, *patches*); if the input is a\nsequence of arrays, then the return value is a tuple\n([*n0*, *n1*, ...], *bins*, [*patches0*, *patches1*, ...]).\n\nMasked arrays are not supported.\n\nParameters\n----------\nx : (n,) array or sequence of (n,) arrays\n    Input values, this takes either a single array or a sequence of\n    arrays which are not required to be of the same length.\n\nbins : int or sequence or str, default: :rc:`hist.bins`\n    If *bins* is an integer, it defines the number of equal-width bins\n    in the range.\n\n    If *bins* is a sequence, it defines the bin edges, including the\n    left edge of the first bin and the right edge of the last bin;\n    in this case, bins may be unequally spaced.  All but the last\n    (righthand-most) bin is half-open.  In other words, if *bins* is::\n\n        [1, 2, 3, 4]\n\n    then the first bin is ``[1, 2)`` (including 1, but excluding 2) and\n    the second ``[2, 3)``.  The last bin, however, is ``[3, 4]``, which\n    *includes* 4.\n\n    If *bins* is a string, it is one of the binning strategies\n    supported by `numpy.histogram_bin_edges`: 'auto', 'fd', 'doane',\n    'scott', 'stone', 'rice', 'sturges', or 'sqrt'.\n\nrange : tuple or None, default: None\n    The lower and upper range of the bins. Lower and upper outliers\n    are ignored. If not provided, *range* is ``(x.min(), x.max())``.\n    Range has no effect if *bins* is a sequence.\n\n    If *bins* is a sequence or *range* is specified, autoscaling\n    is based on the specified bin range instead of the\n    range of x.\n\ndensity : bool, default: False\n    If ``True``, draw and return a probability density: each bin\n    will display the bin's raw count divided by the total number of\n    counts *and the bin width*\n    (``density = counts / (sum(counts) * np.diff(bins))``),\n    so that the area under the histogram integrates to 1\n    (``np.sum(density * np.diff(bins)) == 1``).\n\n    If *stacked* is also ``True``, the sum of the histograms is\n    normalized to 1.\n\nweights : (n,) array-like or None, default: None\n    An array of weights, of the same shape as *x*.  Each value in\n    *x* only contributes its associated weight towards the bin count\n    (instead of 1).  If *density* is ``True``, the weights are\n    normalized, so that the integral of the density over the range\n    remains 1.\n\ncumulative : bool or -1, default: False\n    If ``True``, then a histogram is computed where each bin gives the\n    counts in that bin plus all bins for smaller values. The last bin\n    gives the total number of datapoints.\n\n    If *density* is also ``True`` then the histogram is normalized such\n    that the last bin equals 1.\n\n    If *cumulative* is a number less than 0 (e.g., -1), the direction\n    of accumulation is reversed.  In this case, if *density* is also\n    ``True``, then the histogram is normalized such that the first bin\n    equals 1.\n\nbottom : array-like, scalar, or None, default: None\n    Location of the bottom of each bin, i.e. bins are drawn from\n    ``bottom`` to ``bottom + hist(x, bins)`` If a scalar, the bottom\n    of each bin is shifted by the same amount. If an array, each bin\n    is shifted independently and the length of bottom must match the\n    number of bins. If None, defaults to 0.\n\nhisttype : {'bar', 'barstacked', 'step', 'stepfilled'}, default: 'bar'\n    The type of histogram to draw.\n\n    - 'bar' is a traditional bar-type histogram.  If multiple data\n      are given the bars are arranged side by side.\n    - 'barstacked' is a bar-type histogram where multiple\n      data are stacked on top of each other.\n    - 'step' generates a lineplot that is by default unfilled.\n    - 'stepfilled' generates a lineplot that is by default filled.\n\nalign : {'left', 'mid', 'right'}, default: 'mid'\n    The horizontal alignment of the histogram bars.\n\n    - 'left': bars are centered on the left bin edges.\n    - 'mid': bars are centered between the bin edges.\n    - 'right': bars are centered on the right bin edges.\n\norientation : {'vertical', 'horizontal'}, default: 'vertical'\n    If 'horizontal', `~.Axes.barh` will be used for bar-type histograms\n    and the *bottom* kwarg will be the left edges.\n\nrwidth : float or None, default: None\n    The relative width of the bars as a fraction of the bin width.  If\n    ``None``, automatically compute the width.\n\n    Ignored if *histtype* is 'step' or 'stepfilled'.\n\nlog : bool, default: False\n    If ``True``, the histogram axis will be set to a log scale.\n\ncolor : color or array-like of colors or None, default: None\n    Color or sequence of colors, one per dataset.  Default (``None``)\n    uses the standard line color sequence.\n\nlabel : str or None, default: None\n    String, or sequence of strings to match multiple datasets.  Bar\n    charts yield multiple patches per dataset, but only the first gets\n    the label, so that `~.Axes.legend` will work as expected.\n\nstacked : bool, default: False\n    If ``True``, multiple data are stacked on top of each other If\n    ``False`` multiple data are arranged side by side if histtype is\n    'bar' or on top of each other if histtype is 'step'\n\nReturns\n-------\nn : array or list of arrays\n    The values of the histogram bins. See *density* and *weights* for a\n    description of the possible semantics.  If input *x* is an array,\n    then this is an array of length *nbins*. If input is a sequence of\n    arrays ``[data1, data2, ...]``, then this is a list of arrays with\n    the values of the histograms for each of the arrays in the same\n    order.  The dtype of the array *n* (or of its element arrays) will\n    always be float even if no weighting or normalization is used.\n\nbins : array\n    The edges of the bins. Length nbins + 1 (nbins left edges and right\n    edge of last bin).  Always a single array even when multiple data\n    sets are passed in.\n\npatches : `.BarContainer` or list of a single `.Polygon` or list of such objects\n    Container of individual artists used to create the histogram\n    or list of such containers if there are multiple input datasets.\n\nOther Parameters\n----------------\ndata : indexable object, optional\n    If given, the following parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` (unless this raises an exception):\n\n    *x*, *weights*\n\n**kwargs\n    `~matplotlib.patches.Patch` properties\n\nSee Also\n--------\nhist2d : 2D histogram with rectangular bins\nhexbin : 2D histogram with hexagonal bins\nstairs : Plot a pre-computed histogram\nbar : Plot a pre-computed histogram\n\nNotes\n-----\nFor large numbers of bins (>1000), plotting can be significantly\naccelerated by using `~.Axes.stairs` to plot a pre-computed histogram\n(``plt.stairs(*np.histogram(data))``), or by setting *histtype* to\n'step' or 'stepfilled' rather than 'bar' or 'barstacked'.", "parameters": {"type": "object", "properties": {"x": {}, "bins": {"type": "NoneType", "default": null}, "range": {"type": "NoneType", "default": null}, "density": {"type": "bool", "default": false}, "weights": {"type": "NoneType", "default": null}, "cumulative": {"type": "bool", "default": false}, "bottom": {"type": "NoneType", "default": null}, "histtype": {"type": "str", "default": "bar"}, "align": {"type": "str", "default": "mid"}, "orientation": {"type": "str", "default": "vertical"}, "rwidth": {"type": "NoneType", "default": null}, "log": {"type": "bool", "default": false}, "color": {"type": "NoneType", "default": null}, "label": {"type": "NoneType", "default": null}, "stacked": {"type": "bool", "default": false}, "data": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/239", "data": {"name": "numpy.std", "type": "callable", "signature": "(a)", "description": "Compute the standard deviation along the specified axis.\n\nReturns the standard deviation, a measure of the spread of a distribution,\nof the array elements. The standard deviation is computed for the\nflattened array by default, otherwise over the specified axis.\n\nParameters\n----------\na : array_like\n    Calculate the standard deviation of these values.\naxis : None or int or tuple of ints, optional\n    Axis or axes along which the standard deviation is computed. The\n    default is to compute the standard deviation of the flattened array.\n\n    .. versionadded:: 1.7.0\n\n    If this is a tuple of ints, a standard deviation is performed over\n    multiple axes, instead of a single axis or all the axes as before.\ndtype : dtype, optional\n    Type to use in computing the standard deviation. For arrays of\n    integer type the default is float64, for arrays of float types it is\n    the same as the array type.\nout : ndarray, optional\n    Alternative output array in which to place the result. It must have\n    the same shape as the expected output but the type (of the calculated\n    values) will be cast if necessary.\nddof : int, optional\n    Means Delta Degrees of Freedom.  The divisor used in calculations\n    is ``N - ddof``, where ``N`` represents the number of elements.\n    By default `ddof` is zero.\nkeepdims : bool, optional\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\n    If the default value is passed, then `keepdims` will not be\n    passed through to the `std` method of sub-classes of\n    `ndarray`, however any non-default value will be.  If the\n    sub-class' method does not implement `keepdims` any\n    exceptions will be raised.\n\nwhere : array_like of bool, optional\n    Elements to include in the standard deviation.\n    See `~numpy.ufunc.reduce` for details.\n\n    .. versionadded:: 1.20.0\n\nReturns\n-------\nstandard_deviation : ndarray, see dtype parameter above.\n    If `out` is None, return a new array containing the standard deviation,\n    otherwise return a reference to the output array.\n\nSee Also\n--------\nvar, mean, nanmean, nanstd, nanvar\n:ref:`ufuncs-output-type`\n\nNotes\n-----\nThe standard deviation is the square root of the average of the squared\ndeviations from the mean, i.e., ``std = sqrt(mean(x))``, where\n``x = abs(a - a.mean())**2``.\n\nThe average squared deviation is typically calculated as ``x.sum() / N``,\nwhere ``N = len(x)``. If, however, `ddof` is specified, the divisor\n``N - ddof`` is used instead. In standard statistical practice, ``ddof=1``\nprovides an unbiased estimator of the variance of the infinite population.\n``ddof=0`` provides a maximum likelihood estimate of the variance for\nnormally distributed variables. The standard deviation computed in this\nfunction is the square root of the estimated variance, so even with\n``ddof=1``, it will not be an unbiased estimate of the standard deviation\nper se.\n\nNote that, for complex numbers, `std` takes the absolute\nvalue before squaring, so that the result is always real and nonnegative.\n\nFor floating-point input, the *std* is computed using the same\nprecision the input has. Depending on the input data, this can cause\nthe results to be inaccurate, especially for float32 (see example below).\nSpecifying a higher-accuracy accumulator using the `dtype` keyword can\nalleviate this issue.\n\nExamples\n--------\n>>> a = np.array([[1, 2], [3, 4]])\n>>> np.std(a)\n1.1180339887498949 # may vary\n>>> np.std(a, axis=0)\narray([1.,  1.])\n>>> np.std(a, axis=1)\narray([0.5,  0.5])\n\nIn single precision, std() can be inaccurate:\n\n>>> a = np.zeros((2, 512*512), dtype=np.float32)\n>>> a[0, :] = 1.0\n>>> a[1, :] = 0.1\n>>> np.std(a)\n0.45000005\n\nComputing the standard deviation in float64 is more accurate:\n\n>>> np.std(a, dtype=np.float64)\n0.44999999925494177 # may vary\n\nSpecifying a where argument:\n\n>>> a = np.array([[14, 8, 11, 10], [7, 9, 10, 11], [10, 15, 5, 10]])\n>>> np.std(a)\n2.614064523559687 # may vary\n>>> np.std(a, where=[[True], [True], [False]])\n2.0", "parameters": {"type": "object", "properties": {"a": {}}}}}
{"task_id": "BigCodeBench/239", "data": {"name": "numpy.min", "type": "callable", "signature": "(a)", "description": "Return the minimum of an array or minimum along an axis.\n\nParameters\n----------\na : array_like\n    Input data.\naxis : None or int or tuple of ints, optional\n    Axis or axes along which to operate.  By default, flattened input is\n    used.\n\n    .. versionadded:: 1.7.0\n\n    If this is a tuple of ints, the minimum is selected over multiple axes,\n    instead of a single axis or all the axes as before.\nout : ndarray, optional\n    Alternative output array in which to place the result.  Must\n    be of the same shape and buffer length as the expected output.\n    See :ref:`ufuncs-output-type` for more details.\n\nkeepdims : bool, optional\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\n    If the default value is passed, then `keepdims` will not be\n    passed through to the `amin` method of sub-classes of\n    `ndarray`, however any non-default value will be.  If the\n    sub-class' method does not implement `keepdims` any\n    exceptions will be raised.\n\ninitial : scalar, optional\n    The maximum value of an output element. Must be present to allow\n    computation on empty slice. See `~numpy.ufunc.reduce` for details.\n\n    .. versionadded:: 1.15.0\n\nwhere : array_like of bool, optional\n    Elements to compare for the minimum. See `~numpy.ufunc.reduce`\n    for details.\n\n    .. versionadded:: 1.17.0\n\nReturns\n-------\namin : ndarray or scalar\n    Minimum of `a`. If `axis` is None, the result is a scalar value.\n    If `axis` is given, the result is an array of dimension\n    ``a.ndim - 1``.\n\nSee Also\n--------\namax :\n    The maximum value of an array along a given axis, propagating any NaNs.\nnanmin :\n    The minimum value of an array along a given axis, ignoring any NaNs.\nminimum :\n    Element-wise minimum of two arrays, propagating any NaNs.\nfmin :\n    Element-wise minimum of two arrays, ignoring any NaNs.\nargmin :\n    Return the indices of the minimum values.\n\nnanmax, maximum, fmax\n\nNotes\n-----\nNaN values are propagated, that is if at least one item is NaN, the\ncorresponding min value will be NaN as well. To ignore NaN values\n(MATLAB behavior), please use nanmin.\n\nDon't use `amin` for element-wise comparison of 2 arrays; when\n``a.shape[0]`` is 2, ``minimum(a[0], a[1])`` is faster than\n``amin(a, axis=0)``.\n\nExamples\n--------\n>>> a = np.arange(4).reshape((2,2))\n>>> a\narray([[0, 1],\n       [2, 3]])\n>>> np.amin(a)           # Minimum of the flattened array\n0\n>>> np.amin(a, axis=0)   # Minima along the first axis\narray([0, 1])\n>>> np.amin(a, axis=1)   # Minima along the second axis\narray([0, 2])\n>>> np.amin(a, where=[False, True], initial=10, axis=0)\narray([10,  1])\n\n>>> b = np.arange(5, dtype=float)\n>>> b[2] = np.NaN\n>>> np.amin(b)\nnan\n>>> np.amin(b, where=~np.isnan(b), initial=10)\n0.0\n>>> np.nanmin(b)\n0.0\n\n>>> np.amin([[-50], [10]], axis=-1, initial=0)\narray([-50,   0])\n\nNotice that the initial value is used as one of the elements for which the\nminimum is determined, unlike for the default argument Python's max\nfunction, which is only used for empty iterables.\n\nNotice that this isn't the same as Python's ``default`` argument.\n\n>>> np.amin([6], initial=5)\n5\n>>> min([6], default=5)\n6", "parameters": {"type": "object", "properties": {"a": {}}}}}
{"task_id": "BigCodeBench/239", "data": {"name": "numpy.max", "type": "callable", "signature": "(a)", "description": "Return the maximum of an array or maximum along an axis.\n\nParameters\n----------\na : array_like\n    Input data.\naxis : None or int or tuple of ints, optional\n    Axis or axes along which to operate.  By default, flattened input is\n    used.\n\n    .. versionadded:: 1.7.0\n\n    If this is a tuple of ints, the maximum is selected over multiple axes,\n    instead of a single axis or all the axes as before.\nout : ndarray, optional\n    Alternative output array in which to place the result.  Must\n    be of the same shape and buffer length as the expected output.\n    See :ref:`ufuncs-output-type` for more details.\n\nkeepdims : bool, optional\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\n    If the default value is passed, then `keepdims` will not be\n    passed through to the `amax` method of sub-classes of\n    `ndarray`, however any non-default value will be.  If the\n    sub-class' method does not implement `keepdims` any\n    exceptions will be raised.\n\ninitial : scalar, optional\n    The minimum value of an output element. Must be present to allow\n    computation on empty slice. See `~numpy.ufunc.reduce` for details.\n\n    .. versionadded:: 1.15.0\n\nwhere : array_like of bool, optional\n    Elements to compare for the maximum. See `~numpy.ufunc.reduce`\n    for details.\n\n    .. versionadded:: 1.17.0\n\nReturns\n-------\namax : ndarray or scalar\n    Maximum of `a`. If `axis` is None, the result is a scalar value.\n    If `axis` is given, the result is an array of dimension\n    ``a.ndim - 1``.\n\nSee Also\n--------\namin :\n    The minimum value of an array along a given axis, propagating any NaNs.\nnanmax :\n    The maximum value of an array along a given axis, ignoring any NaNs.\nmaximum :\n    Element-wise maximum of two arrays, propagating any NaNs.\nfmax :\n    Element-wise maximum of two arrays, ignoring any NaNs.\nargmax :\n    Return the indices of the maximum values.\n\nnanmin, minimum, fmin\n\nNotes\n-----\nNaN values are propagated, that is if at least one item is NaN, the\ncorresponding max value will be NaN as well. To ignore NaN values\n(MATLAB behavior), please use nanmax.\n\nDon't use `amax` for element-wise comparison of 2 arrays; when\n``a.shape[0]`` is 2, ``maximum(a[0], a[1])`` is faster than\n``amax(a, axis=0)``.\n\nExamples\n--------\n>>> a = np.arange(4).reshape((2,2))\n>>> a\narray([[0, 1],\n       [2, 3]])\n>>> np.amax(a)           # Maximum of the flattened array\n3\n>>> np.amax(a, axis=0)   # Maxima along the first axis\narray([2, 3])\n>>> np.amax(a, axis=1)   # Maxima along the second axis\narray([1, 3])\n>>> np.amax(a, where=[False, True], initial=-1, axis=0)\narray([-1,  3])\n>>> b = np.arange(5, dtype=float)\n>>> b[2] = np.NaN\n>>> np.amax(b)\nnan\n>>> np.amax(b, where=~np.isnan(b), initial=-1)\n4.0\n>>> np.nanmax(b)\n4.0\n\nYou can use an initial value to compute the maximum of an empty slice, or\nto initialize it to a different value:\n\n>>> np.amax([[-50], [10]], axis=-1, initial=0)\narray([ 0, 10])\n\nNotice that the initial value is used as one of the elements for which the\nmaximum is determined, unlike for the default argument Python's max\nfunction, which is only used for empty iterables.\n\n>>> np.amax([5], initial=6)\n6\n>>> max([5], default=6)\n5", "parameters": {"type": "object", "properties": {"a": {}}}}}
{"task_id": "BigCodeBench/239", "data": {"name": "numpy.mean", "type": "callable", "signature": "(a)", "description": "Compute the arithmetic mean along the specified axis.\n\nReturns the average of the array elements.  The average is taken over\nthe flattened array by default, otherwise over the specified axis.\n`float64` intermediate and return values are used for integer inputs.\n\nParameters\n----------\na : array_like\n    Array containing numbers whose mean is desired. If `a` is not an\n    array, a conversion is attempted.\naxis : None or int or tuple of ints, optional\n    Axis or axes along which the means are computed. The default is to\n    compute the mean of the flattened array.\n\n    .. versionadded:: 1.7.0\n\n    If this is a tuple of ints, a mean is performed over multiple axes,\n    instead of a single axis or all the axes as before.\ndtype : data-type, optional\n    Type to use in computing the mean.  For integer inputs, the default\n    is `float64`; for floating point inputs, it is the same as the\n    input dtype.\nout : ndarray, optional\n    Alternate output array in which to place the result.  The default\n    is ``None``; if provided, it must have the same shape as the\n    expected output, but the type will be cast if necessary.\n    See :ref:`ufuncs-output-type` for more details.\n\nkeepdims : bool, optional\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\n    If the default value is passed, then `keepdims` will not be\n    passed through to the `mean` method of sub-classes of\n    `ndarray`, however any non-default value will be.  If the\n    sub-class' method does not implement `keepdims` any\n    exceptions will be raised.\n\nwhere : array_like of bool, optional\n    Elements to include in the mean. See `~numpy.ufunc.reduce` for details.\n\n    .. versionadded:: 1.20.0\n\nReturns\n-------\nm : ndarray, see dtype parameter above\n    If `out=None`, returns a new array containing the mean values,\n    otherwise a reference to the output array is returned.\n\nSee Also\n--------\naverage : Weighted average\nstd, var, nanmean, nanstd, nanvar\n\nNotes\n-----\nThe arithmetic mean is the sum of the elements along the axis divided\nby the number of elements.\n\nNote that for floating-point input, the mean is computed using the\nsame precision the input has.  Depending on the input data, this can\ncause the results to be inaccurate, especially for `float32` (see\nexample below).  Specifying a higher-precision accumulator using the\n`dtype` keyword can alleviate this issue.\n\nBy default, `float16` results are computed using `float32` intermediates\nfor extra precision.\n\nExamples\n--------\n>>> a = np.array([[1, 2], [3, 4]])\n>>> np.mean(a)\n2.5\n>>> np.mean(a, axis=0)\narray([2., 3.])\n>>> np.mean(a, axis=1)\narray([1.5, 3.5])\n\nIn single precision, `mean` can be inaccurate:\n\n>>> a = np.zeros((2, 512*512), dtype=np.float32)\n>>> a[0, :] = 1.0\n>>> a[1, :] = 0.1\n>>> np.mean(a)\n0.54999924\n\nComputing the mean in float64 is more accurate:\n\n>>> np.mean(a, dtype=np.float64)\n0.55000000074505806 # may vary\n\nSpecifying a where argument:\n\n>>> a = np.array([[5, 9, 13], [14, 10, 12], [11, 15, 19]])\n>>> np.mean(a)\n12.0\n>>> np.mean(a, where=[[True], [False], [False]])\n9.0", "parameters": {"type": "object", "properties": {"a": {}}}}}
{"task_id": "BigCodeBench/239", "data": {"name": "numpy.linspace", "type": "callable", "signature": "(start, stop, num=50)", "description": "Return evenly spaced numbers over a specified interval.\n\nReturns `num` evenly spaced samples, calculated over the\ninterval [`start`, `stop`].\n\nThe endpoint of the interval can optionally be excluded.\n\n.. versionchanged:: 1.16.0\n    Non-scalar `start` and `stop` are now supported.\n\n.. versionchanged:: 1.20.0\n    Values are rounded towards ``-inf`` instead of ``0`` when an\n    integer ``dtype`` is specified. The old behavior can\n    still be obtained with ``np.linspace(start, stop, num).astype(int)``\n\nParameters\n----------\nstart : array_like\n    The starting value of the sequence.\nstop : array_like\n    The end value of the sequence, unless `endpoint` is set to False.\n    In that case, the sequence consists of all but the last of ``num + 1``\n    evenly spaced samples, so that `stop` is excluded.  Note that the step\n    size changes when `endpoint` is False.\nnum : int, optional\n    Number of samples to generate. Default is 50. Must be non-negative.\nendpoint : bool, optional\n    If True, `stop` is the last sample. Otherwise, it is not included.\n    Default is True.\nretstep : bool, optional\n    If True, return (`samples`, `step`), where `step` is the spacing\n    between samples.\ndtype : dtype, optional\n    The type of the output array.  If `dtype` is not given, the data type\n    is inferred from `start` and `stop`. The inferred dtype will never be\n    an integer; `float` is chosen even if the arguments would produce an\n    array of integers.\n\n    .. versionadded:: 1.9.0\n\naxis : int, optional\n    The axis in the result to store the samples.  Relevant only if start\n    or stop are array-like.  By default (0), the samples will be along a\n    new axis inserted at the beginning. Use -1 to get an axis at the end.\n\n    .. versionadded:: 1.16.0\n\nReturns\n-------\nsamples : ndarray\n    There are `num` equally spaced samples in the closed interval\n    ``[start, stop]`` or the half-open interval ``[start, stop)``\n    (depending on whether `endpoint` is True or False).\nstep : float, optional\n    Only returned if `retstep` is True\n\n    Size of spacing between samples.\n\n\nSee Also\n--------\narange : Similar to `linspace`, but uses a step size (instead of the\n         number of samples).\ngeomspace : Similar to `linspace`, but with numbers spaced evenly on a log\n            scale (a geometric progression).\nlogspace : Similar to `geomspace`, but with the end points specified as\n           logarithms.\n\nExamples\n--------\n>>> np.linspace(2.0, 3.0, num=5)\narray([2.  , 2.25, 2.5 , 2.75, 3.  ])\n>>> np.linspace(2.0, 3.0, num=5, endpoint=False)\narray([2. ,  2.2,  2.4,  2.6,  2.8])\n>>> np.linspace(2.0, 3.0, num=5, retstep=True)\n(array([2.  ,  2.25,  2.5 ,  2.75,  3.  ]), 0.25)\n\nGraphical illustration:\n\n>>> import matplotlib.pyplot as plt\n>>> N = 8\n>>> y = np.zeros(N)\n>>> x1 = np.linspace(0, 10, N, endpoint=True)\n>>> x2 = np.linspace(0, 10, N, endpoint=False)\n>>> plt.plot(x1, y, 'o')\n[<matplotlib.lines.Line2D object at 0x...>]\n>>> plt.plot(x2, y + 0.5, 'o')\n[<matplotlib.lines.Line2D object at 0x...>]\n>>> plt.ylim([-0.5, 1])\n(-0.5, 1)\n>>> plt.show()", "parameters": {"type": "object", "properties": {"start": {}, "stop": {}, "num": {"type": "integer", "default": 50}}}}}
{"task_id": "BigCodeBench/239", "data": {"name": "numpy.array", "type": "callable", "signature": "(*args, **kwargs)", "description": "array(object, dtype=None, *, copy=True, order='K', subok=False, ndmin=0,\n      like=None)\n\nCreate an array.\n\nParameters\n----------\nobject : array_like\n    An array, any object exposing the array interface, an object whose\n    __array__ method returns an array, or any (nested) sequence.\n    If object is a scalar, a 0-dimensional array containing object is\n    returned.\ndtype : data-type, optional\n    The desired data-type for the array.  If not given, then the type will\n    be determined as the minimum type required to hold the objects in the\n    sequence.\ncopy : bool, optional\n    If true (default), then the object is copied.  Otherwise, a copy will\n    only be made if __array__ returns a copy, if obj is a nested sequence,\n    or if a copy is needed to satisfy any of the other requirements\n    (`dtype`, `order`, etc.).\norder : {'K', 'A', 'C', 'F'}, optional\n    Specify the memory layout of the array. If object is not an array, the\n    newly created array will be in C order (row major) unless 'F' is\n    specified, in which case it will be in Fortran order (column major).\n    If object is an array the following holds.\n\n    ===== ========= ===================================================\n    order  no copy                     copy=True\n    ===== ========= ===================================================\n    'K'   unchanged F & C order preserved, otherwise most similar order\n    'A'   unchanged F order if input is F and not C, otherwise C order\n    'C'   C order   C order\n    'F'   F order   F order\n    ===== ========= ===================================================\n\n    When ``copy=False`` and a copy is made for other reasons, the result is\n    the same as if ``copy=True``, with some exceptions for 'A', see the\n    Notes section. The default order is 'K'.\nsubok : bool, optional\n    If True, then sub-classes will be passed-through, otherwise\n    the returned array will be forced to be a base-class array (default).\nndmin : int, optional\n    Specifies the minimum number of dimensions that the resulting\n    array should have.  Ones will be prepended to the shape as\n    needed to meet this requirement.\nlike : array_like, optional\n    Reference object to allow the creation of arrays which are not\n    NumPy arrays. If an array-like passed in as ``like`` supports\n    the ``__array_function__`` protocol, the result will be defined\n    by it. In this case, it ensures the creation of an array object\n    compatible with that passed in via this argument.\n\n    .. versionadded:: 1.20.0\n\nReturns\n-------\nout : ndarray\n    An array object satisfying the specified requirements.\n\nSee Also\n--------\nempty_like : Return an empty array with shape and type of input.\nones_like : Return an array of ones with shape and type of input.\nzeros_like : Return an array of zeros with shape and type of input.\nfull_like : Return a new array with shape of input filled with value.\nempty : Return a new uninitialized array.\nones : Return a new array setting values to one.\nzeros : Return a new array setting values to zero.\nfull : Return a new array of given shape filled with value.\n\n\nNotes\n-----\nWhen order is 'A' and `object` is an array in neither 'C' nor 'F' order,\nand a copy is forced by a change in dtype, then the order of the result is\nnot necessarily 'C' as expected. This is likely a bug.\n\nExamples\n--------\n>>> np.array([1, 2, 3])\narray([1, 2, 3])\n\nUpcasting:\n\n>>> np.array([1, 2, 3.0])\narray([ 1.,  2.,  3.])\n\nMore than one dimension:\n\n>>> np.array([[1, 2], [3, 4]])\narray([[1, 2],\n       [3, 4]])\n\nMinimum dimensions 2:\n\n>>> np.array([1, 2, 3], ndmin=2)\narray([[1, 2, 3]])\n\nType provided:\n\n>>> np.array([1, 2, 3], dtype=complex)\narray([ 1.+0.j,  2.+0.j,  3.+0.j])\n\nData-type consisting of more than one element:\n\n>>> x = np.array([(1,2),(3,4)],dtype=[('a','<i4'),('b','<i4')])\n>>> x['a']\narray([1, 3])\n\nCreating an array from sub-classes:\n\n>>> np.array(np.mat('1 2; 3 4'))\narray([[1, 2],\n       [3, 4]])\n\n>>> np.array(np.mat('1 2; 3 4'), subok=True)\nmatrix([[1, 2],\n        [3, 4]])", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/239", "data": {"name": "scipy.stats.norm", "type": "callable", "signature": "(*args, **kwds)", "description": "A normal continuous random variable.\n\nThe location (``loc``) keyword specifies the mean.\nThe scale (``scale``) keyword specifies the standard deviation.\n\nAs an instance of the `rv_continuous` class, `norm` object inherits from it\na collection of generic methods (see below for the full list),\nand completes them with details specific for this particular distribution.\n\nMethods\n-------\nrvs(loc=0, scale=1, size=1, random_state=None)\n    Random variates.\npdf(x, loc=0, scale=1)\n    Probability density function.\nlogpdf(x, loc=0, scale=1)\n    Log of the probability density function.\ncdf(x, loc=0, scale=1)\n    Cumulative distribution function.\nlogcdf(x, loc=0, scale=1)\n    Log of the cumulative distribution function.\nsf(x, loc=0, scale=1)\n    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\nlogsf(x, loc=0, scale=1)\n    Log of the survival function.\nppf(q, loc=0, scale=1)\n    Percent point function (inverse of ``cdf`` --- percentiles).\nisf(q, loc=0, scale=1)\n    Inverse survival function (inverse of ``sf``).\nmoment(order, loc=0, scale=1)\n    Non-central moment of the specified order.\nstats(loc=0, scale=1, moments='mv')\n    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\nentropy(loc=0, scale=1)\n    (Differential) entropy of the RV.\nfit(data)\n    Parameter estimates for generic data.\n    See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n    keyword arguments.\nexpect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n    Expected value of a function (of one argument) with respect to the distribution.\nmedian(loc=0, scale=1)\n    Median of the distribution.\nmean(loc=0, scale=1)\n    Mean of the distribution.\nvar(loc=0, scale=1)\n    Variance of the distribution.\nstd(loc=0, scale=1)\n    Standard deviation of the distribution.\ninterval(confidence, loc=0, scale=1)\n    Confidence interval with equal areas around the median.\n\nNotes\n-----\nThe probability density function for `norm` is:\n\n.. math::\n\n    f(x) = \\frac{\\exp(-x^2/2)}{\\sqrt{2\\pi}}\n\nfor a real number :math:`x`.\n\nThe probability density above is defined in the \"standardized\" form. To shift\nand/or scale the distribution use the ``loc`` and ``scale`` parameters.\nSpecifically, ``norm.pdf(x, loc, scale)`` is identically\nequivalent to ``norm.pdf(y) / scale`` with\n``y = (x - loc) / scale``. Note that shifting the location of a distribution\ndoes not make it a \"noncentral\" distribution; noncentral generalizations of\nsome distributions are available in separate classes.\n\nExamples\n--------\n>>> import numpy as np\n>>> from scipy.stats import norm\n>>> import matplotlib.pyplot as plt\n>>> fig, ax = plt.subplots(1, 1)\n\nCalculate the first four moments:\n\n\n>>> mean, var, skew, kurt = norm.stats(moments='mvsk')\n\nDisplay the probability density function (``pdf``):\n\n>>> x = np.linspace(norm.ppf(0.01),\n...                 norm.ppf(0.99), 100)\n>>> ax.plot(x, norm.pdf(x),\n...        'r-', lw=5, alpha=0.6, label='norm pdf')\n\nAlternatively, the distribution object can be called (as a function)\nto fix the shape, location and scale parameters. This returns a \"frozen\"\nRV object holding the given parameters fixed.\n\nFreeze the distribution and display the frozen ``pdf``:\n\n>>> rv = norm()\n>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n\nCheck accuracy of ``cdf`` and ``ppf``:\n\n>>> vals = norm.ppf([0.001, 0.5, 0.999])\n>>> np.allclose([0.001, 0.5, 0.999], norm.cdf(vals))\nTrue\n\nGenerate random numbers:\n\n>>> r = norm.rvs(size=1000)\n\nAnd compare the histogram:\n\n>>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n>>> ax.set_xlim([x[0], x[-1]])\n>>> ax.legend(loc='best', frameon=False)\n>>> plt.show()", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/239", "data": {"name": "scipy.stats.norm.pdf", "type": "callable", "signature": "(x, *args, **kwds)", "description": "Probability density function at x of the given RV.\n\nParameters\n----------\nx : array_like\n    quantiles\narg1, arg2, arg3,... : array_like\n    The shape parameter(s) for the distribution (see docstring of the\n    instance object for more information)\nloc : array_like, optional\n    location parameter (default=0)\nscale : array_like, optional\n    scale parameter (default=1)\n\nReturns\n-------\npdf : ndarray\n    Probability density function evaluated at x", "parameters": {"type": "object", "properties": {"x": {}}}}}
{"task_id": "BigCodeBench/241", "data": {"name": "matplotlib.pyplot.subplots", "type": "class", "signature": "()", "description": "Create a figure and a set of subplots.\n\nThis utility wrapper makes it convenient to create common layouts of\nsubplots, including the enclosing figure object, in a single call.\n\nParameters\n----------\nnrows, ncols : int, default: 1\n    Number of rows/columns of the subplot grid.\n\nsharex, sharey : bool or {'none', 'all', 'row', 'col'}, default: False\n    Controls sharing of properties among x (*sharex*) or y (*sharey*)\n    axes:\n\n    - True or 'all': x- or y-axis will be shared among all subplots.\n    - False or 'none': each subplot x- or y-axis will be independent.\n    - 'row': each subplot row will share an x- or y-axis.\n    - 'col': each subplot column will share an x- or y-axis.\n\n    When subplots have a shared x-axis along a column, only the x tick\n    labels of the bottom subplot are created. Similarly, when subplots\n    have a shared y-axis along a row, only the y tick labels of the first\n    column subplot are created. To later turn other subplots' ticklabels\n    on, use `~matplotlib.axes.Axes.tick_params`.\n\n    When subplots have a shared axis that has units, calling\n    `~matplotlib.axis.Axis.set_units` will update each axis with the\n    new units.\n\nsqueeze : bool, default: True\n    - If True, extra dimensions are squeezed out from the returned\n      array of `~matplotlib.axes.Axes`:\n\n      - if only one subplot is constructed (nrows=ncols=1), the\n        resulting single Axes object is returned as a scalar.\n      - for Nx1 or 1xM subplots, the returned object is a 1D numpy\n        object array of Axes objects.\n      - for NxM, subplots with N>1 and M>1 are returned as a 2D array.\n\n    - If False, no squeezing at all is done: the returned Axes object is\n      always a 2D array containing Axes instances, even if it ends up\n      being 1x1.\n\nwidth_ratios : array-like of length *ncols*, optional\n    Defines the relative widths of the columns. Each column gets a\n    relative width of ``width_ratios[i] / sum(width_ratios)``.\n    If not given, all columns will have the same width.  Equivalent\n    to ``gridspec_kw={'width_ratios': [...]}``.\n\nheight_ratios : array-like of length *nrows*, optional\n    Defines the relative heights of the rows. Each row gets a\n    relative height of ``height_ratios[i] / sum(height_ratios)``.\n    If not given, all rows will have the same height. Convenience\n    for ``gridspec_kw={'height_ratios': [...]}``.\n\nsubplot_kw : dict, optional\n    Dict with keywords passed to the\n    `~matplotlib.figure.Figure.add_subplot` call used to create each\n    subplot.\n\ngridspec_kw : dict, optional\n    Dict with keywords passed to the `~matplotlib.gridspec.GridSpec`\n    constructor used to create the grid the subplots are placed on.\n\n**fig_kw\n    All additional keyword arguments are passed to the\n    `.pyplot.figure` call.\n\nReturns\n-------\nfig : `.Figure`\n\nax : `~matplotlib.axes.Axes` or array of Axes\n    *ax* can be either a single `~.axes.Axes` object, or an array of Axes\n    objects if more than one subplot was created.  The dimensions of the\n    resulting array can be controlled with the squeeze keyword, see above.\n\n    Typical idioms for handling the return value are::\n\n        # using the variable ax for single a Axes\n        fig, ax = plt.subplots()\n\n        # using the variable axs for multiple Axes\n        fig, axs = plt.subplots(2, 2)\n\n        # using tuple unpacking for multiple Axes\n        fig, (ax1, ax2) = plt.subplots(1, 2)\n        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n\n    The names ``ax`` and pluralized ``axs`` are preferred over ``axes``\n    because for the latter it's not clear if it refers to a single\n    `~.axes.Axes` instance or a collection of these.\n\nSee Also\n--------\n.pyplot.figure\n.pyplot.subplot\n.pyplot.axes\n.Figure.subplots\n.Figure.add_subplot\n\nExamples\n--------\n::\n\n    # First create some toy data:\n    x = np.linspace(0, 2*np.pi, 400)\n    y = np.sin(x**2)\n\n    # Create just a figure and only one subplot\n    fig, ax = plt.subplots()\n    ax.plot(x, y)\n    ax.set_title('Simple plot')\n\n    # Create two subplots and unpack the output array immediately\n    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n    ax1.plot(x, y)\n    ax1.set_title('Sharing Y axis')\n    ax2.scatter(x, y)\n\n    # Create four polar axes and access them through the returned array\n    fig, axs = plt.subplots(2, 2, subplot_kw=dict(projection=\"polar\"))\n    axs[0, 0].plot(x, y)\n    axs[1, 1].scatter(x, y)\n\n    # Share a X axis with each column of subplots\n    plt.subplots(2, 2, sharex='col')\n\n    # Share a Y axis with each row of subplots\n    plt.subplots(2, 2, sharey='row')\n\n    # Share both X and Y axes with all subplots\n    plt.subplots(2, 2, sharex='all', sharey='all')\n\n    # Note that this is the same as\n    plt.subplots(2, 2, sharex=True, sharey=True)\n\n    # Create figure number 10 with a single subplot\n    # and clears it if it already exists.\n    fig, ax = plt.subplots(num=10, clear=True)", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/241", "data": {"name": "matplotlib.pyplot.subplots[1].legend", "type": "method", "signature": "(*args, **kwargs)", "description": "Place a legend on the Axes.\n\nCall signatures::\n\n    legend()\n    legend(handles, labels)\n    legend(handles=handles)\n    legend(labels)\n\nThe call signatures correspond to the following different ways to use\nthis method:\n\n**1. Automatic detection of elements to be shown in the legend**\n\nThe elements to be added to the legend are automatically determined,\nwhen you do not pass in any extra arguments.\n\nIn this case, the labels are taken from the artist. You can specify\nthem either at artist creation or by calling the\n:meth:`~.Artist.set_label` method on the artist::\n\n    ax.plot([1, 2, 3], label='Inline label')\n    ax.legend()\n\nor::\n\n    line, = ax.plot([1, 2, 3])\n    line.set_label('Label via method')\n    ax.legend()\n\n.. note::\n    Specific artists can be excluded from the automatic legend element\n    selection by using a label starting with an underscore, \"_\".\n    A string starting with an underscore is the default label for all\n    artists, so calling `.Axes.legend` without any arguments and\n    without setting the labels manually will result in no legend being\n    drawn.\n\n\n**2. Explicitly listing the artists and labels in the legend**\n\nFor full control of which artists have a legend entry, it is possible\nto pass an iterable of legend artists followed by an iterable of\nlegend labels respectively::\n\n    ax.legend([line1, line2, line3], ['label1', 'label2', 'label3'])\n\n\n**3. Explicitly listing the artists in the legend**\n\nThis is similar to 2, but the labels are taken from the artists'\nlabel properties. Example::\n\n    line1, = ax.plot([1, 2, 3], label='label1')\n    line2, = ax.plot([1, 2, 3], label='label2')\n    ax.legend(handles=[line1, line2])\n\n\n**4. Labeling existing plot elements**\n\n.. admonition:: Discouraged\n\n    This call signature is discouraged, because the relation between\n    plot elements and labels is only implicit by their order and can\n    easily be mixed up.\n\nTo make a legend for all artists on an Axes, call this function with\nan iterable of strings, one for each legend item. For example::\n\n    ax.plot([1, 2, 3])\n    ax.plot([5, 6, 7])\n    ax.legend(['First line', 'Second line'])\n\n\nParameters\n----------\nhandles : list of (`.Artist` or tuple of `.Artist`), optional\n    A list of Artists (lines, patches) to be added to the legend.\n    Use this together with *labels*, if you need full control on what\n    is shown in the legend and the automatic mechanism described above\n    is not sufficient.\n\n    The length of handles and labels should be the same in this\n    case. If they are not, they are truncated to the smaller length.\n\n    If an entry contains a tuple, then the legend handler for all Artists in the\n    tuple will be placed alongside a single label.\n\nlabels : list of str, optional\n    A list of labels to show next to the artists.\n    Use this together with *handles*, if you need full control on what\n    is shown in the legend and the automatic mechanism described above\n    is not sufficient.\n\nReturns\n-------\n`~matplotlib.legend.Legend`\n\nOther Parameters\n----------------\n\nloc : str or pair of floats, default: :rc:`legend.loc`\n    The location of the legend.\n\n    The strings ``'upper left'``, ``'upper right'``, ``'lower left'``,\n    ``'lower right'`` place the legend at the corresponding corner of the\n    axes.\n\n    The strings ``'upper center'``, ``'lower center'``, ``'center left'``,\n    ``'center right'`` place the legend at the center of the corresponding edge\n    of the axes.\n\n    The string ``'center'`` places the legend at the center of the axes.\n\n    The string ``'best'`` places the legend at the location, among the nine\n    locations defined so far, with the minimum overlap with other drawn\n    artists.  This option can be quite slow for plots with large amounts of\n    data; your plotting speed may benefit from providing a specific location.\n\n    The location can also be a 2-tuple giving the coordinates of the lower-left\n    corner of the legend in axes coordinates (in which case *bbox_to_anchor*\n    will be ignored).\n\n    For back-compatibility, ``'center right'`` (but no other location) can also\n    be spelled ``'right'``, and each \"string\" location can also be given as a\n    numeric value:\n\n    ==================   =============\n    Location String      Location Code\n    ==================   =============\n    'best' (Axes only)   0\n    'upper right'        1\n    'upper left'         2\n    'lower left'         3\n    'lower right'        4\n    'right'              5\n    'center left'        6\n    'center right'       7\n    'lower center'       8\n    'upper center'       9\n    'center'             10\n    ==================   =============\n    \nbbox_to_anchor : `.BboxBase`, 2-tuple, or 4-tuple of floats\n    Box that is used to position the legend in conjunction with *loc*.\n    Defaults to `axes.bbox` (if called as a method to `.Axes.legend`) or\n    `figure.bbox` (if `.Figure.legend`).  This argument allows arbitrary\n    placement of the legend.\n\n    Bbox coordinates are interpreted in the coordinate system given by\n    *bbox_transform*, with the default transform\n    Axes or Figure coordinates, depending on which ``legend`` is called.\n\n    If a 4-tuple or `.BboxBase` is given, then it specifies the bbox\n    ``(x, y, width, height)`` that the legend is placed in.\n    To put the legend in the best location in the bottom right\n    quadrant of the axes (or figure)::\n\n        loc='best', bbox_to_anchor=(0.5, 0., 0.5, 0.5)\n\n    A 2-tuple ``(x, y)`` places the corner of the legend specified by *loc* at\n    x, y.  For example, to put the legend's upper right-hand corner in the\n    center of the axes (or figure) the following keywords can be used::\n\n        loc='upper right', bbox_to_anchor=(0.5, 0.5)\n\nncols : int, default: 1\n    The number of columns that the legend has.\n\n    For backward compatibility, the spelling *ncol* is also supported\n    but it is discouraged. If both are given, *ncols* takes precedence.\n\nprop : None or `~matplotlib.font_manager.FontProperties` or dict\n    The font properties of the legend. If None (default), the current\n    :data:`matplotlib.rcParams` will be used.\n\nfontsize : int or {'xx-small', 'x-small', 'small', 'medium', 'large', 'x-large', 'xx-large'}\n    The font size of the legend. If the value is numeric the size will be the\n    absolute font size in points. String values are relative to the current\n    default font size. This argument is only used if *prop* is not specified.\n\nlabelcolor : str or list, default: :rc:`legend.labelcolor`\n    The color of the text in the legend. Either a valid color string\n    (for example, 'red'), or a list of color strings. The labelcolor can\n    also be made to match the color of the line or marker using 'linecolor',\n    'markerfacecolor' (or 'mfc'), or 'markeredgecolor' (or 'mec').\n\n    Labelcolor can be set globally using :rc:`legend.labelcolor`. If None,\n    use :rc:`text.color`.\n\nnumpoints : int, default: :rc:`legend.numpoints`\n    The number of marker points in the legend when creating a legend\n    entry for a `.Line2D` (line).\n\nscatterpoints : int, default: :rc:`legend.scatterpoints`\n    The number of marker points in the legend when creating\n    a legend entry for a `.PathCollection` (scatter plot).\n\nscatteryoffsets : iterable of floats, default: ``[0.375, 0.5, 0.3125]``\n    The vertical offset (relative to the font size) for the markers\n    created for a scatter plot legend entry. 0.0 is at the base the\n    legend text, and 1.0 is at the top. To draw all markers at the\n    same height, set to ``[0.5]``.\n\nmarkerscale : float, default: :rc:`legend.markerscale`\n    The relative size of legend markers compared to the originally drawn ones.\n\nmarkerfirst : bool, default: True\n    If *True*, legend marker is placed to the left of the legend label.\n    If *False*, legend marker is placed to the right of the legend label.\n\nreverse : bool, default: False\n    If *True*, the legend labels are displayed in reverse order from the input.\n    If *False*, the legend labels are displayed in the same order as the input.\n\n    .. versionadded:: 3.7\n\nframeon : bool, default: :rc:`legend.frameon`\n    Whether the legend should be drawn on a patch (frame).\n\nfancybox : bool, default: :rc:`legend.fancybox`\n    Whether round edges should be enabled around the `.FancyBboxPatch` which\n    makes up the legend's background.\n\nshadow : None, bool or dict, default: :rc:`legend.shadow`\n    Whether to draw a shadow behind the legend.\n    The shadow can be configured using `.Patch` keywords.\n    Customization via :rc:`legend.shadow` is currently not supported.\n\nframealpha : float, default: :rc:`legend.framealpha`\n    The alpha transparency of the legend's background.\n    If *shadow* is activated and *framealpha* is ``None``, the default value is\n    ignored.\n\nfacecolor : \"inherit\" or color, default: :rc:`legend.facecolor`\n    The legend's background color.\n    If ``\"inherit\"``, use :rc:`axes.facecolor`.\n\nedgecolor : \"inherit\" or color, default: :rc:`legend.edgecolor`\n    The legend's background patch edge color.\n    If ``\"inherit\"``, use take :rc:`axes.edgecolor`.\n\nmode : {\"expand\", None}\n    If *mode* is set to ``\"expand\"`` the legend will be horizontally\n    expanded to fill the axes area (or *bbox_to_anchor* if defines\n    the legend's size).\n\nbbox_transform : None or `~matplotlib.transforms.Transform`\n    The transform for the bounding box (*bbox_to_anchor*). For a value\n    of ``None`` (default) the Axes'\n    :data:`~matplotlib.axes.Axes.transAxes` transform will be used.\n\ntitle : str or None\n    The legend's title. Default is no title (``None``).\n\ntitle_fontproperties : None or `~matplotlib.font_manager.FontProperties` or dict\n    The font properties of the legend's title. If None (default), the\n    *title_fontsize* argument will be used if present; if *title_fontsize* is\n    also None, the current :rc:`legend.title_fontsize` will be used.\n\ntitle_fontsize : int or {'xx-small', 'x-small', 'small', 'medium', 'large', 'x-large', 'xx-large'}, default: :rc:`legend.title_fontsize`\n    The font size of the legend's title.\n    Note: This cannot be combined with *title_fontproperties*. If you want\n    to set the fontsize alongside other font properties, use the *size*\n    parameter in *title_fontproperties*.\n\nalignment : {'center', 'left', 'right'}, default: 'center'\n    The alignment of the legend title and the box of entries. The entries\n    are aligned as a single block, so that markers always lined up.\n\nborderpad : float, default: :rc:`legend.borderpad`\n    The fractional whitespace inside the legend border, in font-size units.\n\nlabelspacing : float, default: :rc:`legend.labelspacing`\n    The vertical space between the legend entries, in font-size units.\n\nhandlelength : float, default: :rc:`legend.handlelength`\n    The length of the legend handles, in font-size units.\n\nhandleheight : float, default: :rc:`legend.handleheight`\n    The height of the legend handles, in font-size units.\n\nhandletextpad : float, default: :rc:`legend.handletextpad`\n    The pad between the legend handle and text, in font-size units.\n\nborderaxespad : float, default: :rc:`legend.borderaxespad`\n    The pad between the axes and legend border, in font-size units.\n\ncolumnspacing : float, default: :rc:`legend.columnspacing`\n    The spacing between columns, in font-size units.\n\nhandler_map : dict or None\n    The custom dictionary mapping instances or types to a legend\n    handler. This *handler_map* updates the default handler map\n    found at `matplotlib.legend.Legend.get_legend_handler_map`.\n\ndraggable : bool, default: False\n    Whether the legend can be dragged with the mouse.\n\n\nSee Also\n--------\n.Figure.legend\n\nNotes\n-----\nSome artists are not supported by this function.  See\n:ref:`legend_guide` for details.\n\nExamples\n--------\n.. plot:: gallery/text_labels_and_annotations/legend.py", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/241", "data": {"name": "matplotlib.pyplot.subplots[1].plot", "type": "method", "signature": "(*args, scalex=True, scaley=True, data=None, **kwargs)", "description": "Plot y versus x as lines and/or markers.\n\nCall signatures::\n\n    plot([x], y, [fmt], *, data=None, **kwargs)\n    plot([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n\nThe coordinates of the points or line nodes are given by *x*, *y*.\n\nThe optional parameter *fmt* is a convenient way for defining basic\nformatting like color, marker and linestyle. It's a shortcut string\nnotation described in the *Notes* section below.\n\n>>> plot(x, y)        # plot x and y using default line style and color\n>>> plot(x, y, 'bo')  # plot x and y using blue circle markers\n>>> plot(y)           # plot y using x as index array 0..N-1\n>>> plot(y, 'r+')     # ditto, but with red plusses\n\nYou can use `.Line2D` properties as keyword arguments for more\ncontrol on the appearance. Line properties and *fmt* can be mixed.\nThe following two calls yield identical results:\n\n>>> plot(x, y, 'go--', linewidth=2, markersize=12)\n>>> plot(x, y, color='green', marker='o', linestyle='dashed',\n...      linewidth=2, markersize=12)\n\nWhen conflicting with *fmt*, keyword arguments take precedence.\n\n\n**Plotting labelled data**\n\nThere's a convenient way for plotting objects with labelled data (i.e.\ndata that can be accessed by index ``obj['y']``). Instead of giving\nthe data in *x* and *y*, you can provide the object in the *data*\nparameter and just give the labels for *x* and *y*::\n\n>>> plot('xlabel', 'ylabel', data=obj)\n\nAll indexable objects are supported. This could e.g. be a `dict`, a\n`pandas.DataFrame` or a structured numpy array.\n\n\n**Plotting multiple sets of data**\n\nThere are various ways to plot multiple sets of data.\n\n- The most straight forward way is just to call `plot` multiple times.\n  Example:\n\n  >>> plot(x1, y1, 'bo')\n  >>> plot(x2, y2, 'go')\n\n- If *x* and/or *y* are 2D arrays a separate data set will be drawn\n  for every column. If both *x* and *y* are 2D, they must have the\n  same shape. If only one of them is 2D with shape (N, m) the other\n  must have length N and will be used for every data set m.\n\n  Example:\n\n  >>> x = [1, 2, 3]\n  >>> y = np.array([[1, 2], [3, 4], [5, 6]])\n  >>> plot(x, y)\n\n  is equivalent to:\n\n  >>> for col in range(y.shape[1]):\n  ...     plot(x, y[:, col])\n\n- The third way is to specify multiple sets of *[x]*, *y*, *[fmt]*\n  groups::\n\n  >>> plot(x1, y1, 'g^', x2, y2, 'g-')\n\n  In this case, any additional keyword argument applies to all\n  datasets. Also, this syntax cannot be combined with the *data*\n  parameter.\n\nBy default, each line is assigned a different style specified by a\n'style cycle'. The *fmt* and line property parameters are only\nnecessary if you want explicit deviations from these defaults.\nAlternatively, you can also change the style cycle using\n:rc:`axes.prop_cycle`.\n\n\nParameters\n----------\nx, y : array-like or scalar\n    The horizontal / vertical coordinates of the data points.\n    *x* values are optional and default to ``range(len(y))``.\n\n    Commonly, these parameters are 1D arrays.\n\n    They can also be scalars, or two-dimensional (in that case, the\n    columns represent separate data sets).\n\n    These arguments cannot be passed as keywords.\n\nfmt : str, optional\n    A format string, e.g. 'ro' for red circles. See the *Notes*\n    section for a full description of the format strings.\n\n    Format strings are just an abbreviation for quickly setting\n    basic line properties. All of these and more can also be\n    controlled by keyword arguments.\n\n    This argument cannot be passed as keyword.\n\ndata : indexable object, optional\n    An object with labelled data. If given, provide the label names to\n    plot in *x* and *y*.\n\n    .. note::\n        Technically there's a slight ambiguity in calls where the\n        second label is a valid *fmt*. ``plot('n', 'o', data=obj)``\n        could be ``plt(x, y)`` or ``plt(y, fmt)``. In such cases,\n        the former interpretation is chosen, but a warning is issued.\n        You may suppress the warning by adding an empty format string\n        ``plot('n', 'o', '', data=obj)``.\n\nReturns\n-------\nlist of `.Line2D`\n    A list of lines representing the plotted data.\n\nOther Parameters\n----------------\nscalex, scaley : bool, default: True\n    These parameters determine if the view limits are adapted to the\n    data limits. The values are passed on to\n    `~.axes.Axes.autoscale_view`.\n\n**kwargs : `~matplotlib.lines.Line2D` properties, optional\n    *kwargs* are used to specify properties like a line label (for\n    auto legends), linewidth, antialiasing, marker face color.\n    Example::\n\n    >>> plot([1, 2, 3], [1, 2, 3], 'go-', label='line 1', linewidth=2)\n    >>> plot([1, 2, 3], [1, 4, 9], 'rs', label='line 2')\n\n    If you specify multiple lines with one plot call, the kwargs apply\n    to all those lines. In case the label object is iterable, each\n    element is used as labels for each set of data.\n\n    Here is a list of available `.Line2D` properties:\n\n    Properties:\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: scalar or None\n    animated: bool\n    antialiased or aa: bool\n    clip_box: `~matplotlib.transforms.BboxBase` or None\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    color or c: color\n    dash_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    dash_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    dashes: sequence of floats (on/off ink in points) or (None, None)\n    data: (2, N) array or two 1D arrays\n    drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n    figure: `~matplotlib.figure.Figure`\n    fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n    gapcolor: color or None\n    gid: str\n    in_layout: bool\n    label: object\n    linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n    linewidth or lw: float\n    marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n    markeredgecolor or mec: color\n    markeredgewidth or mew: float\n    markerfacecolor or mfc: color\n    markerfacecoloralt or mfcalt: color\n    markersize or ms: float\n    markevery: None or int or (int, int) or slice or list[int] or float or (float, float) or list[bool]\n    mouseover: bool\n    path_effects: list of `.AbstractPathEffect`\n    picker: float or callable[[Artist, Event], tuple[bool, dict]]\n    pickradius: float\n    rasterized: bool\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    solid_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    solid_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    transform: unknown\n    url: str\n    visible: bool\n    xdata: 1D array\n    ydata: 1D array\n    zorder: float\n\nSee Also\n--------\nscatter : XY scatter plot with markers of varying size and/or color (\n    sometimes also called bubble chart).\n\nNotes\n-----\n**Format Strings**\n\nA format string consists of a part for color, marker and line::\n\n    fmt = '[marker][line][color]'\n\nEach of them is optional. If not provided, the value from the style\ncycle is used. Exception: If ``line`` is given, but no ``marker``,\nthe data will be a line without markers.\n\nOther combinations such as ``[color][marker][line]`` are also\nsupported, but note that their parsing may be ambiguous.\n\n**Markers**\n\n=============   ===============================\ncharacter       description\n=============   ===============================\n``'.'``         point marker\n``','``         pixel marker\n``'o'``         circle marker\n``'v'``         triangle_down marker\n``'^'``         triangle_up marker\n``'<'``         triangle_left marker\n``'>'``         triangle_right marker\n``'1'``         tri_down marker\n``'2'``         tri_up marker\n``'3'``         tri_left marker\n``'4'``         tri_right marker\n``'8'``         octagon marker\n``'s'``         square marker\n``'p'``         pentagon marker\n``'P'``         plus (filled) marker\n``'*'``         star marker\n``'h'``         hexagon1 marker\n``'H'``         hexagon2 marker\n``'+'``         plus marker\n``'x'``         x marker\n``'X'``         x (filled) marker\n``'D'``         diamond marker\n``'d'``         thin_diamond marker\n``'|'``         vline marker\n``'_'``         hline marker\n=============   ===============================\n\n**Line Styles**\n\n=============    ===============================\ncharacter        description\n=============    ===============================\n``'-'``          solid line style\n``'--'``         dashed line style\n``'-.'``         dash-dot line style\n``':'``          dotted line style\n=============    ===============================\n\nExample format strings::\n\n    'b'    # blue markers with default shape\n    'or'   # red circles\n    '-g'   # green solid line\n    '--'   # dashed line with default color\n    '^k:'  # black triangle_up markers connected by a dotted line\n\n**Colors**\n\nThe supported color abbreviations are the single letter codes\n\n=============    ===============================\ncharacter        color\n=============    ===============================\n``'b'``          blue\n``'g'``          green\n``'r'``          red\n``'c'``          cyan\n``'m'``          magenta\n``'y'``          yellow\n``'k'``          black\n``'w'``          white\n=============    ===============================\n\nand the ``'CN'`` colors that index into the default property cycle.\n\nIf the color is the only part of the format string, you can\nadditionally use any  `matplotlib.colors` spec, e.g. full names\n(``'green'``) or hex strings (``'#008000'``).", "parameters": {"type": "object", "properties": {"scalex": {"type": "bool", "default": true}, "scaley": {"type": "bool", "default": true}, "data": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/241", "data": {"name": "matplotlib.pyplot.subplots[1].set_title", "type": "method", "signature": "(label, fontdict=None, loc=None, pad=None, *, y=None, **kwargs)", "description": "Set a title for the Axes.\n\nSet one of the three available Axes titles. The available titles\nare positioned above the Axes in the center, flush with the left\nedge, and flush with the right edge.\n\nParameters\n----------\nlabel : str\n    Text to use for the title\n\nfontdict : dict\n\n    .. admonition:: Discouraged\n\n       The use of *fontdict* is discouraged. Parameters should be passed as\n       individual keyword arguments or using dictionary-unpacking\n       ``set_title(..., **fontdict)``.\n\n    A dictionary controlling the appearance of the title text,\n    the default *fontdict* is::\n\n       {'fontsize': rcParams['axes.titlesize'],\n        'fontweight': rcParams['axes.titleweight'],\n        'color': rcParams['axes.titlecolor'],\n        'verticalalignment': 'baseline',\n        'horizontalalignment': loc}\n\nloc : {'center', 'left', 'right'}, default: :rc:`axes.titlelocation`\n    Which title to set.\n\ny : float, default: :rc:`axes.titley`\n    Vertical Axes location for the title (1.0 is the top).  If\n    None (the default) and :rc:`axes.titley` is also None, y is\n    determined automatically to avoid decorators on the Axes.\n\npad : float, default: :rc:`axes.titlepad`\n    The offset of the title from the top of the Axes, in points.\n\nReturns\n-------\n`.Text`\n    The matplotlib text instance representing the title\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    Other keyword arguments are text properties, see `.Text` for a list\n    of valid text properties.", "parameters": {"type": "object", "properties": {"label": {}, "fontdict": {"type": "NoneType", "default": null}, "loc": {"type": "NoneType", "default": null}, "pad": {"type": "NoneType", "default": null}, "y": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/241", "data": {"name": "numpy.array", "type": "callable", "signature": "(*args, **kwargs)", "description": "array(object, dtype=None, *, copy=True, order='K', subok=False, ndmin=0,\n      like=None)\n\nCreate an array.\n\nParameters\n----------\nobject : array_like\n    An array, any object exposing the array interface, an object whose\n    __array__ method returns an array, or any (nested) sequence.\n    If object is a scalar, a 0-dimensional array containing object is\n    returned.\ndtype : data-type, optional\n    The desired data-type for the array.  If not given, then the type will\n    be determined as the minimum type required to hold the objects in the\n    sequence.\ncopy : bool, optional\n    If true (default), then the object is copied.  Otherwise, a copy will\n    only be made if __array__ returns a copy, if obj is a nested sequence,\n    or if a copy is needed to satisfy any of the other requirements\n    (`dtype`, `order`, etc.).\norder : {'K', 'A', 'C', 'F'}, optional\n    Specify the memory layout of the array. If object is not an array, the\n    newly created array will be in C order (row major) unless 'F' is\n    specified, in which case it will be in Fortran order (column major).\n    If object is an array the following holds.\n\n    ===== ========= ===================================================\n    order  no copy                     copy=True\n    ===== ========= ===================================================\n    'K'   unchanged F & C order preserved, otherwise most similar order\n    'A'   unchanged F order if input is F and not C, otherwise C order\n    'C'   C order   C order\n    'F'   F order   F order\n    ===== ========= ===================================================\n\n    When ``copy=False`` and a copy is made for other reasons, the result is\n    the same as if ``copy=True``, with some exceptions for 'A', see the\n    Notes section. The default order is 'K'.\nsubok : bool, optional\n    If True, then sub-classes will be passed-through, otherwise\n    the returned array will be forced to be a base-class array (default).\nndmin : int, optional\n    Specifies the minimum number of dimensions that the resulting\n    array should have.  Ones will be prepended to the shape as\n    needed to meet this requirement.\nlike : array_like, optional\n    Reference object to allow the creation of arrays which are not\n    NumPy arrays. If an array-like passed in as ``like`` supports\n    the ``__array_function__`` protocol, the result will be defined\n    by it. In this case, it ensures the creation of an array object\n    compatible with that passed in via this argument.\n\n    .. versionadded:: 1.20.0\n\nReturns\n-------\nout : ndarray\n    An array object satisfying the specified requirements.\n\nSee Also\n--------\nempty_like : Return an empty array with shape and type of input.\nones_like : Return an array of ones with shape and type of input.\nzeros_like : Return an array of zeros with shape and type of input.\nfull_like : Return a new array with shape of input filled with value.\nempty : Return a new uninitialized array.\nones : Return a new array setting values to one.\nzeros : Return a new array setting values to zero.\nfull : Return a new array of given shape filled with value.\n\n\nNotes\n-----\nWhen order is 'A' and `object` is an array in neither 'C' nor 'F' order,\nand a copy is forced by a change in dtype, then the order of the result is\nnot necessarily 'C' as expected. This is likely a bug.\n\nExamples\n--------\n>>> np.array([1, 2, 3])\narray([1, 2, 3])\n\nUpcasting:\n\n>>> np.array([1, 2, 3.0])\narray([ 1.,  2.,  3.])\n\nMore than one dimension:\n\n>>> np.array([[1, 2], [3, 4]])\narray([[1, 2],\n       [3, 4]])\n\nMinimum dimensions 2:\n\n>>> np.array([1, 2, 3], ndmin=2)\narray([[1, 2, 3]])\n\nType provided:\n\n>>> np.array([1, 2, 3], dtype=complex)\narray([ 1.+0.j,  2.+0.j,  3.+0.j])\n\nData-type consisting of more than one element:\n\n>>> x = np.array([(1,2),(3,4)],dtype=[('a','<i4'),('b','<i4')])\n>>> x['a']\narray([1, 3])\n\nCreating an array from sub-classes:\n\n>>> np.array(np.mat('1 2; 3 4'))\narray([[1, 2],\n       [3, 4]])\n\n>>> np.array(np.mat('1 2; 3 4'), subok=True)\nmatrix([[1, 2],\n        [3, 4]])", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/241", "data": {"name": "numpy.array.size", "type": "callable", "signature": "(*args, **kwargs)", "description": "array(object, dtype=None, *, copy=True, order='K', subok=False, ndmin=0,\n      like=None)\n\nCreate an array.\n\nParameters\n----------\nobject : array_like\n    An array, any object exposing the array interface, an object whose\n    __array__ method returns an array, or any (nested) sequence.\n    If object is a scalar, a 0-dimensional array containing object is\n    returned.\ndtype : data-type, optional\n    The desired data-type for the array.  If not given, then the type will\n    be determined as the minimum type required to hold the objects in the\n    sequence.\ncopy : bool, optional\n    If true (default), then the object is copied.  Otherwise, a copy will\n    only be made if __array__ returns a copy, if obj is a nested sequence,\n    or if a copy is needed to satisfy any of the other requirements\n    (`dtype`, `order`, etc.).\norder : {'K', 'A', 'C', 'F'}, optional\n    Specify the memory layout of the array. If object is not an array, the\n    newly created array will be in C order (row major) unless 'F' is\n    specified, in which case it will be in Fortran order (column major).\n    If object is an array the following holds.\n\n    ===== ========= ===================================================\n    order  no copy                     copy=True\n    ===== ========= ===================================================\n    'K'   unchanged F & C order preserved, otherwise most similar order\n    'A'   unchanged F order if input is F and not C, otherwise C order\n    'C'   C order   C order\n    'F'   F order   F order\n    ===== ========= ===================================================\n\n    When ``copy=False`` and a copy is made for other reasons, the result is\n    the same as if ``copy=True``, with some exceptions for 'A', see the\n    Notes section. The default order is 'K'.\nsubok : bool, optional\n    If True, then sub-classes will be passed-through, otherwise\n    the returned array will be forced to be a base-class array (default).\nndmin : int, optional\n    Specifies the minimum number of dimensions that the resulting\n    array should have.  Ones will be prepended to the shape as\n    needed to meet this requirement.\nlike : array_like, optional\n    Reference object to allow the creation of arrays which are not\n    NumPy arrays. If an array-like passed in as ``like`` supports\n    the ``__array_function__`` protocol, the result will be defined\n    by it. In this case, it ensures the creation of an array object\n    compatible with that passed in via this argument.\n\n    .. versionadded:: 1.20.0\n\nReturns\n-------\nout : ndarray\n    An array object satisfying the specified requirements.\n\nSee Also\n--------\nempty_like : Return an empty array with shape and type of input.\nones_like : Return an array of ones with shape and type of input.\nzeros_like : Return an array of zeros with shape and type of input.\nfull_like : Return a new array with shape of input filled with value.\nempty : Return a new uninitialized array.\nones : Return a new array setting values to one.\nzeros : Return a new array setting values to zero.\nfull : Return a new array of given shape filled with value.\n\n\nNotes\n-----\nWhen order is 'A' and `object` is an array in neither 'C' nor 'F' order,\nand a copy is forced by a change in dtype, then the order of the result is\nnot necessarily 'C' as expected. This is likely a bug.\n\nExamples\n--------\n>>> np.array([1, 2, 3])\narray([1, 2, 3])\n\nUpcasting:\n\n>>> np.array([1, 2, 3.0])\narray([ 1.,  2.,  3.])\n\nMore than one dimension:\n\n>>> np.array([[1, 2], [3, 4]])\narray([[1, 2],\n       [3, 4]])\n\nMinimum dimensions 2:\n\n>>> np.array([1, 2, 3], ndmin=2)\narray([[1, 2, 3]])\n\nType provided:\n\n>>> np.array([1, 2, 3], dtype=complex)\narray([ 1.+0.j,  2.+0.j,  3.+0.j])\n\nData-type consisting of more than one element:\n\n>>> x = np.array([(1,2),(3,4)],dtype=[('a','<i4'),('b','<i4')])\n>>> x['a']\narray([1, 3])\n\nCreating an array from sub-classes:\n\n>>> np.array(np.mat('1 2; 3 4'))\narray([[1, 2],\n       [3, 4]])\n\n>>> np.array(np.mat('1 2; 3 4'), subok=True)\nmatrix([[1, 2],\n        [3, 4]])", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/241", "data": {"name": "sklearn.preprocessing.normalize", "type": "callable", "signature": "(X)", "description": "Scale input vectors individually to unit norm (vector length).\n\nRead more in the :ref:`User Guide <preprocessing_normalization>`.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The data to normalize, element by element.\n    scipy.sparse matrices should be in CSR format to avoid an\n    un-necessary copy.\n\nnorm : {'l1', 'l2', 'max'}, default='l2'\n    The norm to use to normalize each non zero sample (or each non-zero\n    feature if axis is 0).\n\naxis : {0, 1}, default=1\n    Define axis used to normalize the data along. If 1, independently\n    normalize each sample, otherwise (if 0) normalize each feature.\n\ncopy : bool, default=True\n    If False, try to avoid a copy and normalize in place.\n    This is not guaranteed to always work in place; e.g. if the data is\n    a numpy array with an int dtype, a copy will be returned even with\n    copy=False.\n\nreturn_norm : bool, default=False\n    Whether to return the computed norms.\n\nReturns\n-------\nX : {ndarray, sparse matrix} of shape (n_samples, n_features)\n    Normalized input X.\n\nnorms : ndarray of shape (n_samples, ) if axis=1 else (n_features, )\n    An array of norms along given axis for X.\n    When X is sparse, a NotImplementedError will be raised\n    for norm 'l1' or 'l2'.\n\nSee Also\n--------\nNormalizer : Performs normalization using the Transformer API\n    (e.g. as part of a preprocessing :class:`~sklearn.pipeline.Pipeline`).\n\nNotes\n-----\nFor a comparison of the different scalers, transformers, and normalizers,\nsee: :ref:`sphx_glr_auto_examples_preprocessing_plot_all_scaling.py`.\n\nExamples\n--------\n>>> from sklearn.preprocessing import normalize\n>>> X = [[-2, 1, 2], [-1, 0, 1]]\n>>> normalize(X, norm=\"l1\")  # L1 normalization each row independently\narray([[-0.4,  0.2,  0.4],\n       [-0.5,  0. ,  0.5]])\n>>> normalize(X, norm=\"l2\")  # L2 normalization each row independently\narray([[-0.66...,  0.33...,  0.66...],\n       [-0.70...,  0.     ,  0.70...]])", "parameters": {"type": "object", "properties": {"X": {}}}}}
{"task_id": "BigCodeBench/267", "data": {"name": "data.values()"}}
{"task_id": "BigCodeBench/267", "data": {"name": "matplotlib.pyplot.subplots", "type": "class", "signature": "(**fig_kw) -> 'tuple[Figure)", "description": "Create a figure and a set of subplots.\n\nThis utility wrapper makes it convenient to create common layouts of\nsubplots, including the enclosing figure object, in a single call.\n\nParameters\n----------\nnrows, ncols : int, default: 1\n    Number of rows/columns of the subplot grid.\n\nsharex, sharey : bool or {'none', 'all', 'row', 'col'}, default: False\n    Controls sharing of properties among x (*sharex*) or y (*sharey*)\n    axes:\n\n    - True or 'all': x- or y-axis will be shared among all subplots.\n    - False or 'none': each subplot x- or y-axis will be independent.\n    - 'row': each subplot row will share an x- or y-axis.\n    - 'col': each subplot column will share an x- or y-axis.\n\n    When subplots have a shared x-axis along a column, only the x tick\n    labels of the bottom subplot are created. Similarly, when subplots\n    have a shared y-axis along a row, only the y tick labels of the first\n    column subplot are created. To later turn other subplots' ticklabels\n    on, use `~matplotlib.axes.Axes.tick_params`.\n\n    When subplots have a shared axis that has units, calling\n    `~matplotlib.axis.Axis.set_units` will update each axis with the\n    new units.\n\nsqueeze : bool, default: True\n    - If True, extra dimensions are squeezed out from the returned\n      array of `~matplotlib.axes.Axes`:\n\n      - if only one subplot is constructed (nrows=ncols=1), the\n        resulting single Axes object is returned as a scalar.\n      - for Nx1 or 1xM subplots, the returned object is a 1D numpy\n        object array of Axes objects.\n      - for NxM, subplots with N>1 and M>1 are returned as a 2D array.\n\n    - If False, no squeezing at all is done: the returned Axes object is\n      always a 2D array containing Axes instances, even if it ends up\n      being 1x1.\n\nwidth_ratios : array-like of length *ncols*, optional\n    Defines the relative widths of the columns. Each column gets a\n    relative width of ``width_ratios[i] / sum(width_ratios)``.\n    If not given, all columns will have the same width.  Equivalent\n    to ``gridspec_kw={'width_ratios': [...]}``.\n\nheight_ratios : array-like of length *nrows*, optional\n    Defines the relative heights of the rows. Each row gets a\n    relative height of ``height_ratios[i] / sum(height_ratios)``.\n    If not given, all rows will have the same height. Convenience\n    for ``gridspec_kw={'height_ratios': [...]}``.\n\nsubplot_kw : dict, optional\n    Dict with keywords passed to the\n    `~matplotlib.figure.Figure.add_subplot` call used to create each\n    subplot.\n\ngridspec_kw : dict, optional\n    Dict with keywords passed to the `~matplotlib.gridspec.GridSpec`\n    constructor used to create the grid the subplots are placed on.\n\n**fig_kw\n    All additional keyword arguments are passed to the\n    `.pyplot.figure` call.\n\nReturns\n-------\nfig : `.Figure`\n\nax : `~matplotlib.axes.Axes` or array of Axes\n    *ax* can be either a single `~.axes.Axes` object, or an array of Axes\n    objects if more than one subplot was created.  The dimensions of the\n    resulting array can be controlled with the squeeze keyword, see above.\n\n    Typical idioms for handling the return value are::\n\n        # using the variable ax for single a Axes\n        fig, ax = plt.subplots()\n\n        # using the variable axs for multiple Axes\n        fig, axs = plt.subplots(2, 2)\n\n        # using tuple unpacking for multiple Axes\n        fig, (ax1, ax2) = plt.subplots(1, 2)\n        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n\n    The names ``ax`` and pluralized ``axs`` are preferred over ``axes``\n    because for the latter it's not clear if it refers to a single\n    `~.axes.Axes` instance or a collection of these.\n\nSee Also\n--------\n.pyplot.figure\n.pyplot.subplot\n.pyplot.axes\n.Figure.subplots\n.Figure.add_subplot\n\nExamples\n--------\n::\n\n    # First create some toy data:\n    x = np.linspace(0, 2*np.pi, 400)\n    y = np.sin(x**2)\n\n    # Create just a figure and only one subplot\n    fig, ax = plt.subplots()\n    ax.plot(x, y)\n    ax.set_title('Simple plot')\n\n    # Create two subplots and unpack the output array immediately\n    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n    ax1.plot(x, y)\n    ax1.set_title('Sharing Y axis')\n    ax2.scatter(x, y)\n\n    # Create four polar axes and access them through the returned array\n    fig, axs = plt.subplots(2, 2, subplot_kw=dict(projection=\"polar\"))\n    axs[0, 0].plot(x, y)\n    axs[1, 1].scatter(x, y)\n\n    # Share a X axis with each column of subplots\n    plt.subplots(2, 2, sharex='col')\n\n    # Share a Y axis with each row of subplots\n    plt.subplots(2, 2, sharey='row')\n\n    # Share both X and Y axes with all subplots\n    plt.subplots(2, 2, sharex='all', sharey='all')\n\n    # Note that this is the same as\n    plt.subplots(2, 2, sharex=True, sharey=True)\n\n    # Create figure number 10 with a single subplot\n    # and clears it if it already exists.\n    fig, ax = plt.subplots(num=10, clear=True)", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/267", "data": {"name": "matplotlib.pyplot.subplots[1].plot", "type": "method", "signature": "(*args, scalex=True, scaley=True, data=None, **kwargs)", "description": "Plot y versus x as lines and/or markers.\n\nCall signatures::\n\n    plot([x], y, [fmt], *, data=None, **kwargs)\n    plot([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n\nThe coordinates of the points or line nodes are given by *x*, *y*.\n\nThe optional parameter *fmt* is a convenient way for defining basic\nformatting like color, marker and linestyle. It's a shortcut string\nnotation described in the *Notes* section below.\n\n>>> plot(x, y)        # plot x and y using default line style and color\n>>> plot(x, y, 'bo')  # plot x and y using blue circle markers\n>>> plot(y)           # plot y using x as index array 0..N-1\n>>> plot(y, 'r+')     # ditto, but with red plusses\n\nYou can use `.Line2D` properties as keyword arguments for more\ncontrol on the appearance. Line properties and *fmt* can be mixed.\nThe following two calls yield identical results:\n\n>>> plot(x, y, 'go--', linewidth=2, markersize=12)\n>>> plot(x, y, color='green', marker='o', linestyle='dashed',\n...      linewidth=2, markersize=12)\n\nWhen conflicting with *fmt*, keyword arguments take precedence.\n\n\n**Plotting labelled data**\n\nThere's a convenient way for plotting objects with labelled data (i.e.\ndata that can be accessed by index ``obj['y']``). Instead of giving\nthe data in *x* and *y*, you can provide the object in the *data*\nparameter and just give the labels for *x* and *y*::\n\n>>> plot('xlabel', 'ylabel', data=obj)\n\nAll indexable objects are supported. This could e.g. be a `dict`, a\n`pandas.DataFrame` or a structured numpy array.\n\n\n**Plotting multiple sets of data**\n\nThere are various ways to plot multiple sets of data.\n\n- The most straight forward way is just to call `plot` multiple times.\n  Example:\n\n  >>> plot(x1, y1, 'bo')\n  >>> plot(x2, y2, 'go')\n\n- If *x* and/or *y* are 2D arrays a separate data set will be drawn\n  for every column. If both *x* and *y* are 2D, they must have the\n  same shape. If only one of them is 2D with shape (N, m) the other\n  must have length N and will be used for every data set m.\n\n  Example:\n\n  >>> x = [1, 2, 3]\n  >>> y = np.array([[1, 2], [3, 4], [5, 6]])\n  >>> plot(x, y)\n\n  is equivalent to:\n\n  >>> for col in range(y.shape[1]):\n  ...     plot(x, y[:, col])\n\n- The third way is to specify multiple sets of *[x]*, *y*, *[fmt]*\n  groups::\n\n  >>> plot(x1, y1, 'g^', x2, y2, 'g-')\n\n  In this case, any additional keyword argument applies to all\n  datasets. Also, this syntax cannot be combined with the *data*\n  parameter.\n\nBy default, each line is assigned a different style specified by a\n'style cycle'. The *fmt* and line property parameters are only\nnecessary if you want explicit deviations from these defaults.\nAlternatively, you can also change the style cycle using\n:rc:`axes.prop_cycle`.\n\n\nParameters\n----------\nx, y : array-like or scalar\n    The horizontal / vertical coordinates of the data points.\n    *x* values are optional and default to ``range(len(y))``.\n\n    Commonly, these parameters are 1D arrays.\n\n    They can also be scalars, or two-dimensional (in that case, the\n    columns represent separate data sets).\n\n    These arguments cannot be passed as keywords.\n\nfmt : str, optional\n    A format string, e.g. 'ro' for red circles. See the *Notes*\n    section for a full description of the format strings.\n\n    Format strings are just an abbreviation for quickly setting\n    basic line properties. All of these and more can also be\n    controlled by keyword arguments.\n\n    This argument cannot be passed as keyword.\n\ndata : indexable object, optional\n    An object with labelled data. If given, provide the label names to\n    plot in *x* and *y*.\n\n    .. note::\n        Technically there's a slight ambiguity in calls where the\n        second label is a valid *fmt*. ``plot('n', 'o', data=obj)``\n        could be ``plt(x, y)`` or ``plt(y, fmt)``. In such cases,\n        the former interpretation is chosen, but a warning is issued.\n        You may suppress the warning by adding an empty format string\n        ``plot('n', 'o', '', data=obj)``.\n\nReturns\n-------\nlist of `.Line2D`\n    A list of lines representing the plotted data.\n\nOther Parameters\n----------------\nscalex, scaley : bool, default: True\n    These parameters determine if the view limits are adapted to the\n    data limits. The values are passed on to\n    `~.axes.Axes.autoscale_view`.\n\n**kwargs : `~matplotlib.lines.Line2D` properties, optional\n    *kwargs* are used to specify properties like a line label (for\n    auto legends), linewidth, antialiasing, marker face color.\n    Example::\n\n    >>> plot([1, 2, 3], [1, 2, 3], 'go-', label='line 1', linewidth=2)\n    >>> plot([1, 2, 3], [1, 4, 9], 'rs', label='line 2')\n\n    If you specify multiple lines with one plot call, the kwargs apply\n    to all those lines. In case the label object is iterable, each\n    element is used as labels for each set of data.\n\n    Here is a list of available `.Line2D` properties:\n\n    Properties:\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: scalar or None\n    animated: bool\n    antialiased or aa: bool\n    clip_box: `~matplotlib.transforms.BboxBase` or None\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    color or c: color\n    dash_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    dash_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    dashes: sequence of floats (on/off ink in points) or (None, None)\n    data: (2, N) array or two 1D arrays\n    drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n    figure: `~matplotlib.figure.Figure`\n    fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n    gapcolor: color or None\n    gid: str\n    in_layout: bool\n    label: object\n    linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n    linewidth or lw: float\n    marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n    markeredgecolor or mec: color\n    markeredgewidth or mew: float\n    markerfacecolor or mfc: color\n    markerfacecoloralt or mfcalt: color\n    markersize or ms: float\n    markevery: None or int or (int, int) or slice or list[int] or float or (float, float) or list[bool]\n    mouseover: bool\n    path_effects: list of `.AbstractPathEffect`\n    picker: float or callable[[Artist, Event], tuple[bool, dict]]\n    pickradius: float\n    rasterized: bool\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    solid_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    solid_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    transform: unknown\n    url: str\n    visible: bool\n    xdata: 1D array\n    ydata: 1D array\n    zorder: float\n\nSee Also\n--------\nscatter : XY scatter plot with markers of varying size and/or color (\n    sometimes also called bubble chart).\n\nNotes\n-----\n**Format Strings**\n\nA format string consists of a part for color, marker and line::\n\n    fmt = '[marker][line][color]'\n\nEach of them is optional. If not provided, the value from the style\ncycle is used. Exception: If ``line`` is given, but no ``marker``,\nthe data will be a line without markers.\n\nOther combinations such as ``[color][marker][line]`` are also\nsupported, but note that their parsing may be ambiguous.\n\n**Markers**\n\n=============   ===============================\ncharacter       description\n=============   ===============================\n``'.'``         point marker\n``','``         pixel marker\n``'o'``         circle marker\n``'v'``         triangle_down marker\n``'^'``         triangle_up marker\n``'<'``         triangle_left marker\n``'>'``         triangle_right marker\n``'1'``         tri_down marker\n``'2'``         tri_up marker\n``'3'``         tri_left marker\n``'4'``         tri_right marker\n``'8'``         octagon marker\n``'s'``         square marker\n``'p'``         pentagon marker\n``'P'``         plus (filled) marker\n``'*'``         star marker\n``'h'``         hexagon1 marker\n``'H'``         hexagon2 marker\n``'+'``         plus marker\n``'x'``         x marker\n``'X'``         x (filled) marker\n``'D'``         diamond marker\n``'d'``         thin_diamond marker\n``'|'``         vline marker\n``'_'``         hline marker\n=============   ===============================\n\n**Line Styles**\n\n=============    ===============================\ncharacter        description\n=============    ===============================\n``'-'``          solid line style\n``'--'``         dashed line style\n``'-.'``         dash-dot line style\n``':'``          dotted line style\n=============    ===============================\n\nExample format strings::\n\n    'b'    # blue markers with default shape\n    'or'   # red circles\n    '-g'   # green solid line\n    '--'   # dashed line with default color\n    '^k:'  # black triangle_up markers connected by a dotted line\n\n**Colors**\n\nThe supported color abbreviations are the single letter codes\n\n=============    ===============================\ncharacter        color\n=============    ===============================\n``'b'``          blue\n``'g'``          green\n``'r'``          red\n``'c'``          cyan\n``'m'``          magenta\n``'y'``          yellow\n``'k'``          black\n``'w'``          white\n=============    ===============================\n\nand the ``'CN'`` colors that index into the default property cycle.\n\nIf the color is the only part of the format string, you can\nadditionally use any  `matplotlib.colors` spec, e.g. full names\n(``'green'``) or hex strings (``'#008000'``).", "parameters": {"type": "object", "properties": {"scalex": {"type": "bool", "default": true}, "scaley": {"type": "bool", "default": true}, "data": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/267", "data": {"name": "matplotlib.pyplot.subplots[1].set_xlabel", "type": "method", "signature": "(xlabel, fontdict=None, labelpad=None, *, loc=None, **kwargs)", "description": "Set the label for the x-axis.\n\nParameters\n----------\nxlabel : str\n    The label text.\n\nlabelpad : float, default: :rc:`axes.labelpad`\n    Spacing in points from the Axes bounding box including ticks\n    and tick labels.  If None, the previous value is left as is.\n\nloc : {'left', 'center', 'right'}, default: :rc:`xaxis.labellocation`\n    The label position. This is a high-level alternative for passing\n    parameters *x* and *horizontalalignment*.\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    `.Text` properties control the appearance of the label.\n\nSee Also\n--------\ntext : Documents the properties supported by `.Text`.", "parameters": {"type": "object", "properties": {"xlabel": {}, "fontdict": {"type": "NoneType", "default": null}, "labelpad": {"type": "NoneType", "default": null}, "loc": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/267", "data": {"name": "matplotlib.pyplot.subplots[1].set_title", "type": "method", "signature": "(label, fontdict=None, loc=None, pad=None, *, y=None, **kwargs)", "description": "Set a title for the Axes.\n\nSet one of the three available Axes titles. The available titles\nare positioned above the Axes in the center, flush with the left\nedge, and flush with the right edge.\n\nParameters\n----------\nlabel : str\n    Text to use for the title\n\nfontdict : dict\n\n    .. admonition:: Discouraged\n\n       The use of *fontdict* is discouraged. Parameters should be passed as\n       individual keyword arguments or using dictionary-unpacking\n       ``set_title(..., **fontdict)``.\n\n    A dictionary controlling the appearance of the title text,\n    the default *fontdict* is::\n\n       {'fontsize': rcParams['axes.titlesize'],\n        'fontweight': rcParams['axes.titleweight'],\n        'color': rcParams['axes.titlecolor'],\n        'verticalalignment': 'baseline',\n        'horizontalalignment': loc}\n\nloc : {'center', 'left', 'right'}, default: :rc:`axes.titlelocation`\n    Which title to set.\n\ny : float, default: :rc:`axes.titley`\n    Vertical Axes location for the title (1.0 is the top).  If\n    None (the default) and :rc:`axes.titley` is also None, y is\n    determined automatically to avoid decorators on the Axes.\n\npad : float, default: :rc:`axes.titlepad`\n    The offset of the title from the top of the Axes, in points.\n\nReturns\n-------\n`.Text`\n    The matplotlib text instance representing the title\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    Other keyword arguments are text properties, see `.Text` for a list\n    of valid text properties.", "parameters": {"type": "object", "properties": {"label": {}, "fontdict": {"type": "NoneType", "default": null}, "loc": {"type": "NoneType", "default": null}, "pad": {"type": "NoneType", "default": null}, "y": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/267", "data": {"name": "matplotlib.pyplot.subplots[1].set_ylabel", "type": "method", "signature": "(ylabel, fontdict=None, labelpad=None, *, loc=None, **kwargs)", "description": "Set the label for the y-axis.\n\nParameters\n----------\nylabel : str\n    The label text.\n\nlabelpad : float, default: :rc:`axes.labelpad`\n    Spacing in points from the Axes bounding box including ticks\n    and tick labels.  If None, the previous value is left as is.\n\nloc : {'bottom', 'center', 'top'}, default: :rc:`yaxis.labellocation`\n    The label position. This is a high-level alternative for passing\n    parameters *y* and *horizontalalignment*.\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    `.Text` properties control the appearance of the label.\n\nSee Also\n--------\ntext : Documents the properties supported by `.Text`.", "parameters": {"type": "object", "properties": {"ylabel": {}, "fontdict": {"type": "NoneType", "default": null}, "labelpad": {"type": "NoneType", "default": null}, "loc": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/267", "data": {"name": "numpy.pi", "type": "constant", "signature": null, "description": "Convert a string or number to a floating point number, if possible.", "value": "3.141592653589793", "parameters": {"type": "constant"}}}
{"task_id": "BigCodeBench/267", "data": {"name": "numpy.abs", "type": "callable", "signature": "(*args, **kwargs)", "description": "absolute(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n\nCalculate the absolute value element-wise.\n\n``np.abs`` is a shorthand for this function.\n\nParameters\n----------\nx : array_like\n    Input array.\nout : ndarray, None, or tuple of ndarray and None, optional\n    A location into which the result is stored. If provided, it must have\n    a shape that the inputs broadcast to. If not provided or None,\n    a freshly-allocated array is returned. A tuple (possible only as a\n    keyword argument) must have length equal to the number of outputs.\nwhere : array_like, optional\n    This condition is broadcast over the input. At locations where the\n    condition is True, the `out` array will be set to the ufunc result.\n    Elsewhere, the `out` array will retain its original value.\n    Note that if an uninitialized `out` array is created via the default\n    ``out=None``, locations within it where the condition is False will\n    remain uninitialized.\n**kwargs\n    For other keyword-only arguments, see the\n    :ref:`ufunc docs <ufuncs.kwargs>`.\n\nReturns\n-------\nabsolute : ndarray\n    An ndarray containing the absolute value of\n    each element in `x`.  For complex input, ``a + ib``, the\n    absolute value is :math:`\\sqrt{ a^2 + b^2 }`.\n    This is a scalar if `x` is a scalar.\n\nExamples\n--------\n>>> x = np.array([-1.2, 1.2])\n>>> np.absolute(x)\narray([ 1.2,  1.2])\n>>> np.absolute(1.2 + 1j)\n1.5620499351813308\n\nPlot the function over ``[-10, 10]``:\n\n>>> import matplotlib.pyplot as plt\n\n>>> x = np.linspace(start=-10, stop=10, num=101)\n>>> plt.plot(x, np.absolute(x))\n>>> plt.show()\n\nPlot the function over the complex plane:\n\n>>> xx = x + 1j * x[:, np.newaxis]\n>>> plt.imshow(np.abs(xx), extent=[-10, 10, -10, 10], cmap='gray')\n>>> plt.show()\n\nThe `abs` function can be used as a shorthand for ``np.absolute`` on\nndarrays.\n\n>>> x = np.array([-1.2, 1.2])\n>>> abs(x)\narray([1.2, 1.2])", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/267", "data": {"name": "numpy.outer", "type": "callable", "signature": "(a, b)", "description": "Compute the outer product of two vectors.\n\nGiven two vectors, ``a = [a0, a1, ..., aM]`` and\n``b = [b0, b1, ..., bN]``,\nthe outer product [1]_ is::\n\n  [[a0*b0  a0*b1 ... a0*bN ]\n   [a1*b0    .\n   [ ...          .\n   [aM*b0            aM*bN ]]\n\nParameters\n----------\na : (M,) array_like\n    First input vector.  Input is flattened if\n    not already 1-dimensional.\nb : (N,) array_like\n    Second input vector.  Input is flattened if\n    not already 1-dimensional.\nout : (M, N) ndarray, optional\n    A location where the result is stored\n\n    .. versionadded:: 1.9.0\n\nReturns\n-------\nout : (M, N) ndarray\n    ``out[i, j] = a[i] * b[j]``\n\nSee also\n--------\ninner\neinsum : ``einsum('i,j->ij', a.ravel(), b.ravel())`` is the equivalent.\nufunc.outer : A generalization to dimensions other than 1D and other\n              operations. ``np.multiply.outer(a.ravel(), b.ravel())``\n              is the equivalent.\ntensordot : ``np.tensordot(a.ravel(), b.ravel(), axes=((), ()))``\n            is the equivalent.\n\nReferences\n----------\n.. [1] : G. H. Golub and C. F. Van Loan, *Matrix Computations*, 3rd\n         ed., Baltimore, MD, Johns Hopkins University Press, 1996,\n         pg. 8.\n\nExamples\n--------\nMake a (*very* coarse) grid for computing a Mandelbrot set:\n\n>>> rl = np.outer(np.ones((5,)), np.linspace(-2, 2, 5))\n>>> rl\narray([[-2., -1.,  0.,  1.,  2.],\n       [-2., -1.,  0.,  1.,  2.],\n       [-2., -1.,  0.,  1.,  2.],\n       [-2., -1.,  0.,  1.,  2.],\n       [-2., -1.,  0.,  1.,  2.]])\n>>> im = np.outer(1j*np.linspace(2, -2, 5), np.ones((5,)))\n>>> im\narray([[0.+2.j, 0.+2.j, 0.+2.j, 0.+2.j, 0.+2.j],\n       [0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j],\n       [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n       [0.-1.j, 0.-1.j, 0.-1.j, 0.-1.j, 0.-1.j],\n       [0.-2.j, 0.-2.j, 0.-2.j, 0.-2.j, 0.-2.j]])\n>>> grid = rl + im\n>>> grid\narray([[-2.+2.j, -1.+2.j,  0.+2.j,  1.+2.j,  2.+2.j],\n       [-2.+1.j, -1.+1.j,  0.+1.j,  1.+1.j,  2.+1.j],\n       [-2.+0.j, -1.+0.j,  0.+0.j,  1.+0.j,  2.+0.j],\n       [-2.-1.j, -1.-1.j,  0.-1.j,  1.-1.j,  2.-1.j],\n       [-2.-2.j, -1.-2.j,  0.-2.j,  1.-2.j,  2.-2.j]])\n\nAn example using a \"vector\" of letters:\n\n>>> x = np.array(['a', 'b', 'c'], dtype=object)\n>>> np.outer(x, [1, 2, 3])\narray([['a', 'aa', 'aaa'],\n       ['b', 'bb', 'bbb'],\n       ['c', 'cc', 'ccc']], dtype=object)", "parameters": {"type": "object", "properties": {"a": {}, "b": {}}}}}
{"task_id": "BigCodeBench/267", "data": {"name": "numpy.array", "type": "callable", "signature": "(*args, **kwargs)", "description": "array(object, dtype=None, *, copy=True, order='K', subok=False, ndmin=0,\n      like=None)\n\nCreate an array.\n\nParameters\n----------\nobject : array_like\n    An array, any object exposing the array interface, an object whose\n    __array__ method returns an array, or any (nested) sequence.\n    If object is a scalar, a 0-dimensional array containing object is\n    returned.\ndtype : data-type, optional\n    The desired data-type for the array.  If not given, then the type will\n    be determined as the minimum type required to hold the objects in the\n    sequence.\ncopy : bool, optional\n    If true (default), then the object is copied.  Otherwise, a copy will\n    only be made if __array__ returns a copy, if obj is a nested sequence,\n    or if a copy is needed to satisfy any of the other requirements\n    (`dtype`, `order`, etc.).\norder : {'K', 'A', 'C', 'F'}, optional\n    Specify the memory layout of the array. If object is not an array, the\n    newly created array will be in C order (row major) unless 'F' is\n    specified, in which case it will be in Fortran order (column major).\n    If object is an array the following holds.\n\n    ===== ========= ===================================================\n    order  no copy                     copy=True\n    ===== ========= ===================================================\n    'K'   unchanged F & C order preserved, otherwise most similar order\n    'A'   unchanged F order if input is F and not C, otherwise C order\n    'C'   C order   C order\n    'F'   F order   F order\n    ===== ========= ===================================================\n\n    When ``copy=False`` and a copy is made for other reasons, the result is\n    the same as if ``copy=True``, with some exceptions for 'A', see the\n    Notes section. The default order is 'K'.\nsubok : bool, optional\n    If True, then sub-classes will be passed-through, otherwise\n    the returned array will be forced to be a base-class array (default).\nndmin : int, optional\n    Specifies the minimum number of dimensions that the resulting\n    array should have.  Ones will be prepended to the shape as\n    needed to meet this requirement.\nlike : array_like, optional\n    Reference object to allow the creation of arrays which are not\n    NumPy arrays. If an array-like passed in as ``like`` supports\n    the ``__array_function__`` protocol, the result will be defined\n    by it. In this case, it ensures the creation of an array object\n    compatible with that passed in via this argument.\n\n    .. versionadded:: 1.20.0\n\nReturns\n-------\nout : ndarray\n    An array object satisfying the specified requirements.\n\nSee Also\n--------\nempty_like : Return an empty array with shape and type of input.\nones_like : Return an array of ones with shape and type of input.\nzeros_like : Return an array of zeros with shape and type of input.\nfull_like : Return a new array with shape of input filled with value.\nempty : Return a new uninitialized array.\nones : Return a new array setting values to one.\nzeros : Return a new array setting values to zero.\nfull : Return a new array of given shape filled with value.\n\n\nNotes\n-----\nWhen order is 'A' and `object` is an array in neither 'C' nor 'F' order,\nand a copy is forced by a change in dtype, then the order of the result is\nnot necessarily 'C' as expected. This is likely a bug.\n\nExamples\n--------\n>>> np.array([1, 2, 3])\narray([1, 2, 3])\n\nUpcasting:\n\n>>> np.array([1, 2, 3.0])\narray([ 1.,  2.,  3.])\n\nMore than one dimension:\n\n>>> np.array([[1, 2], [3, 4]])\narray([[1, 2],\n       [3, 4]])\n\nMinimum dimensions 2:\n\n>>> np.array([1, 2, 3], ndmin=2)\narray([[1, 2, 3]])\n\nType provided:\n\n>>> np.array([1, 2, 3], dtype=complex)\narray([ 1.+0.j,  2.+0.j,  3.+0.j])\n\nData-type consisting of more than one element:\n\n>>> x = np.array([(1,2),(3,4)],dtype=[('a','<i4'),('b','<i4')])\n>>> x['a']\narray([1, 3])\n\nCreating an array from sub-classes:\n\n>>> np.array(np.mat('1 2; 3 4'))\narray([[1, 2],\n       [3, 4]])\n\n>>> np.array(np.mat('1 2; 3 4'), subok=True)\nmatrix([[1, 2],\n        [3, 4]])", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/267", "data": {"name": "numpy.sin", "type": "callable", "signature": "(*args, **kwargs)", "description": "sin(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n\nTrigonometric sine, element-wise.\n\nParameters\n----------\nx : array_like\n    Angle, in radians (:math:`2 \\pi` rad equals 360 degrees).\nout : ndarray, None, or tuple of ndarray and None, optional\n    A location into which the result is stored. If provided, it must have\n    a shape that the inputs broadcast to. If not provided or None,\n    a freshly-allocated array is returned. A tuple (possible only as a\n    keyword argument) must have length equal to the number of outputs.\nwhere : array_like, optional\n    This condition is broadcast over the input. At locations where the\n    condition is True, the `out` array will be set to the ufunc result.\n    Elsewhere, the `out` array will retain its original value.\n    Note that if an uninitialized `out` array is created via the default\n    ``out=None``, locations within it where the condition is False will\n    remain uninitialized.\n**kwargs\n    For other keyword-only arguments, see the\n    :ref:`ufunc docs <ufuncs.kwargs>`.\n\nReturns\n-------\ny : array_like\n    The sine of each element of x.\n    This is a scalar if `x` is a scalar.\n\nSee Also\n--------\narcsin, sinh, cos\n\nNotes\n-----\nThe sine is one of the fundamental functions of trigonometry (the\nmathematical study of triangles).  Consider a circle of radius 1\ncentered on the origin.  A ray comes in from the :math:`+x` axis, makes\nan angle at the origin (measured counter-clockwise from that axis), and\ndeparts from the origin.  The :math:`y` coordinate of the outgoing\nray's intersection with the unit circle is the sine of that angle.  It\nranges from -1 for :math:`x=3\\pi / 2` to +1 for :math:`\\pi / 2.`  The\nfunction has zeroes where the angle is a multiple of :math:`\\pi`.\nSines of angles between :math:`\\pi` and :math:`2\\pi` are negative.\nThe numerous properties of the sine and related functions are included\nin any standard trigonometry text.\n\nExamples\n--------\nPrint sine of one angle:\n\n>>> np.sin(np.pi/2.)\n1.0\n\nPrint sines of an array of angles given in degrees:\n\n>>> np.sin(np.array((0., 30., 45., 60., 90.)) * np.pi / 180. )\narray([ 0.        ,  0.5       ,  0.70710678,  0.8660254 ,  1.        ])\n\nPlot the sine function:\n\n>>> import matplotlib.pylab as plt\n>>> x = np.linspace(-np.pi, np.pi, 201)\n>>> plt.plot(x, np.sin(x))\n>>> plt.xlabel('Angle [rad]')\n>>> plt.ylabel('sin(x)')\n>>> plt.axis('tight')\n>>> plt.show()", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/267", "data": {"name": "numpy.linspace", "type": "callable", "signature": "(start, stop, num=50, endpoint=True)", "description": "Return evenly spaced numbers over a specified interval.\n\nReturns `num` evenly spaced samples, calculated over the\ninterval [`start`, `stop`].\n\nThe endpoint of the interval can optionally be excluded.\n\n.. versionchanged:: 1.16.0\n    Non-scalar `start` and `stop` are now supported.\n\n.. versionchanged:: 1.20.0\n    Values are rounded towards ``-inf`` instead of ``0`` when an\n    integer ``dtype`` is specified. The old behavior can\n    still be obtained with ``np.linspace(start, stop, num).astype(int)``\n\nParameters\n----------\nstart : array_like\n    The starting value of the sequence.\nstop : array_like\n    The end value of the sequence, unless `endpoint` is set to False.\n    In that case, the sequence consists of all but the last of ``num + 1``\n    evenly spaced samples, so that `stop` is excluded.  Note that the step\n    size changes when `endpoint` is False.\nnum : int, optional\n    Number of samples to generate. Default is 50. Must be non-negative.\nendpoint : bool, optional\n    If True, `stop` is the last sample. Otherwise, it is not included.\n    Default is True.\nretstep : bool, optional\n    If True, return (`samples`, `step`), where `step` is the spacing\n    between samples.\ndtype : dtype, optional\n    The type of the output array.  If `dtype` is not given, the data type\n    is inferred from `start` and `stop`. The inferred dtype will never be\n    an integer; `float` is chosen even if the arguments would produce an\n    array of integers.\n\n    .. versionadded:: 1.9.0\n\naxis : int, optional\n    The axis in the result to store the samples.  Relevant only if start\n    or stop are array-like.  By default (0), the samples will be along a\n    new axis inserted at the beginning. Use -1 to get an axis at the end.\n\n    .. versionadded:: 1.16.0\n\nReturns\n-------\nsamples : ndarray\n    There are `num` equally spaced samples in the closed interval\n    ``[start, stop]`` or the half-open interval ``[start, stop)``\n    (depending on whether `endpoint` is True or False).\nstep : float, optional\n    Only returned if `retstep` is True\n\n    Size of spacing between samples.\n\n\nSee Also\n--------\narange : Similar to `linspace`, but uses a step size (instead of the\n         number of samples).\ngeomspace : Similar to `linspace`, but with numbers spaced evenly on a log\n            scale (a geometric progression).\nlogspace : Similar to `geomspace`, but with the end points specified as\n           logarithms.\n\nExamples\n--------\n>>> np.linspace(2.0, 3.0, num=5)\narray([2.  , 2.25, 2.5 , 2.75, 3.  ])\n>>> np.linspace(2.0, 3.0, num=5, endpoint=False)\narray([2. ,  2.2,  2.4,  2.6,  2.8])\n>>> np.linspace(2.0, 3.0, num=5, retstep=True)\n(array([2.  ,  2.25,  2.5 ,  2.75,  3.  ]), 0.25)\n\nGraphical illustration:\n\n>>> import matplotlib.pyplot as plt\n>>> N = 8\n>>> y = np.zeros(N)\n>>> x1 = np.linspace(0, 10, N, endpoint=True)\n>>> x2 = np.linspace(0, 10, N, endpoint=False)\n>>> plt.plot(x1, y, 'o')\n[<matplotlib.lines.Line2D object at 0x...>]\n>>> plt.plot(x2, y + 0.5, 'o')\n[<matplotlib.lines.Line2D object at 0x...>]\n>>> plt.ylim([-0.5, 1])\n(-0.5, 1)\n>>> plt.show()", "parameters": {"type": "object", "properties": {"start": {}, "stop": {}, "num": {"type": "integer", "default": 50}, "endpoint": {"type": "bool", "default": true}}}}}
{"task_id": "BigCodeBench/267", "data": {"name": "scipy.fftpack.fft", "type": "callable", "signature": "(x)", "description": "Return discrete Fourier transform of real or complex sequence.\n\nThe returned complex array contains ``y(0), y(1),..., y(n-1)``, where\n\n``y(j) = (x * exp(-2*pi*sqrt(-1)*j*np.arange(n)/n)).sum()``.\n\nParameters\n----------\nx : array_like\n    Array to Fourier transform.\nn : int, optional\n    Length of the Fourier transform. If ``n < x.shape[axis]``, `x` is\n    truncated. If ``n > x.shape[axis]``, `x` is zero-padded. The\n    default results in ``n = x.shape[axis]``.\naxis : int, optional\n    Axis along which the fft's are computed; the default is over the\n    last axis (i.e., ``axis=-1``).\noverwrite_x : bool, optional\n    If True, the contents of `x` can be destroyed; the default is False.\n\nReturns\n-------\nz : complex ndarray\n    with the elements::\n\n        [y(0),y(1),..,y(n/2),y(1-n/2),...,y(-1)]        if n is even\n        [y(0),y(1),..,y((n-1)/2),y(-(n-1)/2),...,y(-1)]  if n is odd\n\n    where::\n\n        y(j) = sum[k=0..n-1] x[k] * exp(-sqrt(-1)*j*k* 2*pi/n), j = 0..n-1\n\nSee Also\n--------\nifft : Inverse FFT\nrfft : FFT of a real sequence\n\nNotes\n-----\nThe packing of the result is \"standard\": If ``A = fft(a, n)``, then\n``A[0]`` contains the zero-frequency term, ``A[1:n/2]`` contains the\npositive-frequency terms, and ``A[n/2:]`` contains the negative-frequency\nterms, in order of decreasingly negative frequency. So ,for an 8-point\ntransform, the frequencies of the result are [0, 1, 2, 3, -4, -3, -2, -1].\nTo rearrange the fft output so that the zero-frequency component is\ncentered, like [-4, -3, -2, -1,  0,  1,  2,  3], use `fftshift`.\n\nBoth single and double precision routines are implemented. Half precision\ninputs will be converted to single precision. Non-floating-point inputs\nwill be converted to double precision. Long-double precision inputs are\nnot supported.\n\nThis function is most efficient when `n` is a power of two, and least\nefficient when `n` is prime.\n\nNote that if ``x`` is real-valued, then ``A[j] == A[n-j].conjugate()``.\nIf ``x`` is real-valued and ``n`` is even, then ``A[n/2]`` is real.\n\nIf the data type of `x` is real, a \"real FFT\" algorithm is automatically\nused, which roughly halves the computation time. To increase efficiency\na little further, use `rfft`, which does the same calculation, but only\noutputs half of the symmetrical spectrum. If the data is both real and\nsymmetrical, the `dct` can again double the efficiency by generating\nhalf of the spectrum from half of the signal.\n\nExamples\n--------\n>>> import numpy as np\n>>> from scipy.fftpack import fft, ifft\n>>> x = np.arange(5)\n>>> np.allclose(fft(ifft(x)), x, atol=1e-15)  # within numerical accuracy.\nTrue", "parameters": {"type": "object", "properties": {"x": {}}}}}
{"task_id": "BigCodeBench/273", "data": {"name": "cgi.parse_header", "type": "callable", "signature": "(line)", "description": "Parse a Content-type like header.\n\nReturn the main content-type and a dictionary of options.", "parameters": {"type": "object", "properties": {"line": {}}}}}
{"task_id": "BigCodeBench/273", "data": {"name": "http.server.BaseHTTPRequestHandler", "type": "class", "signature": "(request, client_address, server)", "description": "HTTP request handler base class.\n\nThe following explanation of HTTP serves to guide you through the\ncode as well as to expose any misunderstandings I may have about\nHTTP (so you don't need to read the code to figure out I'm wrong\n:-).\n\nHTTP (HyperText Transfer Protocol) is an extensible protocol on\ntop of a reliable stream transport (e.g. TCP/IP).  The protocol\nrecognizes three parts to a request:\n\n1. One line identifying the request type and path\n2. An optional set of RFC-822-style headers\n3. An optional data part\n\nThe headers and data are separated by a blank line.\n\nThe first line of the request has the form\n\n<command> <path> <version>\n\nwhere <command> is a (case-sensitive) keyword such as GET or POST,\n<path> is a string containing path information for the request,\nand <version> should be the string \"HTTP/1.0\" or \"HTTP/1.1\".\n<path> is encoded using the URL encoding scheme (using %xx to signify\nthe ASCII character with hex code xx).\n\nThe specification specifies that lines are separated by CRLF but\nfor compatibility with the widest range of clients recommends\nservers also handle LF.  Similarly, whitespace in the request line\nis treated sensibly (allowing multiple spaces between components\nand allowing trailing whitespace).\n\nSimilarly, for output, lines ought to be separated by CRLF pairs\nbut most clients grok LF characters just fine.\n\nIf the first line of the request has the form\n\n<command> <path>\n\n(i.e. <version> is left out) then this is assumed to be an HTTP\n0.9 request; this form has no optional headers and data part and\nthe reply consists of just the data.\n\nThe reply form of the HTTP 1.x protocol again has three parts:\n\n1. One line giving the response code\n2. An optional set of RFC-822-style headers\n3. The data\n\nAgain, the headers and data are separated by a blank line.\n\nThe response code line has the form\n\n<version> <responsecode> <responsestring>\n\nwhere <version> is the protocol version (\"HTTP/1.0\" or \"HTTP/1.1\"),\n<responsecode> is a 3-digit response code indicating success or\nfailure of the request, and <responsestring> is an optional\nhuman-readable string explaining what the response code means.\n\nThis server parses the request and the headers, and then calls a\nfunction specific to the request type (<command>).  Specifically,\na request SPAM will be handled by a method do_SPAM().  If no\nsuch method exists the server sends an error response to the\nclient.  If it exists, it is called with no arguments:\n\ndo_SPAM()\n\nNote that the request name is case sensitive (i.e. SPAM and spam\nare different requests).\n\nThe various request details are stored in instance variables:\n\n- client_address is the client IP address in the form (host,\nport);\n\n- command, path and version are the broken-down request line;\n\n- headers is an instance of email.message.Message (or a derived\nclass) containing the header information;\n\n- rfile is a file object open for reading positioned at the\nstart of the optional input data part;\n\n- wfile is a file object open for writing.\n\nIT IS IMPORTANT TO ADHERE TO THE PROTOCOL FOR WRITING!\n\nThe first thing to be written must be the response line.  Then\nfollow 0 or more header lines, then a blank line, and then the\nactual data (if any).  The meaning of the header lines depends on\nthe command executed by the server; in most cases, when data is\nreturned, there should be at least one header line of the form\n\nContent-type: <type>/<subtype>\n\nwhere <type> and <subtype> should be registered MIME types,\ne.g. \"text/html\" or \"text/plain\".", "parameters": {"type": "object", "properties": {"request": {}, "client_address": {}, "server": {}}}}}
{"task_id": "BigCodeBench/273", "data": {"name": "http.server.BaseHTTPRequestHandler.rfile", "type": "class", "signature": "(request, client_address, server)", "description": "HTTP request handler base class.\n\nThe following explanation of HTTP serves to guide you through the\ncode as well as to expose any misunderstandings I may have about\nHTTP (so you don't need to read the code to figure out I'm wrong\n:-).\n\nHTTP (HyperText Transfer Protocol) is an extensible protocol on\ntop of a reliable stream transport (e.g. TCP/IP).  The protocol\nrecognizes three parts to a request:\n\n1. One line identifying the request type and path\n2. An optional set of RFC-822-style headers\n3. An optional data part\n\nThe headers and data are separated by a blank line.\n\nThe first line of the request has the form\n\n<command> <path> <version>\n\nwhere <command> is a (case-sensitive) keyword such as GET or POST,\n<path> is a string containing path information for the request,\nand <version> should be the string \"HTTP/1.0\" or \"HTTP/1.1\".\n<path> is encoded using the URL encoding scheme (using %xx to signify\nthe ASCII character with hex code xx).\n\nThe specification specifies that lines are separated by CRLF but\nfor compatibility with the widest range of clients recommends\nservers also handle LF.  Similarly, whitespace in the request line\nis treated sensibly (allowing multiple spaces between components\nand allowing trailing whitespace).\n\nSimilarly, for output, lines ought to be separated by CRLF pairs\nbut most clients grok LF characters just fine.\n\nIf the first line of the request has the form\n\n<command> <path>\n\n(i.e. <version> is left out) then this is assumed to be an HTTP\n0.9 request; this form has no optional headers and data part and\nthe reply consists of just the data.\n\nThe reply form of the HTTP 1.x protocol again has three parts:\n\n1. One line giving the response code\n2. An optional set of RFC-822-style headers\n3. The data\n\nAgain, the headers and data are separated by a blank line.\n\nThe response code line has the form\n\n<version> <responsecode> <responsestring>\n\nwhere <version> is the protocol version (\"HTTP/1.0\" or \"HTTP/1.1\"),\n<responsecode> is a 3-digit response code indicating success or\nfailure of the request, and <responsestring> is an optional\nhuman-readable string explaining what the response code means.\n\nThis server parses the request and the headers, and then calls a\nfunction specific to the request type (<command>).  Specifically,\na request SPAM will be handled by a method do_SPAM().  If no\nsuch method exists the server sends an error response to the\nclient.  If it exists, it is called with no arguments:\n\ndo_SPAM()\n\nNote that the request name is case sensitive (i.e. SPAM and spam\nare different requests).\n\nThe various request details are stored in instance variables:\n\n- client_address is the client IP address in the form (host,\nport);\n\n- command, path and version are the broken-down request line;\n\n- headers is an instance of email.message.Message (or a derived\nclass) containing the header information;\n\n- rfile is a file object open for reading positioned at the\nstart of the optional input data part;\n\n- wfile is a file object open for writing.\n\nIT IS IMPORTANT TO ADHERE TO THE PROTOCOL FOR WRITING!\n\nThe first thing to be written must be the response line.  Then\nfollow 0 or more header lines, then a blank line, and then the\nactual data (if any).  The meaning of the header lines depends on\nthe command executed by the server; in most cases, when data is\nreturned, there should be at least one header line of the form\n\nContent-type: <type>/<subtype>\n\nwhere <type> and <subtype> should be registered MIME types,\ne.g. \"text/html\" or \"text/plain\"."}}
{"task_id": "BigCodeBench/273", "data": {"name": "http.server.BaseHTTPRequestHandler.rfile.read", "type": "class", "signature": "(request)", "description": "HTTP request handler base class.\n\nThe following explanation of HTTP serves to guide you through the\ncode as well as to expose any misunderstandings I may have about\nHTTP (so you don't need to read the code to figure out I'm wrong\n:-).\n\nHTTP (HyperText Transfer Protocol) is an extensible protocol on\ntop of a reliable stream transport (e.g. TCP/IP).  The protocol\nrecognizes three parts to a request:\n\n1. One line identifying the request type and path\n2. An optional set of RFC-822-style headers\n3. An optional data part\n\nThe headers and data are separated by a blank line.\n\nThe first line of the request has the form\n\n<command> <path> <version>\n\nwhere <command> is a (case-sensitive) keyword such as GET or POST,\n<path> is a string containing path information for the request,\nand <version> should be the string \"HTTP/1.0\" or \"HTTP/1.1\".\n<path> is encoded using the URL encoding scheme (using %xx to signify\nthe ASCII character with hex code xx).\n\nThe specification specifies that lines are separated by CRLF but\nfor compatibility with the widest range of clients recommends\nservers also handle LF.  Similarly, whitespace in the request line\nis treated sensibly (allowing multiple spaces between components\nand allowing trailing whitespace).\n\nSimilarly, for output, lines ought to be separated by CRLF pairs\nbut most clients grok LF characters just fine.\n\nIf the first line of the request has the form\n\n<command> <path>\n\n(i.e. <version> is left out) then this is assumed to be an HTTP\n0.9 request; this form has no optional headers and data part and\nthe reply consists of just the data.\n\nThe reply form of the HTTP 1.x protocol again has three parts:\n\n1. One line giving the response code\n2. An optional set of RFC-822-style headers\n3. The data\n\nAgain, the headers and data are separated by a blank line.\n\nThe response code line has the form\n\n<version> <responsecode> <responsestring>\n\nwhere <version> is the protocol version (\"HTTP/1.0\" or \"HTTP/1.1\"),\n<responsecode> is a 3-digit response code indicating success or\nfailure of the request, and <responsestring> is an optional\nhuman-readable string explaining what the response code means.\n\nThis server parses the request and the headers, and then calls a\nfunction specific to the request type (<command>).  Specifically,\na request SPAM will be handled by a method do_SPAM().  If no\nsuch method exists the server sends an error response to the\nclient.  If it exists, it is called with no arguments:\n\ndo_SPAM()\n\nNote that the request name is case sensitive (i.e. SPAM and spam\nare different requests).\n\nThe various request details are stored in instance variables:\n\n- client_address is the client IP address in the form (host,\nport);\n\n- command, path and version are the broken-down request line;\n\n- headers is an instance of email.message.Message (or a derived\nclass) containing the header information;\n\n- rfile is a file object open for reading positioned at the\nstart of the optional input data part;\n\n- wfile is a file object open for writing.\n\nIT IS IMPORTANT TO ADHERE TO THE PROTOCOL FOR WRITING!\n\nThe first thing to be written must be the response line.  Then\nfollow 0 or more header lines, then a blank line, and then the\nactual data (if any).  The meaning of the header lines depends on\nthe command executed by the server; in most cases, when data is\nreturned, there should be at least one header line of the form\n\nContent-type: <type>/<subtype>\n\nwhere <type> and <subtype> should be registered MIME types,\ne.g. \"text/html\" or \"text/plain\".", "parameters": {"type": "object", "properties": {"request": {}}}}}
{"task_id": "BigCodeBench/273", "data": {"name": "http.server.BaseHTTPRequestHandler.wfile", "type": "class", "signature": "(request, client_address, server)", "description": "HTTP request handler base class.\n\nThe following explanation of HTTP serves to guide you through the\ncode as well as to expose any misunderstandings I may have about\nHTTP (so you don't need to read the code to figure out I'm wrong\n:-).\n\nHTTP (HyperText Transfer Protocol) is an extensible protocol on\ntop of a reliable stream transport (e.g. TCP/IP).  The protocol\nrecognizes three parts to a request:\n\n1. One line identifying the request type and path\n2. An optional set of RFC-822-style headers\n3. An optional data part\n\nThe headers and data are separated by a blank line.\n\nThe first line of the request has the form\n\n<command> <path> <version>\n\nwhere <command> is a (case-sensitive) keyword such as GET or POST,\n<path> is a string containing path information for the request,\nand <version> should be the string \"HTTP/1.0\" or \"HTTP/1.1\".\n<path> is encoded using the URL encoding scheme (using %xx to signify\nthe ASCII character with hex code xx).\n\nThe specification specifies that lines are separated by CRLF but\nfor compatibility with the widest range of clients recommends\nservers also handle LF.  Similarly, whitespace in the request line\nis treated sensibly (allowing multiple spaces between components\nand allowing trailing whitespace).\n\nSimilarly, for output, lines ought to be separated by CRLF pairs\nbut most clients grok LF characters just fine.\n\nIf the first line of the request has the form\n\n<command> <path>\n\n(i.e. <version> is left out) then this is assumed to be an HTTP\n0.9 request; this form has no optional headers and data part and\nthe reply consists of just the data.\n\nThe reply form of the HTTP 1.x protocol again has three parts:\n\n1. One line giving the response code\n2. An optional set of RFC-822-style headers\n3. The data\n\nAgain, the headers and data are separated by a blank line.\n\nThe response code line has the form\n\n<version> <responsecode> <responsestring>\n\nwhere <version> is the protocol version (\"HTTP/1.0\" or \"HTTP/1.1\"),\n<responsecode> is a 3-digit response code indicating success or\nfailure of the request, and <responsestring> is an optional\nhuman-readable string explaining what the response code means.\n\nThis server parses the request and the headers, and then calls a\nfunction specific to the request type (<command>).  Specifically,\na request SPAM will be handled by a method do_SPAM().  If no\nsuch method exists the server sends an error response to the\nclient.  If it exists, it is called with no arguments:\n\ndo_SPAM()\n\nNote that the request name is case sensitive (i.e. SPAM and spam\nare different requests).\n\nThe various request details are stored in instance variables:\n\n- client_address is the client IP address in the form (host,\nport);\n\n- command, path and version are the broken-down request line;\n\n- headers is an instance of email.message.Message (or a derived\nclass) containing the header information;\n\n- rfile is a file object open for reading positioned at the\nstart of the optional input data part;\n\n- wfile is a file object open for writing.\n\nIT IS IMPORTANT TO ADHERE TO THE PROTOCOL FOR WRITING!\n\nThe first thing to be written must be the response line.  Then\nfollow 0 or more header lines, then a blank line, and then the\nactual data (if any).  The meaning of the header lines depends on\nthe command executed by the server; in most cases, when data is\nreturned, there should be at least one header line of the form\n\nContent-type: <type>/<subtype>\n\nwhere <type> and <subtype> should be registered MIME types,\ne.g. \"text/html\" or \"text/plain\"."}}
{"task_id": "BigCodeBench/273", "data": {"name": "http.server.BaseHTTPRequestHandler.wfile.write", "type": "class", "signature": "(request)", "description": "HTTP request handler base class.\n\nThe following explanation of HTTP serves to guide you through the\ncode as well as to expose any misunderstandings I may have about\nHTTP (so you don't need to read the code to figure out I'm wrong\n:-).\n\nHTTP (HyperText Transfer Protocol) is an extensible protocol on\ntop of a reliable stream transport (e.g. TCP/IP).  The protocol\nrecognizes three parts to a request:\n\n1. One line identifying the request type and path\n2. An optional set of RFC-822-style headers\n3. An optional data part\n\nThe headers and data are separated by a blank line.\n\nThe first line of the request has the form\n\n<command> <path> <version>\n\nwhere <command> is a (case-sensitive) keyword such as GET or POST,\n<path> is a string containing path information for the request,\nand <version> should be the string \"HTTP/1.0\" or \"HTTP/1.1\".\n<path> is encoded using the URL encoding scheme (using %xx to signify\nthe ASCII character with hex code xx).\n\nThe specification specifies that lines are separated by CRLF but\nfor compatibility with the widest range of clients recommends\nservers also handle LF.  Similarly, whitespace in the request line\nis treated sensibly (allowing multiple spaces between components\nand allowing trailing whitespace).\n\nSimilarly, for output, lines ought to be separated by CRLF pairs\nbut most clients grok LF characters just fine.\n\nIf the first line of the request has the form\n\n<command> <path>\n\n(i.e. <version> is left out) then this is assumed to be an HTTP\n0.9 request; this form has no optional headers and data part and\nthe reply consists of just the data.\n\nThe reply form of the HTTP 1.x protocol again has three parts:\n\n1. One line giving the response code\n2. An optional set of RFC-822-style headers\n3. The data\n\nAgain, the headers and data are separated by a blank line.\n\nThe response code line has the form\n\n<version> <responsecode> <responsestring>\n\nwhere <version> is the protocol version (\"HTTP/1.0\" or \"HTTP/1.1\"),\n<responsecode> is a 3-digit response code indicating success or\nfailure of the request, and <responsestring> is an optional\nhuman-readable string explaining what the response code means.\n\nThis server parses the request and the headers, and then calls a\nfunction specific to the request type (<command>).  Specifically,\na request SPAM will be handled by a method do_SPAM().  If no\nsuch method exists the server sends an error response to the\nclient.  If it exists, it is called with no arguments:\n\ndo_SPAM()\n\nNote that the request name is case sensitive (i.e. SPAM and spam\nare different requests).\n\nThe various request details are stored in instance variables:\n\n- client_address is the client IP address in the form (host,\nport);\n\n- command, path and version are the broken-down request line;\n\n- headers is an instance of email.message.Message (or a derived\nclass) containing the header information;\n\n- rfile is a file object open for reading positioned at the\nstart of the optional input data part;\n\n- wfile is a file object open for writing.\n\nIT IS IMPORTANT TO ADHERE TO THE PROTOCOL FOR WRITING!\n\nThe first thing to be written must be the response line.  Then\nfollow 0 or more header lines, then a blank line, and then the\nactual data (if any).  The meaning of the header lines depends on\nthe command executed by the server; in most cases, when data is\nreturned, there should be at least one header line of the form\n\nContent-type: <type>/<subtype>\n\nwhere <type> and <subtype> should be registered MIME types,\ne.g. \"text/html\" or \"text/plain\".", "parameters": {"type": "object", "properties": {"request": {}}}}}
{"task_id": "BigCodeBench/273", "data": {"name": "http.server.BaseHTTPRequestHandler.headers", "type": "class", "signature": "(request, client_address, server)", "description": "HTTP request handler base class.\n\nThe following explanation of HTTP serves to guide you through the\ncode as well as to expose any misunderstandings I may have about\nHTTP (so you don't need to read the code to figure out I'm wrong\n:-).\n\nHTTP (HyperText Transfer Protocol) is an extensible protocol on\ntop of a reliable stream transport (e.g. TCP/IP).  The protocol\nrecognizes three parts to a request:\n\n1. One line identifying the request type and path\n2. An optional set of RFC-822-style headers\n3. An optional data part\n\nThe headers and data are separated by a blank line.\n\nThe first line of the request has the form\n\n<command> <path> <version>\n\nwhere <command> is a (case-sensitive) keyword such as GET or POST,\n<path> is a string containing path information for the request,\nand <version> should be the string \"HTTP/1.0\" or \"HTTP/1.1\".\n<path> is encoded using the URL encoding scheme (using %xx to signify\nthe ASCII character with hex code xx).\n\nThe specification specifies that lines are separated by CRLF but\nfor compatibility with the widest range of clients recommends\nservers also handle LF.  Similarly, whitespace in the request line\nis treated sensibly (allowing multiple spaces between components\nand allowing trailing whitespace).\n\nSimilarly, for output, lines ought to be separated by CRLF pairs\nbut most clients grok LF characters just fine.\n\nIf the first line of the request has the form\n\n<command> <path>\n\n(i.e. <version> is left out) then this is assumed to be an HTTP\n0.9 request; this form has no optional headers and data part and\nthe reply consists of just the data.\n\nThe reply form of the HTTP 1.x protocol again has three parts:\n\n1. One line giving the response code\n2. An optional set of RFC-822-style headers\n3. The data\n\nAgain, the headers and data are separated by a blank line.\n\nThe response code line has the form\n\n<version> <responsecode> <responsestring>\n\nwhere <version> is the protocol version (\"HTTP/1.0\" or \"HTTP/1.1\"),\n<responsecode> is a 3-digit response code indicating success or\nfailure of the request, and <responsestring> is an optional\nhuman-readable string explaining what the response code means.\n\nThis server parses the request and the headers, and then calls a\nfunction specific to the request type (<command>).  Specifically,\na request SPAM will be handled by a method do_SPAM().  If no\nsuch method exists the server sends an error response to the\nclient.  If it exists, it is called with no arguments:\n\ndo_SPAM()\n\nNote that the request name is case sensitive (i.e. SPAM and spam\nare different requests).\n\nThe various request details are stored in instance variables:\n\n- client_address is the client IP address in the form (host,\nport);\n\n- command, path and version are the broken-down request line;\n\n- headers is an instance of email.message.Message (or a derived\nclass) containing the header information;\n\n- rfile is a file object open for reading positioned at the\nstart of the optional input data part;\n\n- wfile is a file object open for writing.\n\nIT IS IMPORTANT TO ADHERE TO THE PROTOCOL FOR WRITING!\n\nThe first thing to be written must be the response line.  Then\nfollow 0 or more header lines, then a blank line, and then the\nactual data (if any).  The meaning of the header lines depends on\nthe command executed by the server; in most cases, when data is\nreturned, there should be at least one header line of the form\n\nContent-type: <type>/<subtype>\n\nwhere <type> and <subtype> should be registered MIME types,\ne.g. \"text/html\" or \"text/plain\"."}}
{"task_id": "BigCodeBench/273", "data": {"name": "http.server.BaseHTTPRequestHandler.headers.get", "type": "class", "signature": "(request)", "description": "HTTP request handler base class.\n\nThe following explanation of HTTP serves to guide you through the\ncode as well as to expose any misunderstandings I may have about\nHTTP (so you don't need to read the code to figure out I'm wrong\n:-).\n\nHTTP (HyperText Transfer Protocol) is an extensible protocol on\ntop of a reliable stream transport (e.g. TCP/IP).  The protocol\nrecognizes three parts to a request:\n\n1. One line identifying the request type and path\n2. An optional set of RFC-822-style headers\n3. An optional data part\n\nThe headers and data are separated by a blank line.\n\nThe first line of the request has the form\n\n<command> <path> <version>\n\nwhere <command> is a (case-sensitive) keyword such as GET or POST,\n<path> is a string containing path information for the request,\nand <version> should be the string \"HTTP/1.0\" or \"HTTP/1.1\".\n<path> is encoded using the URL encoding scheme (using %xx to signify\nthe ASCII character with hex code xx).\n\nThe specification specifies that lines are separated by CRLF but\nfor compatibility with the widest range of clients recommends\nservers also handle LF.  Similarly, whitespace in the request line\nis treated sensibly (allowing multiple spaces between components\nand allowing trailing whitespace).\n\nSimilarly, for output, lines ought to be separated by CRLF pairs\nbut most clients grok LF characters just fine.\n\nIf the first line of the request has the form\n\n<command> <path>\n\n(i.e. <version> is left out) then this is assumed to be an HTTP\n0.9 request; this form has no optional headers and data part and\nthe reply consists of just the data.\n\nThe reply form of the HTTP 1.x protocol again has three parts:\n\n1. One line giving the response code\n2. An optional set of RFC-822-style headers\n3. The data\n\nAgain, the headers and data are separated by a blank line.\n\nThe response code line has the form\n\n<version> <responsecode> <responsestring>\n\nwhere <version> is the protocol version (\"HTTP/1.0\" or \"HTTP/1.1\"),\n<responsecode> is a 3-digit response code indicating success or\nfailure of the request, and <responsestring> is an optional\nhuman-readable string explaining what the response code means.\n\nThis server parses the request and the headers, and then calls a\nfunction specific to the request type (<command>).  Specifically,\na request SPAM will be handled by a method do_SPAM().  If no\nsuch method exists the server sends an error response to the\nclient.  If it exists, it is called with no arguments:\n\ndo_SPAM()\n\nNote that the request name is case sensitive (i.e. SPAM and spam\nare different requests).\n\nThe various request details are stored in instance variables:\n\n- client_address is the client IP address in the form (host,\nport);\n\n- command, path and version are the broken-down request line;\n\n- headers is an instance of email.message.Message (or a derived\nclass) containing the header information;\n\n- rfile is a file object open for reading positioned at the\nstart of the optional input data part;\n\n- wfile is a file object open for writing.\n\nIT IS IMPORTANT TO ADHERE TO THE PROTOCOL FOR WRITING!\n\nThe first thing to be written must be the response line.  Then\nfollow 0 or more header lines, then a blank line, and then the\nactual data (if any).  The meaning of the header lines depends on\nthe command executed by the server; in most cases, when data is\nreturned, there should be at least one header line of the form\n\nContent-type: <type>/<subtype>\n\nwhere <type> and <subtype> should be registered MIME types,\ne.g. \"text/html\" or \"text/plain\".", "parameters": {"type": "object", "properties": {"request": {}}}}}
{"task_id": "BigCodeBench/273", "data": {"name": "http.server.BaseHTTPRequestHandler.end_headers", "type": "callable", "signature": "(self)", "description": "Send the blank line ending the MIME headers.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/273", "data": {"name": "http.server.BaseHTTPRequestHandler.send_response", "type": "callable", "signature": "(self, code)", "description": "Add the response header to the headers buffer and log the\nresponse code.\n\nAlso send two standard headers with the server software\nversion and the current date.", "parameters": {"type": "object", "properties": {"code": {}}}}}
{"task_id": "BigCodeBench/273", "data": {"name": "http.server.BaseHTTPRequestHandler.send_error", "type": "callable", "signature": "(self, code, message=None)", "description": "Send and log an error reply.\n\nArguments are\n* code:    an HTTP error code\n           3 digits\n* message: a simple optional 1 line reason phrase.\n           *( HTAB / SP / VCHAR / %x80-FF )\n           defaults to short entry matching the response code\n* explain: a detailed message defaults to the long entry\n           matching the response code.\n\nThis sends an error response (so it must be called before any\noutput has been generated), logs the error, and finally sends\na piece of HTML explaining the error to the user.", "parameters": {"type": "object", "properties": {"code": {}, "message": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/273", "data": {"name": "http.server.BaseHTTPRequestHandler.send_header", "type": "callable", "signature": "(self, keyword, value)", "description": "Send a MIME header to the headers buffer.", "parameters": {"type": "object", "properties": {"keyword": {}, "value": {}}}}}
{"task_id": "BigCodeBench/273", "data": {"name": "json.dumps", "type": "callable", "signature": "(obj)", "description": "Serialize ``obj`` to a JSON formatted ``str``.\n\nIf ``skipkeys`` is true then ``dict`` keys that are not basic types\n(``str``, ``int``, ``float``, ``bool``, ``None``) will be skipped\ninstead of raising a ``TypeError``.\n\nIf ``ensure_ascii`` is false, then the return value can contain non-ASCII\ncharacters if they appear in strings contained in ``obj``. Otherwise, all\nsuch characters are escaped in JSON strings.\n\nIf ``check_circular`` is false, then the circular reference check\nfor container types will be skipped and a circular reference will\nresult in an ``OverflowError`` (or worse).\n\nIf ``allow_nan`` is false, then it will be a ``ValueError`` to\nserialize out of range ``float`` values (``nan``, ``inf``, ``-inf``) in\nstrict compliance of the JSON specification, instead of using the\nJavaScript equivalents (``NaN``, ``Infinity``, ``-Infinity``).\n\nIf ``indent`` is a non-negative integer, then JSON array elements and\nobject members will be pretty-printed with that indent level. An indent\nlevel of 0 will only insert newlines. ``None`` is the most compact\nrepresentation.\n\nIf specified, ``separators`` should be an ``(item_separator, key_separator)``\ntuple.  The default is ``(', ', ': ')`` if *indent* is ``None`` and\n``(',', ': ')`` otherwise.  To get the most compact JSON representation,\nyou should specify ``(',', ':')`` to eliminate whitespace.\n\n``default(obj)`` is a function that should return a serializable version\nof obj or raise TypeError. The default simply raises TypeError.\n\nIf *sort_keys* is true (default: ``False``), then the output of\ndictionaries will be sorted by key.\n\nTo use a custom ``JSONEncoder`` subclass (e.g. one that overrides the\n``.default()`` method to serialize additional types), specify it with\nthe ``cls`` kwarg; otherwise ``JSONEncoder`` is used.", "parameters": {"type": "object", "properties": {"obj": {}}}}}
{"task_id": "BigCodeBench/273", "data": {"name": "json.loads", "type": "callable", "signature": "(s)", "description": "Deserialize ``s`` (a ``str``, ``bytes`` or ``bytearray`` instance\ncontaining a JSON document) to a Python object.\n\n``object_hook`` is an optional function that will be called with the\nresult of any object literal decode (a ``dict``). The return value of\n``object_hook`` will be used instead of the ``dict``. This feature\ncan be used to implement custom decoders (e.g. JSON-RPC class hinting).\n\n``object_pairs_hook`` is an optional function that will be called with the\nresult of any object literal decoded with an ordered list of pairs.  The\nreturn value of ``object_pairs_hook`` will be used instead of the ``dict``.\nThis feature can be used to implement custom decoders.  If ``object_hook``\nis also defined, the ``object_pairs_hook`` takes priority.\n\n``parse_float``, if specified, will be called with the string\nof every JSON float to be decoded. By default this is equivalent to\nfloat(num_str). This can be used to use another datatype or parser\nfor JSON floats (e.g. decimal.Decimal).\n\n``parse_int``, if specified, will be called with the string\nof every JSON int to be decoded. By default this is equivalent to\nint(num_str). This can be used to use another datatype or parser\nfor JSON integers (e.g. float).\n\n``parse_constant``, if specified, will be called with one of the\nfollowing strings: -Infinity, Infinity, NaN.\nThis can be used to raise an exception if invalid JSON numbers\nare encountered.\n\nTo use a custom ``JSONDecoder`` subclass, specify it with the ``cls``\nkwarg; otherwise ``JSONDecoder`` is used.", "parameters": {"type": "object", "properties": {"s": {}}}}}
{"task_id": "BigCodeBench/274", "data": {"name": "smtplib.SMTP", "type": "class", "signature": "(host='', port=0, local_hostname=None, timeout=<object object at 0x7fa57439cb10>, source_address=None)", "description": "This class manages a connection to an SMTP or ESMTP server.\nSMTP Objects:\n    SMTP objects have the following attributes:\n        helo_resp\n            This is the message given by the server in response to the\n            most recent HELO command.\n\n        ehlo_resp\n            This is the message given by the server in response to the\n            most recent EHLO command. This is usually multiline.\n\n        does_esmtp\n            This is a True value _after you do an EHLO command_, if the\n            server supports ESMTP.\n\n        esmtp_features\n            This is a dictionary, which, if the server supports ESMTP,\n            will _after you do an EHLO command_, contain the names of the\n            SMTP service extensions this server supports, and their\n            parameters (if any).\n\n            Note, all extension names are mapped to lower case in the\n            dictionary.\n\n    See each method's docstrings for details.  In general, there is a\n    method of the same name to perform each SMTP command.  There is also a\n    method called 'sendmail' that will do an entire mail transaction.\n    ", "parameters": {"type": "object", "properties": {"host": {"type": "str", "default": ""}, "port": {"type": "integer", "default": 0}, "local_hostname": {"type": "NoneType", "default": null}, "timeout": {"type": "str", "default": "<object object at 0x7fa57439cb10>"}, "source_address": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/274", "data": {"name": "smtplib.SMTP.starttls", "type": "callable", "signature": "(self)", "description": "Puts the connection to the SMTP server into TLS mode.\n\nIf there has been no previous EHLO or HELO command this session, this\nmethod tries ESMTP EHLO first.\n\nIf the server supports TLS, this will encrypt the rest of the SMTP\nsession. If you provide the keyfile and certfile parameters,\nthe identity of the SMTP server and client can be checked. This,\nhowever, depends on whether the socket module really checks the\ncertificates.\n\nThis method may raise the following exceptions:\n\n SMTPHeloError            The server didn't reply properly to\n                          the helo greeting.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/274", "data": {"name": "smtplib.SMTP.login", "type": "callable", "signature": "(self, user, password)", "description": "Log in on an SMTP server that requires authentication.\n\nThe arguments are:\n    - user:         The user name to authenticate with.\n    - password:     The password for the authentication.\n\nKeyword arguments:\n    - initial_response_ok: Allow sending the RFC 4954 initial-response\n      to the AUTH command, if the authentication methods supports it.\n\nIf there has been no previous EHLO or HELO command this session, this\nmethod tries ESMTP EHLO first.\n\nThis method will return normally if the authentication was successful.\n\nThis method may raise the following exceptions:\n\n SMTPHeloError            The server didn't reply properly to\n                          the helo greeting.\n SMTPAuthenticationError  The server didn't accept the username/\n                          password combination.\n SMTPNotSupportedError    The AUTH command is not supported by the\n                          server.\n SMTPException            No suitable authentication method was\n                          found.", "parameters": {"type": "object", "properties": {"user": {}, "password": {}}}}}
{"task_id": "BigCodeBench/274", "data": {"name": "smtplib.SMTP.sendmail", "type": "callable", "signature": "(self, from_addr, to_addrs, msg)", "description": "This command performs an entire mail transaction.\n\nThe arguments are:\n    - from_addr    : The address sending this mail.\n    - to_addrs     : A list of addresses to send this mail to.  A bare\n                     string will be treated as a list with 1 address.\n    - msg          : The message to send.\n    - mail_options : List of ESMTP options (such as 8bitmime) for the\n                     mail command.\n    - rcpt_options : List of ESMTP options (such as DSN commands) for\n                     all the rcpt commands.\n\nmsg may be a string containing characters in the ASCII range, or a byte\nstring.  A string is encoded to bytes using the ascii codec, and lone\n\\r and \\n characters are converted to \\r\\n characters.\n\nIf there has been no previous EHLO or HELO command this session, this\nmethod tries ESMTP EHLO first.  If the server does ESMTP, message size\nand each of the specified options will be passed to it.  If EHLO\nfails, HELO will be tried and ESMTP options suppressed.\n\nThis method will return normally if the mail is accepted for at least\none recipient.  It returns a dictionary, with one entry for each\nrecipient that was refused.  Each entry contains a tuple of the SMTP\nerror code and the accompanying error message sent by the server.\n\nThis method may raise the following exceptions:\n\n SMTPHeloError          The server didn't reply properly to\n                        the helo greeting.\n SMTPRecipientsRefused  The server rejected ALL recipients\n                        (no mail was sent).\n SMTPSenderRefused      The server didn't accept the from_addr.\n SMTPDataError          The server replied with an unexpected\n                        error code (other than a refusal of\n                        a recipient).\n SMTPNotSupportedError  The mail_options parameter includes 'SMTPUTF8'\n                        but the SMTPUTF8 extension is not supported by\n                        the server.\n\nNote: the connection will be open even after an exception is raised.\n\nExample:\n\n >>> import smtplib\n >>> s=smtplib.SMTP(\"localhost\")\n >>> tolist=[\"one@one.org\",\"two@two.org\",\"three@three.org\",\"four@four.org\"]\n >>> msg = '''\\\n ... From: Me@my.org\n ... Subject: testin'...\n ...\n ... This is a test '''\n >>> s.sendmail(\"me@my.org\",tolist,msg)\n { \"three@three.org\" : ( 550 ,\"User unknown\" ) }\n >>> s.quit()\n\nIn the above example, the message was accepted for delivery to three\nof the four addresses, and one was rejected, with the error code\n550.  If all addresses are accepted, then the method will return an\nempty dictionary.", "parameters": {"type": "object", "properties": {"from_addr": {}, "to_addrs": {}, "msg": {}}}}}
{"task_id": "BigCodeBench/274", "data": {"name": "cgi.parse_header", "type": "callable", "signature": "(line)", "description": "Parse a Content-type like header.\n\nReturn the main content-type and a dictionary of options.", "parameters": {"type": "object", "properties": {"line": {}}}}}
{"task_id": "BigCodeBench/274", "data": {"name": "email.mime.text.MIMEText", "type": "class", "signature": "(_text)", "description": "Class for generating text/* type MIME documents.", "parameters": {"type": "object", "properties": {"_text": {}}}}}
{"task_id": "BigCodeBench/274", "data": {"name": "email.mime.text.MIMEText.as_string", "type": "callable", "signature": "(self)", "description": "Return the entire formatted message as a string.\n\nOptional 'unixfrom', when true, means include the Unix From_ envelope\nheader.  For backward compatibility reasons, if maxheaderlen is\nnot specified it defaults to 0, so you must override it explicitly\nif you want a different maxheaderlen.  'policy' is passed to the\nGenerator instance used to serialize the message; if it is not\nspecified the policy associated with the message instance is used.\n\nIf the message object contains binary data that is not encoded\naccording to RFC standards, the non-compliant data will be replaced by\nunicode \"unknown character\" code points.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/274", "data": {"name": "http.server.BaseHTTPRequestHandler", "type": "class", "signature": "(request, client_address, server)", "description": "HTTP request handler base class.\n\nThe following explanation of HTTP serves to guide you through the\ncode as well as to expose any misunderstandings I may have about\nHTTP (so you don't need to read the code to figure out I'm wrong\n:-).\n\nHTTP (HyperText Transfer Protocol) is an extensible protocol on\ntop of a reliable stream transport (e.g. TCP/IP).  The protocol\nrecognizes three parts to a request:\n\n1. One line identifying the request type and path\n2. An optional set of RFC-822-style headers\n3. An optional data part\n\nThe headers and data are separated by a blank line.\n\nThe first line of the request has the form\n\n<command> <path> <version>\n\nwhere <command> is a (case-sensitive) keyword such as GET or POST,\n<path> is a string containing path information for the request,\nand <version> should be the string \"HTTP/1.0\" or \"HTTP/1.1\".\n<path> is encoded using the URL encoding scheme (using %xx to signify\nthe ASCII character with hex code xx).\n\nThe specification specifies that lines are separated by CRLF but\nfor compatibility with the widest range of clients recommends\nservers also handle LF.  Similarly, whitespace in the request line\nis treated sensibly (allowing multiple spaces between components\nand allowing trailing whitespace).\n\nSimilarly, for output, lines ought to be separated by CRLF pairs\nbut most clients grok LF characters just fine.\n\nIf the first line of the request has the form\n\n<command> <path>\n\n(i.e. <version> is left out) then this is assumed to be an HTTP\n0.9 request; this form has no optional headers and data part and\nthe reply consists of just the data.\n\nThe reply form of the HTTP 1.x protocol again has three parts:\n\n1. One line giving the response code\n2. An optional set of RFC-822-style headers\n3. The data\n\nAgain, the headers and data are separated by a blank line.\n\nThe response code line has the form\n\n<version> <responsecode> <responsestring>\n\nwhere <version> is the protocol version (\"HTTP/1.0\" or \"HTTP/1.1\"),\n<responsecode> is a 3-digit response code indicating success or\nfailure of the request, and <responsestring> is an optional\nhuman-readable string explaining what the response code means.\n\nThis server parses the request and the headers, and then calls a\nfunction specific to the request type (<command>).  Specifically,\na request SPAM will be handled by a method do_SPAM().  If no\nsuch method exists the server sends an error response to the\nclient.  If it exists, it is called with no arguments:\n\ndo_SPAM()\n\nNote that the request name is case sensitive (i.e. SPAM and spam\nare different requests).\n\nThe various request details are stored in instance variables:\n\n- client_address is the client IP address in the form (host,\nport);\n\n- command, path and version are the broken-down request line;\n\n- headers is an instance of email.message.Message (or a derived\nclass) containing the header information;\n\n- rfile is a file object open for reading positioned at the\nstart of the optional input data part;\n\n- wfile is a file object open for writing.\n\nIT IS IMPORTANT TO ADHERE TO THE PROTOCOL FOR WRITING!\n\nThe first thing to be written must be the response line.  Then\nfollow 0 or more header lines, then a blank line, and then the\nactual data (if any).  The meaning of the header lines depends on\nthe command executed by the server; in most cases, when data is\nreturned, there should be at least one header line of the form\n\nContent-type: <type>/<subtype>\n\nwhere <type> and <subtype> should be registered MIME types,\ne.g. \"text/html\" or \"text/plain\".", "parameters": {"type": "object", "properties": {"request": {}, "client_address": {}, "server": {}}}}}
{"task_id": "BigCodeBench/274", "data": {"name": "http.server.BaseHTTPRequestHandler.rfile", "type": "class", "signature": "(request, client_address, server)", "description": "HTTP request handler base class.\n\nThe following explanation of HTTP serves to guide you through the\ncode as well as to expose any misunderstandings I may have about\nHTTP (so you don't need to read the code to figure out I'm wrong\n:-).\n\nHTTP (HyperText Transfer Protocol) is an extensible protocol on\ntop of a reliable stream transport (e.g. TCP/IP).  The protocol\nrecognizes three parts to a request:\n\n1. One line identifying the request type and path\n2. An optional set of RFC-822-style headers\n3. An optional data part\n\nThe headers and data are separated by a blank line.\n\nThe first line of the request has the form\n\n<command> <path> <version>\n\nwhere <command> is a (case-sensitive) keyword such as GET or POST,\n<path> is a string containing path information for the request,\nand <version> should be the string \"HTTP/1.0\" or \"HTTP/1.1\".\n<path> is encoded using the URL encoding scheme (using %xx to signify\nthe ASCII character with hex code xx).\n\nThe specification specifies that lines are separated by CRLF but\nfor compatibility with the widest range of clients recommends\nservers also handle LF.  Similarly, whitespace in the request line\nis treated sensibly (allowing multiple spaces between components\nand allowing trailing whitespace).\n\nSimilarly, for output, lines ought to be separated by CRLF pairs\nbut most clients grok LF characters just fine.\n\nIf the first line of the request has the form\n\n<command> <path>\n\n(i.e. <version> is left out) then this is assumed to be an HTTP\n0.9 request; this form has no optional headers and data part and\nthe reply consists of just the data.\n\nThe reply form of the HTTP 1.x protocol again has three parts:\n\n1. One line giving the response code\n2. An optional set of RFC-822-style headers\n3. The data\n\nAgain, the headers and data are separated by a blank line.\n\nThe response code line has the form\n\n<version> <responsecode> <responsestring>\n\nwhere <version> is the protocol version (\"HTTP/1.0\" or \"HTTP/1.1\"),\n<responsecode> is a 3-digit response code indicating success or\nfailure of the request, and <responsestring> is an optional\nhuman-readable string explaining what the response code means.\n\nThis server parses the request and the headers, and then calls a\nfunction specific to the request type (<command>).  Specifically,\na request SPAM will be handled by a method do_SPAM().  If no\nsuch method exists the server sends an error response to the\nclient.  If it exists, it is called with no arguments:\n\ndo_SPAM()\n\nNote that the request name is case sensitive (i.e. SPAM and spam\nare different requests).\n\nThe various request details are stored in instance variables:\n\n- client_address is the client IP address in the form (host,\nport);\n\n- command, path and version are the broken-down request line;\n\n- headers is an instance of email.message.Message (or a derived\nclass) containing the header information;\n\n- rfile is a file object open for reading positioned at the\nstart of the optional input data part;\n\n- wfile is a file object open for writing.\n\nIT IS IMPORTANT TO ADHERE TO THE PROTOCOL FOR WRITING!\n\nThe first thing to be written must be the response line.  Then\nfollow 0 or more header lines, then a blank line, and then the\nactual data (if any).  The meaning of the header lines depends on\nthe command executed by the server; in most cases, when data is\nreturned, there should be at least one header line of the form\n\nContent-type: <type>/<subtype>\n\nwhere <type> and <subtype> should be registered MIME types,\ne.g. \"text/html\" or \"text/plain\"."}}
{"task_id": "BigCodeBench/274", "data": {"name": "http.server.BaseHTTPRequestHandler.rfile.read", "type": "class", "signature": "(request)", "description": "HTTP request handler base class.\n\nThe following explanation of HTTP serves to guide you through the\ncode as well as to expose any misunderstandings I may have about\nHTTP (so you don't need to read the code to figure out I'm wrong\n:-).\n\nHTTP (HyperText Transfer Protocol) is an extensible protocol on\ntop of a reliable stream transport (e.g. TCP/IP).  The protocol\nrecognizes three parts to a request:\n\n1. One line identifying the request type and path\n2. An optional set of RFC-822-style headers\n3. An optional data part\n\nThe headers and data are separated by a blank line.\n\nThe first line of the request has the form\n\n<command> <path> <version>\n\nwhere <command> is a (case-sensitive) keyword such as GET or POST,\n<path> is a string containing path information for the request,\nand <version> should be the string \"HTTP/1.0\" or \"HTTP/1.1\".\n<path> is encoded using the URL encoding scheme (using %xx to signify\nthe ASCII character with hex code xx).\n\nThe specification specifies that lines are separated by CRLF but\nfor compatibility with the widest range of clients recommends\nservers also handle LF.  Similarly, whitespace in the request line\nis treated sensibly (allowing multiple spaces between components\nand allowing trailing whitespace).\n\nSimilarly, for output, lines ought to be separated by CRLF pairs\nbut most clients grok LF characters just fine.\n\nIf the first line of the request has the form\n\n<command> <path>\n\n(i.e. <version> is left out) then this is assumed to be an HTTP\n0.9 request; this form has no optional headers and data part and\nthe reply consists of just the data.\n\nThe reply form of the HTTP 1.x protocol again has three parts:\n\n1. One line giving the response code\n2. An optional set of RFC-822-style headers\n3. The data\n\nAgain, the headers and data are separated by a blank line.\n\nThe response code line has the form\n\n<version> <responsecode> <responsestring>\n\nwhere <version> is the protocol version (\"HTTP/1.0\" or \"HTTP/1.1\"),\n<responsecode> is a 3-digit response code indicating success or\nfailure of the request, and <responsestring> is an optional\nhuman-readable string explaining what the response code means.\n\nThis server parses the request and the headers, and then calls a\nfunction specific to the request type (<command>).  Specifically,\na request SPAM will be handled by a method do_SPAM().  If no\nsuch method exists the server sends an error response to the\nclient.  If it exists, it is called with no arguments:\n\ndo_SPAM()\n\nNote that the request name is case sensitive (i.e. SPAM and spam\nare different requests).\n\nThe various request details are stored in instance variables:\n\n- client_address is the client IP address in the form (host,\nport);\n\n- command, path and version are the broken-down request line;\n\n- headers is an instance of email.message.Message (or a derived\nclass) containing the header information;\n\n- rfile is a file object open for reading positioned at the\nstart of the optional input data part;\n\n- wfile is a file object open for writing.\n\nIT IS IMPORTANT TO ADHERE TO THE PROTOCOL FOR WRITING!\n\nThe first thing to be written must be the response line.  Then\nfollow 0 or more header lines, then a blank line, and then the\nactual data (if any).  The meaning of the header lines depends on\nthe command executed by the server; in most cases, when data is\nreturned, there should be at least one header line of the form\n\nContent-type: <type>/<subtype>\n\nwhere <type> and <subtype> should be registered MIME types,\ne.g. \"text/html\" or \"text/plain\".", "parameters": {"type": "object", "properties": {"request": {}}}}}
{"task_id": "BigCodeBench/274", "data": {"name": "http.server.BaseHTTPRequestHandler.headers", "type": "class", "signature": "(request, client_address, server)", "description": "HTTP request handler base class.\n\nThe following explanation of HTTP serves to guide you through the\ncode as well as to expose any misunderstandings I may have about\nHTTP (so you don't need to read the code to figure out I'm wrong\n:-).\n\nHTTP (HyperText Transfer Protocol) is an extensible protocol on\ntop of a reliable stream transport (e.g. TCP/IP).  The protocol\nrecognizes three parts to a request:\n\n1. One line identifying the request type and path\n2. An optional set of RFC-822-style headers\n3. An optional data part\n\nThe headers and data are separated by a blank line.\n\nThe first line of the request has the form\n\n<command> <path> <version>\n\nwhere <command> is a (case-sensitive) keyword such as GET or POST,\n<path> is a string containing path information for the request,\nand <version> should be the string \"HTTP/1.0\" or \"HTTP/1.1\".\n<path> is encoded using the URL encoding scheme (using %xx to signify\nthe ASCII character with hex code xx).\n\nThe specification specifies that lines are separated by CRLF but\nfor compatibility with the widest range of clients recommends\nservers also handle LF.  Similarly, whitespace in the request line\nis treated sensibly (allowing multiple spaces between components\nand allowing trailing whitespace).\n\nSimilarly, for output, lines ought to be separated by CRLF pairs\nbut most clients grok LF characters just fine.\n\nIf the first line of the request has the form\n\n<command> <path>\n\n(i.e. <version> is left out) then this is assumed to be an HTTP\n0.9 request; this form has no optional headers and data part and\nthe reply consists of just the data.\n\nThe reply form of the HTTP 1.x protocol again has three parts:\n\n1. One line giving the response code\n2. An optional set of RFC-822-style headers\n3. The data\n\nAgain, the headers and data are separated by a blank line.\n\nThe response code line has the form\n\n<version> <responsecode> <responsestring>\n\nwhere <version> is the protocol version (\"HTTP/1.0\" or \"HTTP/1.1\"),\n<responsecode> is a 3-digit response code indicating success or\nfailure of the request, and <responsestring> is an optional\nhuman-readable string explaining what the response code means.\n\nThis server parses the request and the headers, and then calls a\nfunction specific to the request type (<command>).  Specifically,\na request SPAM will be handled by a method do_SPAM().  If no\nsuch method exists the server sends an error response to the\nclient.  If it exists, it is called with no arguments:\n\ndo_SPAM()\n\nNote that the request name is case sensitive (i.e. SPAM and spam\nare different requests).\n\nThe various request details are stored in instance variables:\n\n- client_address is the client IP address in the form (host,\nport);\n\n- command, path and version are the broken-down request line;\n\n- headers is an instance of email.message.Message (or a derived\nclass) containing the header information;\n\n- rfile is a file object open for reading positioned at the\nstart of the optional input data part;\n\n- wfile is a file object open for writing.\n\nIT IS IMPORTANT TO ADHERE TO THE PROTOCOL FOR WRITING!\n\nThe first thing to be written must be the response line.  Then\nfollow 0 or more header lines, then a blank line, and then the\nactual data (if any).  The meaning of the header lines depends on\nthe command executed by the server; in most cases, when data is\nreturned, there should be at least one header line of the form\n\nContent-type: <type>/<subtype>\n\nwhere <type> and <subtype> should be registered MIME types,\ne.g. \"text/html\" or \"text/plain\"."}}
{"task_id": "BigCodeBench/274", "data": {"name": "http.server.BaseHTTPRequestHandler.headers.get", "type": "class", "signature": "(request)", "description": "HTTP request handler base class.\n\nThe following explanation of HTTP serves to guide you through the\ncode as well as to expose any misunderstandings I may have about\nHTTP (so you don't need to read the code to figure out I'm wrong\n:-).\n\nHTTP (HyperText Transfer Protocol) is an extensible protocol on\ntop of a reliable stream transport (e.g. TCP/IP).  The protocol\nrecognizes three parts to a request:\n\n1. One line identifying the request type and path\n2. An optional set of RFC-822-style headers\n3. An optional data part\n\nThe headers and data are separated by a blank line.\n\nThe first line of the request has the form\n\n<command> <path> <version>\n\nwhere <command> is a (case-sensitive) keyword such as GET or POST,\n<path> is a string containing path information for the request,\nand <version> should be the string \"HTTP/1.0\" or \"HTTP/1.1\".\n<path> is encoded using the URL encoding scheme (using %xx to signify\nthe ASCII character with hex code xx).\n\nThe specification specifies that lines are separated by CRLF but\nfor compatibility with the widest range of clients recommends\nservers also handle LF.  Similarly, whitespace in the request line\nis treated sensibly (allowing multiple spaces between components\nand allowing trailing whitespace).\n\nSimilarly, for output, lines ought to be separated by CRLF pairs\nbut most clients grok LF characters just fine.\n\nIf the first line of the request has the form\n\n<command> <path>\n\n(i.e. <version> is left out) then this is assumed to be an HTTP\n0.9 request; this form has no optional headers and data part and\nthe reply consists of just the data.\n\nThe reply form of the HTTP 1.x protocol again has three parts:\n\n1. One line giving the response code\n2. An optional set of RFC-822-style headers\n3. The data\n\nAgain, the headers and data are separated by a blank line.\n\nThe response code line has the form\n\n<version> <responsecode> <responsestring>\n\nwhere <version> is the protocol version (\"HTTP/1.0\" or \"HTTP/1.1\"),\n<responsecode> is a 3-digit response code indicating success or\nfailure of the request, and <responsestring> is an optional\nhuman-readable string explaining what the response code means.\n\nThis server parses the request and the headers, and then calls a\nfunction specific to the request type (<command>).  Specifically,\na request SPAM will be handled by a method do_SPAM().  If no\nsuch method exists the server sends an error response to the\nclient.  If it exists, it is called with no arguments:\n\ndo_SPAM()\n\nNote that the request name is case sensitive (i.e. SPAM and spam\nare different requests).\n\nThe various request details are stored in instance variables:\n\n- client_address is the client IP address in the form (host,\nport);\n\n- command, path and version are the broken-down request line;\n\n- headers is an instance of email.message.Message (or a derived\nclass) containing the header information;\n\n- rfile is a file object open for reading positioned at the\nstart of the optional input data part;\n\n- wfile is a file object open for writing.\n\nIT IS IMPORTANT TO ADHERE TO THE PROTOCOL FOR WRITING!\n\nThe first thing to be written must be the response line.  Then\nfollow 0 or more header lines, then a blank line, and then the\nactual data (if any).  The meaning of the header lines depends on\nthe command executed by the server; in most cases, when data is\nreturned, there should be at least one header line of the form\n\nContent-type: <type>/<subtype>\n\nwhere <type> and <subtype> should be registered MIME types,\ne.g. \"text/html\" or \"text/plain\".", "parameters": {"type": "object", "properties": {"request": {}}}}}
{"task_id": "BigCodeBench/274", "data": {"name": "http.server.BaseHTTPRequestHandler.end_headers", "type": "callable", "signature": "(self)", "description": "Send the blank line ending the MIME headers.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/274", "data": {"name": "http.server.BaseHTTPRequestHandler.send_response", "type": "callable", "signature": "(self, code)", "description": "Add the response header to the headers buffer and log the\nresponse code.\n\nAlso send two standard headers with the server software\nversion and the current date.", "parameters": {"type": "object", "properties": {"code": {}}}}}
{"task_id": "BigCodeBench/274", "data": {"name": "json.loads", "type": "callable", "signature": "(s)", "description": "Deserialize ``s`` (a ``str``, ``bytes`` or ``bytearray`` instance\ncontaining a JSON document) to a Python object.\n\n``object_hook`` is an optional function that will be called with the\nresult of any object literal decode (a ``dict``). The return value of\n``object_hook`` will be used instead of the ``dict``. This feature\ncan be used to implement custom decoders (e.g. JSON-RPC class hinting).\n\n``object_pairs_hook`` is an optional function that will be called with the\nresult of any object literal decoded with an ordered list of pairs.  The\nreturn value of ``object_pairs_hook`` will be used instead of the ``dict``.\nThis feature can be used to implement custom decoders.  If ``object_hook``\nis also defined, the ``object_pairs_hook`` takes priority.\n\n``parse_float``, if specified, will be called with the string\nof every JSON float to be decoded. By default this is equivalent to\nfloat(num_str). This can be used to use another datatype or parser\nfor JSON floats (e.g. decimal.Decimal).\n\n``parse_int``, if specified, will be called with the string\nof every JSON int to be decoded. By default this is equivalent to\nint(num_str). This can be used to use another datatype or parser\nfor JSON integers (e.g. float).\n\n``parse_constant``, if specified, will be called with one of the\nfollowing strings: -Infinity, Infinity, NaN.\nThis can be used to raise an exception if invalid JSON numbers\nare encountered.\n\nTo use a custom ``JSONDecoder`` subclass, specify it with the ``cls``\nkwarg; otherwise ``JSONDecoder`` is used.", "parameters": {"type": "object", "properties": {"s": {}}}}}
{"task_id": "BigCodeBench/287", "data": {"name": "collections.Counter", "type": "class", "signature": "(iterable=None, /, **kwds)", "description": "Dict subclass for counting hashable items.  Sometimes called a bag\nor multiset.  Elements are stored as dictionary keys and their counts\nare stored as dictionary values.\n\n>>> c = Counter('abcdeabcdabcaba')  # count elements from a string\n\n>>> c.most_common(3)                # three most common elements\n[('a', 5), ('b', 4), ('c', 3)]\n>>> sorted(c)                       # list all unique elements\n['a', 'b', 'c', 'd', 'e']\n>>> ''.join(sorted(c.elements()))   # list elements with repetitions\n'aaaaabbbbcccdde'\n>>> sum(c.values())                 # total of all counts\n15\n\n>>> c['a']                          # count of letter 'a'\n5\n>>> for elem in 'shazam':           # update counts from an iterable\n...     c[elem] += 1                # by adding 1 to each element's count\n>>> c['a']                          # now there are seven 'a'\n7\n>>> del c['b']                      # remove all 'b'\n>>> c['b']                          # now there are zero 'b'\n0\n\n>>> d = Counter('simsalabim')       # make another counter\n>>> c.update(d)                     # add in the second counter\n>>> c['a']                          # now there are nine 'a'\n9\n\n>>> c.clear()                       # empty the counter\n>>> c\nCounter()\n\nNote:  If a count is set to zero or reduced to zero, it will remain\nin the counter until the entry is deleted or the counter is cleared:\n\n>>> c = Counter('aaabbc')\n>>> c['b'] -= 2                     # reduce the count of 'b' by two\n>>> c.most_common()                 # 'b' is still in, but its count is zero\n[('a', 3), ('c', 1), ('b', 0)]", "parameters": {"type": "object", "properties": {"iterable": {"type": "NoneType", "default": null}, "/": {}}}}}
{"task_id": "BigCodeBench/287", "data": {"name": "collections.Counter.update", "type": "callable", "signature": "(self, iterable=None)", "description": "Like dict.update() but add counts instead of replacing them.\n\nSource can be an iterable, a dictionary, or another Counter instance.\n\n>>> c = Counter('which')\n>>> c.update('witch')           # add elements from another iterable\n>>> d = Counter('watch')\n>>> c.update(d)                 # add elements from another counter\n>>> c['h']                      # four 'h' in which, witch, and watch\n4", "parameters": {"type": "object", "properties": {"iterable": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/287", "data": {"name": "file_name.endswith('.txt')"}}
{"task_id": "BigCodeBench/287", "data": {"name": "json.dump", "type": "callable", "signature": "(obj, fp)", "description": "Serialize ``obj`` as a JSON formatted stream to ``fp`` (a\n``.write()``-supporting file-like object).\n\nIf ``skipkeys`` is true then ``dict`` keys that are not basic types\n(``str``, ``int``, ``float``, ``bool``, ``None``) will be skipped\ninstead of raising a ``TypeError``.\n\nIf ``ensure_ascii`` is false, then the strings written to ``fp`` can\ncontain non-ASCII characters if they appear in strings contained in\n``obj``. Otherwise, all such characters are escaped in JSON strings.\n\nIf ``check_circular`` is false, then the circular reference check\nfor container types will be skipped and a circular reference will\nresult in an ``OverflowError`` (or worse).\n\nIf ``allow_nan`` is false, then it will be a ``ValueError`` to\nserialize out of range ``float`` values (``nan``, ``inf``, ``-inf``)\nin strict compliance of the JSON specification, instead of using the\nJavaScript equivalents (``NaN``, ``Infinity``, ``-Infinity``).\n\nIf ``indent`` is a non-negative integer, then JSON array elements and\nobject members will be pretty-printed with that indent level. An indent\nlevel of 0 will only insert newlines. ``None`` is the most compact\nrepresentation.\n\nIf specified, ``separators`` should be an ``(item_separator, key_separator)``\ntuple.  The default is ``(', ', ': ')`` if *indent* is ``None`` and\n``(',', ': ')`` otherwise.  To get the most compact JSON representation,\nyou should specify ``(',', ':')`` to eliminate whitespace.\n\n``default(obj)`` is a function that should return a serializable version\nof obj or raise TypeError. The default simply raises TypeError.\n\nIf *sort_keys* is true (default: ``False``), then the output of\ndictionaries will be sorted by key.\n\nTo use a custom ``JSONEncoder`` subclass (e.g. one that overrides the\n``.default()`` method to serialize additional types), specify it with\nthe ``cls`` kwarg; otherwise ``JSONEncoder`` is used.", "parameters": {"type": "object", "properties": {"obj": {}, "fp": {}}}}}
{"task_id": "BigCodeBench/287", "data": {"name": "open.read()"}}
{"task_id": "BigCodeBench/287", "data": {"name": "os.listdir", "type": "callable", "signature": "(path=None)", "description": "Return a list containing the names of the files in the directory.\n\npath can be specified as either str, bytes, or a path-like object.  If path is bytes,\n  the filenames returned will also be bytes; in all other circumstances\n  the filenames returned will be str.\nIf path is None, uses the path='.'.\nOn some platforms, path may also be specified as an open file descriptor;\\\n  the file descriptor must refer to a directory.\n  If this functionality is unavailable, using it raises NotImplementedError.\n\nThe list is in arbitrary order.  It does not include the special\nentries '.' and '..' even if they are present in the directory.", "parameters": {"type": "object", "properties": {"path": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/287", "data": {"name": "os.path.join", "type": "callable", "signature": "(a, *p)", "description": "Join two or more pathname components, inserting '/' as needed.\nIf any component is an absolute path, all previous path components\nwill be discarded.  An empty last part will result in a path that\nends with a separator.", "parameters": {"type": "object", "properties": {"a": {}}}}}
{"task_id": "BigCodeBench/302", "data": {"name": "df.index"}}
{"task_id": "BigCodeBench/302", "data": {"name": "matplotlib.pyplot.figure", "type": "callable", "signature": "()", "description": "Create a new figure, or activate an existing figure.\n\nParameters\n----------\nnum : int or str or `.Figure` or `.SubFigure`, optional\n    A unique identifier for the figure.\n\n    If a figure with that identifier already exists, this figure is made\n    active and returned. An integer refers to the ``Figure.number``\n    attribute, a string refers to the figure label.\n\n    If there is no figure with the identifier or *num* is not given, a new\n    figure is created, made active and returned.  If *num* is an int, it\n    will be used for the ``Figure.number`` attribute, otherwise, an\n    auto-generated integer value is used (starting at 1 and incremented\n    for each new figure). If *num* is a string, the figure label and the\n    window title is set to this value.  If num is a ``SubFigure``, its\n    parent ``Figure`` is activated.\n\nfigsize : (float, float), default: :rc:`figure.figsize`\n    Width, height in inches.\n\ndpi : float, default: :rc:`figure.dpi`\n    The resolution of the figure in dots-per-inch.\n\nfacecolor : color, default: :rc:`figure.facecolor`\n    The background color.\n\nedgecolor : color, default: :rc:`figure.edgecolor`\n    The border color.\n\nframeon : bool, default: True\n    If False, suppress drawing the figure frame.\n\nFigureClass : subclass of `~matplotlib.figure.Figure`\n    If set, an instance of this subclass will be created, rather than a\n    plain `.Figure`.\n\nclear : bool, default: False\n    If True and the figure already exists, then it is cleared.\n\nlayout : {'constrained', 'compressed', 'tight', 'none', `.LayoutEngine`, None}, default: None\n    The layout mechanism for positioning of plot elements to avoid\n    overlapping Axes decorations (labels, ticks, etc). Note that layout\n    managers can measurably slow down figure display.\n\n    - 'constrained': The constrained layout solver adjusts axes sizes\n      to avoid overlapping axes decorations.  Can handle complex plot\n      layouts and colorbars, and is thus recommended.\n\n      See :ref:`constrainedlayout_guide`\n      for examples.\n\n    - 'compressed': uses the same algorithm as 'constrained', but\n      removes extra space between fixed-aspect-ratio Axes.  Best for\n      simple grids of axes.\n\n    - 'tight': Use the tight layout mechanism. This is a relatively\n      simple algorithm that adjusts the subplot parameters so that\n      decorations do not overlap. See `.Figure.set_tight_layout` for\n      further details.\n\n    - 'none': Do not use a layout engine.\n\n    - A `.LayoutEngine` instance. Builtin layout classes are\n      `.ConstrainedLayoutEngine` and `.TightLayoutEngine`, more easily\n      accessible by 'constrained' and 'tight'.  Passing an instance\n      allows third parties to provide their own layout engine.\n\n    If not given, fall back to using the parameters *tight_layout* and\n    *constrained_layout*, including their config defaults\n    :rc:`figure.autolayout` and :rc:`figure.constrained_layout.use`.\n\n**kwargs\n    Additional keyword arguments are passed to the `.Figure` constructor.\n\nReturns\n-------\n`~matplotlib.figure.Figure`\n\nNotes\n-----\nA newly created figure is passed to the `~.FigureCanvasBase.new_manager`\nmethod or the `new_figure_manager` function provided by the current\nbackend, which install a canvas and a manager on the figure.\n\nOnce this is done, :rc:`figure.hooks` are called, one at a time, on the\nfigure; these hooks allow arbitrary customization of the figure (e.g.,\nattaching callbacks) or of associated elements (e.g., modifying the\ntoolbar).  See :doc:`/gallery/user_interfaces/mplcvd` for an example of\ntoolbar customization.\n\nIf you are creating many figures, make sure you explicitly call\n`.pyplot.close` on the figures you are not using, because this will\nenable pyplot to properly clean up the memory.\n\n`~matplotlib.rcParams` defines the default values, which can be modified\nin the matplotlibrc file.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/302", "data": {"name": "matplotlib.pyplot.title", "type": "callable", "signature": "(label: 'str')", "description": "Set a title for the Axes.\n\nSet one of the three available Axes titles. The available titles\nare positioned above the Axes in the center, flush with the left\nedge, and flush with the right edge.\n\nParameters\n----------\nlabel : str\n    Text to use for the title\n\nfontdict : dict\n\n    .. admonition:: Discouraged\n\n       The use of *fontdict* is discouraged. Parameters should be passed as\n       individual keyword arguments or using dictionary-unpacking\n       ``set_title(..., **fontdict)``.\n\n    A dictionary controlling the appearance of the title text,\n    the default *fontdict* is::\n\n       {'fontsize': rcParams['axes.titlesize'],\n        'fontweight': rcParams['axes.titleweight'],\n        'color': rcParams['axes.titlecolor'],\n        'verticalalignment': 'baseline',\n        'horizontalalignment': loc}\n\nloc : {'center', 'left', 'right'}, default: :rc:`axes.titlelocation`\n    Which title to set.\n\ny : float, default: :rc:`axes.titley`\n    Vertical Axes location for the title (1.0 is the top).  If\n    None (the default) and :rc:`axes.titley` is also None, y is\n    determined automatically to avoid decorators on the Axes.\n\npad : float, default: :rc:`axes.titlepad`\n    The offset of the title from the top of the Axes, in points.\n\nReturns\n-------\n`.Text`\n    The matplotlib text instance representing the title\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    Other keyword arguments are text properties, see `.Text` for a list\n    of valid text properties.", "parameters": {"type": "object", "properties": {"label": {"type": "str"}}}}}
{"task_id": "BigCodeBench/302", "data": {"name": "pandas.to_datetime", "type": "callable", "signature": "(arg: 'DatetimeScalarOrArrayConvertible | DictConvertible')", "description": "Convert argument to datetime.\n\nThis function converts a scalar, array-like, :class:`Series` or\n:class:`DataFrame`/dict-like to a pandas datetime object.\n\nParameters\n----------\narg : int, float, str, datetime, list, tuple, 1-d array, Series, DataFrame/dict-like\n    The object to convert to a datetime. If a :class:`DataFrame` is provided, the\n    method expects minimally the following columns: :const:`\"year\"`,\n    :const:`\"month\"`, :const:`\"day\"`. The column \"year\"\n    must be specified in 4-digit format.\nerrors : {'ignore', 'raise', 'coerce'}, default 'raise'\n    - If :const:`'raise'`, then invalid parsing will raise an exception.\n    - If :const:`'coerce'`, then invalid parsing will be set as :const:`NaT`.\n    - If :const:`'ignore'`, then invalid parsing will return the input.\ndayfirst : bool, default False\n    Specify a date parse order if `arg` is str or is list-like.\n    If :const:`True`, parses dates with the day first, e.g. :const:`\"10/11/12\"`\n    is parsed as :const:`2012-11-10`.\n\n    .. warning::\n\n        ``dayfirst=True`` is not strict, but will prefer to parse\n        with day first.\n\nyearfirst : bool, default False\n    Specify a date parse order if `arg` is str or is list-like.\n\n    - If :const:`True` parses dates with the year first, e.g.\n      :const:`\"10/11/12\"` is parsed as :const:`2010-11-12`.\n    - If both `dayfirst` and `yearfirst` are :const:`True`, `yearfirst` is\n      preceded (same as :mod:`dateutil`).\n\n    .. warning::\n\n        ``yearfirst=True`` is not strict, but will prefer to parse\n        with year first.\n\nutc : bool, default False\n    Control timezone-related parsing, localization and conversion.\n\n    - If :const:`True`, the function *always* returns a timezone-aware\n      UTC-localized :class:`Timestamp`, :class:`Series` or\n      :class:`DatetimeIndex`. To do this, timezone-naive inputs are\n      *localized* as UTC, while timezone-aware inputs are *converted* to UTC.\n\n    - If :const:`False` (default), inputs will not be coerced to UTC.\n      Timezone-naive inputs will remain naive, while timezone-aware ones\n      will keep their time offsets. Limitations exist for mixed\n      offsets (typically, daylight savings), see :ref:`Examples\n      <to_datetime_tz_examples>` section for details.\n\n    .. warning::\n\n        In a future version of pandas, parsing datetimes with mixed time\n        zones will raise an error unless `utc=True`.\n        Please specify `utc=True` to opt in to the new behaviour\n        and silence this warning. To create a `Series` with mixed offsets and\n        `object` dtype, please use `apply` and `datetime.datetime.strptime`.\n\n    See also: pandas general documentation about `timezone conversion and\n    localization\n    <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html\n    #time-zone-handling>`_.\n\nformat : str, default None\n    The strftime to parse time, e.g. :const:`\"%d/%m/%Y\"`. See\n    `strftime documentation\n    <https://docs.python.org/3/library/datetime.html\n    #strftime-and-strptime-behavior>`_ for more information on choices, though\n    note that :const:`\"%f\"` will parse all the way up to nanoseconds.\n    You can also pass:\n\n    - \"ISO8601\", to parse any `ISO8601 <https://en.wikipedia.org/wiki/ISO_8601>`_\n      time string (not necessarily in exactly the same format);\n    - \"mixed\", to infer the format for each element individually. This is risky,\n      and you should probably use it along with `dayfirst`.\n\n    .. note::\n\n        If a :class:`DataFrame` is passed, then `format` has no effect.\n\nexact : bool, default True\n    Control how `format` is used:\n\n    - If :const:`True`, require an exact `format` match.\n    - If :const:`False`, allow the `format` to match anywhere in the target\n      string.\n\n    Cannot be used alongside ``format='ISO8601'`` or ``format='mixed'``.\nunit : str, default 'ns'\n    The unit of the arg (D,s,ms,us,ns) denote the unit, which is an\n    integer or float number. This will be based off the origin.\n    Example, with ``unit='ms'`` and ``origin='unix'``, this would calculate\n    the number of milliseconds to the unix epoch start.\ninfer_datetime_format : bool, default False\n    If :const:`True` and no `format` is given, attempt to infer the format\n    of the datetime strings based on the first non-NaN element,\n    and if it can be inferred, switch to a faster method of parsing them.\n    In some cases this can increase the parsing speed by ~5-10x.\n\n    .. deprecated:: 2.0.0\n        A strict version of this argument is now the default, passing it has\n        no effect.\n\norigin : scalar, default 'unix'\n    Define the reference date. The numeric values would be parsed as number\n    of units (defined by `unit`) since this reference date.\n\n    - If :const:`'unix'` (or POSIX) time; origin is set to 1970-01-01.\n    - If :const:`'julian'`, unit must be :const:`'D'`, and origin is set to\n      beginning of Julian Calendar. Julian day number :const:`0` is assigned\n      to the day starting at noon on January 1, 4713 BC.\n    - If Timestamp convertible (Timestamp, dt.datetime, np.datetimt64 or date\n      string), origin is set to Timestamp identified by origin.\n    - If a float or integer, origin is the difference\n      (in units determined by the ``unit`` argument) relative to 1970-01-01.\ncache : bool, default True\n    If :const:`True`, use a cache of unique, converted dates to apply the\n    datetime conversion. May produce significant speed-up when parsing\n    duplicate date strings, especially ones with timezone offsets. The cache\n    is only used when there are at least 50 values. The presence of\n    out-of-bounds values will render the cache unusable and may slow down\n    parsing.\n\nReturns\n-------\ndatetime\n    If parsing succeeded.\n    Return type depends on input (types in parenthesis correspond to\n    fallback in case of unsuccessful timezone or out-of-range timestamp\n    parsing):\n\n    - scalar: :class:`Timestamp` (or :class:`datetime.datetime`)\n    - array-like: :class:`DatetimeIndex` (or :class:`Series` with\n      :class:`object` dtype containing :class:`datetime.datetime`)\n    - Series: :class:`Series` of :class:`datetime64` dtype (or\n      :class:`Series` of :class:`object` dtype containing\n      :class:`datetime.datetime`)\n    - DataFrame: :class:`Series` of :class:`datetime64` dtype (or\n      :class:`Series` of :class:`object` dtype containing\n      :class:`datetime.datetime`)\n\nRaises\n------\nParserError\n    When parsing a date from string fails.\nValueError\n    When another datetime conversion error happens. For example when one\n    of 'year', 'month', day' columns is missing in a :class:`DataFrame`, or\n    when a Timezone-aware :class:`datetime.datetime` is found in an array-like\n    of mixed time offsets, and ``utc=False``.\n\nSee Also\n--------\nDataFrame.astype : Cast argument to a specified dtype.\nto_timedelta : Convert argument to timedelta.\nconvert_dtypes : Convert dtypes.\n\nNotes\n-----\n\nMany input types are supported, and lead to different output types:\n\n- **scalars** can be int, float, str, datetime object (from stdlib :mod:`datetime`\n  module or :mod:`numpy`). They are converted to :class:`Timestamp` when\n  possible, otherwise they are converted to :class:`datetime.datetime`.\n  None/NaN/null scalars are converted to :const:`NaT`.\n\n- **array-like** can contain int, float, str, datetime objects. They are\n  converted to :class:`DatetimeIndex` when possible, otherwise they are\n  converted to :class:`Index` with :class:`object` dtype, containing\n  :class:`datetime.datetime`. None/NaN/null entries are converted to\n  :const:`NaT` in both cases.\n\n- **Series** are converted to :class:`Series` with :class:`datetime64`\n  dtype when possible, otherwise they are converted to :class:`Series` with\n  :class:`object` dtype, containing :class:`datetime.datetime`. None/NaN/null\n  entries are converted to :const:`NaT` in both cases.\n\n- **DataFrame/dict-like** are converted to :class:`Series` with\n  :class:`datetime64` dtype. For each row a datetime is created from assembling\n  the various dataframe columns. Column keys can be common abbreviations\n  like ['year', 'month', 'day', 'minute', 'second', 'ms', 'us', 'ns']) or\n  plurals of the same.\n\nThe following causes are responsible for :class:`datetime.datetime` objects\nbeing returned (possibly inside an :class:`Index` or a :class:`Series` with\n:class:`object` dtype) instead of a proper pandas designated type\n(:class:`Timestamp`, :class:`DatetimeIndex` or :class:`Series`\nwith :class:`datetime64` dtype):\n\n- when any input element is before :const:`Timestamp.min` or after\n  :const:`Timestamp.max`, see `timestamp limitations\n  <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html\n  #timeseries-timestamp-limits>`_.\n\n- when ``utc=False`` (default) and the input is an array-like or\n  :class:`Series` containing mixed naive/aware datetime, or aware with mixed\n  time offsets. Note that this happens in the (quite frequent) situation when\n  the timezone has a daylight savings policy. In that case you may wish to\n  use ``utc=True``.\n\nExamples\n--------\n\n**Handling various input formats**\n\nAssembling a datetime from multiple columns of a :class:`DataFrame`. The keys\ncan be common abbreviations like ['year', 'month', 'day', 'minute', 'second',\n'ms', 'us', 'ns']) or plurals of the same\n\n>>> df = pd.DataFrame({'year': [2015, 2016],\n...                    'month': [2, 3],\n...                    'day': [4, 5]})\n>>> pd.to_datetime(df)\n0   2015-02-04\n1   2016-03-05\ndtype: datetime64[ns]\n\nUsing a unix epoch time\n\n>>> pd.to_datetime(1490195805, unit='s')\nTimestamp('2017-03-22 15:16:45')\n>>> pd.to_datetime(1490195805433502912, unit='ns')\nTimestamp('2017-03-22 15:16:45.433502912')\n\n.. warning:: For float arg, precision rounding might happen. To prevent\n    unexpected behavior use a fixed-width exact type.\n\nUsing a non-unix epoch origin\n\n>>> pd.to_datetime([1, 2, 3], unit='D',\n...                origin=pd.Timestamp('1960-01-01'))\nDatetimeIndex(['1960-01-02', '1960-01-03', '1960-01-04'],\n              dtype='datetime64[ns]', freq=None)\n\n**Differences with strptime behavior**\n\n:const:`\"%f\"` will parse all the way up to nanoseconds.\n\n>>> pd.to_datetime('2018-10-26 12:00:00.0000000011',\n...                format='%Y-%m-%d %H:%M:%S.%f')\nTimestamp('2018-10-26 12:00:00.000000001')\n\n**Non-convertible date/times**\n\nPassing ``errors='coerce'`` will force an out-of-bounds date to :const:`NaT`,\nin addition to forcing non-dates (or non-parseable dates) to :const:`NaT`.\n\n>>> pd.to_datetime('13000101', format='%Y%m%d', errors='coerce')\nNaT\n\n.. _to_datetime_tz_examples:\n\n**Timezones and time offsets**\n\nThe default behaviour (``utc=False``) is as follows:\n\n- Timezone-naive inputs are converted to timezone-naive :class:`DatetimeIndex`:\n\n>>> pd.to_datetime(['2018-10-26 12:00:00', '2018-10-26 13:00:15'])\nDatetimeIndex(['2018-10-26 12:00:00', '2018-10-26 13:00:15'],\n              dtype='datetime64[ns]', freq=None)\n\n- Timezone-aware inputs *with constant time offset* are converted to\n  timezone-aware :class:`DatetimeIndex`:\n\n>>> pd.to_datetime(['2018-10-26 12:00 -0500', '2018-10-26 13:00 -0500'])\nDatetimeIndex(['2018-10-26 12:00:00-05:00', '2018-10-26 13:00:00-05:00'],\n              dtype='datetime64[ns, UTC-05:00]', freq=None)\n\n- However, timezone-aware inputs *with mixed time offsets* (for example\n  issued from a timezone with daylight savings, such as Europe/Paris)\n  are **not successfully converted** to a :class:`DatetimeIndex`.\n  Parsing datetimes with mixed time zones will show a warning unless\n  `utc=True`. If you specify `utc=False` the warning below will be shown\n  and a simple :class:`Index` containing :class:`datetime.datetime`\n  objects will be returned:\n\n>>> pd.to_datetime(['2020-10-25 02:00 +0200',\n...                 '2020-10-25 04:00 +0100'])  # doctest: +SKIP\nFutureWarning: In a future version of pandas, parsing datetimes with mixed\ntime zones will raise an error unless `utc=True`. Please specify `utc=True`\nto opt in to the new behaviour and silence this warning. To create a `Series`\nwith mixed offsets and `object` dtype, please use `apply` and\n`datetime.datetime.strptime`.\nIndex([2020-10-25 02:00:00+02:00, 2020-10-25 04:00:00+01:00],\n      dtype='object')\n\n- A mix of timezone-aware and timezone-naive inputs is also converted to\n  a simple :class:`Index` containing :class:`datetime.datetime` objects:\n\n>>> from datetime import datetime\n>>> pd.to_datetime([\"2020-01-01 01:00:00-01:00\",\n...                 datetime(2020, 1, 1, 3, 0)])  # doctest: +SKIP\nFutureWarning: In a future version of pandas, parsing datetimes with mixed\ntime zones will raise an error unless `utc=True`. Please specify `utc=True`\nto opt in to the new behaviour and silence this warning. To create a `Series`\nwith mixed offsets and `object` dtype, please use `apply` and\n`datetime.datetime.strptime`.\nIndex([2020-01-01 01:00:00-01:00, 2020-01-01 03:00:00], dtype='object')\n\n|\n\nSetting ``utc=True`` solves most of the above issues:\n\n- Timezone-naive inputs are *localized* as UTC\n\n>>> pd.to_datetime(['2018-10-26 12:00', '2018-10-26 13:00'], utc=True)\nDatetimeIndex(['2018-10-26 12:00:00+00:00', '2018-10-26 13:00:00+00:00'],\n              dtype='datetime64[ns, UTC]', freq=None)\n\n- Timezone-aware inputs are *converted* to UTC (the output represents the\n  exact same datetime, but viewed from the UTC time offset `+00:00`).\n\n>>> pd.to_datetime(['2018-10-26 12:00 -0530', '2018-10-26 12:00 -0500'],\n...                utc=True)\nDatetimeIndex(['2018-10-26 17:30:00+00:00', '2018-10-26 17:00:00+00:00'],\n              dtype='datetime64[ns, UTC]', freq=None)\n\n- Inputs can contain both string or datetime, the above\n  rules still apply\n\n>>> pd.to_datetime(['2018-10-26 12:00', datetime(2020, 1, 1, 18)], utc=True)\nDatetimeIndex(['2018-10-26 12:00:00+00:00', '2020-01-01 18:00:00+00:00'],\n              dtype='datetime64[ns, UTC]', freq=None)", "parameters": {"type": "object", "properties": {"arg": {"type": ["dictconvertible", "datetimescalarorarrayconvertible"]}}}}}
{"task_id": "BigCodeBench/302", "data": {"name": "pandas.concat", "type": "callable", "signature": "(objs: 'Iterable[Series | DataFrame] | Mapping[HashableT, axis: 'Axis' = 0)", "description": "Concatenate pandas objects along a particular axis.\n\nAllows optional set logic along the other axes.\n\nCan also add a layer of hierarchical indexing on the concatenation axis,\nwhich may be useful if the labels are the same (or overlapping) on\nthe passed axis number.\n\nParameters\n----------\nobjs : a sequence or mapping of Series or DataFrame objects\n    If a mapping is passed, the sorted keys will be used as the `keys`\n    argument, unless it is passed, in which case the values will be\n    selected (see below). Any None objects will be dropped silently unless\n    they are all None in which case a ValueError will be raised.\naxis : {0/'index', 1/'columns'}, default 0\n    The axis to concatenate along.\njoin : {'inner', 'outer'}, default 'outer'\n    How to handle indexes on other axis (or axes).\nignore_index : bool, default False\n    If True, do not use the index values along the concatenation axis. The\n    resulting axis will be labeled 0, ..., n - 1. This is useful if you are\n    concatenating objects where the concatenation axis does not have\n    meaningful indexing information. Note the index values on the other\n    axes are still respected in the join.\nkeys : sequence, default None\n    If multiple levels passed, should contain tuples. Construct\n    hierarchical index using the passed keys as the outermost level.\nlevels : list of sequences, default None\n    Specific levels (unique values) to use for constructing a\n    MultiIndex. Otherwise they will be inferred from the keys.\nnames : list, default None\n    Names for the levels in the resulting hierarchical index.\nverify_integrity : bool, default False\n    Check whether the new concatenated axis contains duplicates. This can\n    be very expensive relative to the actual data concatenation.\nsort : bool, default False\n    Sort non-concatenation axis if it is not already aligned. One exception to\n    this is when the non-concatentation axis is a DatetimeIndex and join='outer'\n    and the axis is not already aligned. In that case, the non-concatenation\n    axis is always sorted lexicographically.\ncopy : bool, default True\n    If False, do not copy data unnecessarily.\n\nReturns\n-------\nobject, type of objs\n    When concatenating all ``Series`` along the index (axis=0), a\n    ``Series`` is returned. When ``objs`` contains at least one\n    ``DataFrame``, a ``DataFrame`` is returned. When concatenating along\n    the columns (axis=1), a ``DataFrame`` is returned.\n\nSee Also\n--------\nDataFrame.join : Join DataFrames using indexes.\nDataFrame.merge : Merge DataFrames by indexes or columns.\n\nNotes\n-----\nThe keys, levels, and names arguments are all optional.\n\nA walkthrough of how this method fits in with other tools for combining\npandas objects can be found `here\n<https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html>`__.\n\nIt is not recommended to build DataFrames by adding single rows in a\nfor loop. Build a list of rows and make a DataFrame in a single concat.\n\nExamples\n--------\nCombine two ``Series``.\n\n>>> s1 = pd.Series(['a', 'b'])\n>>> s2 = pd.Series(['c', 'd'])\n>>> pd.concat([s1, s2])\n0    a\n1    b\n0    c\n1    d\ndtype: object\n\nClear the existing index and reset it in the result\nby setting the ``ignore_index`` option to ``True``.\n\n>>> pd.concat([s1, s2], ignore_index=True)\n0    a\n1    b\n2    c\n3    d\ndtype: object\n\nAdd a hierarchical index at the outermost level of\nthe data with the ``keys`` option.\n\n>>> pd.concat([s1, s2], keys=['s1', 's2'])\ns1  0    a\n    1    b\ns2  0    c\n    1    d\ndtype: object\n\nLabel the index keys you create with the ``names`` option.\n\n>>> pd.concat([s1, s2], keys=['s1', 's2'],\n...           names=['Series name', 'Row ID'])\nSeries name  Row ID\ns1           0         a\n             1         b\ns2           0         c\n             1         d\ndtype: object\n\nCombine two ``DataFrame`` objects with identical columns.\n\n>>> df1 = pd.DataFrame([['a', 1], ['b', 2]],\n...                    columns=['letter', 'number'])\n>>> df1\n  letter  number\n0      a       1\n1      b       2\n>>> df2 = pd.DataFrame([['c', 3], ['d', 4]],\n...                    columns=['letter', 'number'])\n>>> df2\n  letter  number\n0      c       3\n1      d       4\n>>> pd.concat([df1, df2])\n  letter  number\n0      a       1\n1      b       2\n0      c       3\n1      d       4\n\nCombine ``DataFrame`` objects with overlapping columns\nand return everything. Columns outside the intersection will\nbe filled with ``NaN`` values.\n\n>>> df3 = pd.DataFrame([['c', 3, 'cat'], ['d', 4, 'dog']],\n...                    columns=['letter', 'number', 'animal'])\n>>> df3\n  letter  number animal\n0      c       3    cat\n1      d       4    dog\n>>> pd.concat([df1, df3], sort=False)\n  letter  number animal\n0      a       1    NaN\n1      b       2    NaN\n0      c       3    cat\n1      d       4    dog\n\nCombine ``DataFrame`` objects with overlapping columns\nand return only those that are shared by passing ``inner`` to\nthe ``join`` keyword argument.\n\n>>> pd.concat([df1, df3], join=\"inner\")\n  letter  number\n0      a       1\n1      b       2\n0      c       3\n1      d       4\n\nCombine ``DataFrame`` objects horizontally along the x axis by\npassing in ``axis=1``.\n\n>>> df4 = pd.DataFrame([['bird', 'polly'], ['monkey', 'george']],\n...                    columns=['animal', 'name'])\n>>> pd.concat([df1, df4], axis=1)\n  letter  number  animal    name\n0      a       1    bird   polly\n1      b       2  monkey  george\n\nPrevent the result from including duplicate index values with the\n``verify_integrity`` option.\n\n>>> df5 = pd.DataFrame([1], index=['a'])\n>>> df5\n   0\na  1\n>>> df6 = pd.DataFrame([2], index=['a'])\n>>> df6\n   0\na  2\n>>> pd.concat([df5, df6], verify_integrity=True)\nTraceback (most recent call last):\n    ...\nValueError: Indexes have overlapping values: ['a']\n\nAppend a single row to the end of a ``DataFrame`` object.\n\n>>> df7 = pd.DataFrame({'a': 1, 'b': 2}, index=[0])\n>>> df7\n    a   b\n0   1   2\n>>> new_row = pd.Series({'a': 3, 'b': 4})\n>>> new_row\na    3\nb    4\ndtype: int64\n>>> pd.concat([df7, new_row.to_frame().T], ignore_index=True)\n    a   b\n0   1   2\n1   3   4", "parameters": {"type": "object", "properties": {"objs": {"type": ["dataframe]", "iterable[series", "mapping[hashablet"]}, "axis": {"type": "axis", "default": 0}}}}}
{"task_id": "BigCodeBench/302", "data": {"name": "pandas.concat.iloc", "type": "callable", "signature": "(objs: 'Iterable[Series | DataFrame] | Mapping[HashableT, Series | DataFrame]', *, axis: 'Axis' = 0, join: 'str' = 'outer', ignore_index: 'bool' = False, keys: 'Iterable[Hashable] | None' = None, levels=None, names: 'list[HashableT] | None' = None, verify_integrity: 'bool' = False, sort: 'bool' = False, copy: 'bool | None' = None) -> 'DataFrame | Series'", "description": "Concatenate pandas objects along a particular axis.\n\nAllows optional set logic along the other axes.\n\nCan also add a layer of hierarchical indexing on the concatenation axis,\nwhich may be useful if the labels are the same (or overlapping) on\nthe passed axis number.\n\nParameters\n----------\nobjs : a sequence or mapping of Series or DataFrame objects\n    If a mapping is passed, the sorted keys will be used as the `keys`\n    argument, unless it is passed, in which case the values will be\n    selected (see below). Any None objects will be dropped silently unless\n    they are all None in which case a ValueError will be raised.\naxis : {0/'index', 1/'columns'}, default 0\n    The axis to concatenate along.\njoin : {'inner', 'outer'}, default 'outer'\n    How to handle indexes on other axis (or axes).\nignore_index : bool, default False\n    If True, do not use the index values along the concatenation axis. The\n    resulting axis will be labeled 0, ..., n - 1. This is useful if you are\n    concatenating objects where the concatenation axis does not have\n    meaningful indexing information. Note the index values on the other\n    axes are still respected in the join.\nkeys : sequence, default None\n    If multiple levels passed, should contain tuples. Construct\n    hierarchical index using the passed keys as the outermost level.\nlevels : list of sequences, default None\n    Specific levels (unique values) to use for constructing a\n    MultiIndex. Otherwise they will be inferred from the keys.\nnames : list, default None\n    Names for the levels in the resulting hierarchical index.\nverify_integrity : bool, default False\n    Check whether the new concatenated axis contains duplicates. This can\n    be very expensive relative to the actual data concatenation.\nsort : bool, default False\n    Sort non-concatenation axis if it is not already aligned. One exception to\n    this is when the non-concatentation axis is a DatetimeIndex and join='outer'\n    and the axis is not already aligned. In that case, the non-concatenation\n    axis is always sorted lexicographically.\ncopy : bool, default True\n    If False, do not copy data unnecessarily.\n\nReturns\n-------\nobject, type of objs\n    When concatenating all ``Series`` along the index (axis=0), a\n    ``Series`` is returned. When ``objs`` contains at least one\n    ``DataFrame``, a ``DataFrame`` is returned. When concatenating along\n    the columns (axis=1), a ``DataFrame`` is returned.\n\nSee Also\n--------\nDataFrame.join : Join DataFrames using indexes.\nDataFrame.merge : Merge DataFrames by indexes or columns.\n\nNotes\n-----\nThe keys, levels, and names arguments are all optional.\n\nA walkthrough of how this method fits in with other tools for combining\npandas objects can be found `here\n<https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html>`__.\n\nIt is not recommended to build DataFrames by adding single rows in a\nfor loop. Build a list of rows and make a DataFrame in a single concat.\n\nExamples\n--------\nCombine two ``Series``.\n\n>>> s1 = pd.Series(['a', 'b'])\n>>> s2 = pd.Series(['c', 'd'])\n>>> pd.concat([s1, s2])\n0    a\n1    b\n0    c\n1    d\ndtype: object\n\nClear the existing index and reset it in the result\nby setting the ``ignore_index`` option to ``True``.\n\n>>> pd.concat([s1, s2], ignore_index=True)\n0    a\n1    b\n2    c\n3    d\ndtype: object\n\nAdd a hierarchical index at the outermost level of\nthe data with the ``keys`` option.\n\n>>> pd.concat([s1, s2], keys=['s1', 's2'])\ns1  0    a\n    1    b\ns2  0    c\n    1    d\ndtype: object\n\nLabel the index keys you create with the ``names`` option.\n\n>>> pd.concat([s1, s2], keys=['s1', 's2'],\n...           names=['Series name', 'Row ID'])\nSeries name  Row ID\ns1           0         a\n             1         b\ns2           0         c\n             1         d\ndtype: object\n\nCombine two ``DataFrame`` objects with identical columns.\n\n>>> df1 = pd.DataFrame([['a', 1], ['b', 2]],\n...                    columns=['letter', 'number'])\n>>> df1\n  letter  number\n0      a       1\n1      b       2\n>>> df2 = pd.DataFrame([['c', 3], ['d', 4]],\n...                    columns=['letter', 'number'])\n>>> df2\n  letter  number\n0      c       3\n1      d       4\n>>> pd.concat([df1, df2])\n  letter  number\n0      a       1\n1      b       2\n0      c       3\n1      d       4\n\nCombine ``DataFrame`` objects with overlapping columns\nand return everything. Columns outside the intersection will\nbe filled with ``NaN`` values.\n\n>>> df3 = pd.DataFrame([['c', 3, 'cat'], ['d', 4, 'dog']],\n...                    columns=['letter', 'number', 'animal'])\n>>> df3\n  letter  number animal\n0      c       3    cat\n1      d       4    dog\n>>> pd.concat([df1, df3], sort=False)\n  letter  number animal\n0      a       1    NaN\n1      b       2    NaN\n0      c       3    cat\n1      d       4    dog\n\nCombine ``DataFrame`` objects with overlapping columns\nand return only those that are shared by passing ``inner`` to\nthe ``join`` keyword argument.\n\n>>> pd.concat([df1, df3], join=\"inner\")\n  letter  number\n0      a       1\n1      b       2\n0      c       3\n1      d       4\n\nCombine ``DataFrame`` objects horizontally along the x axis by\npassing in ``axis=1``.\n\n>>> df4 = pd.DataFrame([['bird', 'polly'], ['monkey', 'george']],\n...                    columns=['animal', 'name'])\n>>> pd.concat([df1, df4], axis=1)\n  letter  number  animal    name\n0      a       1    bird   polly\n1      b       2  monkey  george\n\nPrevent the result from including duplicate index values with the\n``verify_integrity`` option.\n\n>>> df5 = pd.DataFrame([1], index=['a'])\n>>> df5\n   0\na  1\n>>> df6 = pd.DataFrame([2], index=['a'])\n>>> df6\n   0\na  2\n>>> pd.concat([df5, df6], verify_integrity=True)\nTraceback (most recent call last):\n    ...\nValueError: Indexes have overlapping values: ['a']\n\nAppend a single row to the end of a ``DataFrame`` object.\n\n>>> df7 = pd.DataFrame({'a': 1, 'b': 2}, index=[0])\n>>> df7\n    a   b\n0   1   2\n>>> new_row = pd.Series({'a': 3, 'b': 4})\n>>> new_row\na    3\nb    4\ndtype: int64\n>>> pd.concat([df7, new_row.to_frame().T], ignore_index=True)\n    a   b\n0   1   2\n1   3   4", "parameters": {"type": "object", "properties": {"objs": {"type": ["dataframe]", "iterable[series", "mapping[hashablet, series"]}, "axis": {"type": "axis", "default": 0}, "join": {"type": "str", "default": "outer"}, "ignore_index": {"type": "bool", "default": false}, "keys": {"type": ["iterable[hashable]", "null"], "default": null}, "levels": {"type": "NoneType", "default": null}, "names": {"type": ["list[hashablet]", "null"], "default": null}, "verify_integrity": {"type": "bool", "default": false}, "sort": {"type": "bool", "default": false}, "copy": {"type": ["bool", "null"], "default": null}}}}}
{"task_id": "BigCodeBench/302", "data": {"name": "seaborn.heatmap", "type": "callable", "signature": "(data, cmap=None, annot=None, **kwargs)", "description": "Plot rectangular data as a color-encoded matrix.\n\nThis is an Axes-level function and will draw the heatmap into the\ncurrently-active Axes if none is provided to the ``ax`` argument.  Part of\nthis Axes space will be taken and used to plot a colormap, unless ``cbar``\nis False or a separate Axes is provided to ``cbar_ax``.\n\nParameters\n----------\ndata : rectangular dataset\n    2D dataset that can be coerced into an ndarray. If a Pandas DataFrame\n    is provided, the index/column information will be used to label the\n    columns and rows.\nvmin, vmax : floats, optional\n    Values to anchor the colormap, otherwise they are inferred from the\n    data and other keyword arguments.\ncmap : matplotlib colormap name or object, or list of colors, optional\n    The mapping from data values to color space. If not provided, the\n    default will depend on whether ``center`` is set.\ncenter : float, optional\n    The value at which to center the colormap when plotting divergent data.\n    Using this parameter will change the default ``cmap`` if none is\n    specified.\nrobust : bool, optional\n    If True and ``vmin`` or ``vmax`` are absent, the colormap range is\n    computed with robust quantiles instead of the extreme values.\nannot : bool or rectangular dataset, optional\n    If True, write the data value in each cell. If an array-like with the\n    same shape as ``data``, then use this to annotate the heatmap instead\n    of the data. Note that DataFrames will match on position, not index.\nfmt : str, optional\n    String formatting code to use when adding annotations.\nannot_kws : dict of key, value mappings, optional\n    Keyword arguments for :meth:`matplotlib.axes.Axes.text` when ``annot``\n    is True.\nlinewidths : float, optional\n    Width of the lines that will divide each cell.\nlinecolor : color, optional\n    Color of the lines that will divide each cell.\ncbar : bool, optional\n    Whether to draw a colorbar.\ncbar_kws : dict of key, value mappings, optional\n    Keyword arguments for :meth:`matplotlib.figure.Figure.colorbar`.\ncbar_ax : matplotlib Axes, optional\n    Axes in which to draw the colorbar, otherwise take space from the\n    main Axes.\nsquare : bool, optional\n    If True, set the Axes aspect to \"equal\" so each cell will be\n    square-shaped.\nxticklabels, yticklabels : \"auto\", bool, list-like, or int, optional\n    If True, plot the column names of the dataframe. If False, don't plot\n    the column names. If list-like, plot these alternate labels as the\n    xticklabels. If an integer, use the column names but plot only every\n    n label. If \"auto\", try to densely plot non-overlapping labels.\nmask : bool array or DataFrame, optional\n    If passed, data will not be shown in cells where ``mask`` is True.\n    Cells with missing values are automatically masked.\nax : matplotlib Axes, optional\n    Axes in which to draw the plot, otherwise use the currently-active\n    Axes.\nkwargs : other keyword arguments\n    All other keyword arguments are passed to\n    :meth:`matplotlib.axes.Axes.pcolormesh`.\n\nReturns\n-------\nax : matplotlib Axes\n    Axes object with the heatmap.\n\nSee Also\n--------\nclustermap : Plot a matrix using hierarchical clustering to arrange the\n             rows and columns.\n\nExamples\n--------\n\n.. include:: ../docstrings/heatmap.rst", "parameters": {"type": "object", "properties": {"data": {}, "cmap": {"type": "NoneType", "default": null}, "annot": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/308", "data": {"name": "pandas.DataFrame", "type": "class", "signature": "(data=None, index: 'Axes | None' = None, columns: 'Axes | None' = None, dtype: 'Dtype | None' = None, copy: 'bool | None' = None) -> 'None'", "description": "Two-dimensional, size-mutable, potentially heterogeneous tabular data.\n\nData structure also contains labeled axes (rows and columns).\nArithmetic operations align on both row and column labels. Can be\nthought of as a dict-like container for Series objects. The primary\npandas data structure.\n\nParameters\n----------\ndata : ndarray (structured or homogeneous), Iterable, dict, or DataFrame\n    Dict can contain Series, arrays, constants, dataclass or list-like objects. If\n    data is a dict, column order follows insertion-order. If a dict contains Series\n    which have an index defined, it is aligned by its index. This alignment also\n    occurs if data is a Series or a DataFrame itself. Alignment is done on\n    Series/DataFrame inputs.\n\n    If data is a list of dicts, column order follows insertion-order.\n\nindex : Index or array-like\n    Index to use for resulting frame. Will default to RangeIndex if\n    no indexing information part of input data and no index provided.\ncolumns : Index or array-like\n    Column labels to use for resulting frame when data does not have them,\n    defaulting to RangeIndex(0, 1, 2, ..., n). If data contains column labels,\n    will perform column selection instead.\ndtype : dtype, default None\n    Data type to force. Only a single dtype is allowed. If None, infer.\ncopy : bool or None, default None\n    Copy data from inputs.\n    For dict data, the default of None behaves like ``copy=True``.  For DataFrame\n    or 2d ndarray input, the default of None behaves like ``copy=False``.\n    If data is a dict containing one or more Series (possibly of different dtypes),\n    ``copy=False`` will ensure that these inputs are not copied.\n\n    .. versionchanged:: 1.3.0\n\nSee Also\n--------\nDataFrame.from_records : Constructor from tuples, also record arrays.\nDataFrame.from_dict : From dicts of Series, arrays, or dicts.\nread_csv : Read a comma-separated values (csv) file into DataFrame.\nread_table : Read general delimited file into DataFrame.\nread_clipboard : Read text from clipboard into DataFrame.\n\nNotes\n-----\nPlease reference the :ref:`User Guide <basics.dataframe>` for more information.\n\nExamples\n--------\nConstructing DataFrame from a dictionary.\n\n>>> d = {'col1': [1, 2], 'col2': [3, 4]}\n>>> df = pd.DataFrame(data=d)\n>>> df\n   col1  col2\n0     1     3\n1     2     4\n\nNotice that the inferred dtype is int64.\n\n>>> df.dtypes\ncol1    int64\ncol2    int64\ndtype: object\n\nTo enforce a single dtype:\n\n>>> df = pd.DataFrame(data=d, dtype=np.int8)\n>>> df.dtypes\ncol1    int8\ncol2    int8\ndtype: object\n\nConstructing DataFrame from a dictionary including Series:\n\n>>> d = {'col1': [0, 1, 2, 3], 'col2': pd.Series([2, 3], index=[2, 3])}\n>>> pd.DataFrame(data=d, index=[0, 1, 2, 3])\n   col1  col2\n0     0   NaN\n1     1   NaN\n2     2   2.0\n3     3   3.0\n\nConstructing DataFrame from numpy ndarray:\n\n>>> df2 = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),\n...                    columns=['a', 'b', 'c'])\n>>> df2\n   a  b  c\n0  1  2  3\n1  4  5  6\n2  7  8  9\n\nConstructing DataFrame from a numpy ndarray that has labeled columns:\n\n>>> data = np.array([(1, 2, 3), (4, 5, 6), (7, 8, 9)],\n...                 dtype=[(\"a\", \"i4\"), (\"b\", \"i4\"), (\"c\", \"i4\")])\n>>> df3 = pd.DataFrame(data, columns=['c', 'a'])\n...\n>>> df3\n   c  a\n0  3  1\n1  6  4\n2  9  7\n\nConstructing DataFrame from dataclass:\n\n>>> from dataclasses import make_dataclass\n>>> Point = make_dataclass(\"Point\", [(\"x\", int), (\"y\", int)])\n>>> pd.DataFrame([Point(0, 0), Point(0, 3), Point(2, 3)])\n   x  y\n0  0  0\n1  0  3\n2  2  3\n\nConstructing DataFrame from Series/DataFrame:\n\n>>> ser = pd.Series([1, 2, 3], index=[\"a\", \"b\", \"c\"])\n>>> df = pd.DataFrame(data=ser, index=[\"a\", \"c\"])\n>>> df\n   0\na  1\nc  3\n\n>>> df1 = pd.DataFrame([1, 2, 3], index=[\"a\", \"b\", \"c\"], columns=[\"x\"])\n>>> df2 = pd.DataFrame(data=df1, index=[\"a\", \"c\"])\n>>> df2\n   x\na  1\nc  3", "parameters": {"type": "object", "properties": {"data": {"type": "NoneType", "default": null}, "index": {"type": ["axes", "null"], "default": null}, "columns": {"type": ["axes", "null"], "default": null}, "dtype": {"type": ["dtype", "null"], "default": null}, "copy": {"type": ["bool", "null"], "default": null}}}}}
{"task_id": "BigCodeBench/308", "data": {"name": "pandas.DataFrame.loc", "type": "constant", "signature": null, "description": "Access a group of rows and columns by label(s) or a boolean array.\n\n``.loc[]`` is primarily label based, but may also be used with a\nboolean array.\n\nAllowed inputs are:\n\n- A single label, e.g. ``5`` or ``'a'``, (note that ``5`` is\n  interpreted as a *label* of the index, and **never** as an\n  integer position along the index).\n- A list or array of labels, e.g. ``['a', 'b', 'c']``.\n- A slice object with labels, e.g. ``'a':'f'``.\n\n  .. warning:: Note that contrary to usual python slices, **both** the\n      start and the stop are included\n\n- A boolean array of the same length as the axis being sliced,\n  e.g. ``[True, False, True]``.\n- An alignable boolean Series. The index of the key will be aligned before\n  masking.\n- An alignable Index. The Index of the returned selection will be the input.\n- A ``callable`` function with one argument (the calling Series or\n  DataFrame) and that returns valid output for indexing (one of the above)\n\nSee more at :ref:`Selection by Label <indexing.label>`.\n\nRaises\n------\nKeyError\n    If any items are not found.\nIndexingError\n    If an indexed key is passed and its index is unalignable to the frame index.\n\nSee Also\n--------\nDataFrame.at : Access a single value for a row/column label pair.\nDataFrame.iloc : Access group of rows and columns by integer position(s).\nDataFrame.xs : Returns a cross-section (row(s) or column(s)) from the\n               Series/DataFrame.\nSeries.loc : Access group of values using labels.\n\nExamples\n--------\n**Getting values**\n\n>>> df = pd.DataFrame([[1, 2], [4, 5], [7, 8]],\n...                   index=['cobra', 'viper', 'sidewinder'],\n...                   columns=['max_speed', 'shield'])\n>>> df\n            max_speed  shield\ncobra               1       2\nviper               4       5\nsidewinder          7       8\n\nSingle label. Note this returns the row as a Series.\n\n>>> df.loc['viper']\nmax_speed    4\nshield       5\nName: viper, dtype: int64\n\nList of labels. Note using ``[[]]`` returns a DataFrame.\n\n>>> df.loc[['viper', 'sidewinder']]\n            max_speed  shield\nviper               4       5\nsidewinder          7       8\n\nSingle label for row and column\n\n>>> df.loc['cobra', 'shield']\n2\n\nSlice with labels for row and single label for column. As mentioned\nabove, note that both the start and stop of the slice are included.\n\n>>> df.loc['cobra':'viper', 'max_speed']\ncobra    1\nviper    4\nName: max_speed, dtype: int64\n\nBoolean list with the same length as the row axis\n\n>>> df.loc[[False, False, True]]\n            max_speed  shield\nsidewinder          7       8\n\nAlignable boolean Series:\n\n>>> df.loc[pd.Series([False, True, False],\n...                  index=['viper', 'sidewinder', 'cobra'])]\n                     max_speed  shield\nsidewinder          7       8\n\nIndex (same behavior as ``df.reindex``)\n\n>>> df.loc[pd.Index([\"cobra\", \"viper\"], name=\"foo\")]\n       max_speed  shield\nfoo\ncobra          1       2\nviper          4       5\n\nConditional that returns a boolean Series\n\n>>> df.loc[df['shield'] > 6]\n            max_speed  shield\nsidewinder          7       8\n\nConditional that returns a boolean Series with column labels specified\n\n>>> df.loc[df['shield'] > 6, ['max_speed']]\n            max_speed\nsidewinder          7\n\nMultiple conditional using ``&`` that returns a boolean Series\n\n>>> df.loc[(df['max_speed'] > 1) & (df['shield'] < 8)]\n            max_speed  shield\nviper          4       5\n\nMultiple conditional using ``|`` that returns a boolean Series\n\n>>> df.loc[(df['max_speed'] > 4) | (df['shield'] < 5)]\n            max_speed  shield\ncobra               1       2\nsidewinder          7       8\n\nPlease ensure that each condition is wrapped in parentheses ``()``.\nSee the :ref:`user guide<indexing.boolean>`\nfor more details and explanations of Boolean indexing.\n\n.. note::\n    If you find yourself using 3 or more conditionals in ``.loc[]``,\n    consider using :ref:`advanced indexing<advanced.advanced_hierarchical>`.\n\n    See below for using ``.loc[]`` on MultiIndex DataFrames.\n\nCallable that returns a boolean Series\n\n>>> df.loc[lambda df: df['shield'] == 8]\n            max_speed  shield\nsidewinder          7       8\n\n**Setting values**\n\nSet value for all items matching the list of labels\n\n>>> df.loc[['viper', 'sidewinder'], ['shield']] = 50\n>>> df\n            max_speed  shield\ncobra               1       2\nviper               4      50\nsidewinder          7      50\n\nSet value for an entire row\n\n>>> df.loc['cobra'] = 10\n>>> df\n            max_speed  shield\ncobra              10      10\nviper               4      50\nsidewinder          7      50\n\nSet value for an entire column\n\n>>> df.loc[:, 'max_speed'] = 30\n>>> df\n            max_speed  shield\ncobra              30      10\nviper              30      50\nsidewinder         30      50\n\nSet value for rows matching callable condition\n\n>>> df.loc[df['shield'] > 35] = 0\n>>> df\n            max_speed  shield\ncobra              30      10\nviper               0       0\nsidewinder          0       0\n\nAdd value matching location\n\n>>> df.loc[\"viper\", \"shield\"] += 5\n>>> df\n            max_speed  shield\ncobra              30      10\nviper               0       5\nsidewinder          0       0\n\nSetting using a ``Series`` or a ``DataFrame`` sets the values matching the\nindex labels, not the index positions.\n\n>>> shuffled_df = df.loc[[\"viper\", \"cobra\", \"sidewinder\"]]\n>>> df.loc[:] += shuffled_df\n>>> df\n            max_speed  shield\ncobra              60      20\nviper               0      10\nsidewinder          0       0\n\n**Getting values on a DataFrame with an index that has integer labels**\n\nAnother example using integers for the index\n\n>>> df = pd.DataFrame([[1, 2], [4, 5], [7, 8]],\n...                   index=[7, 8, 9], columns=['max_speed', 'shield'])\n>>> df\n   max_speed  shield\n7          1       2\n8          4       5\n9          7       8\n\nSlice with integer labels for rows. As mentioned above, note that both\nthe start and stop of the slice are included.\n\n>>> df.loc[7:9]\n   max_speed  shield\n7          1       2\n8          4       5\n9          7       8\n\n**Getting values with a MultiIndex**\n\nA number of examples using a DataFrame with a MultiIndex\n\n>>> tuples = [\n...     ('cobra', 'mark i'), ('cobra', 'mark ii'),\n...     ('sidewinder', 'mark i'), ('sidewinder', 'mark ii'),\n...     ('viper', 'mark ii'), ('viper', 'mark iii')\n... ]\n>>> index = pd.MultiIndex.from_tuples(tuples)\n>>> values = [[12, 2], [0, 4], [10, 20],\n...           [1, 4], [7, 1], [16, 36]]\n>>> df = pd.DataFrame(values, columns=['max_speed', 'shield'], index=index)\n>>> df\n                     max_speed  shield\ncobra      mark i           12       2\n           mark ii           0       4\nsidewinder mark i           10      20\n           mark ii           1       4\nviper      mark ii           7       1\n           mark iii         16      36\n\nSingle label. Note this returns a DataFrame with a single index.\n\n>>> df.loc['cobra']\n         max_speed  shield\nmark i          12       2\nmark ii          0       4\n\nSingle index tuple. Note this returns a Series.\n\n>>> df.loc[('cobra', 'mark ii')]\nmax_speed    0\nshield       4\nName: (cobra, mark ii), dtype: int64\n\nSingle label for row and column. Similar to passing in a tuple, this\nreturns a Series.\n\n>>> df.loc['cobra', 'mark i']\nmax_speed    12\nshield        2\nName: (cobra, mark i), dtype: int64\n\nSingle tuple. Note using ``[[]]`` returns a DataFrame.\n\n>>> df.loc[[('cobra', 'mark ii')]]\n               max_speed  shield\ncobra mark ii          0       4\n\nSingle tuple for the index with a single label for the column\n\n>>> df.loc[('cobra', 'mark i'), 'shield']\n2\n\nSlice from index tuple to single label\n\n>>> df.loc[('cobra', 'mark i'):'viper']\n                     max_speed  shield\ncobra      mark i           12       2\n           mark ii           0       4\nsidewinder mark i           10      20\n           mark ii           1       4\nviper      mark ii           7       1\n           mark iii         16      36\n\nSlice from index tuple to index tuple\n\n>>> df.loc[('cobra', 'mark i'):('viper', 'mark ii')]\n                    max_speed  shield\ncobra      mark i          12       2\n           mark ii          0       4\nsidewinder mark i          10      20\n           mark ii          1       4\nviper      mark ii          7       1\n\nPlease see the :ref:`user guide<advanced.advanced_hierarchical>`\nfor more details and explanations of advanced indexing.", "value": "<property object at 0x7fa4c5d0da80>", "parameters": {"type": "constant"}}}
{"task_id": "BigCodeBench/308", "data": {"name": "pandas.DataFrame.apply", "type": "callable", "signature": "(self, func: 'AggFuncType', axis: 'Axis' = 0, **kwargs)", "description": "Apply a function along an axis of the DataFrame.\n\nObjects passed to the function are Series objects whose index is\neither the DataFrame's index (``axis=0``) or the DataFrame's columns\n(``axis=1``). By default (``result_type=None``), the final return type\nis inferred from the return type of the applied function. Otherwise,\nit depends on the `result_type` argument.\n\nParameters\n----------\nfunc : function\n    Function to apply to each column or row.\naxis : {0 or 'index', 1 or 'columns'}, default 0\n    Axis along which the function is applied:\n\n    * 0 or 'index': apply function to each column.\n    * 1 or 'columns': apply function to each row.\n\nraw : bool, default False\n    Determines if row or column is passed as a Series or ndarray object:\n\n    * ``False`` : passes each row or column as a Series to the\n      function.\n    * ``True`` : the passed function will receive ndarray objects\n      instead.\n      If you are just applying a NumPy reduction function this will\n      achieve much better performance.\n\nresult_type : {'expand', 'reduce', 'broadcast', None}, default None\n    These only act when ``axis=1`` (columns):\n\n    * 'expand' : list-like results will be turned into columns.\n    * 'reduce' : returns a Series if possible rather than expanding\n      list-like results. This is the opposite of 'expand'.\n    * 'broadcast' : results will be broadcast to the original shape\n      of the DataFrame, the original index and columns will be\n      retained.\n\n    The default behaviour (None) depends on the return value of the\n    applied function: list-like results will be returned as a Series\n    of those. However if the apply function returns a Series these\n    are expanded to columns.\nargs : tuple\n    Positional arguments to pass to `func` in addition to the\n    array/series.\nby_row : False or \"compat\", default \"compat\"\n    Only has an effect when ``func`` is a listlike or dictlike of funcs\n    and the func isn't a string.\n    If \"compat\", will if possible first translate the func into pandas\n    methods (e.g. ``Series().apply(np.sum)`` will be translated to\n    ``Series().sum()``). If that doesn't work, will try call to apply again with\n    ``by_row=True`` and if that fails, will call apply again with\n    ``by_row=False`` (backward compatible).\n    If False, the funcs will be passed the whole Series at once.\n\n    .. versionadded:: 2.1.0\n\nengine : {'python', 'numba'}, default 'python'\n    Choose between the python (default) engine or the numba engine in apply.\n\n    The numba engine will attempt to JIT compile the passed function,\n    which may result in speedups for large DataFrames.\n    It also supports the following engine_kwargs :\n\n    - nopython (compile the function in nopython mode)\n    - nogil (release the GIL inside the JIT compiled function)\n    - parallel (try to apply the function in parallel over the DataFrame)\n\n      Note: Due to limitations within numba/how pandas interfaces with numba,\n      you should only use this if raw=True\n\n    Note: The numba compiler only supports a subset of\n    valid Python/numpy operations.\n\n    Please read more about the `supported python features\n    <https://numba.pydata.org/numba-doc/dev/reference/pysupported.html>`_\n    and `supported numpy features\n    <https://numba.pydata.org/numba-doc/dev/reference/numpysupported.html>`_\n    in numba to learn what you can or cannot use in the passed function.\n\n    .. versionadded:: 2.2.0\n\nengine_kwargs : dict\n    Pass keyword arguments to the engine.\n    This is currently only used by the numba engine,\n    see the documentation for the engine argument for more information.\n**kwargs\n    Additional keyword arguments to pass as keywords arguments to\n    `func`.\n\nReturns\n-------\nSeries or DataFrame\n    Result of applying ``func`` along the given axis of the\n    DataFrame.\n\nSee Also\n--------\nDataFrame.map: For elementwise operations.\nDataFrame.aggregate: Only perform aggregating type operations.\nDataFrame.transform: Only perform transforming type operations.\n\nNotes\n-----\nFunctions that mutate the passed object can produce unexpected\nbehavior or errors and are not supported. See :ref:`gotchas.udf-mutation`\nfor more details.\n\nExamples\n--------\n>>> df = pd.DataFrame([[4, 9]] * 3, columns=['A', 'B'])\n>>> df\n   A  B\n0  4  9\n1  4  9\n2  4  9\n\nUsing a numpy universal function (in this case the same as\n``np.sqrt(df)``):\n\n>>> df.apply(np.sqrt)\n     A    B\n0  2.0  3.0\n1  2.0  3.0\n2  2.0  3.0\n\nUsing a reducing function on either axis\n\n>>> df.apply(np.sum, axis=0)\nA    12\nB    27\ndtype: int64\n\n>>> df.apply(np.sum, axis=1)\n0    13\n1    13\n2    13\ndtype: int64\n\nReturning a list-like will result in a Series\n\n>>> df.apply(lambda x: [1, 2], axis=1)\n0    [1, 2]\n1    [1, 2]\n2    [1, 2]\ndtype: object\n\nPassing ``result_type='expand'`` will expand list-like results\nto columns of a Dataframe\n\n>>> df.apply(lambda x: [1, 2], axis=1, result_type='expand')\n   0  1\n0  1  2\n1  1  2\n2  1  2\n\nReturning a Series inside the function is similar to passing\n``result_type='expand'``. The resulting column names\nwill be the Series index.\n\n>>> df.apply(lambda x: pd.Series([1, 2], index=['foo', 'bar']), axis=1)\n   foo  bar\n0    1    2\n1    1    2\n2    1    2\n\nPassing ``result_type='broadcast'`` will ensure the same shape\nresult, whether list-like or scalar is returned by the function,\nand broadcast it along the axis. The resulting column names will\nbe the originals.\n\n>>> df.apply(lambda x: [1, 2], axis=1, result_type='broadcast')\n   A  B\n0  1  2\n1  1  2\n2  1  2", "parameters": {"type": "object", "properties": {"func": {"type": "aggfunctype"}, "axis": {"type": "axis", "default": 0}}}}}
{"task_id": "BigCodeBench/308", "data": {"name": "random.randint", "type": "callable", "signature": "(a, b)", "description": "Return random integer in range [a, b], including both end points.\n        ", "parameters": {"type": "object", "properties": {"a": {}, "b": {}}}}}
{"task_id": "BigCodeBench/308", "data": {"name": "statistics.mean", "type": "callable", "signature": "(data)", "description": "Return the sample arithmetic mean of data.\n\n>>> mean([1, 2, 3, 4, 4])\n2.8\n\n>>> from fractions import Fraction as F\n>>> mean([F(3, 7), F(1, 21), F(5, 3), F(1, 3)])\nFraction(13, 21)\n\n>>> from decimal import Decimal as D\n>>> mean([D(\"0.5\"), D(\"0.75\"), D(\"0.625\"), D(\"0.375\")])\nDecimal('0.5625')\n\nIf ``data`` is empty, StatisticsError will be raised.", "parameters": {"type": "object", "properties": {"data": {}}}}}
{"task_id": "BigCodeBench/310", "data": {"name": "csv.writer", "type": "callable", "signature": "(*args, **kwargs)", "description": "csv_writer = csv.writer(fileobj [, dialect='excel']\n                            [optional keyword args])\n    for row in sequence:\n        csv_writer.writerow(row)\n\n    [or]\n\n    csv_writer = csv.writer(fileobj [, dialect='excel']\n                            [optional keyword args])\n    csv_writer.writerows(rows)\n\nThe \"fileobj\" argument can be any object that supports the file API.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/310", "data": {"name": "csv.writer.writerows", "type": "callable", "signature": "(*args, **kwargs)", "description": "csv_writer = csv.writer(fileobj [, dialect='excel']\n                            [optional keyword args])\n    for row in sequence:\n        csv_writer.writerow(row)\n\n    [or]\n\n    csv_writer = csv.writer(fileobj [, dialect='excel']\n                            [optional keyword args])\n    csv_writer.writerows(rows)\n\nThe \"fileobj\" argument can be any object that supports the file API.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/310", "data": {"name": "csv.writer.writerow", "type": "callable", "signature": "(*args, **kwargs)", "description": "csv_writer = csv.writer(fileobj [, dialect='excel']\n                            [optional keyword args])\n    for row in sequence:\n        csv_writer.writerow(row)\n\n    [or]\n\n    csv_writer = csv.writer(fileobj [, dialect='excel']\n                            [optional keyword args])\n    csv_writer.writerows(rows)\n\nThe \"fileobj\" argument can be any object that supports the file API.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/310", "data": {"name": "os.getcwd", "type": "callable", "signature": "()", "description": "Return a unicode string representing the current working directory.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/310", "data": {"name": "os.path.join", "type": "callable", "signature": "(a, *p)", "description": "Join two or more pathname components, inserting '/' as needed.\nIf any component is an absolute path, all previous path components\nwill be discarded.  An empty last part will result in a path that\nends with a separator.", "parameters": {"type": "object", "properties": {"a": {}}}}}
{"task_id": "BigCodeBench/310", "data": {"name": "random.randint", "type": "callable", "signature": "(a, b)", "description": "Return random integer in range [a, b], including both end points.\n        ", "parameters": {"type": "object", "properties": {"a": {}, "b": {}}}}}
{"task_id": "BigCodeBench/310", "data": {"name": "statistics.mean", "type": "callable", "signature": "(data)", "description": "Return the sample arithmetic mean of data.\n\n>>> mean([1, 2, 3, 4, 4])\n2.8\n\n>>> from fractions import Fraction as F\n>>> mean([F(3, 7), F(1, 21), F(5, 3), F(1, 3)])\nFraction(13, 21)\n\n>>> from decimal import Decimal as D\n>>> mean([D(\"0.5\"), D(\"0.75\"), D(\"0.625\"), D(\"0.375\")])\nDecimal('0.5625')\n\nIf ``data`` is empty, StatisticsError will be raised.", "parameters": {"type": "object", "properties": {"data": {}}}}}
{"task_id": "BigCodeBench/313", "data": {"name": "datetime.datetime", "type": "class", "signature": "(self, /, *args, **kwargs)", "description": "datetime(year, month, day[, hour[, minute[, second[, microsecond[,tzinfo]]]]])\n\nThe year, month and day arguments are required. tzinfo may be None, or an\ninstance of a tzinfo subclass. The remaining arguments may be ints.", "parameters": {"type": "object", "properties": {"/": {}}}}}
{"task_id": "BigCodeBench/313", "data": {"name": "datetime.datetime.now", "type": "callable", "signature": "()", "description": "Returns new datetime object representing current time local to tz.\n\n  tz\n    Timezone object.\n\nIf no tz is specified, uses local timezone.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/313", "data": {"name": "filename.split('.')"}}
{"task_id": "BigCodeBench/313", "data": {"name": "open.read()"}}
{"task_id": "BigCodeBench/313", "data": {"name": "os.listdir", "type": "callable", "signature": "(path=None)", "description": "Return a list containing the names of the files in the directory.\n\npath can be specified as either str, bytes, or a path-like object.  If path is bytes,\n  the filenames returned will also be bytes; in all other circumstances\n  the filenames returned will be str.\nIf path is None, uses the path='.'.\nOn some platforms, path may also be specified as an open file descriptor;\\\n  the file descriptor must refer to a directory.\n  If this functionality is unavailable, using it raises NotImplementedError.\n\nThe list is in arbitrary order.  It does not include the special\nentries '.' and '..' even if they are present in the directory.", "parameters": {"type": "object", "properties": {"path": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/313", "data": {"name": "os.path.join", "type": "callable", "signature": "(a, *p)", "description": "Join two or more pathname components, inserting '/' as needed.\nIf any component is an absolute path, all previous path components\nwill be discarded.  An empty last part will result in a path that\nends with a separator.", "parameters": {"type": "object", "properties": {"a": {}}}}}
{"task_id": "BigCodeBench/313", "data": {"name": "os.makedirs", "type": "callable", "signature": "(name)", "description": "makedirs(name [, mode=0o777][, exist_ok=False])\n\nSuper-mkdir; create a leaf directory and all intermediate ones.  Works like\nmkdir, except that any intermediate path segment (not just the rightmost)\nwill be created if it does not exist. If the target directory already\nexists, raise an OSError if exist_ok is False. Otherwise no exception is\nraised.  This is recursive.", "parameters": {"type": "object", "properties": {"name": {}}}}}
{"task_id": "BigCodeBench/313", "data": {"name": "os.path.exists", "type": "callable", "signature": "(path)", "description": "Test whether a path exists.  Returns False for broken symbolic links", "parameters": {"type": "object", "properties": {"path": {}}}}}
{"task_id": "BigCodeBench/313", "data": {"name": "re.search", "type": "callable", "signature": "(pattern, string)", "description": "Scan through string looking for a match to the pattern, returning\na Match object, or None if no match was found.", "parameters": {"type": "object", "properties": {"pattern": {}, "string": {}}}}}
{"task_id": "BigCodeBench/313", "data": {"name": "re.search.group", "type": "callable", "signature": "(pattern)", "description": "Scan through string looking for a match to the pattern, returning\na Match object, or None if no match was found.", "parameters": {"type": "object", "properties": {"pattern": {}}}}}
{"task_id": "BigCodeBench/313", "data": {"name": "shutil.move", "type": "callable", "signature": "(src, dst)", "description": "Recursively move a file or directory to another location. This is\nsimilar to the Unix \"mv\" command. Return the file or directory's\ndestination.\n\nIf the destination is a directory or a symlink to a directory, the source\nis moved inside the directory. The destination path must not already\nexist.\n\nIf the destination already exists but is not a directory, it may be\noverwritten depending on os.rename() semantics.\n\nIf the destination is on our current filesystem, then rename() is used.\nOtherwise, src is copied to the destination and then removed. Symlinks are\nrecreated under the new name if os.rename() fails because of cross\nfilesystem renames.\n\nThe optional `copy_function` argument is a callable that will be used\nto copy the source or it will be delegated to `copytree`.\nBy default, copy2() is used, but any function that supports the same\nsignature (like copy()) can be used.\n\nA lot more could be done here...  A look at a mv.c shows a lot of\nthe issues this implementation glosses over.", "parameters": {"type": "object", "properties": {"src": {}, "dst": {}}}}}
{"task_id": "BigCodeBench/324", "data": {"name": "subprocess.Popen", "type": "class", "signature": "(args, bufsize=-1, executable=None, stdin=None, stdout=None, stderr=None, preexec_fn=None, close_fds=True, shell=False, cwd=None, env=None, universal_newlines=None, startupinfo=None, creationflags=0, restore_signals=True, start_new_session=False, pass_fds=(), *, user=None, group=None, extra_groups=None, encoding=None, errors=None, text=None, umask=-1, pipesize=-1)", "description": "Execute a child program in a new process.\n\nFor a complete description of the arguments see the Python documentation.\n\nArguments:\n  args: A string, or a sequence of program arguments.\n\n  bufsize: supplied as the buffering argument to the open() function when\n      creating the stdin/stdout/stderr pipe file objects\n\n  executable: A replacement program to execute.\n\n  stdin, stdout and stderr: These specify the executed programs' standard\n      input, standard output and standard error file handles, respectively.\n\n  preexec_fn: (POSIX only) An object to be called in the child process\n      just before the child is executed.\n\n  close_fds: Controls closing or inheriting of file descriptors.\n\n  shell: If true, the command will be executed through the shell.\n\n  cwd: Sets the current directory before the child is executed.\n\n  env: Defines the environment variables for the new process.\n\n  text: If true, decode stdin, stdout and stderr using the given encoding\n      (if set) or the system default otherwise.\n\n  universal_newlines: Alias of text, provided for backwards compatibility.\n\n  startupinfo and creationflags (Windows only)\n\n  restore_signals (POSIX only)\n\n  start_new_session (POSIX only)\n\n  group (POSIX only)\n\n  extra_groups (POSIX only)\n\n  user (POSIX only)\n\n  umask (POSIX only)\n\n  pass_fds (POSIX only)\n\n  encoding and errors: Text mode encoding and error handling to use for\n      file objects stdin, stdout and stderr.\n\nAttributes:\n    stdin, stdout, stderr, pid, returncode", "parameters": {"type": "object", "properties": {"args": {}, "bufsize": {"type": "integer", "default": -1}, "executable": {"type": "NoneType", "default": null}, "stdin": {"type": "NoneType", "default": null}, "stdout": {"type": "NoneType", "default": null}, "stderr": {"type": "NoneType", "default": null}, "preexec_fn": {"type": "NoneType", "default": null}, "close_fds": {"type": "bool", "default": true}, "shell": {"type": "bool", "default": false}, "cwd": {"type": "NoneType", "default": null}, "env": {"type": "NoneType", "default": null}, "universal_newlines": {"type": "NoneType", "default": null}, "startupinfo": {"type": "NoneType", "default": null}, "creationflags": {"type": "integer", "default": 0}, "restore_signals": {"type": "bool", "default": true}, "start_new_session": {"type": "bool", "default": false}, "pass_fds": {"type": "str", "default": "()"}, "user": {"type": "NoneType", "default": null}, "group": {"type": "NoneType", "default": null}, "extra_groups": {"type": "NoneType", "default": null}, "encoding": {"type": "NoneType", "default": null}, "errors": {"type": "NoneType", "default": null}, "text": {"type": "NoneType", "default": null}, "umask": {"type": "integer", "default": -1}, "pipesize": {"type": "integer", "default": -1}}}}}
{"task_id": "BigCodeBench/324", "data": {"name": "subprocess.Popen.poll", "type": "callable", "signature": "(self)", "description": "Check if child process has terminated. Set and return returncode\nattribute.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/324", "data": {"name": "exit_codes.append(process.poll())"}}
{"task_id": "BigCodeBench/324", "data": {"name": "thread.join()"}}
{"task_id": "BigCodeBench/324", "data": {"name": "thread.start()"}}
{"task_id": "BigCodeBench/324", "data": {"name": "time.sleep", "type": "callable", "signature": "(*args, **kwargs)", "description": "sleep(seconds)\n\nDelay execution for a given number of seconds.  The argument may be\na floating point number for subsecond precision.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/326", "data": {"name": "subprocess.Popen", "type": "class", "signature": "(args, bufsize=-1, executable=None, stdin=None, stdout=None, stderr=None, preexec_fn=None, close_fds=True, shell=False, cwd=None, env=None, universal_newlines=None, startupinfo=None, creationflags=0, restore_signals=True, start_new_session=False, pass_fds=(), *, user=None, group=None, extra_groups=None, encoding=None, errors=None, text=None, umask=-1, pipesize=-1)", "description": "Execute a child program in a new process.\n\nFor a complete description of the arguments see the Python documentation.\n\nArguments:\n  args: A string, or a sequence of program arguments.\n\n  bufsize: supplied as the buffering argument to the open() function when\n      creating the stdin/stdout/stderr pipe file objects\n\n  executable: A replacement program to execute.\n\n  stdin, stdout and stderr: These specify the executed programs' standard\n      input, standard output and standard error file handles, respectively.\n\n  preexec_fn: (POSIX only) An object to be called in the child process\n      just before the child is executed.\n\n  close_fds: Controls closing or inheriting of file descriptors.\n\n  shell: If true, the command will be executed through the shell.\n\n  cwd: Sets the current directory before the child is executed.\n\n  env: Defines the environment variables for the new process.\n\n  text: If true, decode stdin, stdout and stderr using the given encoding\n      (if set) or the system default otherwise.\n\n  universal_newlines: Alias of text, provided for backwards compatibility.\n\n  startupinfo and creationflags (Windows only)\n\n  restore_signals (POSIX only)\n\n  start_new_session (POSIX only)\n\n  group (POSIX only)\n\n  extra_groups (POSIX only)\n\n  user (POSIX only)\n\n  umask (POSIX only)\n\n  pass_fds (POSIX only)\n\n  encoding and errors: Text mode encoding and error handling to use for\n      file objects stdin, stdout and stderr.\n\nAttributes:\n    stdin, stdout, stderr, pid, returncode", "parameters": {"type": "object", "properties": {"args": {}, "bufsize": {"type": "integer", "default": -1}, "executable": {"type": "NoneType", "default": null}, "stdin": {"type": "NoneType", "default": null}, "stdout": {"type": "NoneType", "default": null}, "stderr": {"type": "NoneType", "default": null}, "preexec_fn": {"type": "NoneType", "default": null}, "close_fds": {"type": "bool", "default": true}, "shell": {"type": "bool", "default": false}, "cwd": {"type": "NoneType", "default": null}, "env": {"type": "NoneType", "default": null}, "universal_newlines": {"type": "NoneType", "default": null}, "startupinfo": {"type": "NoneType", "default": null}, "creationflags": {"type": "integer", "default": 0}, "restore_signals": {"type": "bool", "default": true}, "start_new_session": {"type": "bool", "default": false}, "pass_fds": {"type": "str", "default": "()"}, "user": {"type": "NoneType", "default": null}, "group": {"type": "NoneType", "default": null}, "extra_groups": {"type": "NoneType", "default": null}, "encoding": {"type": "NoneType", "default": null}, "errors": {"type": "NoneType", "default": null}, "text": {"type": "NoneType", "default": null}, "umask": {"type": "integer", "default": -1}, "pipesize": {"type": "integer", "default": -1}}}}}
{"task_id": "BigCodeBench/326", "data": {"name": "subprocess.Popen.wait", "type": "callable", "signature": "(self)", "description": "Wait for child process to terminate; returns self.returncode.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/326", "data": {"name": "glob.glob", "type": "callable", "signature": "(pathname)", "description": "Return a list of paths matching a pathname pattern.\n\nThe pattern may contain simple shell-style wildcards a la\nfnmatch. However, unlike fnmatch, filenames starting with a\ndot are special cases that are not matched by '*' and '?'\npatterns.\n\nIf recursive is true, the pattern '**' will match any files and\nzero or more directories and subdirectories.", "parameters": {"type": "object", "properties": {"pathname": {}}}}}
{"task_id": "BigCodeBench/326", "data": {"name": "os.path.basename", "type": "callable", "signature": "(p)", "description": "Returns the final component of a pathname", "parameters": {"type": "object", "properties": {"p": {}}}}}
{"task_id": "BigCodeBench/326", "data": {"name": "os.path.join", "type": "callable", "signature": "(a, *p)", "description": "Join two or more pathname components, inserting '/' as needed.\nIf any component is an absolute path, all previous path components\nwill be discarded.  An empty last part will result in a path that\nends with a separator.", "parameters": {"type": "object", "properties": {"a": {}}}}}
{"task_id": "BigCodeBench/326", "data": {"name": "results.append((os.path.basename(file_path), None))"}}
{"task_id": "BigCodeBench/326", "data": {"name": "results.append((os.path.basename(file_path), exit_code))"}}
{"task_id": "BigCodeBench/326", "data": {"name": "sys.stderr", "type": "constant", "signature": null, "description": "Character and line based layer over a BufferedIOBase object, buffer.\n\nencoding gives the name of the encoding that the stream will be\ndecoded or encoded with. It defaults to locale.getpreferredencoding(False).\n\nerrors determines the strictness of encoding and decoding (see\nhelp(codecs.Codec) or the documentation for codecs.register) and\ndefaults to \"strict\".\n\nnewline controls how line endings are handled. It can be None, '',\n'\\n', '\\r', and '\\r\\n'.  It works as follows:\n\n* On input, if newline is None, universal newlines mode is\n  enabled. Lines in the input can end in '\\n', '\\r', or '\\r\\n', and\n  these are translated into '\\n' before being returned to the\n  caller. If it is '', universal newline mode is enabled, but line\n  endings are returned to the caller untranslated. If it has any of\n  the other legal values, input lines are only terminated by the given\n  string, and the line ending is returned to the caller untranslated.\n\n* On output, if newline is None, any '\\n' characters written are\n  translated to the system default line separator, os.linesep. If\n  newline is '' or '\\n', no translation takes place. If newline is any\n  of the other legal values, any '\\n' characters written are translated\n  to the given string.\n\nIf line_buffering is True, a call to flush is implied when a call to\nwrite contains a newline character.", "value": "<_io.TextIOWrapper name='<stderr>' mode='w' encoding='utf-8'>", "parameters": {"type": "constant"}}}
{"task_id": "BigCodeBench/341", "data": {"name": "df.empty"}}
{"task_id": "BigCodeBench/341", "data": {"name": "df.columns"}}
{"task_id": "BigCodeBench/341", "data": {"name": "matplotlib.pyplot.subplots", "type": "callable", "signature": "(nrows: 'int' = 1, ncols: 'int' = 1, **fig_kw) -> 'tuple[Figure)", "description": "Create a figure and a set of subplots.\n\nThis utility wrapper makes it convenient to create common layouts of\nsubplots, including the enclosing figure object, in a single call.\n\nParameters\n----------\nnrows, ncols : int, default: 1\n    Number of rows/columns of the subplot grid.\n\nsharex, sharey : bool or {'none', 'all', 'row', 'col'}, default: False\n    Controls sharing of properties among x (*sharex*) or y (*sharey*)\n    axes:\n\n    - True or 'all': x- or y-axis will be shared among all subplots.\n    - False or 'none': each subplot x- or y-axis will be independent.\n    - 'row': each subplot row will share an x- or y-axis.\n    - 'col': each subplot column will share an x- or y-axis.\n\n    When subplots have a shared x-axis along a column, only the x tick\n    labels of the bottom subplot are created. Similarly, when subplots\n    have a shared y-axis along a row, only the y tick labels of the first\n    column subplot are created. To later turn other subplots' ticklabels\n    on, use `~matplotlib.axes.Axes.tick_params`.\n\n    When subplots have a shared axis that has units, calling\n    `~matplotlib.axis.Axis.set_units` will update each axis with the\n    new units.\n\nsqueeze : bool, default: True\n    - If True, extra dimensions are squeezed out from the returned\n      array of `~matplotlib.axes.Axes`:\n\n      - if only one subplot is constructed (nrows=ncols=1), the\n        resulting single Axes object is returned as a scalar.\n      - for Nx1 or 1xM subplots, the returned object is a 1D numpy\n        object array of Axes objects.\n      - for NxM, subplots with N>1 and M>1 are returned as a 2D array.\n\n    - If False, no squeezing at all is done: the returned Axes object is\n      always a 2D array containing Axes instances, even if it ends up\n      being 1x1.\n\nwidth_ratios : array-like of length *ncols*, optional\n    Defines the relative widths of the columns. Each column gets a\n    relative width of ``width_ratios[i] / sum(width_ratios)``.\n    If not given, all columns will have the same width.  Equivalent\n    to ``gridspec_kw={'width_ratios': [...]}``.\n\nheight_ratios : array-like of length *nrows*, optional\n    Defines the relative heights of the rows. Each row gets a\n    relative height of ``height_ratios[i] / sum(height_ratios)``.\n    If not given, all rows will have the same height. Convenience\n    for ``gridspec_kw={'height_ratios': [...]}``.\n\nsubplot_kw : dict, optional\n    Dict with keywords passed to the\n    `~matplotlib.figure.Figure.add_subplot` call used to create each\n    subplot.\n\ngridspec_kw : dict, optional\n    Dict with keywords passed to the `~matplotlib.gridspec.GridSpec`\n    constructor used to create the grid the subplots are placed on.\n\n**fig_kw\n    All additional keyword arguments are passed to the\n    `.pyplot.figure` call.\n\nReturns\n-------\nfig : `.Figure`\n\nax : `~matplotlib.axes.Axes` or array of Axes\n    *ax* can be either a single `~.axes.Axes` object, or an array of Axes\n    objects if more than one subplot was created.  The dimensions of the\n    resulting array can be controlled with the squeeze keyword, see above.\n\n    Typical idioms for handling the return value are::\n\n        # using the variable ax for single a Axes\n        fig, ax = plt.subplots()\n\n        # using the variable axs for multiple Axes\n        fig, axs = plt.subplots(2, 2)\n\n        # using tuple unpacking for multiple Axes\n        fig, (ax1, ax2) = plt.subplots(1, 2)\n        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n\n    The names ``ax`` and pluralized ``axs`` are preferred over ``axes``\n    because for the latter it's not clear if it refers to a single\n    `~.axes.Axes` instance or a collection of these.\n\nSee Also\n--------\n.pyplot.figure\n.pyplot.subplot\n.pyplot.axes\n.Figure.subplots\n.Figure.add_subplot\n\nExamples\n--------\n::\n\n    # First create some toy data:\n    x = np.linspace(0, 2*np.pi, 400)\n    y = np.sin(x**2)\n\n    # Create just a figure and only one subplot\n    fig, ax = plt.subplots()\n    ax.plot(x, y)\n    ax.set_title('Simple plot')\n\n    # Create two subplots and unpack the output array immediately\n    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n    ax1.plot(x, y)\n    ax1.set_title('Sharing Y axis')\n    ax2.scatter(x, y)\n\n    # Create four polar axes and access them through the returned array\n    fig, axs = plt.subplots(2, 2, subplot_kw=dict(projection=\"polar\"))\n    axs[0, 0].plot(x, y)\n    axs[1, 1].scatter(x, y)\n\n    # Share a X axis with each column of subplots\n    plt.subplots(2, 2, sharex='col')\n\n    # Share a Y axis with each row of subplots\n    plt.subplots(2, 2, sharey='row')\n\n    # Share both X and Y axes with all subplots\n    plt.subplots(2, 2, sharex='all', sharey='all')\n\n    # Note that this is the same as\n    plt.subplots(2, 2, sharex=True, sharey=True)\n\n    # Create figure number 10 with a single subplot\n    # and clears it if it already exists.\n    fig, ax = plt.subplots(num=10, clear=True)", "parameters": {"type": "object", "properties": {"nrows": {"type": "integer", "default": 1}, "ncols": {"type": "integer", "default": 1}}}}}
{"task_id": "BigCodeBench/341", "data": {"name": "pandas.api.types.is_numeric_dtype", "type": "callable", "signature": "(arr_or_dtype) -> 'bool)", "description": "Check whether the provided array or dtype is of a numeric dtype.\n\nParameters\n----------\narr_or_dtype : array-like or dtype\n    The array or dtype to check.\n\nReturns\n-------\nboolean\n    Whether or not the array or dtype is of a numeric dtype.\n\nExamples\n--------\n>>> from pandas.api.types import is_numeric_dtype\n>>> is_numeric_dtype(str)\nFalse\n>>> is_numeric_dtype(int)\nTrue\n>>> is_numeric_dtype(float)\nTrue\n>>> is_numeric_dtype(np.uint64)\nTrue\n>>> is_numeric_dtype(np.datetime64)\nFalse\n>>> is_numeric_dtype(np.timedelta64)\nFalse\n>>> is_numeric_dtype(np.array(['a', 'b']))\nFalse\n>>> is_numeric_dtype(pd.Series([1, 2]))\nTrue\n>>> is_numeric_dtype(pd.Index([1, 2.]))\nTrue\n>>> is_numeric_dtype(np.array([], dtype=np.timedelta64))\nFalse", "parameters": {"type": "object", "properties": {"arr_or_dtype": {}}}}}
{"task_id": "BigCodeBench/341", "data": {"name": "seaborn.boxplot", "type": "callable", "signature": "(x=None, ax=None, **kwargs)", "description": "Draw a box plot to show distributions with respect to categories.\n\nA box plot (or box-and-whisker plot) shows the distribution of quantitative\ndata in a way that facilitates comparisons between variables or across\nlevels of a categorical variable. The box shows the quartiles of the\ndataset while the whiskers extend to show the rest of the distribution,\nexcept for points that are determined to be \"outliers\" using a method\nthat is a function of the inter-quartile range.\n\nSee the :ref:`tutorial <categorical_tutorial>` for more information.\n\n.. note::\n    By default, this function treats one of the variables as categorical\n    and draws data at ordinal positions (0, 1, ... n) on the relevant axis.\n    As of version 0.13.0, this can be disabled by setting `native_scale=True`.\n\n\nParameters\n----------\ndata : DataFrame, Series, dict, array, or list of arrays\n    Dataset for plotting. If `x` and `y` are absent, this is\n    interpreted as wide-form. Otherwise it is expected to be long-form.    \nx, y, hue : names of variables in `data` or vector data\n    Inputs for plotting long-form data. See examples for interpretation.    \norder, hue_order : lists of strings\n    Order to plot the categorical levels in; otherwise the levels are\n    inferred from the data objects.    \norient : \"v\" | \"h\" | \"x\" | \"y\"\n    Orientation of the plot (vertical or horizontal). This is usually\n    inferred based on the type of the input variables, but it can be used\n    to resolve ambiguity when both `x` and `y` are numeric or when\n    plotting wide-form data.\n\n    .. versionchanged:: v0.13.0\n        Added 'x'/'y' as options, equivalent to 'v'/'h'.    \ncolor : matplotlib color\n    Single color for the elements in the plot.    \npalette : palette name, list, or dict\n    Colors to use for the different levels of the ``hue`` variable. Should\n    be something that can be interpreted by :func:`color_palette`, or a\n    dictionary mapping hue levels to matplotlib colors.    \nsaturation : float\n    Proportion of the original saturation to draw fill colors in. Large\n    patches often look better with desaturated colors, but set this to\n    `1` if you want the colors to perfectly match the input values.    \nfill : bool\n    If True, use a solid patch. Otherwise, draw as line art.\n\n    .. versionadded:: v0.13.0    \ndodge : \"auto\" or bool\n    When hue mapping is used, whether elements should be narrowed and shifted along\n    the orient axis to eliminate overlap. If `\"auto\"`, set to `True` when the\n    orient variable is crossed with the categorical variable or `False` otherwise.\n\n    .. versionchanged:: 0.13.0\n\n        Added `\"auto\"` mode as a new default.    \nwidth : float\n    Width allotted to each element on the orient axis. When `native_scale=True`,\n    it is relative to the minimum distance between two values in the native scale.    \ngap : float\n    Shrink on the orient axis by this factor to add a gap between dodged elements.\n\n    .. versionadded:: 0.13.0    \nwhis : float or pair of floats\n    Paramater that controls whisker length. If scalar, whiskers are drawn\n    to the farthest datapoint within *whis * IQR* from the nearest hinge.\n    If a tuple, it is interpreted as percentiles that whiskers represent.\nlinecolor : color\n    Color to use for line elements, when `fill` is True.\n\n    .. versionadded:: v0.13.0    \nlinewidth : float\n    Width of the lines that frame the plot elements.    \nfliersize : float\n    Size of the markers used to indicate outlier observations.\nhue_norm : tuple or :class:`matplotlib.colors.Normalize` object\n    Normalization in data units for colormap applied to the `hue`\n    variable when it is numeric. Not relevant if `hue` is categorical.\n\n    .. versionadded:: v0.12.0    \nlog_scale : bool or number, or pair of bools or numbers\n    Set axis scale(s) to log. A single value sets the data axis for any numeric\n    axes in the plot. A pair of values sets each axis independently.\n    Numeric values are interpreted as the desired base (default 10).\n    When `None` or `False`, seaborn defers to the existing Axes scale.\n\n    .. versionadded:: v0.13.0    \nnative_scale : bool\n    When True, numeric or datetime values on the categorical axis will maintain\n    their original scaling rather than being converted to fixed indices.\n\n    .. versionadded:: v0.13.0    \nformatter : callable\n    Function for converting categorical data into strings. Affects both grouping\n    and tick labels.\n\n    .. versionadded:: v0.13.0    \nlegend : \"auto\", \"brief\", \"full\", or False\n    How to draw the legend. If \"brief\", numeric `hue` and `size`\n    variables will be represented with a sample of evenly spaced values.\n    If \"full\", every group will get an entry in the legend. If \"auto\",\n    choose between brief or full representation based on number of levels.\n    If `False`, no legend data is added and no legend is drawn.\n\n    .. versionadded:: v0.13.0    \nax : matplotlib Axes\n    Axes object to draw the plot onto, otherwise uses the current Axes.    \nkwargs : key, value mappings\n    Other keyword arguments are passed through to\n    :meth:`matplotlib.axes.Axes.boxplot`.\n\nReturns\n-------\nax : matplotlib Axes\n    Returns the Axes object with the plot drawn onto it.    \n\nSee Also\n--------\nviolinplot : A combination of boxplot and kernel density estimation.    \nstripplot : A scatterplot where one variable is categorical. Can be used\n            in conjunction with other plots to show each observation.    \nswarmplot : A categorical scatterplot where the points do not overlap. Can\n            be used with other plots to show each observation.    \ncatplot : Combine a categorical plot with a :class:`FacetGrid`.    \n\nExamples\n--------\n.. include:: ../docstrings/boxplot.rst", "parameters": {"type": "object", "properties": {"x": {"type": "NoneType", "default": null}, "ax": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/341", "data": {"name": "seaborn.countplot", "type": "callable", "signature": "(x=None, ax=None, **kwargs)", "description": "Show the counts of observations in each categorical bin using bars.\n\nA count plot can be thought of as a histogram across a categorical, instead\nof quantitative, variable. The basic API and options are identical to those\nfor :func:`barplot`, so you can compare counts across nested variables.\n\nNote that :func:`histplot` function offers similar functionality with additional\nfeatures (e.g. bar stacking), although its default behavior is somewhat different.\n\nSee the :ref:`tutorial <categorical_tutorial>` for more information.\n\n.. note::\n    By default, this function treats one of the variables as categorical\n    and draws data at ordinal positions (0, 1, ... n) on the relevant axis.\n    As of version 0.13.0, this can be disabled by setting `native_scale=True`.\n\n\nParameters\n----------\ndata : DataFrame, Series, dict, array, or list of arrays\n    Dataset for plotting. If `x` and `y` are absent, this is\n    interpreted as wide-form. Otherwise it is expected to be long-form.    \nx, y, hue : names of variables in `data` or vector data\n    Inputs for plotting long-form data. See examples for interpretation.    \norder, hue_order : lists of strings\n    Order to plot the categorical levels in; otherwise the levels are\n    inferred from the data objects.    \norient : \"v\" | \"h\" | \"x\" | \"y\"\n    Orientation of the plot (vertical or horizontal). This is usually\n    inferred based on the type of the input variables, but it can be used\n    to resolve ambiguity when both `x` and `y` are numeric or when\n    plotting wide-form data.\n\n    .. versionchanged:: v0.13.0\n        Added 'x'/'y' as options, equivalent to 'v'/'h'.    \ncolor : matplotlib color\n    Single color for the elements in the plot.    \npalette : palette name, list, or dict\n    Colors to use for the different levels of the ``hue`` variable. Should\n    be something that can be interpreted by :func:`color_palette`, or a\n    dictionary mapping hue levels to matplotlib colors.    \nsaturation : float\n    Proportion of the original saturation to draw fill colors in. Large\n    patches often look better with desaturated colors, but set this to\n    `1` if you want the colors to perfectly match the input values.    \nhue_norm : tuple or :class:`matplotlib.colors.Normalize` object\n    Normalization in data units for colormap applied to the `hue`\n    variable when it is numeric. Not relevant if `hue` is categorical.\n\n    .. versionadded:: v0.12.0    \nstat : {'count', 'percent', 'proportion', 'probability'}\n    Statistic to compute; when not `'count'`, bar heights will be normalized so that\n    they sum to 100 (for `'percent'`) or 1 (otherwise) across the plot.\n\n    .. versionadded:: v0.13.0\nwidth : float\n    Width allotted to each element on the orient axis. When `native_scale=True`,\n    it is relative to the minimum distance between two values in the native scale.    \ndodge : \"auto\" or bool\n    When hue mapping is used, whether elements should be narrowed and shifted along\n    the orient axis to eliminate overlap. If `\"auto\"`, set to `True` when the\n    orient variable is crossed with the categorical variable or `False` otherwise.\n\n    .. versionchanged:: 0.13.0\n\n        Added `\"auto\"` mode as a new default.    \nlog_scale : bool or number, or pair of bools or numbers\n    Set axis scale(s) to log. A single value sets the data axis for any numeric\n    axes in the plot. A pair of values sets each axis independently.\n    Numeric values are interpreted as the desired base (default 10).\n    When `None` or `False`, seaborn defers to the existing Axes scale.\n\n    .. versionadded:: v0.13.0    \nnative_scale : bool\n    When True, numeric or datetime values on the categorical axis will maintain\n    their original scaling rather than being converted to fixed indices.\n\n    .. versionadded:: v0.13.0    \nformatter : callable\n    Function for converting categorical data into strings. Affects both grouping\n    and tick labels.\n\n    .. versionadded:: v0.13.0    \nlegend : \"auto\", \"brief\", \"full\", or False\n    How to draw the legend. If \"brief\", numeric `hue` and `size`\n    variables will be represented with a sample of evenly spaced values.\n    If \"full\", every group will get an entry in the legend. If \"auto\",\n    choose between brief or full representation based on number of levels.\n    If `False`, no legend data is added and no legend is drawn.\n\n    .. versionadded:: v0.13.0    \nax : matplotlib Axes\n    Axes object to draw the plot onto, otherwise uses the current Axes.    \nkwargs : key, value mappings\n    Other parameters are passed through to :class:`matplotlib.patches.Rectangle`.\n\nReturns\n-------\nax : matplotlib Axes\n    Returns the Axes object with the plot drawn onto it.    \n\nSee Also\n--------\nhistplot : Bin and count observations with additional options.\nbarplot : Show point estimates and confidence intervals using bars.    \ncatplot : Combine a categorical plot with a :class:`FacetGrid`.    \n\nExamples\n--------\n.. include:: ../docstrings/countplot.rst", "parameters": {"type": "object", "properties": {"x": {"type": "NoneType", "default": null}, "ax": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/341", "data": {"name": "seaborn.stripplot", "type": "callable", "signature": "(x=None, jitter=True, ax=None, **kwargs)", "description": "Draw a categorical scatterplot using jitter to reduce overplotting.\n\nA strip plot can be drawn on its own, but it is also a good complement\nto a box or violin plot in cases where you want to show all observations\nalong with some representation of the underlying distribution.\n\nSee the :ref:`tutorial <categorical_tutorial>` for more information.\n\n.. note::\n    By default, this function treats one of the variables as categorical\n    and draws data at ordinal positions (0, 1, ... n) on the relevant axis.\n    As of version 0.13.0, this can be disabled by setting `native_scale=True`.\n\n\nParameters\n----------\ndata : DataFrame, Series, dict, array, or list of arrays\n    Dataset for plotting. If `x` and `y` are absent, this is\n    interpreted as wide-form. Otherwise it is expected to be long-form.    \nx, y, hue : names of variables in `data` or vector data\n    Inputs for plotting long-form data. See examples for interpretation.    \norder, hue_order : lists of strings\n    Order to plot the categorical levels in; otherwise the levels are\n    inferred from the data objects.    \njitter : float, `True`/`1` is special-cased\n    Amount of jitter (only along the categorical axis) to apply. This\n    can be useful when you have many points and they overlap, so that\n    it is easier to see the distribution. You can specify the amount\n    of jitter (half the width of the uniform random variable support),\n    or use `True` for a good default.\ndodge : bool\n    When a `hue` variable is assigned, setting this to `True` will\n    separate the strips for different hue levels along the categorical\n    axis and narrow the amount of space allotedto each strip. Otherwise,\n    the points for each level will be plotted in the same strip.\norient : \"v\" | \"h\" | \"x\" | \"y\"\n    Orientation of the plot (vertical or horizontal). This is usually\n    inferred based on the type of the input variables, but it can be used\n    to resolve ambiguity when both `x` and `y` are numeric or when\n    plotting wide-form data.\n\n    .. versionchanged:: v0.13.0\n        Added 'x'/'y' as options, equivalent to 'v'/'h'.    \ncolor : matplotlib color\n    Single color for the elements in the plot.    \npalette : palette name, list, or dict\n    Colors to use for the different levels of the ``hue`` variable. Should\n    be something that can be interpreted by :func:`color_palette`, or a\n    dictionary mapping hue levels to matplotlib colors.    \nsize : float\n    Radius of the markers, in points.\nedgecolor : matplotlib color, \"gray\" is special-cased\n    Color of the lines around each point. If you pass `\"gray\"`, the\n    brightness is determined by the color palette used for the body\n    of the points. Note that `stripplot` has `linewidth=0` by default,\n    so edge colors are only visible with nonzero line width.\nlinewidth : float\n    Width of the lines that frame the plot elements.    \nhue_norm : tuple or :class:`matplotlib.colors.Normalize` object\n    Normalization in data units for colormap applied to the `hue`\n    variable when it is numeric. Not relevant if `hue` is categorical.\n\n    .. versionadded:: v0.12.0    \nlog_scale : bool or number, or pair of bools or numbers\n    Set axis scale(s) to log. A single value sets the data axis for any numeric\n    axes in the plot. A pair of values sets each axis independently.\n    Numeric values are interpreted as the desired base (default 10).\n    When `None` or `False`, seaborn defers to the existing Axes scale.\n\n    .. versionadded:: v0.13.0    \nnative_scale : bool\n    When True, numeric or datetime values on the categorical axis will maintain\n    their original scaling rather than being converted to fixed indices.\n\n    .. versionadded:: v0.13.0    \nformatter : callable\n    Function for converting categorical data into strings. Affects both grouping\n    and tick labels.\n\n    .. versionadded:: v0.13.0    \nlegend : \"auto\", \"brief\", \"full\", or False\n    How to draw the legend. If \"brief\", numeric `hue` and `size`\n    variables will be represented with a sample of evenly spaced values.\n    If \"full\", every group will get an entry in the legend. If \"auto\",\n    choose between brief or full representation based on number of levels.\n    If `False`, no legend data is added and no legend is drawn.\n\n    .. versionadded:: v0.13.0    \nax : matplotlib Axes\n    Axes object to draw the plot onto, otherwise uses the current Axes.    \nkwargs : key, value mappings\n    Other keyword arguments are passed through to\n    :meth:`matplotlib.axes.Axes.scatter`.\n\nReturns\n-------\nax : matplotlib Axes\n    Returns the Axes object with the plot drawn onto it.    \n\nSee Also\n--------\nswarmplot : A categorical scatterplot where the points do not overlap. Can\n            be used with other plots to show each observation.    \nboxplot : A traditional box-and-whisker plot with a similar API.    \nviolinplot : A combination of boxplot and kernel density estimation.    \ncatplot : Combine a categorical plot with a :class:`FacetGrid`.    \n\nExamples\n--------\n.. include:: ../docstrings/stripplot.rst", "parameters": {"type": "object", "properties": {"x": {"type": "NoneType", "default": null}, "jitter": {"type": "bool", "default": true}, "ax": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/346", "data": {"name": "subprocess.Popen", "type": "class", "signature": "(args, bufsize=-1, executable=None, stdin=None, stdout=None, stderr=None, preexec_fn=None, close_fds=True, shell=False, cwd=None, env=None, universal_newlines=None, startupinfo=None, creationflags=0, restore_signals=True, start_new_session=False, pass_fds=(), *, user=None, group=None, extra_groups=None, encoding=None, errors=None, text=None, umask=-1, pipesize=-1)", "description": "Execute a child program in a new process.\n\nFor a complete description of the arguments see the Python documentation.\n\nArguments:\n  args: A string, or a sequence of program arguments.\n\n  bufsize: supplied as the buffering argument to the open() function when\n      creating the stdin/stdout/stderr pipe file objects\n\n  executable: A replacement program to execute.\n\n  stdin, stdout and stderr: These specify the executed programs' standard\n      input, standard output and standard error file handles, respectively.\n\n  preexec_fn: (POSIX only) An object to be called in the child process\n      just before the child is executed.\n\n  close_fds: Controls closing or inheriting of file descriptors.\n\n  shell: If true, the command will be executed through the shell.\n\n  cwd: Sets the current directory before the child is executed.\n\n  env: Defines the environment variables for the new process.\n\n  text: If true, decode stdin, stdout and stderr using the given encoding\n      (if set) or the system default otherwise.\n\n  universal_newlines: Alias of text, provided for backwards compatibility.\n\n  startupinfo and creationflags (Windows only)\n\n  restore_signals (POSIX only)\n\n  start_new_session (POSIX only)\n\n  group (POSIX only)\n\n  extra_groups (POSIX only)\n\n  user (POSIX only)\n\n  umask (POSIX only)\n\n  pass_fds (POSIX only)\n\n  encoding and errors: Text mode encoding and error handling to use for\n      file objects stdin, stdout and stderr.\n\nAttributes:\n    stdin, stdout, stderr, pid, returncode", "parameters": {"type": "object", "properties": {"args": {}, "bufsize": {"type": "integer", "default": -1}, "executable": {"type": "NoneType", "default": null}, "stdin": {"type": "NoneType", "default": null}, "stdout": {"type": "NoneType", "default": null}, "stderr": {"type": "NoneType", "default": null}, "preexec_fn": {"type": "NoneType", "default": null}, "close_fds": {"type": "bool", "default": true}, "shell": {"type": "bool", "default": false}, "cwd": {"type": "NoneType", "default": null}, "env": {"type": "NoneType", "default": null}, "universal_newlines": {"type": "NoneType", "default": null}, "startupinfo": {"type": "NoneType", "default": null}, "creationflags": {"type": "integer", "default": 0}, "restore_signals": {"type": "bool", "default": true}, "start_new_session": {"type": "bool", "default": false}, "pass_fds": {"type": "str", "default": "()"}, "user": {"type": "NoneType", "default": null}, "group": {"type": "NoneType", "default": null}, "extra_groups": {"type": "NoneType", "default": null}, "encoding": {"type": "NoneType", "default": null}, "errors": {"type": "NoneType", "default": null}, "text": {"type": "NoneType", "default": null}, "umask": {"type": "integer", "default": -1}, "pipesize": {"type": "integer", "default": -1}}}}}
{"task_id": "BigCodeBench/346", "data": {"name": "subprocess.Popen.args", "type": "class", "signature": "(args, bufsize=-1, executable=None, stdin=None, stdout=None, stderr=None, preexec_fn=None, close_fds=True, shell=False, cwd=None, env=None, universal_newlines=None, startupinfo=None, creationflags=0, restore_signals=True, start_new_session=False, pass_fds=(), *, user=None, group=None, extra_groups=None, encoding=None, errors=None, text=None, umask=-1, pipesize=-1)", "description": "Execute a child program in a new process.\n\nFor a complete description of the arguments see the Python documentation.\n\nArguments:\n  args: A string, or a sequence of program arguments.\n\n  bufsize: supplied as the buffering argument to the open() function when\n      creating the stdin/stdout/stderr pipe file objects\n\n  executable: A replacement program to execute.\n\n  stdin, stdout and stderr: These specify the executed programs' standard\n      input, standard output and standard error file handles, respectively.\n\n  preexec_fn: (POSIX only) An object to be called in the child process\n      just before the child is executed.\n\n  close_fds: Controls closing or inheriting of file descriptors.\n\n  shell: If true, the command will be executed through the shell.\n\n  cwd: Sets the current directory before the child is executed.\n\n  env: Defines the environment variables for the new process.\n\n  text: If true, decode stdin, stdout and stderr using the given encoding\n      (if set) or the system default otherwise.\n\n  universal_newlines: Alias of text, provided for backwards compatibility.\n\n  startupinfo and creationflags (Windows only)\n\n  restore_signals (POSIX only)\n\n  start_new_session (POSIX only)\n\n  group (POSIX only)\n\n  extra_groups (POSIX only)\n\n  user (POSIX only)\n\n  umask (POSIX only)\n\n  pass_fds (POSIX only)\n\n  encoding and errors: Text mode encoding and error handling to use for\n      file objects stdin, stdout and stderr.\n\nAttributes:\n    stdin, stdout, stderr, pid, returncode", "parameters": {"type": "object", "properties": {"args": {}, "bufsize": {"type": "integer", "default": -1}, "executable": {"type": "NoneType", "default": null}, "stdin": {"type": "NoneType", "default": null}, "stdout": {"type": "NoneType", "default": null}, "stderr": {"type": "NoneType", "default": null}, "preexec_fn": {"type": "NoneType", "default": null}, "close_fds": {"type": "bool", "default": true}, "shell": {"type": "bool", "default": false}, "cwd": {"type": "NoneType", "default": null}, "env": {"type": "NoneType", "default": null}, "universal_newlines": {"type": "NoneType", "default": null}, "startupinfo": {"type": "NoneType", "default": null}, "creationflags": {"type": "integer", "default": 0}, "restore_signals": {"type": "bool", "default": true}, "start_new_session": {"type": "bool", "default": false}, "pass_fds": {"type": "str", "default": "()"}, "user": {"type": "NoneType", "default": null}, "group": {"type": "NoneType", "default": null}, "extra_groups": {"type": "NoneType", "default": null}, "encoding": {"type": "NoneType", "default": null}, "errors": {"type": "NoneType", "default": null}, "text": {"type": "NoneType", "default": null}, "umask": {"type": "integer", "default": -1}, "pipesize": {"type": "integer", "default": -1}}}}}
{"task_id": "BigCodeBench/346", "data": {"name": "subprocess.Popen.poll", "type": "callable", "signature": "(self)", "description": "Check if child process has terminated. Set and return returncode\nattribute.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/346", "data": {"name": "subprocess.Popen.returncode", "type": "class", "signature": "(args, bufsize=-1, executable=None, stdin=None, stdout=None, stderr=None, preexec_fn=None, close_fds=True, shell=False, cwd=None, env=None, universal_newlines=None, startupinfo=None, creationflags=0, restore_signals=True, start_new_session=False, pass_fds=(), *, user=None, group=None, extra_groups=None, encoding=None, errors=None, text=None, umask=-1, pipesize=-1)", "description": "Execute a child program in a new process.\n\nFor a complete description of the arguments see the Python documentation.\n\nArguments:\n  args: A string, or a sequence of program arguments.\n\n  bufsize: supplied as the buffering argument to the open() function when\n      creating the stdin/stdout/stderr pipe file objects\n\n  executable: A replacement program to execute.\n\n  stdin, stdout and stderr: These specify the executed programs' standard\n      input, standard output and standard error file handles, respectively.\n\n  preexec_fn: (POSIX only) An object to be called in the child process\n      just before the child is executed.\n\n  close_fds: Controls closing or inheriting of file descriptors.\n\n  shell: If true, the command will be executed through the shell.\n\n  cwd: Sets the current directory before the child is executed.\n\n  env: Defines the environment variables for the new process.\n\n  text: If true, decode stdin, stdout and stderr using the given encoding\n      (if set) or the system default otherwise.\n\n  universal_newlines: Alias of text, provided for backwards compatibility.\n\n  startupinfo and creationflags (Windows only)\n\n  restore_signals (POSIX only)\n\n  start_new_session (POSIX only)\n\n  group (POSIX only)\n\n  extra_groups (POSIX only)\n\n  user (POSIX only)\n\n  umask (POSIX only)\n\n  pass_fds (POSIX only)\n\n  encoding and errors: Text mode encoding and error handling to use for\n      file objects stdin, stdout and stderr.\n\nAttributes:\n    stdin, stdout, stderr, pid, returncode", "parameters": {"type": "object", "properties": {"args": {}, "bufsize": {"type": "integer", "default": -1}, "executable": {"type": "NoneType", "default": null}, "stdin": {"type": "NoneType", "default": null}, "stdout": {"type": "NoneType", "default": null}, "stderr": {"type": "NoneType", "default": null}, "preexec_fn": {"type": "NoneType", "default": null}, "close_fds": {"type": "bool", "default": true}, "shell": {"type": "bool", "default": false}, "cwd": {"type": "NoneType", "default": null}, "env": {"type": "NoneType", "default": null}, "universal_newlines": {"type": "NoneType", "default": null}, "startupinfo": {"type": "NoneType", "default": null}, "creationflags": {"type": "integer", "default": 0}, "restore_signals": {"type": "bool", "default": true}, "start_new_session": {"type": "bool", "default": false}, "pass_fds": {"type": "str", "default": "()"}, "user": {"type": "NoneType", "default": null}, "group": {"type": "NoneType", "default": null}, "extra_groups": {"type": "NoneType", "default": null}, "encoding": {"type": "NoneType", "default": null}, "errors": {"type": "NoneType", "default": null}, "text": {"type": "NoneType", "default": null}, "umask": {"type": "integer", "default": -1}, "pipesize": {"type": "integer", "default": -1}}}}}
{"task_id": "BigCodeBench/346", "data": {"name": "subprocess.Popen.communicate", "type": "callable", "signature": "(self)", "description": "Interact with process: Send data to stdin and close it.\nRead data from stdout and stderr, until end-of-file is\nreached.  Wait for process to terminate.\n\nThe optional \"input\" argument should be data to be sent to the\nchild process, or None, if no data should be sent to the child.\ncommunicate() returns a tuple (stdout, stderr).\n\nBy default, all communication is in bytes, and therefore any\n\"input\" should be bytes, and the (stdout, stderr) will be bytes.\nIf in text mode (indicated by self.text_mode), any \"input\" should\nbe a string, and (stdout, stderr) will be strings decoded\naccording to locale encoding, or by \"encoding\" if set. Text mode\nis triggered by setting any of text, encoding, errors or\nuniversal_newlines.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/346", "data": {"name": "os.path.isfile", "type": "callable", "signature": "(path)", "description": "Test whether a path is a regular file", "parameters": {"type": "object", "properties": {"path": {}}}}}
{"task_id": "BigCodeBench/346", "data": {"name": "subprocess.PIPE", "type": "constant", "signature": null, "description": "int([x]) -> integer\nint(x, base=10) -> integer\n\nConvert a number or string to an integer, or return 0 if no arguments\nare given.  If x is a number, return x.__int__().  For floating point\nnumbers, this truncates towards zero.\n\nIf x is not a number or if base is given, then x must be a string,\nbytes, or bytearray instance representing an integer literal in the\ngiven base.  The literal can be preceded by '+' or '-' and be surrounded\nby whitespace.  The base defaults to 10.  Valid bases are 0 and 2-36.\nBase 0 means to interpret the base from the string as an integer literal.\n>>> int('0b100', base=0)\n4", "value": "-1", "parameters": {"type": "constant"}}}
{"task_id": "BigCodeBench/346", "data": {"name": "sys.executable", "type": "constant", "signature": null, "description": "str(object='') -> str\nstr(bytes_or_buffer[, encoding[, errors]]) -> str\n\nCreate a new string object from the given object. If encoding or\nerrors is specified, then the object must expose a data buffer\nthat will be decoded using the given encoding and error handler.\nOtherwise, returns the result of object.__str__() (if defined)\nor repr(object).\nencoding defaults to sys.getdefaultencoding().\nerrors defaults to 'strict'.", "value": "/home/aiops/zhuoyt/miniconda3/bin/python", "parameters": {"type": "constant"}}}
{"task_id": "BigCodeBench/346", "data": {"name": "time.sleep", "type": "callable", "signature": "(*args, **kwargs)", "description": "sleep(seconds)\n\nDelay execution for a given number of seconds.  The argument may be\na floating point number for subsecond precision.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/360", "data": {"name": "matplotlib.pyplot.subplots", "type": "class", "signature": "()", "description": "Create a figure and a set of subplots.\n\nThis utility wrapper makes it convenient to create common layouts of\nsubplots, including the enclosing figure object, in a single call.\n\nParameters\n----------\nnrows, ncols : int, default: 1\n    Number of rows/columns of the subplot grid.\n\nsharex, sharey : bool or {'none', 'all', 'row', 'col'}, default: False\n    Controls sharing of properties among x (*sharex*) or y (*sharey*)\n    axes:\n\n    - True or 'all': x- or y-axis will be shared among all subplots.\n    - False or 'none': each subplot x- or y-axis will be independent.\n    - 'row': each subplot row will share an x- or y-axis.\n    - 'col': each subplot column will share an x- or y-axis.\n\n    When subplots have a shared x-axis along a column, only the x tick\n    labels of the bottom subplot are created. Similarly, when subplots\n    have a shared y-axis along a row, only the y tick labels of the first\n    column subplot are created. To later turn other subplots' ticklabels\n    on, use `~matplotlib.axes.Axes.tick_params`.\n\n    When subplots have a shared axis that has units, calling\n    `~matplotlib.axis.Axis.set_units` will update each axis with the\n    new units.\n\nsqueeze : bool, default: True\n    - If True, extra dimensions are squeezed out from the returned\n      array of `~matplotlib.axes.Axes`:\n\n      - if only one subplot is constructed (nrows=ncols=1), the\n        resulting single Axes object is returned as a scalar.\n      - for Nx1 or 1xM subplots, the returned object is a 1D numpy\n        object array of Axes objects.\n      - for NxM, subplots with N>1 and M>1 are returned as a 2D array.\n\n    - If False, no squeezing at all is done: the returned Axes object is\n      always a 2D array containing Axes instances, even if it ends up\n      being 1x1.\n\nwidth_ratios : array-like of length *ncols*, optional\n    Defines the relative widths of the columns. Each column gets a\n    relative width of ``width_ratios[i] / sum(width_ratios)``.\n    If not given, all columns will have the same width.  Equivalent\n    to ``gridspec_kw={'width_ratios': [...]}``.\n\nheight_ratios : array-like of length *nrows*, optional\n    Defines the relative heights of the rows. Each row gets a\n    relative height of ``height_ratios[i] / sum(height_ratios)``.\n    If not given, all rows will have the same height. Convenience\n    for ``gridspec_kw={'height_ratios': [...]}``.\n\nsubplot_kw : dict, optional\n    Dict with keywords passed to the\n    `~matplotlib.figure.Figure.add_subplot` call used to create each\n    subplot.\n\ngridspec_kw : dict, optional\n    Dict with keywords passed to the `~matplotlib.gridspec.GridSpec`\n    constructor used to create the grid the subplots are placed on.\n\n**fig_kw\n    All additional keyword arguments are passed to the\n    `.pyplot.figure` call.\n\nReturns\n-------\nfig : `.Figure`\n\nax : `~matplotlib.axes.Axes` or array of Axes\n    *ax* can be either a single `~.axes.Axes` object, or an array of Axes\n    objects if more than one subplot was created.  The dimensions of the\n    resulting array can be controlled with the squeeze keyword, see above.\n\n    Typical idioms for handling the return value are::\n\n        # using the variable ax for single a Axes\n        fig, ax = plt.subplots()\n\n        # using the variable axs for multiple Axes\n        fig, axs = plt.subplots(2, 2)\n\n        # using tuple unpacking for multiple Axes\n        fig, (ax1, ax2) = plt.subplots(1, 2)\n        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n\n    The names ``ax`` and pluralized ``axs`` are preferred over ``axes``\n    because for the latter it's not clear if it refers to a single\n    `~.axes.Axes` instance or a collection of these.\n\nSee Also\n--------\n.pyplot.figure\n.pyplot.subplot\n.pyplot.axes\n.Figure.subplots\n.Figure.add_subplot\n\nExamples\n--------\n::\n\n    # First create some toy data:\n    x = np.linspace(0, 2*np.pi, 400)\n    y = np.sin(x**2)\n\n    # Create just a figure and only one subplot\n    fig, ax = plt.subplots()\n    ax.plot(x, y)\n    ax.set_title('Simple plot')\n\n    # Create two subplots and unpack the output array immediately\n    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n    ax1.plot(x, y)\n    ax1.set_title('Sharing Y axis')\n    ax2.scatter(x, y)\n\n    # Create four polar axes and access them through the returned array\n    fig, axs = plt.subplots(2, 2, subplot_kw=dict(projection=\"polar\"))\n    axs[0, 0].plot(x, y)\n    axs[1, 1].scatter(x, y)\n\n    # Share a X axis with each column of subplots\n    plt.subplots(2, 2, sharex='col')\n\n    # Share a Y axis with each row of subplots\n    plt.subplots(2, 2, sharey='row')\n\n    # Share both X and Y axes with all subplots\n    plt.subplots(2, 2, sharex='all', sharey='all')\n\n    # Note that this is the same as\n    plt.subplots(2, 2, sharex=True, sharey=True)\n\n    # Create figure number 10 with a single subplot\n    # and clears it if it already exists.\n    fig, ax = plt.subplots(num=10, clear=True)", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/360", "data": {"name": "matplotlib.pyplot.subplots[1].set_ylabel", "type": "method", "signature": "(ylabel, fontdict=None, labelpad=None, *, loc=None, **kwargs)", "description": "Set the label for the y-axis.\n\nParameters\n----------\nylabel : str\n    The label text.\n\nlabelpad : float, default: :rc:`axes.labelpad`\n    Spacing in points from the Axes bounding box including ticks\n    and tick labels.  If None, the previous value is left as is.\n\nloc : {'bottom', 'center', 'top'}, default: :rc:`yaxis.labellocation`\n    The label position. This is a high-level alternative for passing\n    parameters *y* and *horizontalalignment*.\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    `.Text` properties control the appearance of the label.\n\nSee Also\n--------\ntext : Documents the properties supported by `.Text`.", "parameters": {"type": "object", "properties": {"ylabel": {}, "fontdict": {"type": "NoneType", "default": null}, "labelpad": {"type": "NoneType", "default": null}, "loc": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/360", "data": {"name": "matplotlib.pyplot.subplots[1].set_xlabel", "type": "method", "signature": "(xlabel, fontdict=None, labelpad=None, *, loc=None, **kwargs)", "description": "Set the label for the x-axis.\n\nParameters\n----------\nxlabel : str\n    The label text.\n\nlabelpad : float, default: :rc:`axes.labelpad`\n    Spacing in points from the Axes bounding box including ticks\n    and tick labels.  If None, the previous value is left as is.\n\nloc : {'left', 'center', 'right'}, default: :rc:`xaxis.labellocation`\n    The label position. This is a high-level alternative for passing\n    parameters *x* and *horizontalalignment*.\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    `.Text` properties control the appearance of the label.\n\nSee Also\n--------\ntext : Documents the properties supported by `.Text`.", "parameters": {"type": "object", "properties": {"xlabel": {}, "fontdict": {"type": "NoneType", "default": null}, "labelpad": {"type": "NoneType", "default": null}, "loc": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/360", "data": {"name": "matplotlib.pyplot.subplots[1].bar", "type": "method", "signature": "(x, height, width=0.8, bottom=None, *, align='center', data=None, **kwargs)", "description": "Make a bar plot.\n\nThe bars are positioned at *x* with the given *align*\\ment. Their\ndimensions are given by *height* and *width*. The vertical baseline\nis *bottom* (default 0).\n\nMany parameters can take either a single value applying to all bars\nor a sequence of values, one for each bar.\n\nParameters\n----------\nx : float or array-like\n    The x coordinates of the bars. See also *align* for the\n    alignment of the bars to the coordinates.\n\nheight : float or array-like\n    The height(s) of the bars.\n\n    Note that if *bottom* has units (e.g. datetime), *height* should be in\n    units that are a difference from the value of *bottom* (e.g. timedelta).\n\nwidth : float or array-like, default: 0.8\n    The width(s) of the bars.\n\n    Note that if *x* has units (e.g. datetime), then *width* should be in\n    units that are a difference (e.g. timedelta) around the *x* values.\n\nbottom : float or array-like, default: 0\n    The y coordinate(s) of the bottom side(s) of the bars.\n\n    Note that if *bottom* has units, then the y-axis will get a Locator and\n    Formatter appropriate for the units (e.g. dates, or categorical).\n\nalign : {'center', 'edge'}, default: 'center'\n    Alignment of the bars to the *x* coordinates:\n\n    - 'center': Center the base on the *x* positions.\n    - 'edge': Align the left edges of the bars with the *x* positions.\n\n    To align the bars on the right edge pass a negative *width* and\n    ``align='edge'``.\n\nReturns\n-------\n`.BarContainer`\n    Container with all the bars and optionally errorbars.\n\nOther Parameters\n----------------\ncolor : color or list of color, optional\n    The colors of the bar faces.\n\nedgecolor : color or list of color, optional\n    The colors of the bar edges.\n\nlinewidth : float or array-like, optional\n    Width of the bar edge(s). If 0, don't draw edges.\n\ntick_label : str or list of str, optional\n    The tick labels of the bars.\n    Default: None (Use default numeric labels.)\n\nlabel : str or list of str, optional\n    A single label is attached to the resulting `.BarContainer` as a\n    label for the whole dataset.\n    If a list is provided, it must be the same length as *x* and\n    labels the individual bars. Repeated labels are not de-duplicated\n    and will cause repeated label entries, so this is best used when\n    bars also differ in style (e.g., by passing a list to *color*.)\n\nxerr, yerr : float or array-like of shape(N,) or shape(2, N), optional\n    If not *None*, add horizontal / vertical errorbars to the bar tips.\n    The values are +/- sizes relative to the data:\n\n    - scalar: symmetric +/- values for all bars\n    - shape(N,): symmetric +/- values for each bar\n    - shape(2, N): Separate - and + values for each bar. First row\n      contains the lower errors, the second row contains the upper\n      errors.\n    - *None*: No errorbar. (Default)\n\n    See :doc:`/gallery/statistics/errorbar_features` for an example on\n    the usage of *xerr* and *yerr*.\n\necolor : color or list of color, default: 'black'\n    The line color of the errorbars.\n\ncapsize : float, default: :rc:`errorbar.capsize`\n   The length of the error bar caps in points.\n\nerror_kw : dict, optional\n    Dictionary of keyword arguments to be passed to the\n    `~.Axes.errorbar` method. Values of *ecolor* or *capsize* defined\n    here take precedence over the independent keyword arguments.\n\nlog : bool, default: False\n    If *True*, set the y-axis to be log scale.\n\ndata : indexable object, optional\n    If given, all parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` (unless this raises an exception).\n\n**kwargs : `.Rectangle` properties\n\nProperties:\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: scalar or None\n    angle: unknown\n    animated: bool\n    antialiased or aa: bool or None\n    bounds: (left, bottom, width, height)\n    capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    clip_box: `~matplotlib.transforms.BboxBase` or None\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    color: color\n    edgecolor or ec: color or None\n    facecolor or fc: color or None\n    figure: `~matplotlib.figure.Figure`\n    fill: bool\n    gid: str\n    hatch: {'/', '\\\\', '|', '-', '+', 'x', 'o', 'O', '.', '*'}\n    height: unknown\n    in_layout: bool\n    joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    label: object\n    linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n    linewidth or lw: float or None\n    mouseover: bool\n    path_effects: list of `.AbstractPathEffect`\n    picker: None or bool or float or callable\n    rasterized: bool\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    transform: `~matplotlib.transforms.Transform`\n    url: str\n    visible: bool\n    width: unknown\n    x: unknown\n    xy: (float, float)\n    y: unknown\n    zorder: float\n\nSee Also\n--------\nbarh : Plot a horizontal bar plot.\n\nNotes\n-----\nStacked bars can be achieved by passing individual *bottom* values per\nbar. See :doc:`/gallery/lines_bars_and_markers/bar_stacked`.", "parameters": {"type": "object", "properties": {"x": {}, "height": {}, "width": {"type": "float", "default": 0.8}, "bottom": {"type": "NoneType", "default": null}, "align": {"type": "str", "default": "center"}, "data": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/360", "data": {"name": "matplotlib.pyplot.subplots[1].set_title", "type": "method", "signature": "(label, fontdict=None, loc=None, pad=None, *, y=None, **kwargs)", "description": "Set a title for the Axes.\n\nSet one of the three available Axes titles. The available titles\nare positioned above the Axes in the center, flush with the left\nedge, and flush with the right edge.\n\nParameters\n----------\nlabel : str\n    Text to use for the title\n\nfontdict : dict\n\n    .. admonition:: Discouraged\n\n       The use of *fontdict* is discouraged. Parameters should be passed as\n       individual keyword arguments or using dictionary-unpacking\n       ``set_title(..., **fontdict)``.\n\n    A dictionary controlling the appearance of the title text,\n    the default *fontdict* is::\n\n       {'fontsize': rcParams['axes.titlesize'],\n        'fontweight': rcParams['axes.titleweight'],\n        'color': rcParams['axes.titlecolor'],\n        'verticalalignment': 'baseline',\n        'horizontalalignment': loc}\n\nloc : {'center', 'left', 'right'}, default: :rc:`axes.titlelocation`\n    Which title to set.\n\ny : float, default: :rc:`axes.titley`\n    Vertical Axes location for the title (1.0 is the top).  If\n    None (the default) and :rc:`axes.titley` is also None, y is\n    determined automatically to avoid decorators on the Axes.\n\npad : float, default: :rc:`axes.titlepad`\n    The offset of the title from the top of the Axes, in points.\n\nReturns\n-------\n`.Text`\n    The matplotlib text instance representing the title\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    Other keyword arguments are text properties, see `.Text` for a list\n    of valid text properties.", "parameters": {"type": "object", "properties": {"label": {}, "fontdict": {"type": "NoneType", "default": null}, "loc": {"type": "NoneType", "default": null}, "pad": {"type": "NoneType", "default": null}, "y": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/360", "data": {"name": "numpy.std", "type": "callable", "signature": "(a)", "description": "Compute the standard deviation along the specified axis.\n\nReturns the standard deviation, a measure of the spread of a distribution,\nof the array elements. The standard deviation is computed for the\nflattened array by default, otherwise over the specified axis.\n\nParameters\n----------\na : array_like\n    Calculate the standard deviation of these values.\naxis : None or int or tuple of ints, optional\n    Axis or axes along which the standard deviation is computed. The\n    default is to compute the standard deviation of the flattened array.\n\n    .. versionadded:: 1.7.0\n\n    If this is a tuple of ints, a standard deviation is performed over\n    multiple axes, instead of a single axis or all the axes as before.\ndtype : dtype, optional\n    Type to use in computing the standard deviation. For arrays of\n    integer type the default is float64, for arrays of float types it is\n    the same as the array type.\nout : ndarray, optional\n    Alternative output array in which to place the result. It must have\n    the same shape as the expected output but the type (of the calculated\n    values) will be cast if necessary.\nddof : int, optional\n    Means Delta Degrees of Freedom.  The divisor used in calculations\n    is ``N - ddof``, where ``N`` represents the number of elements.\n    By default `ddof` is zero.\nkeepdims : bool, optional\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\n    If the default value is passed, then `keepdims` will not be\n    passed through to the `std` method of sub-classes of\n    `ndarray`, however any non-default value will be.  If the\n    sub-class' method does not implement `keepdims` any\n    exceptions will be raised.\n\nwhere : array_like of bool, optional\n    Elements to include in the standard deviation.\n    See `~numpy.ufunc.reduce` for details.\n\n    .. versionadded:: 1.20.0\n\nReturns\n-------\nstandard_deviation : ndarray, see dtype parameter above.\n    If `out` is None, return a new array containing the standard deviation,\n    otherwise return a reference to the output array.\n\nSee Also\n--------\nvar, mean, nanmean, nanstd, nanvar\n:ref:`ufuncs-output-type`\n\nNotes\n-----\nThe standard deviation is the square root of the average of the squared\ndeviations from the mean, i.e., ``std = sqrt(mean(x))``, where\n``x = abs(a - a.mean())**2``.\n\nThe average squared deviation is typically calculated as ``x.sum() / N``,\nwhere ``N = len(x)``. If, however, `ddof` is specified, the divisor\n``N - ddof`` is used instead. In standard statistical practice, ``ddof=1``\nprovides an unbiased estimator of the variance of the infinite population.\n``ddof=0`` provides a maximum likelihood estimate of the variance for\nnormally distributed variables. The standard deviation computed in this\nfunction is the square root of the estimated variance, so even with\n``ddof=1``, it will not be an unbiased estimate of the standard deviation\nper se.\n\nNote that, for complex numbers, `std` takes the absolute\nvalue before squaring, so that the result is always real and nonnegative.\n\nFor floating-point input, the *std* is computed using the same\nprecision the input has. Depending on the input data, this can cause\nthe results to be inaccurate, especially for float32 (see example below).\nSpecifying a higher-accuracy accumulator using the `dtype` keyword can\nalleviate this issue.\n\nExamples\n--------\n>>> a = np.array([[1, 2], [3, 4]])\n>>> np.std(a)\n1.1180339887498949 # may vary\n>>> np.std(a, axis=0)\narray([1.,  1.])\n>>> np.std(a, axis=1)\narray([0.5,  0.5])\n\nIn single precision, std() can be inaccurate:\n\n>>> a = np.zeros((2, 512*512), dtype=np.float32)\n>>> a[0, :] = 1.0\n>>> a[1, :] = 0.1\n>>> np.std(a)\n0.45000005\n\nComputing the standard deviation in float64 is more accurate:\n\n>>> np.std(a, dtype=np.float64)\n0.44999999925494177 # may vary\n\nSpecifying a where argument:\n\n>>> a = np.array([[14, 8, 11, 10], [7, 9, 10, 11], [10, 15, 5, 10]])\n>>> np.std(a)\n2.614064523559687 # may vary\n>>> np.std(a, where=[[True], [True], [False]])\n2.0", "parameters": {"type": "object", "properties": {"a": {}}}}}
{"task_id": "BigCodeBench/360", "data": {"name": "numpy.mean", "type": "callable", "signature": "(a)", "description": "Compute the arithmetic mean along the specified axis.\n\nReturns the average of the array elements.  The average is taken over\nthe flattened array by default, otherwise over the specified axis.\n`float64` intermediate and return values are used for integer inputs.\n\nParameters\n----------\na : array_like\n    Array containing numbers whose mean is desired. If `a` is not an\n    array, a conversion is attempted.\naxis : None or int or tuple of ints, optional\n    Axis or axes along which the means are computed. The default is to\n    compute the mean of the flattened array.\n\n    .. versionadded:: 1.7.0\n\n    If this is a tuple of ints, a mean is performed over multiple axes,\n    instead of a single axis or all the axes as before.\ndtype : data-type, optional\n    Type to use in computing the mean.  For integer inputs, the default\n    is `float64`; for floating point inputs, it is the same as the\n    input dtype.\nout : ndarray, optional\n    Alternate output array in which to place the result.  The default\n    is ``None``; if provided, it must have the same shape as the\n    expected output, but the type will be cast if necessary.\n    See :ref:`ufuncs-output-type` for more details.\n\nkeepdims : bool, optional\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\n    If the default value is passed, then `keepdims` will not be\n    passed through to the `mean` method of sub-classes of\n    `ndarray`, however any non-default value will be.  If the\n    sub-class' method does not implement `keepdims` any\n    exceptions will be raised.\n\nwhere : array_like of bool, optional\n    Elements to include in the mean. See `~numpy.ufunc.reduce` for details.\n\n    .. versionadded:: 1.20.0\n\nReturns\n-------\nm : ndarray, see dtype parameter above\n    If `out=None`, returns a new array containing the mean values,\n    otherwise a reference to the output array is returned.\n\nSee Also\n--------\naverage : Weighted average\nstd, var, nanmean, nanstd, nanvar\n\nNotes\n-----\nThe arithmetic mean is the sum of the elements along the axis divided\nby the number of elements.\n\nNote that for floating-point input, the mean is computed using the\nsame precision the input has.  Depending on the input data, this can\ncause the results to be inaccurate, especially for `float32` (see\nexample below).  Specifying a higher-precision accumulator using the\n`dtype` keyword can alleviate this issue.\n\nBy default, `float16` results are computed using `float32` intermediates\nfor extra precision.\n\nExamples\n--------\n>>> a = np.array([[1, 2], [3, 4]])\n>>> np.mean(a)\n2.5\n>>> np.mean(a, axis=0)\narray([2., 3.])\n>>> np.mean(a, axis=1)\narray([1.5, 3.5])\n\nIn single precision, `mean` can be inaccurate:\n\n>>> a = np.zeros((2, 512*512), dtype=np.float32)\n>>> a[0, :] = 1.0\n>>> a[1, :] = 0.1\n>>> np.mean(a)\n0.54999924\n\nComputing the mean in float64 is more accurate:\n\n>>> np.mean(a, dtype=np.float64)\n0.55000000074505806 # may vary\n\nSpecifying a where argument:\n\n>>> a = np.array([[5, 9, 13], [14, 10, 12], [11, 15, 19]])\n>>> np.mean(a)\n12.0\n>>> np.mean(a, where=[[True], [False], [False]])\n9.0", "parameters": {"type": "object", "properties": {"a": {}}}}}
{"task_id": "BigCodeBench/360", "data": {"name": "os.path.exists", "type": "callable", "signature": "(path)", "description": "Test whether a path exists.  Returns False for broken symbolic links", "parameters": {"type": "object", "properties": {"path": {}}}}}
{"task_id": "BigCodeBench/360", "data": {"name": "pandas.read_excel", "type": "callable", "signature": "(io, sheet_name: 'str | int | list[IntStrT] | None' = 0)", "description": "Read an Excel file into a ``pandas`` ``DataFrame``.\n\nSupports `xls`, `xlsx`, `xlsm`, `xlsb`, `odf`, `ods` and `odt` file extensions\nread from a local filesystem or URL. Supports an option to read\na single sheet or a list of sheets.\n\nParameters\n----------\nio : str, bytes, ExcelFile, xlrd.Book, path object, or file-like object\n    Any valid string path is acceptable. The string could be a URL. Valid\n    URL schemes include http, ftp, s3, and file. For file URLs, a host is\n    expected. A local file could be: ``file://localhost/path/to/table.xlsx``.\n\n    If you want to pass in a path object, pandas accepts any ``os.PathLike``.\n\n    By file-like object, we refer to objects with a ``read()`` method,\n    such as a file handle (e.g. via builtin ``open`` function)\n    or ``StringIO``.\n\n    .. deprecated:: 2.1.0\n        Passing byte strings is deprecated. To read from a\n        byte string, wrap it in a ``BytesIO`` object.\nsheet_name : str, int, list, or None, default 0\n    Strings are used for sheet names. Integers are used in zero-indexed\n    sheet positions (chart sheets do not count as a sheet position).\n    Lists of strings/integers are used to request multiple sheets.\n    Specify ``None`` to get all worksheets.\n\n    Available cases:\n\n    * Defaults to ``0``: 1st sheet as a `DataFrame`\n    * ``1``: 2nd sheet as a `DataFrame`\n    * ``\"Sheet1\"``: Load sheet with name \"Sheet1\"\n    * ``[0, 1, \"Sheet5\"]``: Load first, second and sheet named \"Sheet5\"\n      as a dict of `DataFrame`\n    * ``None``: All worksheets.\n\nheader : int, list of int, default 0\n    Row (0-indexed) to use for the column labels of the parsed\n    DataFrame. If a list of integers is passed those row positions will\n    be combined into a ``MultiIndex``. Use None if there is no header.\nnames : array-like, default None\n    List of column names to use. If file contains no header row,\n    then you should explicitly pass header=None.\nindex_col : int, str, list of int, default None\n    Column (0-indexed) to use as the row labels of the DataFrame.\n    Pass None if there is no such column.  If a list is passed,\n    those columns will be combined into a ``MultiIndex``.  If a\n    subset of data is selected with ``usecols``, index_col\n    is based on the subset.\n\n    Missing values will be forward filled to allow roundtripping with\n    ``to_excel`` for ``merged_cells=True``. To avoid forward filling the\n    missing values use ``set_index`` after reading the data instead of\n    ``index_col``.\nusecols : str, list-like, or callable, default None\n    * If None, then parse all columns.\n    * If str, then indicates comma separated list of Excel column letters\n      and column ranges (e.g. \"A:E\" or \"A,C,E:F\"). Ranges are inclusive of\n      both sides.\n    * If list of int, then indicates list of column numbers to be parsed\n      (0-indexed).\n    * If list of string, then indicates list of column names to be parsed.\n    * If callable, then evaluate each column name against it and parse the\n      column if the callable returns ``True``.\n\n    Returns a subset of the columns according to behavior above.\ndtype : Type name or dict of column -> type, default None\n    Data type for data or columns. E.g. {'a': np.float64, 'b': np.int32}\n    Use ``object`` to preserve data as stored in Excel and not interpret dtype,\n    which will necessarily result in ``object`` dtype.\n    If converters are specified, they will be applied INSTEAD\n    of dtype conversion.\n    If you use ``None``, it will infer the dtype of each column based on the data.\nengine : {'openpyxl', 'calamine', 'odf', 'pyxlsb', 'xlrd'}, default None\n    If io is not a buffer or path, this must be set to identify io.\n    Engine compatibility :\n\n    - ``openpyxl`` supports newer Excel file formats.\n    - ``calamine`` supports Excel (.xls, .xlsx, .xlsm, .xlsb)\n      and OpenDocument (.ods) file formats.\n    - ``odf`` supports OpenDocument file formats (.odf, .ods, .odt).\n    - ``pyxlsb`` supports Binary Excel files.\n    - ``xlrd`` supports old-style Excel files (.xls).\n\n    When ``engine=None``, the following logic will be used to determine the engine:\n\n    - If ``path_or_buffer`` is an OpenDocument format (.odf, .ods, .odt),\n      then `odf <https://pypi.org/project/odfpy/>`_ will be used.\n    - Otherwise if ``path_or_buffer`` is an xls format, ``xlrd`` will be used.\n    - Otherwise if ``path_or_buffer`` is in xlsb format, ``pyxlsb`` will be used.\n    - Otherwise ``openpyxl`` will be used.\nconverters : dict, default None\n    Dict of functions for converting values in certain columns. Keys can\n    either be integers or column labels, values are functions that take one\n    input argument, the Excel cell content, and return the transformed\n    content.\ntrue_values : list, default None\n    Values to consider as True.\nfalse_values : list, default None\n    Values to consider as False.\nskiprows : list-like, int, or callable, optional\n    Line numbers to skip (0-indexed) or number of lines to skip (int) at the\n    start of the file. If callable, the callable function will be evaluated\n    against the row indices, returning True if the row should be skipped and\n    False otherwise. An example of a valid callable argument would be ``lambda\n    x: x in [0, 2]``.\nnrows : int, default None\n    Number of rows to parse.\nna_values : scalar, str, list-like, or dict, default None\n    Additional strings to recognize as NA/NaN. If dict passed, specific\n    per-column NA values. By default the following values are interpreted\n    as NaN: '', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan',\n    '1.#IND', '1.#QNAN', '<NA>', 'N/A', 'NA', 'NULL', 'NaN', 'None',\n    'n/a', 'nan', 'null'.\nkeep_default_na : bool, default True\n    Whether or not to include the default NaN values when parsing the data.\n    Depending on whether ``na_values`` is passed in, the behavior is as follows:\n\n    * If ``keep_default_na`` is True, and ``na_values`` are specified,\n      ``na_values`` is appended to the default NaN values used for parsing.\n    * If ``keep_default_na`` is True, and ``na_values`` are not specified, only\n      the default NaN values are used for parsing.\n    * If ``keep_default_na`` is False, and ``na_values`` are specified, only\n      the NaN values specified ``na_values`` are used for parsing.\n    * If ``keep_default_na`` is False, and ``na_values`` are not specified, no\n      strings will be parsed as NaN.\n\n    Note that if `na_filter` is passed in as False, the ``keep_default_na`` and\n    ``na_values`` parameters will be ignored.\nna_filter : bool, default True\n    Detect missing value markers (empty strings and the value of na_values). In\n    data without any NAs, passing ``na_filter=False`` can improve the\n    performance of reading a large file.\nverbose : bool, default False\n    Indicate number of NA values placed in non-numeric columns.\nparse_dates : bool, list-like, or dict, default False\n    The behavior is as follows:\n\n    * ``bool``. If True -> try parsing the index.\n    * ``list`` of int or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3\n      each as a separate date column.\n    * ``list`` of lists. e.g.  If [[1, 3]] -> combine columns 1 and 3 and parse as\n      a single date column.\n    * ``dict``, e.g. {'foo' : [1, 3]} -> parse columns 1, 3 as date and call\n      result 'foo'\n\n    If a column or index contains an unparsable date, the entire column or\n    index will be returned unaltered as an object data type. If you don`t want to\n    parse some cells as date just change their type in Excel to \"Text\".\n    For non-standard datetime parsing, use ``pd.to_datetime`` after ``pd.read_excel``.\n\n    Note: A fast-path exists for iso8601-formatted dates.\ndate_parser : function, optional\n    Function to use for converting a sequence of string columns to an array of\n    datetime instances. The default uses ``dateutil.parser.parser`` to do the\n    conversion. Pandas will try to call `date_parser` in three different ways,\n    advancing to the next if an exception occurs: 1) Pass one or more arrays\n    (as defined by `parse_dates`) as arguments; 2) concatenate (row-wise) the\n    string values from the columns defined by `parse_dates` into a single array\n    and pass that; and 3) call `date_parser` once for each row using one or\n    more strings (corresponding to the columns defined by `parse_dates`) as\n    arguments.\n\n    .. deprecated:: 2.0.0\n       Use ``date_format`` instead, or read in as ``object`` and then apply\n       :func:`to_datetime` as-needed.\ndate_format : str or dict of column -> format, default ``None``\n   If used in conjunction with ``parse_dates``, will parse dates according to this\n   format. For anything more complex,\n   please read in as ``object`` and then apply :func:`to_datetime` as-needed.\n\n   .. versionadded:: 2.0.0\nthousands : str, default None\n    Thousands separator for parsing string columns to numeric.  Note that\n    this parameter is only necessary for columns stored as TEXT in Excel,\n    any numeric columns will automatically be parsed, regardless of display\n    format.\ndecimal : str, default '.'\n    Character to recognize as decimal point for parsing string columns to numeric.\n    Note that this parameter is only necessary for columns stored as TEXT in Excel,\n    any numeric columns will automatically be parsed, regardless of display\n    format.(e.g. use ',' for European data).\n\n    .. versionadded:: 1.4.0\n\ncomment : str, default None\n    Comments out remainder of line. Pass a character or characters to this\n    argument to indicate comments in the input file. Any data between the\n    comment string and the end of the current line is ignored.\nskipfooter : int, default 0\n    Rows at the end to skip (0-indexed).\nstorage_options : dict, optional\n    Extra options that make sense for a particular storage connection, e.g.\n    host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n    are forwarded to ``urllib.request.Request`` as header options. For other\n    URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n    forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n    details, and for more examples on storage options refer `here\n    <https://pandas.pydata.org/docs/user_guide/io.html?\n    highlight=storage_options#reading-writing-remote-files>`_.\n\ndtype_backend : {'numpy_nullable', 'pyarrow'}, default 'numpy_nullable'\n    Back-end data type applied to the resultant :class:`DataFrame`\n    (still experimental). Behaviour is as follows:\n\n    * ``\"numpy_nullable\"``: returns nullable-dtype-backed :class:`DataFrame`\n      (default).\n    * ``\"pyarrow\"``: returns pyarrow-backed nullable :class:`ArrowDtype`\n      DataFrame.\n\n    .. versionadded:: 2.0\n\nengine_kwargs : dict, optional\n    Arbitrary keyword arguments passed to excel engine.\n\nReturns\n-------\nDataFrame or dict of DataFrames\n    DataFrame from the passed in Excel file. See notes in sheet_name\n    argument for more information on when a dict of DataFrames is returned.\n\nSee Also\n--------\nDataFrame.to_excel : Write DataFrame to an Excel file.\nDataFrame.to_csv : Write DataFrame to a comma-separated values (csv) file.\nread_csv : Read a comma-separated values (csv) file into DataFrame.\nread_fwf : Read a table of fixed-width formatted lines into DataFrame.\n\nNotes\n-----\nFor specific information on the methods used for each Excel engine, refer to the pandas\n:ref:`user guide <io.excel_reader>`\n\nExamples\n--------\nThe file can be read using the file name as string or an open file object:\n\n>>> pd.read_excel('tmp.xlsx', index_col=0)  # doctest: +SKIP\n       Name  Value\n0   string1      1\n1   string2      2\n2  #Comment      3\n\n>>> pd.read_excel(open('tmp.xlsx', 'rb'),\n...               sheet_name='Sheet3')  # doctest: +SKIP\n   Unnamed: 0      Name  Value\n0           0   string1      1\n1           1   string2      2\n2           2  #Comment      3\n\nIndex and header can be specified via the `index_col` and `header` arguments\n\n>>> pd.read_excel('tmp.xlsx', index_col=None, header=None)  # doctest: +SKIP\n     0         1      2\n0  NaN      Name  Value\n1  0.0   string1      1\n2  1.0   string2      2\n3  2.0  #Comment      3\n\nColumn types are inferred but can be explicitly specified\n\n>>> pd.read_excel('tmp.xlsx', index_col=0,\n...               dtype={'Name': str, 'Value': float})  # doctest: +SKIP\n       Name  Value\n0   string1    1.0\n1   string2    2.0\n2  #Comment    3.0\n\nTrue, False, and NA values, and thousands separators have defaults,\nbut can be explicitly specified, too. Supply the values you would like\nas strings or lists of strings!\n\n>>> pd.read_excel('tmp.xlsx', index_col=0,\n...               na_values=['string1', 'string2'])  # doctest: +SKIP\n       Name  Value\n0       NaN      1\n1       NaN      2\n2  #Comment      3\n\nComment lines in the excel input file can be skipped using the\n``comment`` kwarg.\n\n>>> pd.read_excel('tmp.xlsx', index_col=0, comment='#')  # doctest: +SKIP\n      Name  Value\n0  string1    1.0\n1  string2    2.0\n2     None    NaN", "parameters": {"type": "object", "properties": {"io": {}, "sheet_name": {"type": ["integer", "list[intstrt]", "str", "null"], "default": 0}}}}}
{"task_id": "BigCodeBench/360", "data": {"name": "pandas.read_excel.columns", "type": "callable", "signature": "(io, sheet_name: 'str | int | list[IntStrT] | None' = 0, *, header: 'int | Sequence[int] | None' = 0, names: 'SequenceNotStr[Hashable] | range | None' = None, index_col: 'int | str | Sequence[int] | None' = None, usecols: 'int | str | Sequence[int] | Sequence[str] | Callable[[str], bool] | None' = None, dtype: 'DtypeArg | None' = None, engine: \"Literal['xlrd', 'openpyxl', 'odf', 'pyxlsb', 'calamine'] | None\" = None, converters: 'dict[str, Callable] | dict[int, Callable] | None' = None, true_values: 'Iterable[Hashable] | None' = None, false_values: 'Iterable[Hashable] | None' = None, skiprows: 'Sequence[int] | int | Callable[[int], object] | None' = None, nrows: 'int | None' = None, na_values=None, keep_default_na: 'bool' = True, na_filter: 'bool' = True, verbose: 'bool' = False, parse_dates: 'list | dict | bool' = False, date_parser: 'Callable | lib.NoDefault' = <no_default>, date_format: 'dict[Hashable, str] | str | None' = None, thousands: 'str | None' = None, decimal: 'str' = '.', comment: 'str | None' = None, skipfooter: 'int' = 0, storage_options: 'StorageOptions | None' = None, dtype_backend: 'DtypeBackend | lib.NoDefault' = <no_default>, engine_kwargs: 'dict | None' = None) -> 'DataFrame | dict[IntStrT, DataFrame]'", "description": "Read an Excel file into a ``pandas`` ``DataFrame``.\n\nSupports `xls`, `xlsx`, `xlsm`, `xlsb`, `odf`, `ods` and `odt` file extensions\nread from a local filesystem or URL. Supports an option to read\na single sheet or a list of sheets.\n\nParameters\n----------\nio : str, bytes, ExcelFile, xlrd.Book, path object, or file-like object\n    Any valid string path is acceptable. The string could be a URL. Valid\n    URL schemes include http, ftp, s3, and file. For file URLs, a host is\n    expected. A local file could be: ``file://localhost/path/to/table.xlsx``.\n\n    If you want to pass in a path object, pandas accepts any ``os.PathLike``.\n\n    By file-like object, we refer to objects with a ``read()`` method,\n    such as a file handle (e.g. via builtin ``open`` function)\n    or ``StringIO``.\n\n    .. deprecated:: 2.1.0\n        Passing byte strings is deprecated. To read from a\n        byte string, wrap it in a ``BytesIO`` object.\nsheet_name : str, int, list, or None, default 0\n    Strings are used for sheet names. Integers are used in zero-indexed\n    sheet positions (chart sheets do not count as a sheet position).\n    Lists of strings/integers are used to request multiple sheets.\n    Specify ``None`` to get all worksheets.\n\n    Available cases:\n\n    * Defaults to ``0``: 1st sheet as a `DataFrame`\n    * ``1``: 2nd sheet as a `DataFrame`\n    * ``\"Sheet1\"``: Load sheet with name \"Sheet1\"\n    * ``[0, 1, \"Sheet5\"]``: Load first, second and sheet named \"Sheet5\"\n      as a dict of `DataFrame`\n    * ``None``: All worksheets.\n\nheader : int, list of int, default 0\n    Row (0-indexed) to use for the column labels of the parsed\n    DataFrame. If a list of integers is passed those row positions will\n    be combined into a ``MultiIndex``. Use None if there is no header.\nnames : array-like, default None\n    List of column names to use. If file contains no header row,\n    then you should explicitly pass header=None.\nindex_col : int, str, list of int, default None\n    Column (0-indexed) to use as the row labels of the DataFrame.\n    Pass None if there is no such column.  If a list is passed,\n    those columns will be combined into a ``MultiIndex``.  If a\n    subset of data is selected with ``usecols``, index_col\n    is based on the subset.\n\n    Missing values will be forward filled to allow roundtripping with\n    ``to_excel`` for ``merged_cells=True``. To avoid forward filling the\n    missing values use ``set_index`` after reading the data instead of\n    ``index_col``.\nusecols : str, list-like, or callable, default None\n    * If None, then parse all columns.\n    * If str, then indicates comma separated list of Excel column letters\n      and column ranges (e.g. \"A:E\" or \"A,C,E:F\"). Ranges are inclusive of\n      both sides.\n    * If list of int, then indicates list of column numbers to be parsed\n      (0-indexed).\n    * If list of string, then indicates list of column names to be parsed.\n    * If callable, then evaluate each column name against it and parse the\n      column if the callable returns ``True``.\n\n    Returns a subset of the columns according to behavior above.\ndtype : Type name or dict of column -> type, default None\n    Data type for data or columns. E.g. {'a': np.float64, 'b': np.int32}\n    Use ``object`` to preserve data as stored in Excel and not interpret dtype,\n    which will necessarily result in ``object`` dtype.\n    If converters are specified, they will be applied INSTEAD\n    of dtype conversion.\n    If you use ``None``, it will infer the dtype of each column based on the data.\nengine : {'openpyxl', 'calamine', 'odf', 'pyxlsb', 'xlrd'}, default None\n    If io is not a buffer or path, this must be set to identify io.\n    Engine compatibility :\n\n    - ``openpyxl`` supports newer Excel file formats.\n    - ``calamine`` supports Excel (.xls, .xlsx, .xlsm, .xlsb)\n      and OpenDocument (.ods) file formats.\n    - ``odf`` supports OpenDocument file formats (.odf, .ods, .odt).\n    - ``pyxlsb`` supports Binary Excel files.\n    - ``xlrd`` supports old-style Excel files (.xls).\n\n    When ``engine=None``, the following logic will be used to determine the engine:\n\n    - If ``path_or_buffer`` is an OpenDocument format (.odf, .ods, .odt),\n      then `odf <https://pypi.org/project/odfpy/>`_ will be used.\n    - Otherwise if ``path_or_buffer`` is an xls format, ``xlrd`` will be used.\n    - Otherwise if ``path_or_buffer`` is in xlsb format, ``pyxlsb`` will be used.\n    - Otherwise ``openpyxl`` will be used.\nconverters : dict, default None\n    Dict of functions for converting values in certain columns. Keys can\n    either be integers or column labels, values are functions that take one\n    input argument, the Excel cell content, and return the transformed\n    content.\ntrue_values : list, default None\n    Values to consider as True.\nfalse_values : list, default None\n    Values to consider as False.\nskiprows : list-like, int, or callable, optional\n    Line numbers to skip (0-indexed) or number of lines to skip (int) at the\n    start of the file. If callable, the callable function will be evaluated\n    against the row indices, returning True if the row should be skipped and\n    False otherwise. An example of a valid callable argument would be ``lambda\n    x: x in [0, 2]``.\nnrows : int, default None\n    Number of rows to parse.\nna_values : scalar, str, list-like, or dict, default None\n    Additional strings to recognize as NA/NaN. If dict passed, specific\n    per-column NA values. By default the following values are interpreted\n    as NaN: '', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan',\n    '1.#IND', '1.#QNAN', '<NA>', 'N/A', 'NA', 'NULL', 'NaN', 'None',\n    'n/a', 'nan', 'null'.\nkeep_default_na : bool, default True\n    Whether or not to include the default NaN values when parsing the data.\n    Depending on whether ``na_values`` is passed in, the behavior is as follows:\n\n    * If ``keep_default_na`` is True, and ``na_values`` are specified,\n      ``na_values`` is appended to the default NaN values used for parsing.\n    * If ``keep_default_na`` is True, and ``na_values`` are not specified, only\n      the default NaN values are used for parsing.\n    * If ``keep_default_na`` is False, and ``na_values`` are specified, only\n      the NaN values specified ``na_values`` are used for parsing.\n    * If ``keep_default_na`` is False, and ``na_values`` are not specified, no\n      strings will be parsed as NaN.\n\n    Note that if `na_filter` is passed in as False, the ``keep_default_na`` and\n    ``na_values`` parameters will be ignored.\nna_filter : bool, default True\n    Detect missing value markers (empty strings and the value of na_values). In\n    data without any NAs, passing ``na_filter=False`` can improve the\n    performance of reading a large file.\nverbose : bool, default False\n    Indicate number of NA values placed in non-numeric columns.\nparse_dates : bool, list-like, or dict, default False\n    The behavior is as follows:\n\n    * ``bool``. If True -> try parsing the index.\n    * ``list`` of int or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3\n      each as a separate date column.\n    * ``list`` of lists. e.g.  If [[1, 3]] -> combine columns 1 and 3 and parse as\n      a single date column.\n    * ``dict``, e.g. {'foo' : [1, 3]} -> parse columns 1, 3 as date and call\n      result 'foo'\n\n    If a column or index contains an unparsable date, the entire column or\n    index will be returned unaltered as an object data type. If you don`t want to\n    parse some cells as date just change their type in Excel to \"Text\".\n    For non-standard datetime parsing, use ``pd.to_datetime`` after ``pd.read_excel``.\n\n    Note: A fast-path exists for iso8601-formatted dates.\ndate_parser : function, optional\n    Function to use for converting a sequence of string columns to an array of\n    datetime instances. The default uses ``dateutil.parser.parser`` to do the\n    conversion. Pandas will try to call `date_parser` in three different ways,\n    advancing to the next if an exception occurs: 1) Pass one or more arrays\n    (as defined by `parse_dates`) as arguments; 2) concatenate (row-wise) the\n    string values from the columns defined by `parse_dates` into a single array\n    and pass that; and 3) call `date_parser` once for each row using one or\n    more strings (corresponding to the columns defined by `parse_dates`) as\n    arguments.\n\n    .. deprecated:: 2.0.0\n       Use ``date_format`` instead, or read in as ``object`` and then apply\n       :func:`to_datetime` as-needed.\ndate_format : str or dict of column -> format, default ``None``\n   If used in conjunction with ``parse_dates``, will parse dates according to this\n   format. For anything more complex,\n   please read in as ``object`` and then apply :func:`to_datetime` as-needed.\n\n   .. versionadded:: 2.0.0\nthousands : str, default None\n    Thousands separator for parsing string columns to numeric.  Note that\n    this parameter is only necessary for columns stored as TEXT in Excel,\n    any numeric columns will automatically be parsed, regardless of display\n    format.\ndecimal : str, default '.'\n    Character to recognize as decimal point for parsing string columns to numeric.\n    Note that this parameter is only necessary for columns stored as TEXT in Excel,\n    any numeric columns will automatically be parsed, regardless of display\n    format.(e.g. use ',' for European data).\n\n    .. versionadded:: 1.4.0\n\ncomment : str, default None\n    Comments out remainder of line. Pass a character or characters to this\n    argument to indicate comments in the input file. Any data between the\n    comment string and the end of the current line is ignored.\nskipfooter : int, default 0\n    Rows at the end to skip (0-indexed).\nstorage_options : dict, optional\n    Extra options that make sense for a particular storage connection, e.g.\n    host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n    are forwarded to ``urllib.request.Request`` as header options. For other\n    URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n    forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n    details, and for more examples on storage options refer `here\n    <https://pandas.pydata.org/docs/user_guide/io.html?\n    highlight=storage_options#reading-writing-remote-files>`_.\n\ndtype_backend : {'numpy_nullable', 'pyarrow'}, default 'numpy_nullable'\n    Back-end data type applied to the resultant :class:`DataFrame`\n    (still experimental). Behaviour is as follows:\n\n    * ``\"numpy_nullable\"``: returns nullable-dtype-backed :class:`DataFrame`\n      (default).\n    * ``\"pyarrow\"``: returns pyarrow-backed nullable :class:`ArrowDtype`\n      DataFrame.\n\n    .. versionadded:: 2.0\n\nengine_kwargs : dict, optional\n    Arbitrary keyword arguments passed to excel engine.\n\nReturns\n-------\nDataFrame or dict of DataFrames\n    DataFrame from the passed in Excel file. See notes in sheet_name\n    argument for more information on when a dict of DataFrames is returned.\n\nSee Also\n--------\nDataFrame.to_excel : Write DataFrame to an Excel file.\nDataFrame.to_csv : Write DataFrame to a comma-separated values (csv) file.\nread_csv : Read a comma-separated values (csv) file into DataFrame.\nread_fwf : Read a table of fixed-width formatted lines into DataFrame.\n\nNotes\n-----\nFor specific information on the methods used for each Excel engine, refer to the pandas\n:ref:`user guide <io.excel_reader>`\n\nExamples\n--------\nThe file can be read using the file name as string or an open file object:\n\n>>> pd.read_excel('tmp.xlsx', index_col=0)  # doctest: +SKIP\n       Name  Value\n0   string1      1\n1   string2      2\n2  #Comment      3\n\n>>> pd.read_excel(open('tmp.xlsx', 'rb'),\n...               sheet_name='Sheet3')  # doctest: +SKIP\n   Unnamed: 0      Name  Value\n0           0   string1      1\n1           1   string2      2\n2           2  #Comment      3\n\nIndex and header can be specified via the `index_col` and `header` arguments\n\n>>> pd.read_excel('tmp.xlsx', index_col=None, header=None)  # doctest: +SKIP\n     0         1      2\n0  NaN      Name  Value\n1  0.0   string1      1\n2  1.0   string2      2\n3  2.0  #Comment      3\n\nColumn types are inferred but can be explicitly specified\n\n>>> pd.read_excel('tmp.xlsx', index_col=0,\n...               dtype={'Name': str, 'Value': float})  # doctest: +SKIP\n       Name  Value\n0   string1    1.0\n1   string2    2.0\n2  #Comment    3.0\n\nTrue, False, and NA values, and thousands separators have defaults,\nbut can be explicitly specified, too. Supply the values you would like\nas strings or lists of strings!\n\n>>> pd.read_excel('tmp.xlsx', index_col=0,\n...               na_values=['string1', 'string2'])  # doctest: +SKIP\n       Name  Value\n0       NaN      1\n1       NaN      2\n2  #Comment      3\n\nComment lines in the excel input file can be skipped using the\n``comment`` kwarg.\n\n>>> pd.read_excel('tmp.xlsx', index_col=0, comment='#')  # doctest: +SKIP\n      Name  Value\n0  string1    1.0\n1  string2    2.0\n2     None    NaN", "parameters": {"type": "object", "properties": {"io": {}, "sheet_name": {"type": ["integer", "list[intstrt]", "str", "null"], "default": 0}, "header": {"type": ["integer", "sequence[int]", "null"], "default": 0}, "names": {"type": ["range", "sequencenotstr[hashable]", "null"], "default": null}, "index_col": {"type": ["integer", "sequence[int]", "str", "null"], "default": null}, "usecols": {"type": ["sequence[int]", "callable[[str], bool]", "str", "null", "integer", "sequence[str]"], "default": null}, "dtype": {"type": ["dtypearg", "null"], "default": null}, "engine": {"type": ["none\"", "\"literal['xlrd', 'openpyxl', 'odf', 'pyxlsb', 'calamine']"], "default": null}, "converters": {"type": ["dict[str, callable]", "dict[int, callable]", "null"], "default": null}, "true_values": {"type": ["iterable[hashable]", "null"], "default": null}, "false_values": {"type": ["iterable[hashable]", "null"], "default": null}, "skiprows": {"type": ["callable[[int], object]", "integer", "sequence[int]", "null"], "default": null}, "nrows": {"type": ["integer", "null"], "default": null}, "na_values": {"type": "NoneType", "default": null}, "keep_default_na": {"type": "bool", "default": true}, "na_filter": {"type": "bool", "default": true}, "verbose": {"type": "bool", "default": false}, "parse_dates": {"type": ["list", "bool", "dict"], "default": false}, "date_parser": {"type": ["callable", "lib.nodefault"], "default": "<no_default>"}, "date_format": {"type": ["dict[hashable, str]", "str", "null"], "default": null}, "thousands": {"type": ["str", "null"], "default": null}, "decimal": {"type": "str", "default": "."}, "comment": {"type": ["str", "null"], "default": null}, "skipfooter": {"type": "integer", "default": 0}, "storage_options": {"type": ["storageoptions", "null"], "default": null}, "dtype_backend": {"type": ["dtypebackend", "lib.nodefault"], "default": "<no_default>"}, "engine_kwargs": {"type": ["null", "dict"], "default": null}}}}}
{"task_id": "BigCodeBench/367", "data": {"name": "activity.strftime('%A')"}}
{"task_id": "BigCodeBench/367", "data": {"name": "matplotlib.pyplot.figure", "type": "callable", "signature": "()", "description": "Create a new figure, or activate an existing figure.\n\nParameters\n----------\nnum : int or str or `.Figure` or `.SubFigure`, optional\n    A unique identifier for the figure.\n\n    If a figure with that identifier already exists, this figure is made\n    active and returned. An integer refers to the ``Figure.number``\n    attribute, a string refers to the figure label.\n\n    If there is no figure with the identifier or *num* is not given, a new\n    figure is created, made active and returned.  If *num* is an int, it\n    will be used for the ``Figure.number`` attribute, otherwise, an\n    auto-generated integer value is used (starting at 1 and incremented\n    for each new figure). If *num* is a string, the figure label and the\n    window title is set to this value.  If num is a ``SubFigure``, its\n    parent ``Figure`` is activated.\n\nfigsize : (float, float), default: :rc:`figure.figsize`\n    Width, height in inches.\n\ndpi : float, default: :rc:`figure.dpi`\n    The resolution of the figure in dots-per-inch.\n\nfacecolor : color, default: :rc:`figure.facecolor`\n    The background color.\n\nedgecolor : color, default: :rc:`figure.edgecolor`\n    The border color.\n\nframeon : bool, default: True\n    If False, suppress drawing the figure frame.\n\nFigureClass : subclass of `~matplotlib.figure.Figure`\n    If set, an instance of this subclass will be created, rather than a\n    plain `.Figure`.\n\nclear : bool, default: False\n    If True and the figure already exists, then it is cleared.\n\nlayout : {'constrained', 'compressed', 'tight', 'none', `.LayoutEngine`, None}, default: None\n    The layout mechanism for positioning of plot elements to avoid\n    overlapping Axes decorations (labels, ticks, etc). Note that layout\n    managers can measurably slow down figure display.\n\n    - 'constrained': The constrained layout solver adjusts axes sizes\n      to avoid overlapping axes decorations.  Can handle complex plot\n      layouts and colorbars, and is thus recommended.\n\n      See :ref:`constrainedlayout_guide`\n      for examples.\n\n    - 'compressed': uses the same algorithm as 'constrained', but\n      removes extra space between fixed-aspect-ratio Axes.  Best for\n      simple grids of axes.\n\n    - 'tight': Use the tight layout mechanism. This is a relatively\n      simple algorithm that adjusts the subplot parameters so that\n      decorations do not overlap. See `.Figure.set_tight_layout` for\n      further details.\n\n    - 'none': Do not use a layout engine.\n\n    - A `.LayoutEngine` instance. Builtin layout classes are\n      `.ConstrainedLayoutEngine` and `.TightLayoutEngine`, more easily\n      accessible by 'constrained' and 'tight'.  Passing an instance\n      allows third parties to provide their own layout engine.\n\n    If not given, fall back to using the parameters *tight_layout* and\n    *constrained_layout*, including their config defaults\n    :rc:`figure.autolayout` and :rc:`figure.constrained_layout.use`.\n\n**kwargs\n    Additional keyword arguments are passed to the `.Figure` constructor.\n\nReturns\n-------\n`~matplotlib.figure.Figure`\n\nNotes\n-----\nA newly created figure is passed to the `~.FigureCanvasBase.new_manager`\nmethod or the `new_figure_manager` function provided by the current\nbackend, which install a canvas and a manager on the figure.\n\nOnce this is done, :rc:`figure.hooks` are called, one at a time, on the\nfigure; these hooks allow arbitrary customization of the figure (e.g.,\nattaching callbacks) or of associated elements (e.g., modifying the\ntoolbar).  See :doc:`/gallery/user_interfaces/mplcvd` for an example of\ntoolbar customization.\n\nIf you are creating many figures, make sure you explicitly call\n`.pyplot.close` on the figures you are not using, because this will\nenable pyplot to properly clean up the memory.\n\n`~matplotlib.rcParams` defines the default values, which can be modified\nin the matplotlibrc file.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/367", "data": {"name": "matplotlib.pyplot.subplots", "type": "class", "signature": "()", "description": "Create a figure and a set of subplots.\n\nThis utility wrapper makes it convenient to create common layouts of\nsubplots, including the enclosing figure object, in a single call.\n\nParameters\n----------\nnrows, ncols : int, default: 1\n    Number of rows/columns of the subplot grid.\n\nsharex, sharey : bool or {'none', 'all', 'row', 'col'}, default: False\n    Controls sharing of properties among x (*sharex*) or y (*sharey*)\n    axes:\n\n    - True or 'all': x- or y-axis will be shared among all subplots.\n    - False or 'none': each subplot x- or y-axis will be independent.\n    - 'row': each subplot row will share an x- or y-axis.\n    - 'col': each subplot column will share an x- or y-axis.\n\n    When subplots have a shared x-axis along a column, only the x tick\n    labels of the bottom subplot are created. Similarly, when subplots\n    have a shared y-axis along a row, only the y tick labels of the first\n    column subplot are created. To later turn other subplots' ticklabels\n    on, use `~matplotlib.axes.Axes.tick_params`.\n\n    When subplots have a shared axis that has units, calling\n    `~matplotlib.axis.Axis.set_units` will update each axis with the\n    new units.\n\nsqueeze : bool, default: True\n    - If True, extra dimensions are squeezed out from the returned\n      array of `~matplotlib.axes.Axes`:\n\n      - if only one subplot is constructed (nrows=ncols=1), the\n        resulting single Axes object is returned as a scalar.\n      - for Nx1 or 1xM subplots, the returned object is a 1D numpy\n        object array of Axes objects.\n      - for NxM, subplots with N>1 and M>1 are returned as a 2D array.\n\n    - If False, no squeezing at all is done: the returned Axes object is\n      always a 2D array containing Axes instances, even if it ends up\n      being 1x1.\n\nwidth_ratios : array-like of length *ncols*, optional\n    Defines the relative widths of the columns. Each column gets a\n    relative width of ``width_ratios[i] / sum(width_ratios)``.\n    If not given, all columns will have the same width.  Equivalent\n    to ``gridspec_kw={'width_ratios': [...]}``.\n\nheight_ratios : array-like of length *nrows*, optional\n    Defines the relative heights of the rows. Each row gets a\n    relative height of ``height_ratios[i] / sum(height_ratios)``.\n    If not given, all rows will have the same height. Convenience\n    for ``gridspec_kw={'height_ratios': [...]}``.\n\nsubplot_kw : dict, optional\n    Dict with keywords passed to the\n    `~matplotlib.figure.Figure.add_subplot` call used to create each\n    subplot.\n\ngridspec_kw : dict, optional\n    Dict with keywords passed to the `~matplotlib.gridspec.GridSpec`\n    constructor used to create the grid the subplots are placed on.\n\n**fig_kw\n    All additional keyword arguments are passed to the\n    `.pyplot.figure` call.\n\nReturns\n-------\nfig : `.Figure`\n\nax : `~matplotlib.axes.Axes` or array of Axes\n    *ax* can be either a single `~.axes.Axes` object, or an array of Axes\n    objects if more than one subplot was created.  The dimensions of the\n    resulting array can be controlled with the squeeze keyword, see above.\n\n    Typical idioms for handling the return value are::\n\n        # using the variable ax for single a Axes\n        fig, ax = plt.subplots()\n\n        # using the variable axs for multiple Axes\n        fig, axs = plt.subplots(2, 2)\n\n        # using tuple unpacking for multiple Axes\n        fig, (ax1, ax2) = plt.subplots(1, 2)\n        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n\n    The names ``ax`` and pluralized ``axs`` are preferred over ``axes``\n    because for the latter it's not clear if it refers to a single\n    `~.axes.Axes` instance or a collection of these.\n\nSee Also\n--------\n.pyplot.figure\n.pyplot.subplot\n.pyplot.axes\n.Figure.subplots\n.Figure.add_subplot\n\nExamples\n--------\n::\n\n    # First create some toy data:\n    x = np.linspace(0, 2*np.pi, 400)\n    y = np.sin(x**2)\n\n    # Create just a figure and only one subplot\n    fig, ax = plt.subplots()\n    ax.plot(x, y)\n    ax.set_title('Simple plot')\n\n    # Create two subplots and unpack the output array immediately\n    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n    ax1.plot(x, y)\n    ax1.set_title('Sharing Y axis')\n    ax2.scatter(x, y)\n\n    # Create four polar axes and access them through the returned array\n    fig, axs = plt.subplots(2, 2, subplot_kw=dict(projection=\"polar\"))\n    axs[0, 0].plot(x, y)\n    axs[1, 1].scatter(x, y)\n\n    # Share a X axis with each column of subplots\n    plt.subplots(2, 2, sharex='col')\n\n    # Share a Y axis with each row of subplots\n    plt.subplots(2, 2, sharey='row')\n\n    # Share both X and Y axes with all subplots\n    plt.subplots(2, 2, sharex='all', sharey='all')\n\n    # Note that this is the same as\n    plt.subplots(2, 2, sharex=True, sharey=True)\n\n    # Create figure number 10 with a single subplot\n    # and clears it if it already exists.\n    fig, ax = plt.subplots(num=10, clear=True)", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/367", "data": {"name": "matplotlib.pyplot.subplots[1].bar", "type": "method", "signature": "(x, height, width=0.8, bottom=None, *, align='center', data=None, **kwargs)", "description": "Make a bar plot.\n\nThe bars are positioned at *x* with the given *align*\\ment. Their\ndimensions are given by *height* and *width*. The vertical baseline\nis *bottom* (default 0).\n\nMany parameters can take either a single value applying to all bars\nor a sequence of values, one for each bar.\n\nParameters\n----------\nx : float or array-like\n    The x coordinates of the bars. See also *align* for the\n    alignment of the bars to the coordinates.\n\nheight : float or array-like\n    The height(s) of the bars.\n\n    Note that if *bottom* has units (e.g. datetime), *height* should be in\n    units that are a difference from the value of *bottom* (e.g. timedelta).\n\nwidth : float or array-like, default: 0.8\n    The width(s) of the bars.\n\n    Note that if *x* has units (e.g. datetime), then *width* should be in\n    units that are a difference (e.g. timedelta) around the *x* values.\n\nbottom : float or array-like, default: 0\n    The y coordinate(s) of the bottom side(s) of the bars.\n\n    Note that if *bottom* has units, then the y-axis will get a Locator and\n    Formatter appropriate for the units (e.g. dates, or categorical).\n\nalign : {'center', 'edge'}, default: 'center'\n    Alignment of the bars to the *x* coordinates:\n\n    - 'center': Center the base on the *x* positions.\n    - 'edge': Align the left edges of the bars with the *x* positions.\n\n    To align the bars on the right edge pass a negative *width* and\n    ``align='edge'``.\n\nReturns\n-------\n`.BarContainer`\n    Container with all the bars and optionally errorbars.\n\nOther Parameters\n----------------\ncolor : color or list of color, optional\n    The colors of the bar faces.\n\nedgecolor : color or list of color, optional\n    The colors of the bar edges.\n\nlinewidth : float or array-like, optional\n    Width of the bar edge(s). If 0, don't draw edges.\n\ntick_label : str or list of str, optional\n    The tick labels of the bars.\n    Default: None (Use default numeric labels.)\n\nlabel : str or list of str, optional\n    A single label is attached to the resulting `.BarContainer` as a\n    label for the whole dataset.\n    If a list is provided, it must be the same length as *x* and\n    labels the individual bars. Repeated labels are not de-duplicated\n    and will cause repeated label entries, so this is best used when\n    bars also differ in style (e.g., by passing a list to *color*.)\n\nxerr, yerr : float or array-like of shape(N,) or shape(2, N), optional\n    If not *None*, add horizontal / vertical errorbars to the bar tips.\n    The values are +/- sizes relative to the data:\n\n    - scalar: symmetric +/- values for all bars\n    - shape(N,): symmetric +/- values for each bar\n    - shape(2, N): Separate - and + values for each bar. First row\n      contains the lower errors, the second row contains the upper\n      errors.\n    - *None*: No errorbar. (Default)\n\n    See :doc:`/gallery/statistics/errorbar_features` for an example on\n    the usage of *xerr* and *yerr*.\n\necolor : color or list of color, default: 'black'\n    The line color of the errorbars.\n\ncapsize : float, default: :rc:`errorbar.capsize`\n   The length of the error bar caps in points.\n\nerror_kw : dict, optional\n    Dictionary of keyword arguments to be passed to the\n    `~.Axes.errorbar` method. Values of *ecolor* or *capsize* defined\n    here take precedence over the independent keyword arguments.\n\nlog : bool, default: False\n    If *True*, set the y-axis to be log scale.\n\ndata : indexable object, optional\n    If given, all parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` (unless this raises an exception).\n\n**kwargs : `.Rectangle` properties\n\nProperties:\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: scalar or None\n    angle: unknown\n    animated: bool\n    antialiased or aa: bool or None\n    bounds: (left, bottom, width, height)\n    capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    clip_box: `~matplotlib.transforms.BboxBase` or None\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    color: color\n    edgecolor or ec: color or None\n    facecolor or fc: color or None\n    figure: `~matplotlib.figure.Figure`\n    fill: bool\n    gid: str\n    hatch: {'/', '\\\\', '|', '-', '+', 'x', 'o', 'O', '.', '*'}\n    height: unknown\n    in_layout: bool\n    joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    label: object\n    linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n    linewidth or lw: float or None\n    mouseover: bool\n    path_effects: list of `.AbstractPathEffect`\n    picker: None or bool or float or callable\n    rasterized: bool\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    transform: `~matplotlib.transforms.Transform`\n    url: str\n    visible: bool\n    width: unknown\n    x: unknown\n    xy: (float, float)\n    y: unknown\n    zorder: float\n\nSee Also\n--------\nbarh : Plot a horizontal bar plot.\n\nNotes\n-----\nStacked bars can be achieved by passing individual *bottom* values per\nbar. See :doc:`/gallery/lines_bars_and_markers/bar_stacked`.", "parameters": {"type": "object", "properties": {"x": {}, "height": {}, "width": {"type": "float", "default": 0.8}, "bottom": {"type": "NoneType", "default": null}, "align": {"type": "str", "default": "center"}, "data": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/367", "data": {"name": "matplotlib.pyplot.subplots[1].set_title", "type": "method", "signature": "(label, fontdict=None, loc=None, pad=None, *, y=None, **kwargs)", "description": "Set a title for the Axes.\n\nSet one of the three available Axes titles. The available titles\nare positioned above the Axes in the center, flush with the left\nedge, and flush with the right edge.\n\nParameters\n----------\nlabel : str\n    Text to use for the title\n\nfontdict : dict\n\n    .. admonition:: Discouraged\n\n       The use of *fontdict* is discouraged. Parameters should be passed as\n       individual keyword arguments or using dictionary-unpacking\n       ``set_title(..., **fontdict)``.\n\n    A dictionary controlling the appearance of the title text,\n    the default *fontdict* is::\n\n       {'fontsize': rcParams['axes.titlesize'],\n        'fontweight': rcParams['axes.titleweight'],\n        'color': rcParams['axes.titlecolor'],\n        'verticalalignment': 'baseline',\n        'horizontalalignment': loc}\n\nloc : {'center', 'left', 'right'}, default: :rc:`axes.titlelocation`\n    Which title to set.\n\ny : float, default: :rc:`axes.titley`\n    Vertical Axes location for the title (1.0 is the top).  If\n    None (the default) and :rc:`axes.titley` is also None, y is\n    determined automatically to avoid decorators on the Axes.\n\npad : float, default: :rc:`axes.titlepad`\n    The offset of the title from the top of the Axes, in points.\n\nReturns\n-------\n`.Text`\n    The matplotlib text instance representing the title\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    Other keyword arguments are text properties, see `.Text` for a list\n    of valid text properties.", "parameters": {"type": "object", "properties": {"label": {}, "fontdict": {"type": "NoneType", "default": null}, "loc": {"type": "NoneType", "default": null}, "pad": {"type": "NoneType", "default": null}, "y": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/367", "data": {"name": "matplotlib.pyplot.subplots[1].set_xlabel", "type": "method", "signature": "(xlabel, fontdict=None, labelpad=None, *, loc=None, **kwargs)", "description": "Set the label for the x-axis.\n\nParameters\n----------\nxlabel : str\n    The label text.\n\nlabelpad : float, default: :rc:`axes.labelpad`\n    Spacing in points from the Axes bounding box including ticks\n    and tick labels.  If None, the previous value is left as is.\n\nloc : {'left', 'center', 'right'}, default: :rc:`xaxis.labellocation`\n    The label position. This is a high-level alternative for passing\n    parameters *x* and *horizontalalignment*.\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    `.Text` properties control the appearance of the label.\n\nSee Also\n--------\ntext : Documents the properties supported by `.Text`.", "parameters": {"type": "object", "properties": {"xlabel": {}, "fontdict": {"type": "NoneType", "default": null}, "labelpad": {"type": "NoneType", "default": null}, "loc": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/367", "data": {"name": "matplotlib.pyplot.subplots[1].set_ylabel", "type": "method", "signature": "(ylabel, fontdict=None, labelpad=None, *, loc=None, **kwargs)", "description": "Set the label for the y-axis.\n\nParameters\n----------\nylabel : str\n    The label text.\n\nlabelpad : float, default: :rc:`axes.labelpad`\n    Spacing in points from the Axes bounding box including ticks\n    and tick labels.  If None, the previous value is left as is.\n\nloc : {'bottom', 'center', 'top'}, default: :rc:`yaxis.labellocation`\n    The label position. This is a high-level alternative for passing\n    parameters *y* and *horizontalalignment*.\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    `.Text` properties control the appearance of the label.\n\nSee Also\n--------\ntext : Documents the properties supported by `.Text`.", "parameters": {"type": "object", "properties": {"ylabel": {}, "fontdict": {"type": "NoneType", "default": null}, "labelpad": {"type": "NoneType", "default": null}, "loc": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/368", "data": {"name": "os.listdir", "type": "callable", "signature": "(path=None)", "description": "Return a list containing the names of the files in the directory.\n\npath can be specified as either str, bytes, or a path-like object.  If path is bytes,\n  the filenames returned will also be bytes; in all other circumstances\n  the filenames returned will be str.\nIf path is None, uses the path='.'.\nOn some platforms, path may also be specified as an open file descriptor;\\\n  the file descriptor must refer to a directory.\n  If this functionality is unavailable, using it raises NotImplementedError.\n\nThe list is in arbitrary order.  It does not include the special\nentries '.' and '..' even if they are present in the directory.", "parameters": {"type": "object", "properties": {"path": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/368", "data": {"name": "os.path.join", "type": "callable", "signature": "(a, *p)", "description": "Join two or more pathname components, inserting '/' as needed.\nIf any component is an absolute path, all previous path components\nwill be discarded.  An empty last part will result in a path that\nends with a separator.", "parameters": {"type": "object", "properties": {"a": {}}}}}
{"task_id": "BigCodeBench/368", "data": {"name": "random.seed", "type": "callable", "signature": "(a=None)", "description": "Initialize internal state from a seed.\n\nThe only supported seed types are None, int, float,\nstr, bytes, and bytearray.\n\nNone or no argument seeds from current time or from an operating\nsystem specific randomness source if available.\n\nIf *a* is an int, all bits are used.\n\nFor version 2 (the default), all of the bits are used if *a* is a str,\nbytes, or bytearray.  For version 1 (provided for reproducing random\nsequences from older versions of Python), the algorithm for str and\nbytes generates a narrower range of seeds.", "parameters": {"type": "object", "properties": {"a": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/368", "data": {"name": "random.choice", "type": "callable", "signature": "(seq)", "description": "Choose a random element from a non-empty sequence.", "parameters": {"type": "object", "properties": {"seq": {}}}}}
{"task_id": "BigCodeBench/368", "data": {"name": "shutil.move", "type": "callable", "signature": "(src, dst)", "description": "Recursively move a file or directory to another location. This is\nsimilar to the Unix \"mv\" command. Return the file or directory's\ndestination.\n\nIf the destination is a directory or a symlink to a directory, the source\nis moved inside the directory. The destination path must not already\nexist.\n\nIf the destination already exists but is not a directory, it may be\noverwritten depending on os.rename() semantics.\n\nIf the destination is on our current filesystem, then rename() is used.\nOtherwise, src is copied to the destination and then removed. Symlinks are\nrecreated under the new name if os.rename() fails because of cross\nfilesystem renames.\n\nThe optional `copy_function` argument is a callable that will be used\nto copy the source or it will be delegated to `copytree`.\nBy default, copy2() is used, but any function that supports the same\nsignature (like copy()) can be used.\n\nA lot more could be done here...  A look at a mv.c shows a lot of\nthe issues this implementation glosses over.", "parameters": {"type": "object", "properties": {"src": {}, "dst": {}}}}}
{"task_id": "BigCodeBench/374", "data": {"name": "cell.value"}}
{"task_id": "BigCodeBench/374", "data": {"name": "glob.glob", "type": "callable", "signature": "(pathname)", "description": "Return a list of paths matching a pathname pattern.\n\nThe pattern may contain simple shell-style wildcards a la\nfnmatch. However, unlike fnmatch, filenames starting with a\ndot are special cases that are not matched by '*' and '?'\npatterns.\n\nIf recursive is true, the pattern '**' will match any files and\nzero or more directories and subdirectories.", "parameters": {"type": "object", "properties": {"pathname": {}}}}}
{"task_id": "BigCodeBench/374", "data": {"name": "openpyxl.load_workbook", "type": "callable", "signature": "(filename)", "description": "Open the given filename and return the workbook\n\n:param filename: the path to open or a file-like object\n:type filename: string or a file-like object open in binary mode c.f., :class:`zipfile.ZipFile`\n\n:param read_only: optimised for reading, content cannot be edited\n:type read_only: bool\n\n:param keep_vba: preserve vba content (this does NOT mean you can use it)\n:type keep_vba: bool\n\n:param data_only: controls whether cells with formulae have either the formula (default) or the value stored the last time Excel read the sheet\n:type data_only: bool\n\n:param keep_links: whether links to external workbooks should be preserved. The default is True\n:type keep_links: bool\n\n:param rich_text: if set to True openpyxl will preserve any rich text formatting in cells. The default is False\n:type rich_text: bool\n\n:rtype: :class:`openpyxl.workbook.Workbook`\n\n.. note::\n\n    When using lazy load, all worksheets will be :class:`openpyxl.worksheet.iter_worksheet.IterableWorksheet`\n    and the returned workbook will be read-only.", "parameters": {"type": "object", "properties": {"filename": {}}}}}
{"task_id": "BigCodeBench/374", "data": {"name": "openpyxl.load_workbook.sheetnames", "type": "callable", "signature": "(filename, read_only=False, keep_vba=False, data_only=False, keep_links=True, rich_text=False)", "description": "Open the given filename and return the workbook\n\n:param filename: the path to open or a file-like object\n:type filename: string or a file-like object open in binary mode c.f., :class:`zipfile.ZipFile`\n\n:param read_only: optimised for reading, content cannot be edited\n:type read_only: bool\n\n:param keep_vba: preserve vba content (this does NOT mean you can use it)\n:type keep_vba: bool\n\n:param data_only: controls whether cells with formulae have either the formula (default) or the value stored the last time Excel read the sheet\n:type data_only: bool\n\n:param keep_links: whether links to external workbooks should be preserved. The default is True\n:type keep_links: bool\n\n:param rich_text: if set to True openpyxl will preserve any rich text formatting in cells. The default is False\n:type rich_text: bool\n\n:rtype: :class:`openpyxl.workbook.Workbook`\n\n.. note::\n\n    When using lazy load, all worksheets will be :class:`openpyxl.worksheet.iter_worksheet.IterableWorksheet`\n    and the returned workbook will be read-only.", "parameters": {"type": "object", "properties": {"filename": {}, "read_only": {"type": "bool", "default": false}, "keep_vba": {"type": "bool", "default": false}, "data_only": {"type": "bool", "default": false}, "keep_links": {"type": "bool", "default": true}, "rich_text": {"type": "bool", "default": false}}}}}
{"task_id": "BigCodeBench/374", "data": {"name": "openpyxl.load_workbook.save", "type": "callable", "signature": "(filename)", "description": "Open the given filename and return the workbook\n\n:param filename: the path to open or a file-like object\n:type filename: string or a file-like object open in binary mode c.f., :class:`zipfile.ZipFile`\n\n:param read_only: optimised for reading, content cannot be edited\n:type read_only: bool\n\n:param keep_vba: preserve vba content (this does NOT mean you can use it)\n:type keep_vba: bool\n\n:param data_only: controls whether cells with formulae have either the formula (default) or the value stored the last time Excel read the sheet\n:type data_only: bool\n\n:param keep_links: whether links to external workbooks should be preserved. The default is True\n:type keep_links: bool\n\n:param rich_text: if set to True openpyxl will preserve any rich text formatting in cells. The default is False\n:type rich_text: bool\n\n:rtype: :class:`openpyxl.workbook.Workbook`\n\n.. note::\n\n    When using lazy load, all worksheets will be :class:`openpyxl.worksheet.iter_worksheet.IterableWorksheet`\n    and the returned workbook will be read-only.", "parameters": {"type": "object", "properties": {"filename": {}}}}}
{"task_id": "BigCodeBench/374", "data": {"name": "os.path.isdir", "type": "callable", "signature": "(s)", "description": "Return true if the pathname refers to an existing directory.", "parameters": {"type": "object", "properties": {"s": {}}}}}
{"task_id": "BigCodeBench/374", "data": {"name": "regex.sub", "type": "callable", "signature": "(pattern, repl, string)", "description": "Return the string obtained by replacing the leftmost (or rightmost with a\nreverse pattern) non-overlapping occurrences of the pattern in string by the\nreplacement repl. repl can be either a string or a callable; if a string,\nbackslash escapes in it are processed; if a callable, it's passed the match\nobject and must return a replacement string to be used.", "parameters": {"type": "object", "properties": {"pattern": {}, "repl": {}, "string": {}}}}}
{"task_id": "BigCodeBench/399", "data": {"name": "math.pi", "type": "constant", "signature": null, "description": "Convert a string or number to a floating point number, if possible.", "value": "3.141592653589793", "parameters": {"type": "constant"}}}
{"task_id": "BigCodeBench/399", "data": {"name": "matplotlib.pyplot.figure", "type": "callable", "signature": "()", "description": "Create a new figure, or activate an existing figure.\n\nParameters\n----------\nnum : int or str or `.Figure` or `.SubFigure`, optional\n    A unique identifier for the figure.\n\n    If a figure with that identifier already exists, this figure is made\n    active and returned. An integer refers to the ``Figure.number``\n    attribute, a string refers to the figure label.\n\n    If there is no figure with the identifier or *num* is not given, a new\n    figure is created, made active and returned.  If *num* is an int, it\n    will be used for the ``Figure.number`` attribute, otherwise, an\n    auto-generated integer value is used (starting at 1 and incremented\n    for each new figure). If *num* is a string, the figure label and the\n    window title is set to this value.  If num is a ``SubFigure``, its\n    parent ``Figure`` is activated.\n\nfigsize : (float, float), default: :rc:`figure.figsize`\n    Width, height in inches.\n\ndpi : float, default: :rc:`figure.dpi`\n    The resolution of the figure in dots-per-inch.\n\nfacecolor : color, default: :rc:`figure.facecolor`\n    The background color.\n\nedgecolor : color, default: :rc:`figure.edgecolor`\n    The border color.\n\nframeon : bool, default: True\n    If False, suppress drawing the figure frame.\n\nFigureClass : subclass of `~matplotlib.figure.Figure`\n    If set, an instance of this subclass will be created, rather than a\n    plain `.Figure`.\n\nclear : bool, default: False\n    If True and the figure already exists, then it is cleared.\n\nlayout : {'constrained', 'compressed', 'tight', 'none', `.LayoutEngine`, None}, default: None\n    The layout mechanism for positioning of plot elements to avoid\n    overlapping Axes decorations (labels, ticks, etc). Note that layout\n    managers can measurably slow down figure display.\n\n    - 'constrained': The constrained layout solver adjusts axes sizes\n      to avoid overlapping axes decorations.  Can handle complex plot\n      layouts and colorbars, and is thus recommended.\n\n      See :ref:`constrainedlayout_guide`\n      for examples.\n\n    - 'compressed': uses the same algorithm as 'constrained', but\n      removes extra space between fixed-aspect-ratio Axes.  Best for\n      simple grids of axes.\n\n    - 'tight': Use the tight layout mechanism. This is a relatively\n      simple algorithm that adjusts the subplot parameters so that\n      decorations do not overlap. See `.Figure.set_tight_layout` for\n      further details.\n\n    - 'none': Do not use a layout engine.\n\n    - A `.LayoutEngine` instance. Builtin layout classes are\n      `.ConstrainedLayoutEngine` and `.TightLayoutEngine`, more easily\n      accessible by 'constrained' and 'tight'.  Passing an instance\n      allows third parties to provide their own layout engine.\n\n    If not given, fall back to using the parameters *tight_layout* and\n    *constrained_layout*, including their config defaults\n    :rc:`figure.autolayout` and :rc:`figure.constrained_layout.use`.\n\n**kwargs\n    Additional keyword arguments are passed to the `.Figure` constructor.\n\nReturns\n-------\n`~matplotlib.figure.Figure`\n\nNotes\n-----\nA newly created figure is passed to the `~.FigureCanvasBase.new_manager`\nmethod or the `new_figure_manager` function provided by the current\nbackend, which install a canvas and a manager on the figure.\n\nOnce this is done, :rc:`figure.hooks` are called, one at a time, on the\nfigure; these hooks allow arbitrary customization of the figure (e.g.,\nattaching callbacks) or of associated elements (e.g., modifying the\ntoolbar).  See :doc:`/gallery/user_interfaces/mplcvd` for an example of\ntoolbar customization.\n\nIf you are creating many figures, make sure you explicitly call\n`.pyplot.close` on the figures you are not using, because this will\nenable pyplot to properly clean up the memory.\n\n`~matplotlib.rcParams` defines the default values, which can be modified\nin the matplotlibrc file.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/399", "data": {"name": "matplotlib.pyplot.subplots", "type": "class", "signature": "()", "description": "Create a figure and a set of subplots.\n\nThis utility wrapper makes it convenient to create common layouts of\nsubplots, including the enclosing figure object, in a single call.\n\nParameters\n----------\nnrows, ncols : int, default: 1\n    Number of rows/columns of the subplot grid.\n\nsharex, sharey : bool or {'none', 'all', 'row', 'col'}, default: False\n    Controls sharing of properties among x (*sharex*) or y (*sharey*)\n    axes:\n\n    - True or 'all': x- or y-axis will be shared among all subplots.\n    - False or 'none': each subplot x- or y-axis will be independent.\n    - 'row': each subplot row will share an x- or y-axis.\n    - 'col': each subplot column will share an x- or y-axis.\n\n    When subplots have a shared x-axis along a column, only the x tick\n    labels of the bottom subplot are created. Similarly, when subplots\n    have a shared y-axis along a row, only the y tick labels of the first\n    column subplot are created. To later turn other subplots' ticklabels\n    on, use `~matplotlib.axes.Axes.tick_params`.\n\n    When subplots have a shared axis that has units, calling\n    `~matplotlib.axis.Axis.set_units` will update each axis with the\n    new units.\n\nsqueeze : bool, default: True\n    - If True, extra dimensions are squeezed out from the returned\n      array of `~matplotlib.axes.Axes`:\n\n      - if only one subplot is constructed (nrows=ncols=1), the\n        resulting single Axes object is returned as a scalar.\n      - for Nx1 or 1xM subplots, the returned object is a 1D numpy\n        object array of Axes objects.\n      - for NxM, subplots with N>1 and M>1 are returned as a 2D array.\n\n    - If False, no squeezing at all is done: the returned Axes object is\n      always a 2D array containing Axes instances, even if it ends up\n      being 1x1.\n\nwidth_ratios : array-like of length *ncols*, optional\n    Defines the relative widths of the columns. Each column gets a\n    relative width of ``width_ratios[i] / sum(width_ratios)``.\n    If not given, all columns will have the same width.  Equivalent\n    to ``gridspec_kw={'width_ratios': [...]}``.\n\nheight_ratios : array-like of length *nrows*, optional\n    Defines the relative heights of the rows. Each row gets a\n    relative height of ``height_ratios[i] / sum(height_ratios)``.\n    If not given, all rows will have the same height. Convenience\n    for ``gridspec_kw={'height_ratios': [...]}``.\n\nsubplot_kw : dict, optional\n    Dict with keywords passed to the\n    `~matplotlib.figure.Figure.add_subplot` call used to create each\n    subplot.\n\ngridspec_kw : dict, optional\n    Dict with keywords passed to the `~matplotlib.gridspec.GridSpec`\n    constructor used to create the grid the subplots are placed on.\n\n**fig_kw\n    All additional keyword arguments are passed to the\n    `.pyplot.figure` call.\n\nReturns\n-------\nfig : `.Figure`\n\nax : `~matplotlib.axes.Axes` or array of Axes\n    *ax* can be either a single `~.axes.Axes` object, or an array of Axes\n    objects if more than one subplot was created.  The dimensions of the\n    resulting array can be controlled with the squeeze keyword, see above.\n\n    Typical idioms for handling the return value are::\n\n        # using the variable ax for single a Axes\n        fig, ax = plt.subplots()\n\n        # using the variable axs for multiple Axes\n        fig, axs = plt.subplots(2, 2)\n\n        # using tuple unpacking for multiple Axes\n        fig, (ax1, ax2) = plt.subplots(1, 2)\n        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n\n    The names ``ax`` and pluralized ``axs`` are preferred over ``axes``\n    because for the latter it's not clear if it refers to a single\n    `~.axes.Axes` instance or a collection of these.\n\nSee Also\n--------\n.pyplot.figure\n.pyplot.subplot\n.pyplot.axes\n.Figure.subplots\n.Figure.add_subplot\n\nExamples\n--------\n::\n\n    # First create some toy data:\n    x = np.linspace(0, 2*np.pi, 400)\n    y = np.sin(x**2)\n\n    # Create just a figure and only one subplot\n    fig, ax = plt.subplots()\n    ax.plot(x, y)\n    ax.set_title('Simple plot')\n\n    # Create two subplots and unpack the output array immediately\n    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n    ax1.plot(x, y)\n    ax1.set_title('Sharing Y axis')\n    ax2.scatter(x, y)\n\n    # Create four polar axes and access them through the returned array\n    fig, axs = plt.subplots(2, 2, subplot_kw=dict(projection=\"polar\"))\n    axs[0, 0].plot(x, y)\n    axs[1, 1].scatter(x, y)\n\n    # Share a X axis with each column of subplots\n    plt.subplots(2, 2, sharex='col')\n\n    # Share a Y axis with each row of subplots\n    plt.subplots(2, 2, sharey='row')\n\n    # Share both X and Y axes with all subplots\n    plt.subplots(2, 2, sharex='all', sharey='all')\n\n    # Note that this is the same as\n    plt.subplots(2, 2, sharex=True, sharey=True)\n\n    # Create figure number 10 with a single subplot\n    # and clears it if it already exists.\n    fig, ax = plt.subplots(num=10, clear=True)", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/399", "data": {"name": "matplotlib.pyplot.subplots[1].legend", "type": "method", "signature": "(*args, **kwargs)", "description": "Place a legend on the Axes.\n\nCall signatures::\n\n    legend()\n    legend(handles, labels)\n    legend(handles=handles)\n    legend(labels)\n\nThe call signatures correspond to the following different ways to use\nthis method:\n\n**1. Automatic detection of elements to be shown in the legend**\n\nThe elements to be added to the legend are automatically determined,\nwhen you do not pass in any extra arguments.\n\nIn this case, the labels are taken from the artist. You can specify\nthem either at artist creation or by calling the\n:meth:`~.Artist.set_label` method on the artist::\n\n    ax.plot([1, 2, 3], label='Inline label')\n    ax.legend()\n\nor::\n\n    line, = ax.plot([1, 2, 3])\n    line.set_label('Label via method')\n    ax.legend()\n\n.. note::\n    Specific artists can be excluded from the automatic legend element\n    selection by using a label starting with an underscore, \"_\".\n    A string starting with an underscore is the default label for all\n    artists, so calling `.Axes.legend` without any arguments and\n    without setting the labels manually will result in no legend being\n    drawn.\n\n\n**2. Explicitly listing the artists and labels in the legend**\n\nFor full control of which artists have a legend entry, it is possible\nto pass an iterable of legend artists followed by an iterable of\nlegend labels respectively::\n\n    ax.legend([line1, line2, line3], ['label1', 'label2', 'label3'])\n\n\n**3. Explicitly listing the artists in the legend**\n\nThis is similar to 2, but the labels are taken from the artists'\nlabel properties. Example::\n\n    line1, = ax.plot([1, 2, 3], label='label1')\n    line2, = ax.plot([1, 2, 3], label='label2')\n    ax.legend(handles=[line1, line2])\n\n\n**4. Labeling existing plot elements**\n\n.. admonition:: Discouraged\n\n    This call signature is discouraged, because the relation between\n    plot elements and labels is only implicit by their order and can\n    easily be mixed up.\n\nTo make a legend for all artists on an Axes, call this function with\nan iterable of strings, one for each legend item. For example::\n\n    ax.plot([1, 2, 3])\n    ax.plot([5, 6, 7])\n    ax.legend(['First line', 'Second line'])\n\n\nParameters\n----------\nhandles : list of (`.Artist` or tuple of `.Artist`), optional\n    A list of Artists (lines, patches) to be added to the legend.\n    Use this together with *labels*, if you need full control on what\n    is shown in the legend and the automatic mechanism described above\n    is not sufficient.\n\n    The length of handles and labels should be the same in this\n    case. If they are not, they are truncated to the smaller length.\n\n    If an entry contains a tuple, then the legend handler for all Artists in the\n    tuple will be placed alongside a single label.\n\nlabels : list of str, optional\n    A list of labels to show next to the artists.\n    Use this together with *handles*, if you need full control on what\n    is shown in the legend and the automatic mechanism described above\n    is not sufficient.\n\nReturns\n-------\n`~matplotlib.legend.Legend`\n\nOther Parameters\n----------------\n\nloc : str or pair of floats, default: :rc:`legend.loc`\n    The location of the legend.\n\n    The strings ``'upper left'``, ``'upper right'``, ``'lower left'``,\n    ``'lower right'`` place the legend at the corresponding corner of the\n    axes.\n\n    The strings ``'upper center'``, ``'lower center'``, ``'center left'``,\n    ``'center right'`` place the legend at the center of the corresponding edge\n    of the axes.\n\n    The string ``'center'`` places the legend at the center of the axes.\n\n    The string ``'best'`` places the legend at the location, among the nine\n    locations defined so far, with the minimum overlap with other drawn\n    artists.  This option can be quite slow for plots with large amounts of\n    data; your plotting speed may benefit from providing a specific location.\n\n    The location can also be a 2-tuple giving the coordinates of the lower-left\n    corner of the legend in axes coordinates (in which case *bbox_to_anchor*\n    will be ignored).\n\n    For back-compatibility, ``'center right'`` (but no other location) can also\n    be spelled ``'right'``, and each \"string\" location can also be given as a\n    numeric value:\n\n    ==================   =============\n    Location String      Location Code\n    ==================   =============\n    'best' (Axes only)   0\n    'upper right'        1\n    'upper left'         2\n    'lower left'         3\n    'lower right'        4\n    'right'              5\n    'center left'        6\n    'center right'       7\n    'lower center'       8\n    'upper center'       9\n    'center'             10\n    ==================   =============\n    \nbbox_to_anchor : `.BboxBase`, 2-tuple, or 4-tuple of floats\n    Box that is used to position the legend in conjunction with *loc*.\n    Defaults to `axes.bbox` (if called as a method to `.Axes.legend`) or\n    `figure.bbox` (if `.Figure.legend`).  This argument allows arbitrary\n    placement of the legend.\n\n    Bbox coordinates are interpreted in the coordinate system given by\n    *bbox_transform*, with the default transform\n    Axes or Figure coordinates, depending on which ``legend`` is called.\n\n    If a 4-tuple or `.BboxBase` is given, then it specifies the bbox\n    ``(x, y, width, height)`` that the legend is placed in.\n    To put the legend in the best location in the bottom right\n    quadrant of the axes (or figure)::\n\n        loc='best', bbox_to_anchor=(0.5, 0., 0.5, 0.5)\n\n    A 2-tuple ``(x, y)`` places the corner of the legend specified by *loc* at\n    x, y.  For example, to put the legend's upper right-hand corner in the\n    center of the axes (or figure) the following keywords can be used::\n\n        loc='upper right', bbox_to_anchor=(0.5, 0.5)\n\nncols : int, default: 1\n    The number of columns that the legend has.\n\n    For backward compatibility, the spelling *ncol* is also supported\n    but it is discouraged. If both are given, *ncols* takes precedence.\n\nprop : None or `~matplotlib.font_manager.FontProperties` or dict\n    The font properties of the legend. If None (default), the current\n    :data:`matplotlib.rcParams` will be used.\n\nfontsize : int or {'xx-small', 'x-small', 'small', 'medium', 'large', 'x-large', 'xx-large'}\n    The font size of the legend. If the value is numeric the size will be the\n    absolute font size in points. String values are relative to the current\n    default font size. This argument is only used if *prop* is not specified.\n\nlabelcolor : str or list, default: :rc:`legend.labelcolor`\n    The color of the text in the legend. Either a valid color string\n    (for example, 'red'), or a list of color strings. The labelcolor can\n    also be made to match the color of the line or marker using 'linecolor',\n    'markerfacecolor' (or 'mfc'), or 'markeredgecolor' (or 'mec').\n\n    Labelcolor can be set globally using :rc:`legend.labelcolor`. If None,\n    use :rc:`text.color`.\n\nnumpoints : int, default: :rc:`legend.numpoints`\n    The number of marker points in the legend when creating a legend\n    entry for a `.Line2D` (line).\n\nscatterpoints : int, default: :rc:`legend.scatterpoints`\n    The number of marker points in the legend when creating\n    a legend entry for a `.PathCollection` (scatter plot).\n\nscatteryoffsets : iterable of floats, default: ``[0.375, 0.5, 0.3125]``\n    The vertical offset (relative to the font size) for the markers\n    created for a scatter plot legend entry. 0.0 is at the base the\n    legend text, and 1.0 is at the top. To draw all markers at the\n    same height, set to ``[0.5]``.\n\nmarkerscale : float, default: :rc:`legend.markerscale`\n    The relative size of legend markers compared to the originally drawn ones.\n\nmarkerfirst : bool, default: True\n    If *True*, legend marker is placed to the left of the legend label.\n    If *False*, legend marker is placed to the right of the legend label.\n\nreverse : bool, default: False\n    If *True*, the legend labels are displayed in reverse order from the input.\n    If *False*, the legend labels are displayed in the same order as the input.\n\n    .. versionadded:: 3.7\n\nframeon : bool, default: :rc:`legend.frameon`\n    Whether the legend should be drawn on a patch (frame).\n\nfancybox : bool, default: :rc:`legend.fancybox`\n    Whether round edges should be enabled around the `.FancyBboxPatch` which\n    makes up the legend's background.\n\nshadow : None, bool or dict, default: :rc:`legend.shadow`\n    Whether to draw a shadow behind the legend.\n    The shadow can be configured using `.Patch` keywords.\n    Customization via :rc:`legend.shadow` is currently not supported.\n\nframealpha : float, default: :rc:`legend.framealpha`\n    The alpha transparency of the legend's background.\n    If *shadow* is activated and *framealpha* is ``None``, the default value is\n    ignored.\n\nfacecolor : \"inherit\" or color, default: :rc:`legend.facecolor`\n    The legend's background color.\n    If ``\"inherit\"``, use :rc:`axes.facecolor`.\n\nedgecolor : \"inherit\" or color, default: :rc:`legend.edgecolor`\n    The legend's background patch edge color.\n    If ``\"inherit\"``, use take :rc:`axes.edgecolor`.\n\nmode : {\"expand\", None}\n    If *mode* is set to ``\"expand\"`` the legend will be horizontally\n    expanded to fill the axes area (or *bbox_to_anchor* if defines\n    the legend's size).\n\nbbox_transform : None or `~matplotlib.transforms.Transform`\n    The transform for the bounding box (*bbox_to_anchor*). For a value\n    of ``None`` (default) the Axes'\n    :data:`~matplotlib.axes.Axes.transAxes` transform will be used.\n\ntitle : str or None\n    The legend's title. Default is no title (``None``).\n\ntitle_fontproperties : None or `~matplotlib.font_manager.FontProperties` or dict\n    The font properties of the legend's title. If None (default), the\n    *title_fontsize* argument will be used if present; if *title_fontsize* is\n    also None, the current :rc:`legend.title_fontsize` will be used.\n\ntitle_fontsize : int or {'xx-small', 'x-small', 'small', 'medium', 'large', 'x-large', 'xx-large'}, default: :rc:`legend.title_fontsize`\n    The font size of the legend's title.\n    Note: This cannot be combined with *title_fontproperties*. If you want\n    to set the fontsize alongside other font properties, use the *size*\n    parameter in *title_fontproperties*.\n\nalignment : {'center', 'left', 'right'}, default: 'center'\n    The alignment of the legend title and the box of entries. The entries\n    are aligned as a single block, so that markers always lined up.\n\nborderpad : float, default: :rc:`legend.borderpad`\n    The fractional whitespace inside the legend border, in font-size units.\n\nlabelspacing : float, default: :rc:`legend.labelspacing`\n    The vertical space between the legend entries, in font-size units.\n\nhandlelength : float, default: :rc:`legend.handlelength`\n    The length of the legend handles, in font-size units.\n\nhandleheight : float, default: :rc:`legend.handleheight`\n    The height of the legend handles, in font-size units.\n\nhandletextpad : float, default: :rc:`legend.handletextpad`\n    The pad between the legend handle and text, in font-size units.\n\nborderaxespad : float, default: :rc:`legend.borderaxespad`\n    The pad between the axes and legend border, in font-size units.\n\ncolumnspacing : float, default: :rc:`legend.columnspacing`\n    The spacing between columns, in font-size units.\n\nhandler_map : dict or None\n    The custom dictionary mapping instances or types to a legend\n    handler. This *handler_map* updates the default handler map\n    found at `matplotlib.legend.Legend.get_legend_handler_map`.\n\ndraggable : bool, default: False\n    Whether the legend can be dragged with the mouse.\n\n\nSee Also\n--------\n.Figure.legend\n\nNotes\n-----\nSome artists are not supported by this function.  See\n:ref:`legend_guide` for details.\n\nExamples\n--------\n.. plot:: gallery/text_labels_and_annotations/legend.py", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/399", "data": {"name": "matplotlib.pyplot.subplots[1].plot", "type": "method", "signature": "(*args, scalex=True, scaley=True, data=None, **kwargs)", "description": "Plot y versus x as lines and/or markers.\n\nCall signatures::\n\n    plot([x], y, [fmt], *, data=None, **kwargs)\n    plot([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n\nThe coordinates of the points or line nodes are given by *x*, *y*.\n\nThe optional parameter *fmt* is a convenient way for defining basic\nformatting like color, marker and linestyle. It's a shortcut string\nnotation described in the *Notes* section below.\n\n>>> plot(x, y)        # plot x and y using default line style and color\n>>> plot(x, y, 'bo')  # plot x and y using blue circle markers\n>>> plot(y)           # plot y using x as index array 0..N-1\n>>> plot(y, 'r+')     # ditto, but with red plusses\n\nYou can use `.Line2D` properties as keyword arguments for more\ncontrol on the appearance. Line properties and *fmt* can be mixed.\nThe following two calls yield identical results:\n\n>>> plot(x, y, 'go--', linewidth=2, markersize=12)\n>>> plot(x, y, color='green', marker='o', linestyle='dashed',\n...      linewidth=2, markersize=12)\n\nWhen conflicting with *fmt*, keyword arguments take precedence.\n\n\n**Plotting labelled data**\n\nThere's a convenient way for plotting objects with labelled data (i.e.\ndata that can be accessed by index ``obj['y']``). Instead of giving\nthe data in *x* and *y*, you can provide the object in the *data*\nparameter and just give the labels for *x* and *y*::\n\n>>> plot('xlabel', 'ylabel', data=obj)\n\nAll indexable objects are supported. This could e.g. be a `dict`, a\n`pandas.DataFrame` or a structured numpy array.\n\n\n**Plotting multiple sets of data**\n\nThere are various ways to plot multiple sets of data.\n\n- The most straight forward way is just to call `plot` multiple times.\n  Example:\n\n  >>> plot(x1, y1, 'bo')\n  >>> plot(x2, y2, 'go')\n\n- If *x* and/or *y* are 2D arrays a separate data set will be drawn\n  for every column. If both *x* and *y* are 2D, they must have the\n  same shape. If only one of them is 2D with shape (N, m) the other\n  must have length N and will be used for every data set m.\n\n  Example:\n\n  >>> x = [1, 2, 3]\n  >>> y = np.array([[1, 2], [3, 4], [5, 6]])\n  >>> plot(x, y)\n\n  is equivalent to:\n\n  >>> for col in range(y.shape[1]):\n  ...     plot(x, y[:, col])\n\n- The third way is to specify multiple sets of *[x]*, *y*, *[fmt]*\n  groups::\n\n  >>> plot(x1, y1, 'g^', x2, y2, 'g-')\n\n  In this case, any additional keyword argument applies to all\n  datasets. Also, this syntax cannot be combined with the *data*\n  parameter.\n\nBy default, each line is assigned a different style specified by a\n'style cycle'. The *fmt* and line property parameters are only\nnecessary if you want explicit deviations from these defaults.\nAlternatively, you can also change the style cycle using\n:rc:`axes.prop_cycle`.\n\n\nParameters\n----------\nx, y : array-like or scalar\n    The horizontal / vertical coordinates of the data points.\n    *x* values are optional and default to ``range(len(y))``.\n\n    Commonly, these parameters are 1D arrays.\n\n    They can also be scalars, or two-dimensional (in that case, the\n    columns represent separate data sets).\n\n    These arguments cannot be passed as keywords.\n\nfmt : str, optional\n    A format string, e.g. 'ro' for red circles. See the *Notes*\n    section for a full description of the format strings.\n\n    Format strings are just an abbreviation for quickly setting\n    basic line properties. All of these and more can also be\n    controlled by keyword arguments.\n\n    This argument cannot be passed as keyword.\n\ndata : indexable object, optional\n    An object with labelled data. If given, provide the label names to\n    plot in *x* and *y*.\n\n    .. note::\n        Technically there's a slight ambiguity in calls where the\n        second label is a valid *fmt*. ``plot('n', 'o', data=obj)``\n        could be ``plt(x, y)`` or ``plt(y, fmt)``. In such cases,\n        the former interpretation is chosen, but a warning is issued.\n        You may suppress the warning by adding an empty format string\n        ``plot('n', 'o', '', data=obj)``.\n\nReturns\n-------\nlist of `.Line2D`\n    A list of lines representing the plotted data.\n\nOther Parameters\n----------------\nscalex, scaley : bool, default: True\n    These parameters determine if the view limits are adapted to the\n    data limits. The values are passed on to\n    `~.axes.Axes.autoscale_view`.\n\n**kwargs : `~matplotlib.lines.Line2D` properties, optional\n    *kwargs* are used to specify properties like a line label (for\n    auto legends), linewidth, antialiasing, marker face color.\n    Example::\n\n    >>> plot([1, 2, 3], [1, 2, 3], 'go-', label='line 1', linewidth=2)\n    >>> plot([1, 2, 3], [1, 4, 9], 'rs', label='line 2')\n\n    If you specify multiple lines with one plot call, the kwargs apply\n    to all those lines. In case the label object is iterable, each\n    element is used as labels for each set of data.\n\n    Here is a list of available `.Line2D` properties:\n\n    Properties:\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: scalar or None\n    animated: bool\n    antialiased or aa: bool\n    clip_box: `~matplotlib.transforms.BboxBase` or None\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    color or c: color\n    dash_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    dash_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    dashes: sequence of floats (on/off ink in points) or (None, None)\n    data: (2, N) array or two 1D arrays\n    drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n    figure: `~matplotlib.figure.Figure`\n    fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n    gapcolor: color or None\n    gid: str\n    in_layout: bool\n    label: object\n    linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n    linewidth or lw: float\n    marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n    markeredgecolor or mec: color\n    markeredgewidth or mew: float\n    markerfacecolor or mfc: color\n    markerfacecoloralt or mfcalt: color\n    markersize or ms: float\n    markevery: None or int or (int, int) or slice or list[int] or float or (float, float) or list[bool]\n    mouseover: bool\n    path_effects: list of `.AbstractPathEffect`\n    picker: float or callable[[Artist, Event], tuple[bool, dict]]\n    pickradius: float\n    rasterized: bool\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    solid_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    solid_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    transform: unknown\n    url: str\n    visible: bool\n    xdata: 1D array\n    ydata: 1D array\n    zorder: float\n\nSee Also\n--------\nscatter : XY scatter plot with markers of varying size and/or color (\n    sometimes also called bubble chart).\n\nNotes\n-----\n**Format Strings**\n\nA format string consists of a part for color, marker and line::\n\n    fmt = '[marker][line][color]'\n\nEach of them is optional. If not provided, the value from the style\ncycle is used. Exception: If ``line`` is given, but no ``marker``,\nthe data will be a line without markers.\n\nOther combinations such as ``[color][marker][line]`` are also\nsupported, but note that their parsing may be ambiguous.\n\n**Markers**\n\n=============   ===============================\ncharacter       description\n=============   ===============================\n``'.'``         point marker\n``','``         pixel marker\n``'o'``         circle marker\n``'v'``         triangle_down marker\n``'^'``         triangle_up marker\n``'<'``         triangle_left marker\n``'>'``         triangle_right marker\n``'1'``         tri_down marker\n``'2'``         tri_up marker\n``'3'``         tri_left marker\n``'4'``         tri_right marker\n``'8'``         octagon marker\n``'s'``         square marker\n``'p'``         pentagon marker\n``'P'``         plus (filled) marker\n``'*'``         star marker\n``'h'``         hexagon1 marker\n``'H'``         hexagon2 marker\n``'+'``         plus marker\n``'x'``         x marker\n``'X'``         x (filled) marker\n``'D'``         diamond marker\n``'d'``         thin_diamond marker\n``'|'``         vline marker\n``'_'``         hline marker\n=============   ===============================\n\n**Line Styles**\n\n=============    ===============================\ncharacter        description\n=============    ===============================\n``'-'``          solid line style\n``'--'``         dashed line style\n``'-.'``         dash-dot line style\n``':'``          dotted line style\n=============    ===============================\n\nExample format strings::\n\n    'b'    # blue markers with default shape\n    'or'   # red circles\n    '-g'   # green solid line\n    '--'   # dashed line with default color\n    '^k:'  # black triangle_up markers connected by a dotted line\n\n**Colors**\n\nThe supported color abbreviations are the single letter codes\n\n=============    ===============================\ncharacter        color\n=============    ===============================\n``'b'``          blue\n``'g'``          green\n``'r'``          red\n``'c'``          cyan\n``'m'``          magenta\n``'y'``          yellow\n``'k'``          black\n``'w'``          white\n=============    ===============================\n\nand the ``'CN'`` colors that index into the default property cycle.\n\nIf the color is the only part of the format string, you can\nadditionally use any  `matplotlib.colors` spec, e.g. full names\n(``'green'``) or hex strings (``'#008000'``).", "parameters": {"type": "object", "properties": {"scalex": {"type": "bool", "default": true}, "scaley": {"type": "bool", "default": true}, "data": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/399", "data": {"name": "numpy.cos", "type": "callable", "signature": "(*args, **kwargs)", "description": "cos(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n\nCosine element-wise.\n\nParameters\n----------\nx : array_like\n    Input array in radians.\nout : ndarray, None, or tuple of ndarray and None, optional\n    A location into which the result is stored. If provided, it must have\n    a shape that the inputs broadcast to. If not provided or None,\n    a freshly-allocated array is returned. A tuple (possible only as a\n    keyword argument) must have length equal to the number of outputs.\nwhere : array_like, optional\n    This condition is broadcast over the input. At locations where the\n    condition is True, the `out` array will be set to the ufunc result.\n    Elsewhere, the `out` array will retain its original value.\n    Note that if an uninitialized `out` array is created via the default\n    ``out=None``, locations within it where the condition is False will\n    remain uninitialized.\n**kwargs\n    For other keyword-only arguments, see the\n    :ref:`ufunc docs <ufuncs.kwargs>`.\n\nReturns\n-------\ny : ndarray\n    The corresponding cosine values.\n    This is a scalar if `x` is a scalar.\n\nNotes\n-----\nIf `out` is provided, the function writes the result into it,\nand returns a reference to `out`.  (See Examples)\n\nReferences\n----------\nM. Abramowitz and I. A. Stegun, Handbook of Mathematical Functions.\nNew York, NY: Dover, 1972.\n\nExamples\n--------\n>>> np.cos(np.array([0, np.pi/2, np.pi]))\narray([  1.00000000e+00,   6.12303177e-17,  -1.00000000e+00])\n>>>\n>>> # Example of providing the optional output parameter\n>>> out1 = np.array([0], dtype='d')\n>>> out2 = np.cos([0.1], out1)\n>>> out2 is out1\nTrue\n>>>\n>>> # Example of ValueError due to provision of shape mis-matched `out`\n>>> np.cos(np.zeros((3,3)),np.zeros((2,2)))\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nValueError: operands could not be broadcast together with shapes (3,3) (2,2)", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/399", "data": {"name": "numpy.sin", "type": "callable", "signature": "(*args, **kwargs)", "description": "sin(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n\nTrigonometric sine, element-wise.\n\nParameters\n----------\nx : array_like\n    Angle, in radians (:math:`2 \\pi` rad equals 360 degrees).\nout : ndarray, None, or tuple of ndarray and None, optional\n    A location into which the result is stored. If provided, it must have\n    a shape that the inputs broadcast to. If not provided or None,\n    a freshly-allocated array is returned. A tuple (possible only as a\n    keyword argument) must have length equal to the number of outputs.\nwhere : array_like, optional\n    This condition is broadcast over the input. At locations where the\n    condition is True, the `out` array will be set to the ufunc result.\n    Elsewhere, the `out` array will retain its original value.\n    Note that if an uninitialized `out` array is created via the default\n    ``out=None``, locations within it where the condition is False will\n    remain uninitialized.\n**kwargs\n    For other keyword-only arguments, see the\n    :ref:`ufunc docs <ufuncs.kwargs>`.\n\nReturns\n-------\ny : array_like\n    The sine of each element of x.\n    This is a scalar if `x` is a scalar.\n\nSee Also\n--------\narcsin, sinh, cos\n\nNotes\n-----\nThe sine is one of the fundamental functions of trigonometry (the\nmathematical study of triangles).  Consider a circle of radius 1\ncentered on the origin.  A ray comes in from the :math:`+x` axis, makes\nan angle at the origin (measured counter-clockwise from that axis), and\ndeparts from the origin.  The :math:`y` coordinate of the outgoing\nray's intersection with the unit circle is the sine of that angle.  It\nranges from -1 for :math:`x=3\\pi / 2` to +1 for :math:`\\pi / 2.`  The\nfunction has zeroes where the angle is a multiple of :math:`\\pi`.\nSines of angles between :math:`\\pi` and :math:`2\\pi` are negative.\nThe numerous properties of the sine and related functions are included\nin any standard trigonometry text.\n\nExamples\n--------\nPrint sine of one angle:\n\n>>> np.sin(np.pi/2.)\n1.0\n\nPrint sines of an array of angles given in degrees:\n\n>>> np.sin(np.array((0., 30., 45., 60., 90.)) * np.pi / 180. )\narray([ 0.        ,  0.5       ,  0.70710678,  0.8660254 ,  1.        ])\n\nPlot the sine function:\n\n>>> import matplotlib.pylab as plt\n>>> x = np.linspace(-np.pi, np.pi, 201)\n>>> plt.plot(x, np.sin(x))\n>>> plt.xlabel('Angle [rad]')\n>>> plt.ylabel('sin(x)')\n>>> plt.axis('tight')\n>>> plt.show()", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/399", "data": {"name": "numpy.linspace", "type": "callable", "signature": "(start, stop, num=50)", "description": "Return evenly spaced numbers over a specified interval.\n\nReturns `num` evenly spaced samples, calculated over the\ninterval [`start`, `stop`].\n\nThe endpoint of the interval can optionally be excluded.\n\n.. versionchanged:: 1.16.0\n    Non-scalar `start` and `stop` are now supported.\n\n.. versionchanged:: 1.20.0\n    Values are rounded towards ``-inf`` instead of ``0`` when an\n    integer ``dtype`` is specified. The old behavior can\n    still be obtained with ``np.linspace(start, stop, num).astype(int)``\n\nParameters\n----------\nstart : array_like\n    The starting value of the sequence.\nstop : array_like\n    The end value of the sequence, unless `endpoint` is set to False.\n    In that case, the sequence consists of all but the last of ``num + 1``\n    evenly spaced samples, so that `stop` is excluded.  Note that the step\n    size changes when `endpoint` is False.\nnum : int, optional\n    Number of samples to generate. Default is 50. Must be non-negative.\nendpoint : bool, optional\n    If True, `stop` is the last sample. Otherwise, it is not included.\n    Default is True.\nretstep : bool, optional\n    If True, return (`samples`, `step`), where `step` is the spacing\n    between samples.\ndtype : dtype, optional\n    The type of the output array.  If `dtype` is not given, the data type\n    is inferred from `start` and `stop`. The inferred dtype will never be\n    an integer; `float` is chosen even if the arguments would produce an\n    array of integers.\n\n    .. versionadded:: 1.9.0\n\naxis : int, optional\n    The axis in the result to store the samples.  Relevant only if start\n    or stop are array-like.  By default (0), the samples will be along a\n    new axis inserted at the beginning. Use -1 to get an axis at the end.\n\n    .. versionadded:: 1.16.0\n\nReturns\n-------\nsamples : ndarray\n    There are `num` equally spaced samples in the closed interval\n    ``[start, stop]`` or the half-open interval ``[start, stop)``\n    (depending on whether `endpoint` is True or False).\nstep : float, optional\n    Only returned if `retstep` is True\n\n    Size of spacing between samples.\n\n\nSee Also\n--------\narange : Similar to `linspace`, but uses a step size (instead of the\n         number of samples).\ngeomspace : Similar to `linspace`, but with numbers spaced evenly on a log\n            scale (a geometric progression).\nlogspace : Similar to `geomspace`, but with the end points specified as\n           logarithms.\n\nExamples\n--------\n>>> np.linspace(2.0, 3.0, num=5)\narray([2.  , 2.25, 2.5 , 2.75, 3.  ])\n>>> np.linspace(2.0, 3.0, num=5, endpoint=False)\narray([2. ,  2.2,  2.4,  2.6,  2.8])\n>>> np.linspace(2.0, 3.0, num=5, retstep=True)\n(array([2.  ,  2.25,  2.5 ,  2.75,  3.  ]), 0.25)\n\nGraphical illustration:\n\n>>> import matplotlib.pyplot as plt\n>>> N = 8\n>>> y = np.zeros(N)\n>>> x1 = np.linspace(0, 10, N, endpoint=True)\n>>> x2 = np.linspace(0, 10, N, endpoint=False)\n>>> plt.plot(x1, y, 'o')\n[<matplotlib.lines.Line2D object at 0x...>]\n>>> plt.plot(x2, y + 0.5, 'o')\n[<matplotlib.lines.Line2D object at 0x...>]\n>>> plt.ylim([-0.5, 1])\n(-0.5, 1)\n>>> plt.show()", "parameters": {"type": "object", "properties": {"start": {}, "stop": {}, "num": {"type": "integer", "default": 50}}}}}
{"task_id": "BigCodeBench/401", "data": {"name": "flask.Flask", "type": "class", "signature": "(import_name: 'str', static_url_path: 'str | None' = None, static_folder: 'str | os.PathLike[str] | None' = 'static', static_host: 'str | None' = None, host_matching: 'bool' = False, subdomain_matching: 'bool' = False, template_folder: 'str | os.PathLike[str] | None' = 'templates', instance_path: 'str | None' = None, instance_relative_config: 'bool' = False, root_path: 'str | None' = None)", "description": "The flask object implements a WSGI application and acts as the central\nobject.  It is passed the name of the module or package of the\napplication.  Once it is created it will act as a central registry for\nthe view functions, the URL rules, template configuration and much more.\n\nThe name of the package is used to resolve resources from inside the\npackage or the folder the module is contained in depending on if the\npackage parameter resolves to an actual python package (a folder with\nan :file:`__init__.py` file inside) or a standard module (just a ``.py`` file).\n\nFor more information about resource loading, see :func:`open_resource`.\n\nUsually you create a :class:`Flask` instance in your main module or\nin the :file:`__init__.py` file of your package like this::\n\n    from flask import Flask\n    app = Flask(__name__)\n\n.. admonition:: About the First Parameter\n\n    The idea of the first parameter is to give Flask an idea of what\n    belongs to your application.  This name is used to find resources\n    on the filesystem, can be used by extensions to improve debugging\n    information and a lot more.\n\n    So it's important what you provide there.  If you are using a single\n    module, `__name__` is always the correct value.  If you however are\n    using a package, it's usually recommended to hardcode the name of\n    your package there.\n\n    For example if your application is defined in :file:`yourapplication/app.py`\n    you should create it with one of the two versions below::\n\n        app = Flask('yourapplication')\n        app = Flask(__name__.split('.')[0])\n\n    Why is that?  The application will work even with `__name__`, thanks\n    to how resources are looked up.  However it will make debugging more\n    painful.  Certain extensions can make assumptions based on the\n    import name of your application.  For example the Flask-SQLAlchemy\n    extension will look for the code in your application that triggered\n    an SQL query in debug mode.  If the import name is not properly set\n    up, that debugging information is lost.  (For example it would only\n    pick up SQL queries in `yourapplication.app` and not\n    `yourapplication.views.frontend`)\n\n.. versionadded:: 0.7\n   The `static_url_path`, `static_folder`, and `template_folder`\n   parameters were added.\n\n.. versionadded:: 0.8\n   The `instance_path` and `instance_relative_config` parameters were\n   added.\n\n.. versionadded:: 0.11\n   The `root_path` parameter was added.\n\n.. versionadded:: 1.0\n   The ``host_matching`` and ``static_host`` parameters were added.\n\n.. versionadded:: 1.0\n   The ``subdomain_matching`` parameter was added. Subdomain\n   matching needs to be enabled manually now. Setting\n   :data:`SERVER_NAME` does not implicitly enable it.\n\n:param import_name: the name of the application package\n:param static_url_path: can be used to specify a different path for the\n                        static files on the web.  Defaults to the name\n                        of the `static_folder` folder.\n:param static_folder: The folder with static files that is served at\n    ``static_url_path``. Relative to the application ``root_path``\n    or an absolute path. Defaults to ``'static'``.\n:param static_host: the host to use when adding the static route.\n    Defaults to None. Required when using ``host_matching=True``\n    with a ``static_folder`` configured.\n:param host_matching: set ``url_map.host_matching`` attribute.\n    Defaults to False.\n:param subdomain_matching: consider the subdomain relative to\n    :data:`SERVER_NAME` when matching routes. Defaults to False.\n:param template_folder: the folder that contains the templates that should\n                        be used by the application.  Defaults to\n                        ``'templates'`` folder in the root path of the\n                        application.\n:param instance_path: An alternative instance path for the application.\n                      By default the folder ``'instance'`` next to the\n                      package or module is assumed to be the instance\n                      path.\n:param instance_relative_config: if set to ``True`` relative filenames\n                                 for loading the config are assumed to\n                                 be relative to the instance path instead\n                                 of the application root.\n:param root_path: The path to the root of the application files.\n    This should only be set manually when it can't be detected\n    automatically, such as for namespace packages.", "parameters": {"type": "object", "properties": {"import_name": {"type": "str"}, "static_url_path": {"type": ["str", "null"], "default": null}, "static_folder": {"type": ["null", "str", "string"], "default": "static"}, "static_host": {"type": ["str", "null"], "default": null}, "host_matching": {"type": "bool", "default": false}, "subdomain_matching": {"type": "bool", "default": false}, "template_folder": {"type": ["null", "str", "string"], "default": "templates"}, "instance_path": {"type": ["str", "null"], "default": null}, "instance_relative_config": {"type": "bool", "default": false}, "root_path": {"type": ["str", "null"], "default": null}}}}}
{"task_id": "BigCodeBench/401", "data": {"name": "flask.Flask.config", "type": "class", "signature": "(import_name: 'str', static_url_path: 'str | None' = None, static_folder: 'str | os.PathLike[str] | None' = 'static', static_host: 'str | None' = None, host_matching: 'bool' = False, subdomain_matching: 'bool' = False, template_folder: 'str | os.PathLike[str] | None' = 'templates', instance_path: 'str | None' = None, instance_relative_config: 'bool' = False, root_path: 'str | None' = None)", "description": "The flask object implements a WSGI application and acts as the central\nobject.  It is passed the name of the module or package of the\napplication.  Once it is created it will act as a central registry for\nthe view functions, the URL rules, template configuration and much more.\n\nThe name of the package is used to resolve resources from inside the\npackage or the folder the module is contained in depending on if the\npackage parameter resolves to an actual python package (a folder with\nan :file:`__init__.py` file inside) or a standard module (just a ``.py`` file).\n\nFor more information about resource loading, see :func:`open_resource`.\n\nUsually you create a :class:`Flask` instance in your main module or\nin the :file:`__init__.py` file of your package like this::\n\n    from flask import Flask\n    app = Flask(__name__)\n\n.. admonition:: About the First Parameter\n\n    The idea of the first parameter is to give Flask an idea of what\n    belongs to your application.  This name is used to find resources\n    on the filesystem, can be used by extensions to improve debugging\n    information and a lot more.\n\n    So it's important what you provide there.  If you are using a single\n    module, `__name__` is always the correct value.  If you however are\n    using a package, it's usually recommended to hardcode the name of\n    your package there.\n\n    For example if your application is defined in :file:`yourapplication/app.py`\n    you should create it with one of the two versions below::\n\n        app = Flask('yourapplication')\n        app = Flask(__name__.split('.')[0])\n\n    Why is that?  The application will work even with `__name__`, thanks\n    to how resources are looked up.  However it will make debugging more\n    painful.  Certain extensions can make assumptions based on the\n    import name of your application.  For example the Flask-SQLAlchemy\n    extension will look for the code in your application that triggered\n    an SQL query in debug mode.  If the import name is not properly set\n    up, that debugging information is lost.  (For example it would only\n    pick up SQL queries in `yourapplication.app` and not\n    `yourapplication.views.frontend`)\n\n.. versionadded:: 0.7\n   The `static_url_path`, `static_folder`, and `template_folder`\n   parameters were added.\n\n.. versionadded:: 0.8\n   The `instance_path` and `instance_relative_config` parameters were\n   added.\n\n.. versionadded:: 0.11\n   The `root_path` parameter was added.\n\n.. versionadded:: 1.0\n   The ``host_matching`` and ``static_host`` parameters were added.\n\n.. versionadded:: 1.0\n   The ``subdomain_matching`` parameter was added. Subdomain\n   matching needs to be enabled manually now. Setting\n   :data:`SERVER_NAME` does not implicitly enable it.\n\n:param import_name: the name of the application package\n:param static_url_path: can be used to specify a different path for the\n                        static files on the web.  Defaults to the name\n                        of the `static_folder` folder.\n:param static_folder: The folder with static files that is served at\n    ``static_url_path``. Relative to the application ``root_path``\n    or an absolute path. Defaults to ``'static'``.\n:param static_host: the host to use when adding the static route.\n    Defaults to None. Required when using ``host_matching=True``\n    with a ``static_folder`` configured.\n:param host_matching: set ``url_map.host_matching`` attribute.\n    Defaults to False.\n:param subdomain_matching: consider the subdomain relative to\n    :data:`SERVER_NAME` when matching routes. Defaults to False.\n:param template_folder: the folder that contains the templates that should\n                        be used by the application.  Defaults to\n                        ``'templates'`` folder in the root path of the\n                        application.\n:param instance_path: An alternative instance path for the application.\n                      By default the folder ``'instance'`` next to the\n                      package or module is assumed to be the instance\n                      path.\n:param instance_relative_config: if set to ``True`` relative filenames\n                                 for loading the config are assumed to\n                                 be relative to the instance path instead\n                                 of the application root.\n:param root_path: The path to the root of the application files.\n    This should only be set manually when it can't be detected\n    automatically, such as for namespace packages.", "parameters": {"type": "object", "properties": {"import_name": {"type": "str"}, "static_url_path": {"type": ["str", "null"], "default": null}, "static_folder": {"type": ["null", "str", "string"], "default": "static"}, "static_host": {"type": ["str", "null"], "default": null}, "host_matching": {"type": "bool", "default": false}, "subdomain_matching": {"type": "bool", "default": false}, "template_folder": {"type": ["null", "str", "string"], "default": "templates"}, "instance_path": {"type": ["str", "null"], "default": null}, "instance_relative_config": {"type": "bool", "default": false}, "root_path": {"type": ["str", "null"], "default": null}}}}}
{"task_id": "BigCodeBench/401", "data": {"name": "os.getenv", "type": "callable", "signature": "(key, default=None)", "description": "Get an environment variable, return None if it doesn't exist.\nThe optional second argument can specify an alternate default.\nkey, default and the result are str.", "parameters": {"type": "object", "properties": {"key": {}, "default": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/409", "data": {"name": "numpy.std", "type": "callable", "signature": "(a)", "description": "Compute the standard deviation along the specified axis.\n\nReturns the standard deviation, a measure of the spread of a distribution,\nof the array elements. The standard deviation is computed for the\nflattened array by default, otherwise over the specified axis.\n\nParameters\n----------\na : array_like\n    Calculate the standard deviation of these values.\naxis : None or int or tuple of ints, optional\n    Axis or axes along which the standard deviation is computed. The\n    default is to compute the standard deviation of the flattened array.\n\n    .. versionadded:: 1.7.0\n\n    If this is a tuple of ints, a standard deviation is performed over\n    multiple axes, instead of a single axis or all the axes as before.\ndtype : dtype, optional\n    Type to use in computing the standard deviation. For arrays of\n    integer type the default is float64, for arrays of float types it is\n    the same as the array type.\nout : ndarray, optional\n    Alternative output array in which to place the result. It must have\n    the same shape as the expected output but the type (of the calculated\n    values) will be cast if necessary.\nddof : int, optional\n    Means Delta Degrees of Freedom.  The divisor used in calculations\n    is ``N - ddof``, where ``N`` represents the number of elements.\n    By default `ddof` is zero.\nkeepdims : bool, optional\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\n    If the default value is passed, then `keepdims` will not be\n    passed through to the `std` method of sub-classes of\n    `ndarray`, however any non-default value will be.  If the\n    sub-class' method does not implement `keepdims` any\n    exceptions will be raised.\n\nwhere : array_like of bool, optional\n    Elements to include in the standard deviation.\n    See `~numpy.ufunc.reduce` for details.\n\n    .. versionadded:: 1.20.0\n\nReturns\n-------\nstandard_deviation : ndarray, see dtype parameter above.\n    If `out` is None, return a new array containing the standard deviation,\n    otherwise return a reference to the output array.\n\nSee Also\n--------\nvar, mean, nanmean, nanstd, nanvar\n:ref:`ufuncs-output-type`\n\nNotes\n-----\nThe standard deviation is the square root of the average of the squared\ndeviations from the mean, i.e., ``std = sqrt(mean(x))``, where\n``x = abs(a - a.mean())**2``.\n\nThe average squared deviation is typically calculated as ``x.sum() / N``,\nwhere ``N = len(x)``. If, however, `ddof` is specified, the divisor\n``N - ddof`` is used instead. In standard statistical practice, ``ddof=1``\nprovides an unbiased estimator of the variance of the infinite population.\n``ddof=0`` provides a maximum likelihood estimate of the variance for\nnormally distributed variables. The standard deviation computed in this\nfunction is the square root of the estimated variance, so even with\n``ddof=1``, it will not be an unbiased estimate of the standard deviation\nper se.\n\nNote that, for complex numbers, `std` takes the absolute\nvalue before squaring, so that the result is always real and nonnegative.\n\nFor floating-point input, the *std* is computed using the same\nprecision the input has. Depending on the input data, this can cause\nthe results to be inaccurate, especially for float32 (see example below).\nSpecifying a higher-accuracy accumulator using the `dtype` keyword can\nalleviate this issue.\n\nExamples\n--------\n>>> a = np.array([[1, 2], [3, 4]])\n>>> np.std(a)\n1.1180339887498949 # may vary\n>>> np.std(a, axis=0)\narray([1.,  1.])\n>>> np.std(a, axis=1)\narray([0.5,  0.5])\n\nIn single precision, std() can be inaccurate:\n\n>>> a = np.zeros((2, 512*512), dtype=np.float32)\n>>> a[0, :] = 1.0\n>>> a[1, :] = 0.1\n>>> np.std(a)\n0.45000005\n\nComputing the standard deviation in float64 is more accurate:\n\n>>> np.std(a, dtype=np.float64)\n0.44999999925494177 # may vary\n\nSpecifying a where argument:\n\n>>> a = np.array([[14, 8, 11, 10], [7, 9, 10, 11], [10, 15, 5, 10]])\n>>> np.std(a)\n2.614064523559687 # may vary\n>>> np.std(a, where=[[True], [True], [False]])\n2.0", "parameters": {"type": "object", "properties": {"a": {}}}}}
{"task_id": "BigCodeBench/409", "data": {"name": "numpy.mean", "type": "callable", "signature": "(a)", "description": "Compute the arithmetic mean along the specified axis.\n\nReturns the average of the array elements.  The average is taken over\nthe flattened array by default, otherwise over the specified axis.\n`float64` intermediate and return values are used for integer inputs.\n\nParameters\n----------\na : array_like\n    Array containing numbers whose mean is desired. If `a` is not an\n    array, a conversion is attempted.\naxis : None or int or tuple of ints, optional\n    Axis or axes along which the means are computed. The default is to\n    compute the mean of the flattened array.\n\n    .. versionadded:: 1.7.0\n\n    If this is a tuple of ints, a mean is performed over multiple axes,\n    instead of a single axis or all the axes as before.\ndtype : data-type, optional\n    Type to use in computing the mean.  For integer inputs, the default\n    is `float64`; for floating point inputs, it is the same as the\n    input dtype.\nout : ndarray, optional\n    Alternate output array in which to place the result.  The default\n    is ``None``; if provided, it must have the same shape as the\n    expected output, but the type will be cast if necessary.\n    See :ref:`ufuncs-output-type` for more details.\n\nkeepdims : bool, optional\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\n    If the default value is passed, then `keepdims` will not be\n    passed through to the `mean` method of sub-classes of\n    `ndarray`, however any non-default value will be.  If the\n    sub-class' method does not implement `keepdims` any\n    exceptions will be raised.\n\nwhere : array_like of bool, optional\n    Elements to include in the mean. See `~numpy.ufunc.reduce` for details.\n\n    .. versionadded:: 1.20.0\n\nReturns\n-------\nm : ndarray, see dtype parameter above\n    If `out=None`, returns a new array containing the mean values,\n    otherwise a reference to the output array is returned.\n\nSee Also\n--------\naverage : Weighted average\nstd, var, nanmean, nanstd, nanvar\n\nNotes\n-----\nThe arithmetic mean is the sum of the elements along the axis divided\nby the number of elements.\n\nNote that for floating-point input, the mean is computed using the\nsame precision the input has.  Depending on the input data, this can\ncause the results to be inaccurate, especially for `float32` (see\nexample below).  Specifying a higher-precision accumulator using the\n`dtype` keyword can alleviate this issue.\n\nBy default, `float16` results are computed using `float32` intermediates\nfor extra precision.\n\nExamples\n--------\n>>> a = np.array([[1, 2], [3, 4]])\n>>> np.mean(a)\n2.5\n>>> np.mean(a, axis=0)\narray([2., 3.])\n>>> np.mean(a, axis=1)\narray([1.5, 3.5])\n\nIn single precision, `mean` can be inaccurate:\n\n>>> a = np.zeros((2, 512*512), dtype=np.float32)\n>>> a[0, :] = 1.0\n>>> a[1, :] = 0.1\n>>> np.mean(a)\n0.54999924\n\nComputing the mean in float64 is more accurate:\n\n>>> np.mean(a, dtype=np.float64)\n0.55000000074505806 # may vary\n\nSpecifying a where argument:\n\n>>> a = np.array([[5, 9, 13], [14, 10, 12], [11, 15, 19]])\n>>> np.mean(a)\n12.0\n>>> np.mean(a, where=[[True], [False], [False]])\n9.0", "parameters": {"type": "object", "properties": {"a": {}}}}}
{"task_id": "BigCodeBench/409", "data": {"name": "numpy.median", "type": "callable", "signature": "(a)", "description": "Compute the median along the specified axis.\n\nReturns the median of the array elements.\n\nParameters\n----------\na : array_like\n    Input array or object that can be converted to an array.\naxis : {int, sequence of int, None}, optional\n    Axis or axes along which the medians are computed. The default\n    is to compute the median along a flattened version of the array.\n    A sequence of axes is supported since version 1.9.0.\nout : ndarray, optional\n    Alternative output array in which to place the result. It must\n    have the same shape and buffer length as the expected output,\n    but the type (of the output) will be cast if necessary.\noverwrite_input : bool, optional\n   If True, then allow use of memory of input array `a` for\n   calculations. The input array will be modified by the call to\n   `median`. This will save memory when you do not need to preserve\n   the contents of the input array. Treat the input as undefined,\n   but it will probably be fully or partially sorted. Default is\n   False. If `overwrite_input` is ``True`` and `a` is not already an\n   `ndarray`, an error will be raised.\nkeepdims : bool, optional\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the original `arr`.\n\n    .. versionadded:: 1.9.0\n\nReturns\n-------\nmedian : ndarray\n    A new array holding the result. If the input contains integers\n    or floats smaller than ``float64``, then the output data-type is\n    ``np.float64``.  Otherwise, the data-type of the output is the\n    same as that of the input. If `out` is specified, that array is\n    returned instead.\n\nSee Also\n--------\nmean, percentile\n\nNotes\n-----\nGiven a vector ``V`` of length ``N``, the median of ``V`` is the\nmiddle value of a sorted copy of ``V``, ``V_sorted`` - i\ne., ``V_sorted[(N-1)/2]``, when ``N`` is odd, and the average of the\ntwo middle values of ``V_sorted`` when ``N`` is even.\n\nExamples\n--------\n>>> a = np.array([[10, 7, 4], [3, 2, 1]])\n>>> a\narray([[10,  7,  4],\n       [ 3,  2,  1]])\n>>> np.median(a)\n3.5\n>>> np.median(a, axis=0)\narray([6.5, 4.5, 2.5])\n>>> np.median(a, axis=1)\narray([7.,  2.])\n>>> m = np.median(a, axis=0)\n>>> out = np.zeros_like(m)\n>>> np.median(a, axis=0, out=m)\narray([6.5,  4.5,  2.5])\n>>> m\narray([6.5,  4.5,  2.5])\n>>> b = a.copy()\n>>> np.median(b, axis=1, overwrite_input=True)\narray([7.,  2.])\n>>> assert not np.all(a==b)\n>>> b = a.copy()\n>>> np.median(b, axis=None, overwrite_input=True)\n3.5\n>>> assert not np.all(a==b)", "parameters": {"type": "object", "properties": {"a": {}}}}}
{"task_id": "BigCodeBench/409", "data": {"name": "os.path.exists", "type": "callable", "signature": "(path)", "description": "Test whether a path exists.  Returns False for broken symbolic links", "parameters": {"type": "object", "properties": {"path": {}}}}}
{"task_id": "BigCodeBench/409", "data": {"name": "os.path.join", "type": "callable", "signature": "(a, *p)", "description": "Join two or more pathname components, inserting '/' as needed.\nIf any component is an absolute path, all previous path components\nwill be discarded.  An empty last part will result in a path that\nends with a separator.", "parameters": {"type": "object", "properties": {"a": {}}}}}
{"task_id": "BigCodeBench/409", "data": {"name": "pandas.read_excel", "type": "callable", "signature": "(io)", "description": "Read an Excel file into a ``pandas`` ``DataFrame``.\n\nSupports `xls`, `xlsx`, `xlsm`, `xlsb`, `odf`, `ods` and `odt` file extensions\nread from a local filesystem or URL. Supports an option to read\na single sheet or a list of sheets.\n\nParameters\n----------\nio : str, bytes, ExcelFile, xlrd.Book, path object, or file-like object\n    Any valid string path is acceptable. The string could be a URL. Valid\n    URL schemes include http, ftp, s3, and file. For file URLs, a host is\n    expected. A local file could be: ``file://localhost/path/to/table.xlsx``.\n\n    If you want to pass in a path object, pandas accepts any ``os.PathLike``.\n\n    By file-like object, we refer to objects with a ``read()`` method,\n    such as a file handle (e.g. via builtin ``open`` function)\n    or ``StringIO``.\n\n    .. deprecated:: 2.1.0\n        Passing byte strings is deprecated. To read from a\n        byte string, wrap it in a ``BytesIO`` object.\nsheet_name : str, int, list, or None, default 0\n    Strings are used for sheet names. Integers are used in zero-indexed\n    sheet positions (chart sheets do not count as a sheet position).\n    Lists of strings/integers are used to request multiple sheets.\n    Specify ``None`` to get all worksheets.\n\n    Available cases:\n\n    * Defaults to ``0``: 1st sheet as a `DataFrame`\n    * ``1``: 2nd sheet as a `DataFrame`\n    * ``\"Sheet1\"``: Load sheet with name \"Sheet1\"\n    * ``[0, 1, \"Sheet5\"]``: Load first, second and sheet named \"Sheet5\"\n      as a dict of `DataFrame`\n    * ``None``: All worksheets.\n\nheader : int, list of int, default 0\n    Row (0-indexed) to use for the column labels of the parsed\n    DataFrame. If a list of integers is passed those row positions will\n    be combined into a ``MultiIndex``. Use None if there is no header.\nnames : array-like, default None\n    List of column names to use. If file contains no header row,\n    then you should explicitly pass header=None.\nindex_col : int, str, list of int, default None\n    Column (0-indexed) to use as the row labels of the DataFrame.\n    Pass None if there is no such column.  If a list is passed,\n    those columns will be combined into a ``MultiIndex``.  If a\n    subset of data is selected with ``usecols``, index_col\n    is based on the subset.\n\n    Missing values will be forward filled to allow roundtripping with\n    ``to_excel`` for ``merged_cells=True``. To avoid forward filling the\n    missing values use ``set_index`` after reading the data instead of\n    ``index_col``.\nusecols : str, list-like, or callable, default None\n    * If None, then parse all columns.\n    * If str, then indicates comma separated list of Excel column letters\n      and column ranges (e.g. \"A:E\" or \"A,C,E:F\"). Ranges are inclusive of\n      both sides.\n    * If list of int, then indicates list of column numbers to be parsed\n      (0-indexed).\n    * If list of string, then indicates list of column names to be parsed.\n    * If callable, then evaluate each column name against it and parse the\n      column if the callable returns ``True``.\n\n    Returns a subset of the columns according to behavior above.\ndtype : Type name or dict of column -> type, default None\n    Data type for data or columns. E.g. {'a': np.float64, 'b': np.int32}\n    Use ``object`` to preserve data as stored in Excel and not interpret dtype,\n    which will necessarily result in ``object`` dtype.\n    If converters are specified, they will be applied INSTEAD\n    of dtype conversion.\n    If you use ``None``, it will infer the dtype of each column based on the data.\nengine : {'openpyxl', 'calamine', 'odf', 'pyxlsb', 'xlrd'}, default None\n    If io is not a buffer or path, this must be set to identify io.\n    Engine compatibility :\n\n    - ``openpyxl`` supports newer Excel file formats.\n    - ``calamine`` supports Excel (.xls, .xlsx, .xlsm, .xlsb)\n      and OpenDocument (.ods) file formats.\n    - ``odf`` supports OpenDocument file formats (.odf, .ods, .odt).\n    - ``pyxlsb`` supports Binary Excel files.\n    - ``xlrd`` supports old-style Excel files (.xls).\n\n    When ``engine=None``, the following logic will be used to determine the engine:\n\n    - If ``path_or_buffer`` is an OpenDocument format (.odf, .ods, .odt),\n      then `odf <https://pypi.org/project/odfpy/>`_ will be used.\n    - Otherwise if ``path_or_buffer`` is an xls format, ``xlrd`` will be used.\n    - Otherwise if ``path_or_buffer`` is in xlsb format, ``pyxlsb`` will be used.\n    - Otherwise ``openpyxl`` will be used.\nconverters : dict, default None\n    Dict of functions for converting values in certain columns. Keys can\n    either be integers or column labels, values are functions that take one\n    input argument, the Excel cell content, and return the transformed\n    content.\ntrue_values : list, default None\n    Values to consider as True.\nfalse_values : list, default None\n    Values to consider as False.\nskiprows : list-like, int, or callable, optional\n    Line numbers to skip (0-indexed) or number of lines to skip (int) at the\n    start of the file. If callable, the callable function will be evaluated\n    against the row indices, returning True if the row should be skipped and\n    False otherwise. An example of a valid callable argument would be ``lambda\n    x: x in [0, 2]``.\nnrows : int, default None\n    Number of rows to parse.\nna_values : scalar, str, list-like, or dict, default None\n    Additional strings to recognize as NA/NaN. If dict passed, specific\n    per-column NA values. By default the following values are interpreted\n    as NaN: '', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan',\n    '1.#IND', '1.#QNAN', '<NA>', 'N/A', 'NA', 'NULL', 'NaN', 'None',\n    'n/a', 'nan', 'null'.\nkeep_default_na : bool, default True\n    Whether or not to include the default NaN values when parsing the data.\n    Depending on whether ``na_values`` is passed in, the behavior is as follows:\n\n    * If ``keep_default_na`` is True, and ``na_values`` are specified,\n      ``na_values`` is appended to the default NaN values used for parsing.\n    * If ``keep_default_na`` is True, and ``na_values`` are not specified, only\n      the default NaN values are used for parsing.\n    * If ``keep_default_na`` is False, and ``na_values`` are specified, only\n      the NaN values specified ``na_values`` are used for parsing.\n    * If ``keep_default_na`` is False, and ``na_values`` are not specified, no\n      strings will be parsed as NaN.\n\n    Note that if `na_filter` is passed in as False, the ``keep_default_na`` and\n    ``na_values`` parameters will be ignored.\nna_filter : bool, default True\n    Detect missing value markers (empty strings and the value of na_values). In\n    data without any NAs, passing ``na_filter=False`` can improve the\n    performance of reading a large file.\nverbose : bool, default False\n    Indicate number of NA values placed in non-numeric columns.\nparse_dates : bool, list-like, or dict, default False\n    The behavior is as follows:\n\n    * ``bool``. If True -> try parsing the index.\n    * ``list`` of int or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3\n      each as a separate date column.\n    * ``list`` of lists. e.g.  If [[1, 3]] -> combine columns 1 and 3 and parse as\n      a single date column.\n    * ``dict``, e.g. {'foo' : [1, 3]} -> parse columns 1, 3 as date and call\n      result 'foo'\n\n    If a column or index contains an unparsable date, the entire column or\n    index will be returned unaltered as an object data type. If you don`t want to\n    parse some cells as date just change their type in Excel to \"Text\".\n    For non-standard datetime parsing, use ``pd.to_datetime`` after ``pd.read_excel``.\n\n    Note: A fast-path exists for iso8601-formatted dates.\ndate_parser : function, optional\n    Function to use for converting a sequence of string columns to an array of\n    datetime instances. The default uses ``dateutil.parser.parser`` to do the\n    conversion. Pandas will try to call `date_parser` in three different ways,\n    advancing to the next if an exception occurs: 1) Pass one or more arrays\n    (as defined by `parse_dates`) as arguments; 2) concatenate (row-wise) the\n    string values from the columns defined by `parse_dates` into a single array\n    and pass that; and 3) call `date_parser` once for each row using one or\n    more strings (corresponding to the columns defined by `parse_dates`) as\n    arguments.\n\n    .. deprecated:: 2.0.0\n       Use ``date_format`` instead, or read in as ``object`` and then apply\n       :func:`to_datetime` as-needed.\ndate_format : str or dict of column -> format, default ``None``\n   If used in conjunction with ``parse_dates``, will parse dates according to this\n   format. For anything more complex,\n   please read in as ``object`` and then apply :func:`to_datetime` as-needed.\n\n   .. versionadded:: 2.0.0\nthousands : str, default None\n    Thousands separator for parsing string columns to numeric.  Note that\n    this parameter is only necessary for columns stored as TEXT in Excel,\n    any numeric columns will automatically be parsed, regardless of display\n    format.\ndecimal : str, default '.'\n    Character to recognize as decimal point for parsing string columns to numeric.\n    Note that this parameter is only necessary for columns stored as TEXT in Excel,\n    any numeric columns will automatically be parsed, regardless of display\n    format.(e.g. use ',' for European data).\n\n    .. versionadded:: 1.4.0\n\ncomment : str, default None\n    Comments out remainder of line. Pass a character or characters to this\n    argument to indicate comments in the input file. Any data between the\n    comment string and the end of the current line is ignored.\nskipfooter : int, default 0\n    Rows at the end to skip (0-indexed).\nstorage_options : dict, optional\n    Extra options that make sense for a particular storage connection, e.g.\n    host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n    are forwarded to ``urllib.request.Request`` as header options. For other\n    URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n    forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n    details, and for more examples on storage options refer `here\n    <https://pandas.pydata.org/docs/user_guide/io.html?\n    highlight=storage_options#reading-writing-remote-files>`_.\n\ndtype_backend : {'numpy_nullable', 'pyarrow'}, default 'numpy_nullable'\n    Back-end data type applied to the resultant :class:`DataFrame`\n    (still experimental). Behaviour is as follows:\n\n    * ``\"numpy_nullable\"``: returns nullable-dtype-backed :class:`DataFrame`\n      (default).\n    * ``\"pyarrow\"``: returns pyarrow-backed nullable :class:`ArrowDtype`\n      DataFrame.\n\n    .. versionadded:: 2.0\n\nengine_kwargs : dict, optional\n    Arbitrary keyword arguments passed to excel engine.\n\nReturns\n-------\nDataFrame or dict of DataFrames\n    DataFrame from the passed in Excel file. See notes in sheet_name\n    argument for more information on when a dict of DataFrames is returned.\n\nSee Also\n--------\nDataFrame.to_excel : Write DataFrame to an Excel file.\nDataFrame.to_csv : Write DataFrame to a comma-separated values (csv) file.\nread_csv : Read a comma-separated values (csv) file into DataFrame.\nread_fwf : Read a table of fixed-width formatted lines into DataFrame.\n\nNotes\n-----\nFor specific information on the methods used for each Excel engine, refer to the pandas\n:ref:`user guide <io.excel_reader>`\n\nExamples\n--------\nThe file can be read using the file name as string or an open file object:\n\n>>> pd.read_excel('tmp.xlsx', index_col=0)  # doctest: +SKIP\n       Name  Value\n0   string1      1\n1   string2      2\n2  #Comment      3\n\n>>> pd.read_excel(open('tmp.xlsx', 'rb'),\n...               sheet_name='Sheet3')  # doctest: +SKIP\n   Unnamed: 0      Name  Value\n0           0   string1      1\n1           1   string2      2\n2           2  #Comment      3\n\nIndex and header can be specified via the `index_col` and `header` arguments\n\n>>> pd.read_excel('tmp.xlsx', index_col=None, header=None)  # doctest: +SKIP\n     0         1      2\n0  NaN      Name  Value\n1  0.0   string1      1\n2  1.0   string2      2\n3  2.0  #Comment      3\n\nColumn types are inferred but can be explicitly specified\n\n>>> pd.read_excel('tmp.xlsx', index_col=0,\n...               dtype={'Name': str, 'Value': float})  # doctest: +SKIP\n       Name  Value\n0   string1    1.0\n1   string2    2.0\n2  #Comment    3.0\n\nTrue, False, and NA values, and thousands separators have defaults,\nbut can be explicitly specified, too. Supply the values you would like\nas strings or lists of strings!\n\n>>> pd.read_excel('tmp.xlsx', index_col=0,\n...               na_values=['string1', 'string2'])  # doctest: +SKIP\n       Name  Value\n0       NaN      1\n1       NaN      2\n2  #Comment      3\n\nComment lines in the excel input file can be skipped using the\n``comment`` kwarg.\n\n>>> pd.read_excel('tmp.xlsx', index_col=0, comment='#')  # doctest: +SKIP\n      Name  Value\n0  string1    1.0\n1  string2    2.0\n2     None    NaN", "parameters": {"type": "object", "properties": {"io": {}}}}}
{"task_id": "BigCodeBench/409", "data": {"name": "pandas.read_excel.columns", "type": "callable", "signature": "(io, sheet_name: 'str | int | list[IntStrT] | None' = 0, *, header: 'int | Sequence[int] | None' = 0, names: 'SequenceNotStr[Hashable] | range | None' = None, index_col: 'int | str | Sequence[int] | None' = None, usecols: 'int | str | Sequence[int] | Sequence[str] | Callable[[str], bool] | None' = None, dtype: 'DtypeArg | None' = None, engine: \"Literal['xlrd', 'openpyxl', 'odf', 'pyxlsb', 'calamine'] | None\" = None, converters: 'dict[str, Callable] | dict[int, Callable] | None' = None, true_values: 'Iterable[Hashable] | None' = None, false_values: 'Iterable[Hashable] | None' = None, skiprows: 'Sequence[int] | int | Callable[[int], object] | None' = None, nrows: 'int | None' = None, na_values=None, keep_default_na: 'bool' = True, na_filter: 'bool' = True, verbose: 'bool' = False, parse_dates: 'list | dict | bool' = False, date_parser: 'Callable | lib.NoDefault' = <no_default>, date_format: 'dict[Hashable, str] | str | None' = None, thousands: 'str | None' = None, decimal: 'str' = '.', comment: 'str | None' = None, skipfooter: 'int' = 0, storage_options: 'StorageOptions | None' = None, dtype_backend: 'DtypeBackend | lib.NoDefault' = <no_default>, engine_kwargs: 'dict | None' = None) -> 'DataFrame | dict[IntStrT, DataFrame]'", "description": "Read an Excel file into a ``pandas`` ``DataFrame``.\n\nSupports `xls`, `xlsx`, `xlsm`, `xlsb`, `odf`, `ods` and `odt` file extensions\nread from a local filesystem or URL. Supports an option to read\na single sheet or a list of sheets.\n\nParameters\n----------\nio : str, bytes, ExcelFile, xlrd.Book, path object, or file-like object\n    Any valid string path is acceptable. The string could be a URL. Valid\n    URL schemes include http, ftp, s3, and file. For file URLs, a host is\n    expected. A local file could be: ``file://localhost/path/to/table.xlsx``.\n\n    If you want to pass in a path object, pandas accepts any ``os.PathLike``.\n\n    By file-like object, we refer to objects with a ``read()`` method,\n    such as a file handle (e.g. via builtin ``open`` function)\n    or ``StringIO``.\n\n    .. deprecated:: 2.1.0\n        Passing byte strings is deprecated. To read from a\n        byte string, wrap it in a ``BytesIO`` object.\nsheet_name : str, int, list, or None, default 0\n    Strings are used for sheet names. Integers are used in zero-indexed\n    sheet positions (chart sheets do not count as a sheet position).\n    Lists of strings/integers are used to request multiple sheets.\n    Specify ``None`` to get all worksheets.\n\n    Available cases:\n\n    * Defaults to ``0``: 1st sheet as a `DataFrame`\n    * ``1``: 2nd sheet as a `DataFrame`\n    * ``\"Sheet1\"``: Load sheet with name \"Sheet1\"\n    * ``[0, 1, \"Sheet5\"]``: Load first, second and sheet named \"Sheet5\"\n      as a dict of `DataFrame`\n    * ``None``: All worksheets.\n\nheader : int, list of int, default 0\n    Row (0-indexed) to use for the column labels of the parsed\n    DataFrame. If a list of integers is passed those row positions will\n    be combined into a ``MultiIndex``. Use None if there is no header.\nnames : array-like, default None\n    List of column names to use. If file contains no header row,\n    then you should explicitly pass header=None.\nindex_col : int, str, list of int, default None\n    Column (0-indexed) to use as the row labels of the DataFrame.\n    Pass None if there is no such column.  If a list is passed,\n    those columns will be combined into a ``MultiIndex``.  If a\n    subset of data is selected with ``usecols``, index_col\n    is based on the subset.\n\n    Missing values will be forward filled to allow roundtripping with\n    ``to_excel`` for ``merged_cells=True``. To avoid forward filling the\n    missing values use ``set_index`` after reading the data instead of\n    ``index_col``.\nusecols : str, list-like, or callable, default None\n    * If None, then parse all columns.\n    * If str, then indicates comma separated list of Excel column letters\n      and column ranges (e.g. \"A:E\" or \"A,C,E:F\"). Ranges are inclusive of\n      both sides.\n    * If list of int, then indicates list of column numbers to be parsed\n      (0-indexed).\n    * If list of string, then indicates list of column names to be parsed.\n    * If callable, then evaluate each column name against it and parse the\n      column if the callable returns ``True``.\n\n    Returns a subset of the columns according to behavior above.\ndtype : Type name or dict of column -> type, default None\n    Data type for data or columns. E.g. {'a': np.float64, 'b': np.int32}\n    Use ``object`` to preserve data as stored in Excel and not interpret dtype,\n    which will necessarily result in ``object`` dtype.\n    If converters are specified, they will be applied INSTEAD\n    of dtype conversion.\n    If you use ``None``, it will infer the dtype of each column based on the data.\nengine : {'openpyxl', 'calamine', 'odf', 'pyxlsb', 'xlrd'}, default None\n    If io is not a buffer or path, this must be set to identify io.\n    Engine compatibility :\n\n    - ``openpyxl`` supports newer Excel file formats.\n    - ``calamine`` supports Excel (.xls, .xlsx, .xlsm, .xlsb)\n      and OpenDocument (.ods) file formats.\n    - ``odf`` supports OpenDocument file formats (.odf, .ods, .odt).\n    - ``pyxlsb`` supports Binary Excel files.\n    - ``xlrd`` supports old-style Excel files (.xls).\n\n    When ``engine=None``, the following logic will be used to determine the engine:\n\n    - If ``path_or_buffer`` is an OpenDocument format (.odf, .ods, .odt),\n      then `odf <https://pypi.org/project/odfpy/>`_ will be used.\n    - Otherwise if ``path_or_buffer`` is an xls format, ``xlrd`` will be used.\n    - Otherwise if ``path_or_buffer`` is in xlsb format, ``pyxlsb`` will be used.\n    - Otherwise ``openpyxl`` will be used.\nconverters : dict, default None\n    Dict of functions for converting values in certain columns. Keys can\n    either be integers or column labels, values are functions that take one\n    input argument, the Excel cell content, and return the transformed\n    content.\ntrue_values : list, default None\n    Values to consider as True.\nfalse_values : list, default None\n    Values to consider as False.\nskiprows : list-like, int, or callable, optional\n    Line numbers to skip (0-indexed) or number of lines to skip (int) at the\n    start of the file. If callable, the callable function will be evaluated\n    against the row indices, returning True if the row should be skipped and\n    False otherwise. An example of a valid callable argument would be ``lambda\n    x: x in [0, 2]``.\nnrows : int, default None\n    Number of rows to parse.\nna_values : scalar, str, list-like, or dict, default None\n    Additional strings to recognize as NA/NaN. If dict passed, specific\n    per-column NA values. By default the following values are interpreted\n    as NaN: '', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan',\n    '1.#IND', '1.#QNAN', '<NA>', 'N/A', 'NA', 'NULL', 'NaN', 'None',\n    'n/a', 'nan', 'null'.\nkeep_default_na : bool, default True\n    Whether or not to include the default NaN values when parsing the data.\n    Depending on whether ``na_values`` is passed in, the behavior is as follows:\n\n    * If ``keep_default_na`` is True, and ``na_values`` are specified,\n      ``na_values`` is appended to the default NaN values used for parsing.\n    * If ``keep_default_na`` is True, and ``na_values`` are not specified, only\n      the default NaN values are used for parsing.\n    * If ``keep_default_na`` is False, and ``na_values`` are specified, only\n      the NaN values specified ``na_values`` are used for parsing.\n    * If ``keep_default_na`` is False, and ``na_values`` are not specified, no\n      strings will be parsed as NaN.\n\n    Note that if `na_filter` is passed in as False, the ``keep_default_na`` and\n    ``na_values`` parameters will be ignored.\nna_filter : bool, default True\n    Detect missing value markers (empty strings and the value of na_values). In\n    data without any NAs, passing ``na_filter=False`` can improve the\n    performance of reading a large file.\nverbose : bool, default False\n    Indicate number of NA values placed in non-numeric columns.\nparse_dates : bool, list-like, or dict, default False\n    The behavior is as follows:\n\n    * ``bool``. If True -> try parsing the index.\n    * ``list`` of int or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3\n      each as a separate date column.\n    * ``list`` of lists. e.g.  If [[1, 3]] -> combine columns 1 and 3 and parse as\n      a single date column.\n    * ``dict``, e.g. {'foo' : [1, 3]} -> parse columns 1, 3 as date and call\n      result 'foo'\n\n    If a column or index contains an unparsable date, the entire column or\n    index will be returned unaltered as an object data type. If you don`t want to\n    parse some cells as date just change their type in Excel to \"Text\".\n    For non-standard datetime parsing, use ``pd.to_datetime`` after ``pd.read_excel``.\n\n    Note: A fast-path exists for iso8601-formatted dates.\ndate_parser : function, optional\n    Function to use for converting a sequence of string columns to an array of\n    datetime instances. The default uses ``dateutil.parser.parser`` to do the\n    conversion. Pandas will try to call `date_parser` in three different ways,\n    advancing to the next if an exception occurs: 1) Pass one or more arrays\n    (as defined by `parse_dates`) as arguments; 2) concatenate (row-wise) the\n    string values from the columns defined by `parse_dates` into a single array\n    and pass that; and 3) call `date_parser` once for each row using one or\n    more strings (corresponding to the columns defined by `parse_dates`) as\n    arguments.\n\n    .. deprecated:: 2.0.0\n       Use ``date_format`` instead, or read in as ``object`` and then apply\n       :func:`to_datetime` as-needed.\ndate_format : str or dict of column -> format, default ``None``\n   If used in conjunction with ``parse_dates``, will parse dates according to this\n   format. For anything more complex,\n   please read in as ``object`` and then apply :func:`to_datetime` as-needed.\n\n   .. versionadded:: 2.0.0\nthousands : str, default None\n    Thousands separator for parsing string columns to numeric.  Note that\n    this parameter is only necessary for columns stored as TEXT in Excel,\n    any numeric columns will automatically be parsed, regardless of display\n    format.\ndecimal : str, default '.'\n    Character to recognize as decimal point for parsing string columns to numeric.\n    Note that this parameter is only necessary for columns stored as TEXT in Excel,\n    any numeric columns will automatically be parsed, regardless of display\n    format.(e.g. use ',' for European data).\n\n    .. versionadded:: 1.4.0\n\ncomment : str, default None\n    Comments out remainder of line. Pass a character or characters to this\n    argument to indicate comments in the input file. Any data between the\n    comment string and the end of the current line is ignored.\nskipfooter : int, default 0\n    Rows at the end to skip (0-indexed).\nstorage_options : dict, optional\n    Extra options that make sense for a particular storage connection, e.g.\n    host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n    are forwarded to ``urllib.request.Request`` as header options. For other\n    URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n    forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n    details, and for more examples on storage options refer `here\n    <https://pandas.pydata.org/docs/user_guide/io.html?\n    highlight=storage_options#reading-writing-remote-files>`_.\n\ndtype_backend : {'numpy_nullable', 'pyarrow'}, default 'numpy_nullable'\n    Back-end data type applied to the resultant :class:`DataFrame`\n    (still experimental). Behaviour is as follows:\n\n    * ``\"numpy_nullable\"``: returns nullable-dtype-backed :class:`DataFrame`\n      (default).\n    * ``\"pyarrow\"``: returns pyarrow-backed nullable :class:`ArrowDtype`\n      DataFrame.\n\n    .. versionadded:: 2.0\n\nengine_kwargs : dict, optional\n    Arbitrary keyword arguments passed to excel engine.\n\nReturns\n-------\nDataFrame or dict of DataFrames\n    DataFrame from the passed in Excel file. See notes in sheet_name\n    argument for more information on when a dict of DataFrames is returned.\n\nSee Also\n--------\nDataFrame.to_excel : Write DataFrame to an Excel file.\nDataFrame.to_csv : Write DataFrame to a comma-separated values (csv) file.\nread_csv : Read a comma-separated values (csv) file into DataFrame.\nread_fwf : Read a table of fixed-width formatted lines into DataFrame.\n\nNotes\n-----\nFor specific information on the methods used for each Excel engine, refer to the pandas\n:ref:`user guide <io.excel_reader>`\n\nExamples\n--------\nThe file can be read using the file name as string or an open file object:\n\n>>> pd.read_excel('tmp.xlsx', index_col=0)  # doctest: +SKIP\n       Name  Value\n0   string1      1\n1   string2      2\n2  #Comment      3\n\n>>> pd.read_excel(open('tmp.xlsx', 'rb'),\n...               sheet_name='Sheet3')  # doctest: +SKIP\n   Unnamed: 0      Name  Value\n0           0   string1      1\n1           1   string2      2\n2           2  #Comment      3\n\nIndex and header can be specified via the `index_col` and `header` arguments\n\n>>> pd.read_excel('tmp.xlsx', index_col=None, header=None)  # doctest: +SKIP\n     0         1      2\n0  NaN      Name  Value\n1  0.0   string1      1\n2  1.0   string2      2\n3  2.0  #Comment      3\n\nColumn types are inferred but can be explicitly specified\n\n>>> pd.read_excel('tmp.xlsx', index_col=0,\n...               dtype={'Name': str, 'Value': float})  # doctest: +SKIP\n       Name  Value\n0   string1    1.0\n1   string2    2.0\n2  #Comment    3.0\n\nTrue, False, and NA values, and thousands separators have defaults,\nbut can be explicitly specified, too. Supply the values you would like\nas strings or lists of strings!\n\n>>> pd.read_excel('tmp.xlsx', index_col=0,\n...               na_values=['string1', 'string2'])  # doctest: +SKIP\n       Name  Value\n0       NaN      1\n1       NaN      2\n2  #Comment      3\n\nComment lines in the excel input file can be skipped using the\n``comment`` kwarg.\n\n>>> pd.read_excel('tmp.xlsx', index_col=0, comment='#')  # doctest: +SKIP\n      Name  Value\n0  string1    1.0\n1  string2    2.0\n2     None    NaN", "parameters": {"type": "object", "properties": {"io": {}, "sheet_name": {"type": ["integer", "list[intstrt]", "str", "null"], "default": 0}, "header": {"type": ["integer", "sequence[int]", "null"], "default": 0}, "names": {"type": ["range", "sequencenotstr[hashable]", "null"], "default": null}, "index_col": {"type": ["integer", "sequence[int]", "str", "null"], "default": null}, "usecols": {"type": ["sequence[int]", "callable[[str], bool]", "str", "null", "integer", "sequence[str]"], "default": null}, "dtype": {"type": ["dtypearg", "null"], "default": null}, "engine": {"type": ["none\"", "\"literal['xlrd', 'openpyxl', 'odf', 'pyxlsb', 'calamine']"], "default": null}, "converters": {"type": ["dict[str, callable]", "dict[int, callable]", "null"], "default": null}, "true_values": {"type": ["iterable[hashable]", "null"], "default": null}, "false_values": {"type": ["iterable[hashable]", "null"], "default": null}, "skiprows": {"type": ["callable[[int], object]", "integer", "sequence[int]", "null"], "default": null}, "nrows": {"type": ["integer", "null"], "default": null}, "na_values": {"type": "NoneType", "default": null}, "keep_default_na": {"type": "bool", "default": true}, "na_filter": {"type": "bool", "default": true}, "verbose": {"type": "bool", "default": false}, "parse_dates": {"type": ["list", "bool", "dict"], "default": false}, "date_parser": {"type": ["callable", "lib.nodefault"], "default": "<no_default>"}, "date_format": {"type": ["dict[hashable, str]", "str", "null"], "default": null}, "thousands": {"type": ["str", "null"], "default": null}, "decimal": {"type": "str", "default": "."}, "comment": {"type": ["str", "null"], "default": null}, "skipfooter": {"type": "integer", "default": 0}, "storage_options": {"type": ["storageoptions", "null"], "default": null}, "dtype_backend": {"type": ["dtypebackend", "lib.nodefault"], "default": "<no_default>"}, "engine_kwargs": {"type": ["null", "dict"], "default": null}}}}}
{"task_id": "BigCodeBench/417", "data": {"name": "keras.optimizers.SGD(learning_rate=0.1)"}}
{"task_id": "BigCodeBench/417", "data": {"name": "keras.layers.Dense(input_dim=2, units=1, activation='sigmoid')"}}
{"task_id": "BigCodeBench/417", "data": {"name": "keras.models.Sequential([Dense(input_dim=2, units=1, activation='sigmoid')])"}}
{"task_id": "BigCodeBench/417", "data": {"name": "keras.models.Sequential.compile(loss='binary_crossentropy', optimizer=SGD(learning_rate=0.1))"}}
{"task_id": "BigCodeBench/417", "data": {"name": "keras.models.Sequential.fit(X_train, Y_train, epochs=200, batch_size=1, verbose=0, validation_data=(X_test, Y_test))"}}
{"task_id": "BigCodeBench/417", "data": {"name": "matplotlib.pyplot.subplots", "type": "class", "signature": "()", "description": "Create a figure and a set of subplots.\n\nThis utility wrapper makes it convenient to create common layouts of\nsubplots, including the enclosing figure object, in a single call.\n\nParameters\n----------\nnrows, ncols : int, default: 1\n    Number of rows/columns of the subplot grid.\n\nsharex, sharey : bool or {'none', 'all', 'row', 'col'}, default: False\n    Controls sharing of properties among x (*sharex*) or y (*sharey*)\n    axes:\n\n    - True or 'all': x- or y-axis will be shared among all subplots.\n    - False or 'none': each subplot x- or y-axis will be independent.\n    - 'row': each subplot row will share an x- or y-axis.\n    - 'col': each subplot column will share an x- or y-axis.\n\n    When subplots have a shared x-axis along a column, only the x tick\n    labels of the bottom subplot are created. Similarly, when subplots\n    have a shared y-axis along a row, only the y tick labels of the first\n    column subplot are created. To later turn other subplots' ticklabels\n    on, use `~matplotlib.axes.Axes.tick_params`.\n\n    When subplots have a shared axis that has units, calling\n    `~matplotlib.axis.Axis.set_units` will update each axis with the\n    new units.\n\nsqueeze : bool, default: True\n    - If True, extra dimensions are squeezed out from the returned\n      array of `~matplotlib.axes.Axes`:\n\n      - if only one subplot is constructed (nrows=ncols=1), the\n        resulting single Axes object is returned as a scalar.\n      - for Nx1 or 1xM subplots, the returned object is a 1D numpy\n        object array of Axes objects.\n      - for NxM, subplots with N>1 and M>1 are returned as a 2D array.\n\n    - If False, no squeezing at all is done: the returned Axes object is\n      always a 2D array containing Axes instances, even if it ends up\n      being 1x1.\n\nwidth_ratios : array-like of length *ncols*, optional\n    Defines the relative widths of the columns. Each column gets a\n    relative width of ``width_ratios[i] / sum(width_ratios)``.\n    If not given, all columns will have the same width.  Equivalent\n    to ``gridspec_kw={'width_ratios': [...]}``.\n\nheight_ratios : array-like of length *nrows*, optional\n    Defines the relative heights of the rows. Each row gets a\n    relative height of ``height_ratios[i] / sum(height_ratios)``.\n    If not given, all rows will have the same height. Convenience\n    for ``gridspec_kw={'height_ratios': [...]}``.\n\nsubplot_kw : dict, optional\n    Dict with keywords passed to the\n    `~matplotlib.figure.Figure.add_subplot` call used to create each\n    subplot.\n\ngridspec_kw : dict, optional\n    Dict with keywords passed to the `~matplotlib.gridspec.GridSpec`\n    constructor used to create the grid the subplots are placed on.\n\n**fig_kw\n    All additional keyword arguments are passed to the\n    `.pyplot.figure` call.\n\nReturns\n-------\nfig : `.Figure`\n\nax : `~matplotlib.axes.Axes` or array of Axes\n    *ax* can be either a single `~.axes.Axes` object, or an array of Axes\n    objects if more than one subplot was created.  The dimensions of the\n    resulting array can be controlled with the squeeze keyword, see above.\n\n    Typical idioms for handling the return value are::\n\n        # using the variable ax for single a Axes\n        fig, ax = plt.subplots()\n\n        # using the variable axs for multiple Axes\n        fig, axs = plt.subplots(2, 2)\n\n        # using tuple unpacking for multiple Axes\n        fig, (ax1, ax2) = plt.subplots(1, 2)\n        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n\n    The names ``ax`` and pluralized ``axs`` are preferred over ``axes``\n    because for the latter it's not clear if it refers to a single\n    `~.axes.Axes` instance or a collection of these.\n\nSee Also\n--------\n.pyplot.figure\n.pyplot.subplot\n.pyplot.axes\n.Figure.subplots\n.Figure.add_subplot\n\nExamples\n--------\n::\n\n    # First create some toy data:\n    x = np.linspace(0, 2*np.pi, 400)\n    y = np.sin(x**2)\n\n    # Create just a figure and only one subplot\n    fig, ax = plt.subplots()\n    ax.plot(x, y)\n    ax.set_title('Simple plot')\n\n    # Create two subplots and unpack the output array immediately\n    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n    ax1.plot(x, y)\n    ax1.set_title('Sharing Y axis')\n    ax2.scatter(x, y)\n\n    # Create four polar axes and access them through the returned array\n    fig, axs = plt.subplots(2, 2, subplot_kw=dict(projection=\"polar\"))\n    axs[0, 0].plot(x, y)\n    axs[1, 1].scatter(x, y)\n\n    # Share a X axis with each column of subplots\n    plt.subplots(2, 2, sharex='col')\n\n    # Share a Y axis with each row of subplots\n    plt.subplots(2, 2, sharey='row')\n\n    # Share both X and Y axes with all subplots\n    plt.subplots(2, 2, sharex='all', sharey='all')\n\n    # Note that this is the same as\n    plt.subplots(2, 2, sharex=True, sharey=True)\n\n    # Create figure number 10 with a single subplot\n    # and clears it if it already exists.\n    fig, ax = plt.subplots(num=10, clear=True)", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/417", "data": {"name": "matplotlib.pyplot.subplots[1].set_ylabel", "type": "method", "signature": "(ylabel, fontdict=None, labelpad=None, *, loc=None, **kwargs)", "description": "Set the label for the y-axis.\n\nParameters\n----------\nylabel : str\n    The label text.\n\nlabelpad : float, default: :rc:`axes.labelpad`\n    Spacing in points from the Axes bounding box including ticks\n    and tick labels.  If None, the previous value is left as is.\n\nloc : {'bottom', 'center', 'top'}, default: :rc:`yaxis.labellocation`\n    The label position. This is a high-level alternative for passing\n    parameters *y* and *horizontalalignment*.\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    `.Text` properties control the appearance of the label.\n\nSee Also\n--------\ntext : Documents the properties supported by `.Text`.", "parameters": {"type": "object", "properties": {"ylabel": {}, "fontdict": {"type": "NoneType", "default": null}, "labelpad": {"type": "NoneType", "default": null}, "loc": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/417", "data": {"name": "matplotlib.pyplot.subplots[1].set_xlabel", "type": "method", "signature": "(xlabel, fontdict=None, labelpad=None, *, loc=None, **kwargs)", "description": "Set the label for the x-axis.\n\nParameters\n----------\nxlabel : str\n    The label text.\n\nlabelpad : float, default: :rc:`axes.labelpad`\n    Spacing in points from the Axes bounding box including ticks\n    and tick labels.  If None, the previous value is left as is.\n\nloc : {'left', 'center', 'right'}, default: :rc:`xaxis.labellocation`\n    The label position. This is a high-level alternative for passing\n    parameters *x* and *horizontalalignment*.\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    `.Text` properties control the appearance of the label.\n\nSee Also\n--------\ntext : Documents the properties supported by `.Text`.", "parameters": {"type": "object", "properties": {"xlabel": {}, "fontdict": {"type": "NoneType", "default": null}, "labelpad": {"type": "NoneType", "default": null}, "loc": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/417", "data": {"name": "matplotlib.pyplot.subplots[1].set_title", "type": "method", "signature": "(label, fontdict=None, loc=None, pad=None, *, y=None, **kwargs)", "description": "Set a title for the Axes.\n\nSet one of the three available Axes titles. The available titles\nare positioned above the Axes in the center, flush with the left\nedge, and flush with the right edge.\n\nParameters\n----------\nlabel : str\n    Text to use for the title\n\nfontdict : dict\n\n    .. admonition:: Discouraged\n\n       The use of *fontdict* is discouraged. Parameters should be passed as\n       individual keyword arguments or using dictionary-unpacking\n       ``set_title(..., **fontdict)``.\n\n    A dictionary controlling the appearance of the title text,\n    the default *fontdict* is::\n\n       {'fontsize': rcParams['axes.titlesize'],\n        'fontweight': rcParams['axes.titleweight'],\n        'color': rcParams['axes.titlecolor'],\n        'verticalalignment': 'baseline',\n        'horizontalalignment': loc}\n\nloc : {'center', 'left', 'right'}, default: :rc:`axes.titlelocation`\n    Which title to set.\n\ny : float, default: :rc:`axes.titley`\n    Vertical Axes location for the title (1.0 is the top).  If\n    None (the default) and :rc:`axes.titley` is also None, y is\n    determined automatically to avoid decorators on the Axes.\n\npad : float, default: :rc:`axes.titlepad`\n    The offset of the title from the top of the Axes, in points.\n\nReturns\n-------\n`.Text`\n    The matplotlib text instance representing the title\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    Other keyword arguments are text properties, see `.Text` for a list\n    of valid text properties.", "parameters": {"type": "object", "properties": {"label": {}, "fontdict": {"type": "NoneType", "default": null}, "loc": {"type": "NoneType", "default": null}, "pad": {"type": "NoneType", "default": null}, "y": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/417", "data": {"name": "matplotlib.pyplot.subplots[1].legend", "type": "method", "signature": "(*args, **kwargs)", "description": "Place a legend on the Axes.\n\nCall signatures::\n\n    legend()\n    legend(handles, labels)\n    legend(handles=handles)\n    legend(labels)\n\nThe call signatures correspond to the following different ways to use\nthis method:\n\n**1. Automatic detection of elements to be shown in the legend**\n\nThe elements to be added to the legend are automatically determined,\nwhen you do not pass in any extra arguments.\n\nIn this case, the labels are taken from the artist. You can specify\nthem either at artist creation or by calling the\n:meth:`~.Artist.set_label` method on the artist::\n\n    ax.plot([1, 2, 3], label='Inline label')\n    ax.legend()\n\nor::\n\n    line, = ax.plot([1, 2, 3])\n    line.set_label('Label via method')\n    ax.legend()\n\n.. note::\n    Specific artists can be excluded from the automatic legend element\n    selection by using a label starting with an underscore, \"_\".\n    A string starting with an underscore is the default label for all\n    artists, so calling `.Axes.legend` without any arguments and\n    without setting the labels manually will result in no legend being\n    drawn.\n\n\n**2. Explicitly listing the artists and labels in the legend**\n\nFor full control of which artists have a legend entry, it is possible\nto pass an iterable of legend artists followed by an iterable of\nlegend labels respectively::\n\n    ax.legend([line1, line2, line3], ['label1', 'label2', 'label3'])\n\n\n**3. Explicitly listing the artists in the legend**\n\nThis is similar to 2, but the labels are taken from the artists'\nlabel properties. Example::\n\n    line1, = ax.plot([1, 2, 3], label='label1')\n    line2, = ax.plot([1, 2, 3], label='label2')\n    ax.legend(handles=[line1, line2])\n\n\n**4. Labeling existing plot elements**\n\n.. admonition:: Discouraged\n\n    This call signature is discouraged, because the relation between\n    plot elements and labels is only implicit by their order and can\n    easily be mixed up.\n\nTo make a legend for all artists on an Axes, call this function with\nan iterable of strings, one for each legend item. For example::\n\n    ax.plot([1, 2, 3])\n    ax.plot([5, 6, 7])\n    ax.legend(['First line', 'Second line'])\n\n\nParameters\n----------\nhandles : list of (`.Artist` or tuple of `.Artist`), optional\n    A list of Artists (lines, patches) to be added to the legend.\n    Use this together with *labels*, if you need full control on what\n    is shown in the legend and the automatic mechanism described above\n    is not sufficient.\n\n    The length of handles and labels should be the same in this\n    case. If they are not, they are truncated to the smaller length.\n\n    If an entry contains a tuple, then the legend handler for all Artists in the\n    tuple will be placed alongside a single label.\n\nlabels : list of str, optional\n    A list of labels to show next to the artists.\n    Use this together with *handles*, if you need full control on what\n    is shown in the legend and the automatic mechanism described above\n    is not sufficient.\n\nReturns\n-------\n`~matplotlib.legend.Legend`\n\nOther Parameters\n----------------\n\nloc : str or pair of floats, default: :rc:`legend.loc`\n    The location of the legend.\n\n    The strings ``'upper left'``, ``'upper right'``, ``'lower left'``,\n    ``'lower right'`` place the legend at the corresponding corner of the\n    axes.\n\n    The strings ``'upper center'``, ``'lower center'``, ``'center left'``,\n    ``'center right'`` place the legend at the center of the corresponding edge\n    of the axes.\n\n    The string ``'center'`` places the legend at the center of the axes.\n\n    The string ``'best'`` places the legend at the location, among the nine\n    locations defined so far, with the minimum overlap with other drawn\n    artists.  This option can be quite slow for plots with large amounts of\n    data; your plotting speed may benefit from providing a specific location.\n\n    The location can also be a 2-tuple giving the coordinates of the lower-left\n    corner of the legend in axes coordinates (in which case *bbox_to_anchor*\n    will be ignored).\n\n    For back-compatibility, ``'center right'`` (but no other location) can also\n    be spelled ``'right'``, and each \"string\" location can also be given as a\n    numeric value:\n\n    ==================   =============\n    Location String      Location Code\n    ==================   =============\n    'best' (Axes only)   0\n    'upper right'        1\n    'upper left'         2\n    'lower left'         3\n    'lower right'        4\n    'right'              5\n    'center left'        6\n    'center right'       7\n    'lower center'       8\n    'upper center'       9\n    'center'             10\n    ==================   =============\n    \nbbox_to_anchor : `.BboxBase`, 2-tuple, or 4-tuple of floats\n    Box that is used to position the legend in conjunction with *loc*.\n    Defaults to `axes.bbox` (if called as a method to `.Axes.legend`) or\n    `figure.bbox` (if `.Figure.legend`).  This argument allows arbitrary\n    placement of the legend.\n\n    Bbox coordinates are interpreted in the coordinate system given by\n    *bbox_transform*, with the default transform\n    Axes or Figure coordinates, depending on which ``legend`` is called.\n\n    If a 4-tuple or `.BboxBase` is given, then it specifies the bbox\n    ``(x, y, width, height)`` that the legend is placed in.\n    To put the legend in the best location in the bottom right\n    quadrant of the axes (or figure)::\n\n        loc='best', bbox_to_anchor=(0.5, 0., 0.5, 0.5)\n\n    A 2-tuple ``(x, y)`` places the corner of the legend specified by *loc* at\n    x, y.  For example, to put the legend's upper right-hand corner in the\n    center of the axes (or figure) the following keywords can be used::\n\n        loc='upper right', bbox_to_anchor=(0.5, 0.5)\n\nncols : int, default: 1\n    The number of columns that the legend has.\n\n    For backward compatibility, the spelling *ncol* is also supported\n    but it is discouraged. If both are given, *ncols* takes precedence.\n\nprop : None or `~matplotlib.font_manager.FontProperties` or dict\n    The font properties of the legend. If None (default), the current\n    :data:`matplotlib.rcParams` will be used.\n\nfontsize : int or {'xx-small', 'x-small', 'small', 'medium', 'large', 'x-large', 'xx-large'}\n    The font size of the legend. If the value is numeric the size will be the\n    absolute font size in points. String values are relative to the current\n    default font size. This argument is only used if *prop* is not specified.\n\nlabelcolor : str or list, default: :rc:`legend.labelcolor`\n    The color of the text in the legend. Either a valid color string\n    (for example, 'red'), or a list of color strings. The labelcolor can\n    also be made to match the color of the line or marker using 'linecolor',\n    'markerfacecolor' (or 'mfc'), or 'markeredgecolor' (or 'mec').\n\n    Labelcolor can be set globally using :rc:`legend.labelcolor`. If None,\n    use :rc:`text.color`.\n\nnumpoints : int, default: :rc:`legend.numpoints`\n    The number of marker points in the legend when creating a legend\n    entry for a `.Line2D` (line).\n\nscatterpoints : int, default: :rc:`legend.scatterpoints`\n    The number of marker points in the legend when creating\n    a legend entry for a `.PathCollection` (scatter plot).\n\nscatteryoffsets : iterable of floats, default: ``[0.375, 0.5, 0.3125]``\n    The vertical offset (relative to the font size) for the markers\n    created for a scatter plot legend entry. 0.0 is at the base the\n    legend text, and 1.0 is at the top. To draw all markers at the\n    same height, set to ``[0.5]``.\n\nmarkerscale : float, default: :rc:`legend.markerscale`\n    The relative size of legend markers compared to the originally drawn ones.\n\nmarkerfirst : bool, default: True\n    If *True*, legend marker is placed to the left of the legend label.\n    If *False*, legend marker is placed to the right of the legend label.\n\nreverse : bool, default: False\n    If *True*, the legend labels are displayed in reverse order from the input.\n    If *False*, the legend labels are displayed in the same order as the input.\n\n    .. versionadded:: 3.7\n\nframeon : bool, default: :rc:`legend.frameon`\n    Whether the legend should be drawn on a patch (frame).\n\nfancybox : bool, default: :rc:`legend.fancybox`\n    Whether round edges should be enabled around the `.FancyBboxPatch` which\n    makes up the legend's background.\n\nshadow : None, bool or dict, default: :rc:`legend.shadow`\n    Whether to draw a shadow behind the legend.\n    The shadow can be configured using `.Patch` keywords.\n    Customization via :rc:`legend.shadow` is currently not supported.\n\nframealpha : float, default: :rc:`legend.framealpha`\n    The alpha transparency of the legend's background.\n    If *shadow* is activated and *framealpha* is ``None``, the default value is\n    ignored.\n\nfacecolor : \"inherit\" or color, default: :rc:`legend.facecolor`\n    The legend's background color.\n    If ``\"inherit\"``, use :rc:`axes.facecolor`.\n\nedgecolor : \"inherit\" or color, default: :rc:`legend.edgecolor`\n    The legend's background patch edge color.\n    If ``\"inherit\"``, use take :rc:`axes.edgecolor`.\n\nmode : {\"expand\", None}\n    If *mode* is set to ``\"expand\"`` the legend will be horizontally\n    expanded to fill the axes area (or *bbox_to_anchor* if defines\n    the legend's size).\n\nbbox_transform : None or `~matplotlib.transforms.Transform`\n    The transform for the bounding box (*bbox_to_anchor*). For a value\n    of ``None`` (default) the Axes'\n    :data:`~matplotlib.axes.Axes.transAxes` transform will be used.\n\ntitle : str or None\n    The legend's title. Default is no title (``None``).\n\ntitle_fontproperties : None or `~matplotlib.font_manager.FontProperties` or dict\n    The font properties of the legend's title. If None (default), the\n    *title_fontsize* argument will be used if present; if *title_fontsize* is\n    also None, the current :rc:`legend.title_fontsize` will be used.\n\ntitle_fontsize : int or {'xx-small', 'x-small', 'small', 'medium', 'large', 'x-large', 'xx-large'}, default: :rc:`legend.title_fontsize`\n    The font size of the legend's title.\n    Note: This cannot be combined with *title_fontproperties*. If you want\n    to set the fontsize alongside other font properties, use the *size*\n    parameter in *title_fontproperties*.\n\nalignment : {'center', 'left', 'right'}, default: 'center'\n    The alignment of the legend title and the box of entries. The entries\n    are aligned as a single block, so that markers always lined up.\n\nborderpad : float, default: :rc:`legend.borderpad`\n    The fractional whitespace inside the legend border, in font-size units.\n\nlabelspacing : float, default: :rc:`legend.labelspacing`\n    The vertical space between the legend entries, in font-size units.\n\nhandlelength : float, default: :rc:`legend.handlelength`\n    The length of the legend handles, in font-size units.\n\nhandleheight : float, default: :rc:`legend.handleheight`\n    The height of the legend handles, in font-size units.\n\nhandletextpad : float, default: :rc:`legend.handletextpad`\n    The pad between the legend handle and text, in font-size units.\n\nborderaxespad : float, default: :rc:`legend.borderaxespad`\n    The pad between the axes and legend border, in font-size units.\n\ncolumnspacing : float, default: :rc:`legend.columnspacing`\n    The spacing between columns, in font-size units.\n\nhandler_map : dict or None\n    The custom dictionary mapping instances or types to a legend\n    handler. This *handler_map* updates the default handler map\n    found at `matplotlib.legend.Legend.get_legend_handler_map`.\n\ndraggable : bool, default: False\n    Whether the legend can be dragged with the mouse.\n\n\nSee Also\n--------\n.Figure.legend\n\nNotes\n-----\nSome artists are not supported by this function.  See\n:ref:`legend_guide` for details.\n\nExamples\n--------\n.. plot:: gallery/text_labels_and_annotations/legend.py", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/417", "data": {"name": "matplotlib.pyplot.subplots[1].plot", "type": "method", "signature": "(*args, scalex=True, scaley=True, data=None, **kwargs)", "description": "Plot y versus x as lines and/or markers.\n\nCall signatures::\n\n    plot([x], y, [fmt], *, data=None, **kwargs)\n    plot([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n\nThe coordinates of the points or line nodes are given by *x*, *y*.\n\nThe optional parameter *fmt* is a convenient way for defining basic\nformatting like color, marker and linestyle. It's a shortcut string\nnotation described in the *Notes* section below.\n\n>>> plot(x, y)        # plot x and y using default line style and color\n>>> plot(x, y, 'bo')  # plot x and y using blue circle markers\n>>> plot(y)           # plot y using x as index array 0..N-1\n>>> plot(y, 'r+')     # ditto, but with red plusses\n\nYou can use `.Line2D` properties as keyword arguments for more\ncontrol on the appearance. Line properties and *fmt* can be mixed.\nThe following two calls yield identical results:\n\n>>> plot(x, y, 'go--', linewidth=2, markersize=12)\n>>> plot(x, y, color='green', marker='o', linestyle='dashed',\n...      linewidth=2, markersize=12)\n\nWhen conflicting with *fmt*, keyword arguments take precedence.\n\n\n**Plotting labelled data**\n\nThere's a convenient way for plotting objects with labelled data (i.e.\ndata that can be accessed by index ``obj['y']``). Instead of giving\nthe data in *x* and *y*, you can provide the object in the *data*\nparameter and just give the labels for *x* and *y*::\n\n>>> plot('xlabel', 'ylabel', data=obj)\n\nAll indexable objects are supported. This could e.g. be a `dict`, a\n`pandas.DataFrame` or a structured numpy array.\n\n\n**Plotting multiple sets of data**\n\nThere are various ways to plot multiple sets of data.\n\n- The most straight forward way is just to call `plot` multiple times.\n  Example:\n\n  >>> plot(x1, y1, 'bo')\n  >>> plot(x2, y2, 'go')\n\n- If *x* and/or *y* are 2D arrays a separate data set will be drawn\n  for every column. If both *x* and *y* are 2D, they must have the\n  same shape. If only one of them is 2D with shape (N, m) the other\n  must have length N and will be used for every data set m.\n\n  Example:\n\n  >>> x = [1, 2, 3]\n  >>> y = np.array([[1, 2], [3, 4], [5, 6]])\n  >>> plot(x, y)\n\n  is equivalent to:\n\n  >>> for col in range(y.shape[1]):\n  ...     plot(x, y[:, col])\n\n- The third way is to specify multiple sets of *[x]*, *y*, *[fmt]*\n  groups::\n\n  >>> plot(x1, y1, 'g^', x2, y2, 'g-')\n\n  In this case, any additional keyword argument applies to all\n  datasets. Also, this syntax cannot be combined with the *data*\n  parameter.\n\nBy default, each line is assigned a different style specified by a\n'style cycle'. The *fmt* and line property parameters are only\nnecessary if you want explicit deviations from these defaults.\nAlternatively, you can also change the style cycle using\n:rc:`axes.prop_cycle`.\n\n\nParameters\n----------\nx, y : array-like or scalar\n    The horizontal / vertical coordinates of the data points.\n    *x* values are optional and default to ``range(len(y))``.\n\n    Commonly, these parameters are 1D arrays.\n\n    They can also be scalars, or two-dimensional (in that case, the\n    columns represent separate data sets).\n\n    These arguments cannot be passed as keywords.\n\nfmt : str, optional\n    A format string, e.g. 'ro' for red circles. See the *Notes*\n    section for a full description of the format strings.\n\n    Format strings are just an abbreviation for quickly setting\n    basic line properties. All of these and more can also be\n    controlled by keyword arguments.\n\n    This argument cannot be passed as keyword.\n\ndata : indexable object, optional\n    An object with labelled data. If given, provide the label names to\n    plot in *x* and *y*.\n\n    .. note::\n        Technically there's a slight ambiguity in calls where the\n        second label is a valid *fmt*. ``plot('n', 'o', data=obj)``\n        could be ``plt(x, y)`` or ``plt(y, fmt)``. In such cases,\n        the former interpretation is chosen, but a warning is issued.\n        You may suppress the warning by adding an empty format string\n        ``plot('n', 'o', '', data=obj)``.\n\nReturns\n-------\nlist of `.Line2D`\n    A list of lines representing the plotted data.\n\nOther Parameters\n----------------\nscalex, scaley : bool, default: True\n    These parameters determine if the view limits are adapted to the\n    data limits. The values are passed on to\n    `~.axes.Axes.autoscale_view`.\n\n**kwargs : `~matplotlib.lines.Line2D` properties, optional\n    *kwargs* are used to specify properties like a line label (for\n    auto legends), linewidth, antialiasing, marker face color.\n    Example::\n\n    >>> plot([1, 2, 3], [1, 2, 3], 'go-', label='line 1', linewidth=2)\n    >>> plot([1, 2, 3], [1, 4, 9], 'rs', label='line 2')\n\n    If you specify multiple lines with one plot call, the kwargs apply\n    to all those lines. In case the label object is iterable, each\n    element is used as labels for each set of data.\n\n    Here is a list of available `.Line2D` properties:\n\n    Properties:\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: scalar or None\n    animated: bool\n    antialiased or aa: bool\n    clip_box: `~matplotlib.transforms.BboxBase` or None\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    color or c: color\n    dash_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    dash_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    dashes: sequence of floats (on/off ink in points) or (None, None)\n    data: (2, N) array or two 1D arrays\n    drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n    figure: `~matplotlib.figure.Figure`\n    fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n    gapcolor: color or None\n    gid: str\n    in_layout: bool\n    label: object\n    linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n    linewidth or lw: float\n    marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n    markeredgecolor or mec: color\n    markeredgewidth or mew: float\n    markerfacecolor or mfc: color\n    markerfacecoloralt or mfcalt: color\n    markersize or ms: float\n    markevery: None or int or (int, int) or slice or list[int] or float or (float, float) or list[bool]\n    mouseover: bool\n    path_effects: list of `.AbstractPathEffect`\n    picker: float or callable[[Artist, Event], tuple[bool, dict]]\n    pickradius: float\n    rasterized: bool\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    solid_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    solid_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    transform: unknown\n    url: str\n    visible: bool\n    xdata: 1D array\n    ydata: 1D array\n    zorder: float\n\nSee Also\n--------\nscatter : XY scatter plot with markers of varying size and/or color (\n    sometimes also called bubble chart).\n\nNotes\n-----\n**Format Strings**\n\nA format string consists of a part for color, marker and line::\n\n    fmt = '[marker][line][color]'\n\nEach of them is optional. If not provided, the value from the style\ncycle is used. Exception: If ``line`` is given, but no ``marker``,\nthe data will be a line without markers.\n\nOther combinations such as ``[color][marker][line]`` are also\nsupported, but note that their parsing may be ambiguous.\n\n**Markers**\n\n=============   ===============================\ncharacter       description\n=============   ===============================\n``'.'``         point marker\n``','``         pixel marker\n``'o'``         circle marker\n``'v'``         triangle_down marker\n``'^'``         triangle_up marker\n``'<'``         triangle_left marker\n``'>'``         triangle_right marker\n``'1'``         tri_down marker\n``'2'``         tri_up marker\n``'3'``         tri_left marker\n``'4'``         tri_right marker\n``'8'``         octagon marker\n``'s'``         square marker\n``'p'``         pentagon marker\n``'P'``         plus (filled) marker\n``'*'``         star marker\n``'h'``         hexagon1 marker\n``'H'``         hexagon2 marker\n``'+'``         plus marker\n``'x'``         x marker\n``'X'``         x (filled) marker\n``'D'``         diamond marker\n``'d'``         thin_diamond marker\n``'|'``         vline marker\n``'_'``         hline marker\n=============   ===============================\n\n**Line Styles**\n\n=============    ===============================\ncharacter        description\n=============    ===============================\n``'-'``          solid line style\n``'--'``         dashed line style\n``'-.'``         dash-dot line style\n``':'``          dotted line style\n=============    ===============================\n\nExample format strings::\n\n    'b'    # blue markers with default shape\n    'or'   # red circles\n    '-g'   # green solid line\n    '--'   # dashed line with default color\n    '^k:'  # black triangle_up markers connected by a dotted line\n\n**Colors**\n\nThe supported color abbreviations are the single letter codes\n\n=============    ===============================\ncharacter        color\n=============    ===============================\n``'b'``          blue\n``'g'``          green\n``'r'``          red\n``'c'``          cyan\n``'m'``          magenta\n``'y'``          yellow\n``'k'``          black\n``'w'``          white\n=============    ===============================\n\nand the ``'CN'`` colors that index into the default property cycle.\n\nIf the color is the only part of the format string, you can\nadditionally use any  `matplotlib.colors` spec, e.g. full names\n(``'green'``) or hex strings (``'#008000'``).", "parameters": {"type": "object", "properties": {"scalex": {"type": "bool", "default": true}, "scaley": {"type": "bool", "default": true}, "data": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/417", "data": {"name": "sklearn.model_selection.train_test_split", "type": "callable", "signature": "(*arrays, test_size=None)", "description": "Split arrays or matrices into random train and test subsets.\n\nQuick utility that wraps input validation,\n``next(ShuffleSplit().split(X, y))``, and application to input data\ninto a single call for splitting (and optionally subsampling) data into a\none-liner.\n\nRead more in the :ref:`User Guide <cross_validation>`.\n\nParameters\n----------\n*arrays : sequence of indexables with same length / shape[0]\n    Allowed inputs are lists, numpy arrays, scipy-sparse\n    matrices or pandas dataframes.\n\ntest_size : float or int, default=None\n    If float, should be between 0.0 and 1.0 and represent the proportion\n    of the dataset to include in the test split. If int, represents the\n    absolute number of test samples. If None, the value is set to the\n    complement of the train size. If ``train_size`` is also None, it will\n    be set to 0.25.\n\ntrain_size : float or int, default=None\n    If float, should be between 0.0 and 1.0 and represent the\n    proportion of the dataset to include in the train split. If\n    int, represents the absolute number of train samples. If None,\n    the value is automatically set to the complement of the test size.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the shuffling applied to the data before applying the split.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nshuffle : bool, default=True\n    Whether or not to shuffle the data before splitting. If shuffle=False\n    then stratify must be None.\n\nstratify : array-like, default=None\n    If not None, data is split in a stratified fashion, using this as\n    the class labels.\n    Read more in the :ref:`User Guide <stratification>`.\n\nReturns\n-------\nsplitting : list, length=2 * len(arrays)\n    List containing train-test split of inputs.\n\n    .. versionadded:: 0.16\n        If the input is sparse, the output will be a\n        ``scipy.sparse.csr_matrix``. Else, output type is the same as the\n        input type.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.model_selection import train_test_split\n>>> X, y = np.arange(10).reshape((5, 2)), range(5)\n>>> X\narray([[0, 1],\n       [2, 3],\n       [4, 5],\n       [6, 7],\n       [8, 9]])\n>>> list(y)\n[0, 1, 2, 3, 4]\n\n>>> X_train, X_test, y_train, y_test = train_test_split(\n...     X, y, test_size=0.33, random_state=42)\n...\n>>> X_train\narray([[4, 5],\n       [0, 1],\n       [6, 7]])\n>>> y_train\n[2, 0, 3]\n>>> X_test\narray([[2, 3],\n       [8, 9]])\n>>> y_test\n[1, 4]\n\n>>> train_test_split(y, shuffle=False)\n[[0, 1, 2], [3, 4]]", "parameters": {"type": "object", "properties": {"test_size": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/418", "data": {"name": "matplotlib.pyplot.subplots", "type": "class", "signature": "()", "description": "Create a figure and a set of subplots.\n\nThis utility wrapper makes it convenient to create common layouts of\nsubplots, including the enclosing figure object, in a single call.\n\nParameters\n----------\nnrows, ncols : int, default: 1\n    Number of rows/columns of the subplot grid.\n\nsharex, sharey : bool or {'none', 'all', 'row', 'col'}, default: False\n    Controls sharing of properties among x (*sharex*) or y (*sharey*)\n    axes:\n\n    - True or 'all': x- or y-axis will be shared among all subplots.\n    - False or 'none': each subplot x- or y-axis will be independent.\n    - 'row': each subplot row will share an x- or y-axis.\n    - 'col': each subplot column will share an x- or y-axis.\n\n    When subplots have a shared x-axis along a column, only the x tick\n    labels of the bottom subplot are created. Similarly, when subplots\n    have a shared y-axis along a row, only the y tick labels of the first\n    column subplot are created. To later turn other subplots' ticklabels\n    on, use `~matplotlib.axes.Axes.tick_params`.\n\n    When subplots have a shared axis that has units, calling\n    `~matplotlib.axis.Axis.set_units` will update each axis with the\n    new units.\n\nsqueeze : bool, default: True\n    - If True, extra dimensions are squeezed out from the returned\n      array of `~matplotlib.axes.Axes`:\n\n      - if only one subplot is constructed (nrows=ncols=1), the\n        resulting single Axes object is returned as a scalar.\n      - for Nx1 or 1xM subplots, the returned object is a 1D numpy\n        object array of Axes objects.\n      - for NxM, subplots with N>1 and M>1 are returned as a 2D array.\n\n    - If False, no squeezing at all is done: the returned Axes object is\n      always a 2D array containing Axes instances, even if it ends up\n      being 1x1.\n\nwidth_ratios : array-like of length *ncols*, optional\n    Defines the relative widths of the columns. Each column gets a\n    relative width of ``width_ratios[i] / sum(width_ratios)``.\n    If not given, all columns will have the same width.  Equivalent\n    to ``gridspec_kw={'width_ratios': [...]}``.\n\nheight_ratios : array-like of length *nrows*, optional\n    Defines the relative heights of the rows. Each row gets a\n    relative height of ``height_ratios[i] / sum(height_ratios)``.\n    If not given, all rows will have the same height. Convenience\n    for ``gridspec_kw={'height_ratios': [...]}``.\n\nsubplot_kw : dict, optional\n    Dict with keywords passed to the\n    `~matplotlib.figure.Figure.add_subplot` call used to create each\n    subplot.\n\ngridspec_kw : dict, optional\n    Dict with keywords passed to the `~matplotlib.gridspec.GridSpec`\n    constructor used to create the grid the subplots are placed on.\n\n**fig_kw\n    All additional keyword arguments are passed to the\n    `.pyplot.figure` call.\n\nReturns\n-------\nfig : `.Figure`\n\nax : `~matplotlib.axes.Axes` or array of Axes\n    *ax* can be either a single `~.axes.Axes` object, or an array of Axes\n    objects if more than one subplot was created.  The dimensions of the\n    resulting array can be controlled with the squeeze keyword, see above.\n\n    Typical idioms for handling the return value are::\n\n        # using the variable ax for single a Axes\n        fig, ax = plt.subplots()\n\n        # using the variable axs for multiple Axes\n        fig, axs = plt.subplots(2, 2)\n\n        # using tuple unpacking for multiple Axes\n        fig, (ax1, ax2) = plt.subplots(1, 2)\n        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n\n    The names ``ax`` and pluralized ``axs`` are preferred over ``axes``\n    because for the latter it's not clear if it refers to a single\n    `~.axes.Axes` instance or a collection of these.\n\nSee Also\n--------\n.pyplot.figure\n.pyplot.subplot\n.pyplot.axes\n.Figure.subplots\n.Figure.add_subplot\n\nExamples\n--------\n::\n\n    # First create some toy data:\n    x = np.linspace(0, 2*np.pi, 400)\n    y = np.sin(x**2)\n\n    # Create just a figure and only one subplot\n    fig, ax = plt.subplots()\n    ax.plot(x, y)\n    ax.set_title('Simple plot')\n\n    # Create two subplots and unpack the output array immediately\n    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n    ax1.plot(x, y)\n    ax1.set_title('Sharing Y axis')\n    ax2.scatter(x, y)\n\n    # Create four polar axes and access them through the returned array\n    fig, axs = plt.subplots(2, 2, subplot_kw=dict(projection=\"polar\"))\n    axs[0, 0].plot(x, y)\n    axs[1, 1].scatter(x, y)\n\n    # Share a X axis with each column of subplots\n    plt.subplots(2, 2, sharex='col')\n\n    # Share a Y axis with each row of subplots\n    plt.subplots(2, 2, sharey='row')\n\n    # Share both X and Y axes with all subplots\n    plt.subplots(2, 2, sharex='all', sharey='all')\n\n    # Note that this is the same as\n    plt.subplots(2, 2, sharex=True, sharey=True)\n\n    # Create figure number 10 with a single subplot\n    # and clears it if it already exists.\n    fig, ax = plt.subplots(num=10, clear=True)", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/418", "data": {"name": "matplotlib.pyplot.subplots[1].legend", "type": "method", "signature": "(*args, **kwargs)", "description": "Place a legend on the Axes.\n\nCall signatures::\n\n    legend()\n    legend(handles, labels)\n    legend(handles=handles)\n    legend(labels)\n\nThe call signatures correspond to the following different ways to use\nthis method:\n\n**1. Automatic detection of elements to be shown in the legend**\n\nThe elements to be added to the legend are automatically determined,\nwhen you do not pass in any extra arguments.\n\nIn this case, the labels are taken from the artist. You can specify\nthem either at artist creation or by calling the\n:meth:`~.Artist.set_label` method on the artist::\n\n    ax.plot([1, 2, 3], label='Inline label')\n    ax.legend()\n\nor::\n\n    line, = ax.plot([1, 2, 3])\n    line.set_label('Label via method')\n    ax.legend()\n\n.. note::\n    Specific artists can be excluded from the automatic legend element\n    selection by using a label starting with an underscore, \"_\".\n    A string starting with an underscore is the default label for all\n    artists, so calling `.Axes.legend` without any arguments and\n    without setting the labels manually will result in no legend being\n    drawn.\n\n\n**2. Explicitly listing the artists and labels in the legend**\n\nFor full control of which artists have a legend entry, it is possible\nto pass an iterable of legend artists followed by an iterable of\nlegend labels respectively::\n\n    ax.legend([line1, line2, line3], ['label1', 'label2', 'label3'])\n\n\n**3. Explicitly listing the artists in the legend**\n\nThis is similar to 2, but the labels are taken from the artists'\nlabel properties. Example::\n\n    line1, = ax.plot([1, 2, 3], label='label1')\n    line2, = ax.plot([1, 2, 3], label='label2')\n    ax.legend(handles=[line1, line2])\n\n\n**4. Labeling existing plot elements**\n\n.. admonition:: Discouraged\n\n    This call signature is discouraged, because the relation between\n    plot elements and labels is only implicit by their order and can\n    easily be mixed up.\n\nTo make a legend for all artists on an Axes, call this function with\nan iterable of strings, one for each legend item. For example::\n\n    ax.plot([1, 2, 3])\n    ax.plot([5, 6, 7])\n    ax.legend(['First line', 'Second line'])\n\n\nParameters\n----------\nhandles : list of (`.Artist` or tuple of `.Artist`), optional\n    A list of Artists (lines, patches) to be added to the legend.\n    Use this together with *labels*, if you need full control on what\n    is shown in the legend and the automatic mechanism described above\n    is not sufficient.\n\n    The length of handles and labels should be the same in this\n    case. If they are not, they are truncated to the smaller length.\n\n    If an entry contains a tuple, then the legend handler for all Artists in the\n    tuple will be placed alongside a single label.\n\nlabels : list of str, optional\n    A list of labels to show next to the artists.\n    Use this together with *handles*, if you need full control on what\n    is shown in the legend and the automatic mechanism described above\n    is not sufficient.\n\nReturns\n-------\n`~matplotlib.legend.Legend`\n\nOther Parameters\n----------------\n\nloc : str or pair of floats, default: :rc:`legend.loc`\n    The location of the legend.\n\n    The strings ``'upper left'``, ``'upper right'``, ``'lower left'``,\n    ``'lower right'`` place the legend at the corresponding corner of the\n    axes.\n\n    The strings ``'upper center'``, ``'lower center'``, ``'center left'``,\n    ``'center right'`` place the legend at the center of the corresponding edge\n    of the axes.\n\n    The string ``'center'`` places the legend at the center of the axes.\n\n    The string ``'best'`` places the legend at the location, among the nine\n    locations defined so far, with the minimum overlap with other drawn\n    artists.  This option can be quite slow for plots with large amounts of\n    data; your plotting speed may benefit from providing a specific location.\n\n    The location can also be a 2-tuple giving the coordinates of the lower-left\n    corner of the legend in axes coordinates (in which case *bbox_to_anchor*\n    will be ignored).\n\n    For back-compatibility, ``'center right'`` (but no other location) can also\n    be spelled ``'right'``, and each \"string\" location can also be given as a\n    numeric value:\n\n    ==================   =============\n    Location String      Location Code\n    ==================   =============\n    'best' (Axes only)   0\n    'upper right'        1\n    'upper left'         2\n    'lower left'         3\n    'lower right'        4\n    'right'              5\n    'center left'        6\n    'center right'       7\n    'lower center'       8\n    'upper center'       9\n    'center'             10\n    ==================   =============\n    \nbbox_to_anchor : `.BboxBase`, 2-tuple, or 4-tuple of floats\n    Box that is used to position the legend in conjunction with *loc*.\n    Defaults to `axes.bbox` (if called as a method to `.Axes.legend`) or\n    `figure.bbox` (if `.Figure.legend`).  This argument allows arbitrary\n    placement of the legend.\n\n    Bbox coordinates are interpreted in the coordinate system given by\n    *bbox_transform*, with the default transform\n    Axes or Figure coordinates, depending on which ``legend`` is called.\n\n    If a 4-tuple or `.BboxBase` is given, then it specifies the bbox\n    ``(x, y, width, height)`` that the legend is placed in.\n    To put the legend in the best location in the bottom right\n    quadrant of the axes (or figure)::\n\n        loc='best', bbox_to_anchor=(0.5, 0., 0.5, 0.5)\n\n    A 2-tuple ``(x, y)`` places the corner of the legend specified by *loc* at\n    x, y.  For example, to put the legend's upper right-hand corner in the\n    center of the axes (or figure) the following keywords can be used::\n\n        loc='upper right', bbox_to_anchor=(0.5, 0.5)\n\nncols : int, default: 1\n    The number of columns that the legend has.\n\n    For backward compatibility, the spelling *ncol* is also supported\n    but it is discouraged. If both are given, *ncols* takes precedence.\n\nprop : None or `~matplotlib.font_manager.FontProperties` or dict\n    The font properties of the legend. If None (default), the current\n    :data:`matplotlib.rcParams` will be used.\n\nfontsize : int or {'xx-small', 'x-small', 'small', 'medium', 'large', 'x-large', 'xx-large'}\n    The font size of the legend. If the value is numeric the size will be the\n    absolute font size in points. String values are relative to the current\n    default font size. This argument is only used if *prop* is not specified.\n\nlabelcolor : str or list, default: :rc:`legend.labelcolor`\n    The color of the text in the legend. Either a valid color string\n    (for example, 'red'), or a list of color strings. The labelcolor can\n    also be made to match the color of the line or marker using 'linecolor',\n    'markerfacecolor' (or 'mfc'), or 'markeredgecolor' (or 'mec').\n\n    Labelcolor can be set globally using :rc:`legend.labelcolor`. If None,\n    use :rc:`text.color`.\n\nnumpoints : int, default: :rc:`legend.numpoints`\n    The number of marker points in the legend when creating a legend\n    entry for a `.Line2D` (line).\n\nscatterpoints : int, default: :rc:`legend.scatterpoints`\n    The number of marker points in the legend when creating\n    a legend entry for a `.PathCollection` (scatter plot).\n\nscatteryoffsets : iterable of floats, default: ``[0.375, 0.5, 0.3125]``\n    The vertical offset (relative to the font size) for the markers\n    created for a scatter plot legend entry. 0.0 is at the base the\n    legend text, and 1.0 is at the top. To draw all markers at the\n    same height, set to ``[0.5]``.\n\nmarkerscale : float, default: :rc:`legend.markerscale`\n    The relative size of legend markers compared to the originally drawn ones.\n\nmarkerfirst : bool, default: True\n    If *True*, legend marker is placed to the left of the legend label.\n    If *False*, legend marker is placed to the right of the legend label.\n\nreverse : bool, default: False\n    If *True*, the legend labels are displayed in reverse order from the input.\n    If *False*, the legend labels are displayed in the same order as the input.\n\n    .. versionadded:: 3.7\n\nframeon : bool, default: :rc:`legend.frameon`\n    Whether the legend should be drawn on a patch (frame).\n\nfancybox : bool, default: :rc:`legend.fancybox`\n    Whether round edges should be enabled around the `.FancyBboxPatch` which\n    makes up the legend's background.\n\nshadow : None, bool or dict, default: :rc:`legend.shadow`\n    Whether to draw a shadow behind the legend.\n    The shadow can be configured using `.Patch` keywords.\n    Customization via :rc:`legend.shadow` is currently not supported.\n\nframealpha : float, default: :rc:`legend.framealpha`\n    The alpha transparency of the legend's background.\n    If *shadow* is activated and *framealpha* is ``None``, the default value is\n    ignored.\n\nfacecolor : \"inherit\" or color, default: :rc:`legend.facecolor`\n    The legend's background color.\n    If ``\"inherit\"``, use :rc:`axes.facecolor`.\n\nedgecolor : \"inherit\" or color, default: :rc:`legend.edgecolor`\n    The legend's background patch edge color.\n    If ``\"inherit\"``, use take :rc:`axes.edgecolor`.\n\nmode : {\"expand\", None}\n    If *mode* is set to ``\"expand\"`` the legend will be horizontally\n    expanded to fill the axes area (or *bbox_to_anchor* if defines\n    the legend's size).\n\nbbox_transform : None or `~matplotlib.transforms.Transform`\n    The transform for the bounding box (*bbox_to_anchor*). For a value\n    of ``None`` (default) the Axes'\n    :data:`~matplotlib.axes.Axes.transAxes` transform will be used.\n\ntitle : str or None\n    The legend's title. Default is no title (``None``).\n\ntitle_fontproperties : None or `~matplotlib.font_manager.FontProperties` or dict\n    The font properties of the legend's title. If None (default), the\n    *title_fontsize* argument will be used if present; if *title_fontsize* is\n    also None, the current :rc:`legend.title_fontsize` will be used.\n\ntitle_fontsize : int or {'xx-small', 'x-small', 'small', 'medium', 'large', 'x-large', 'xx-large'}, default: :rc:`legend.title_fontsize`\n    The font size of the legend's title.\n    Note: This cannot be combined with *title_fontproperties*. If you want\n    to set the fontsize alongside other font properties, use the *size*\n    parameter in *title_fontproperties*.\n\nalignment : {'center', 'left', 'right'}, default: 'center'\n    The alignment of the legend title and the box of entries. The entries\n    are aligned as a single block, so that markers always lined up.\n\nborderpad : float, default: :rc:`legend.borderpad`\n    The fractional whitespace inside the legend border, in font-size units.\n\nlabelspacing : float, default: :rc:`legend.labelspacing`\n    The vertical space between the legend entries, in font-size units.\n\nhandlelength : float, default: :rc:`legend.handlelength`\n    The length of the legend handles, in font-size units.\n\nhandleheight : float, default: :rc:`legend.handleheight`\n    The height of the legend handles, in font-size units.\n\nhandletextpad : float, default: :rc:`legend.handletextpad`\n    The pad between the legend handle and text, in font-size units.\n\nborderaxespad : float, default: :rc:`legend.borderaxespad`\n    The pad between the axes and legend border, in font-size units.\n\ncolumnspacing : float, default: :rc:`legend.columnspacing`\n    The spacing between columns, in font-size units.\n\nhandler_map : dict or None\n    The custom dictionary mapping instances or types to a legend\n    handler. This *handler_map* updates the default handler map\n    found at `matplotlib.legend.Legend.get_legend_handler_map`.\n\ndraggable : bool, default: False\n    Whether the legend can be dragged with the mouse.\n\n\nSee Also\n--------\n.Figure.legend\n\nNotes\n-----\nSome artists are not supported by this function.  See\n:ref:`legend_guide` for details.\n\nExamples\n--------\n.. plot:: gallery/text_labels_and_annotations/legend.py", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/418", "data": {"name": "matplotlib.pyplot.subplots[1].set_title", "type": "method", "signature": "(label, fontdict=None, loc=None, pad=None, *, y=None, **kwargs)", "description": "Set a title for the Axes.\n\nSet one of the three available Axes titles. The available titles\nare positioned above the Axes in the center, flush with the left\nedge, and flush with the right edge.\n\nParameters\n----------\nlabel : str\n    Text to use for the title\n\nfontdict : dict\n\n    .. admonition:: Discouraged\n\n       The use of *fontdict* is discouraged. Parameters should be passed as\n       individual keyword arguments or using dictionary-unpacking\n       ``set_title(..., **fontdict)``.\n\n    A dictionary controlling the appearance of the title text,\n    the default *fontdict* is::\n\n       {'fontsize': rcParams['axes.titlesize'],\n        'fontweight': rcParams['axes.titleweight'],\n        'color': rcParams['axes.titlecolor'],\n        'verticalalignment': 'baseline',\n        'horizontalalignment': loc}\n\nloc : {'center', 'left', 'right'}, default: :rc:`axes.titlelocation`\n    Which title to set.\n\ny : float, default: :rc:`axes.titley`\n    Vertical Axes location for the title (1.0 is the top).  If\n    None (the default) and :rc:`axes.titley` is also None, y is\n    determined automatically to avoid decorators on the Axes.\n\npad : float, default: :rc:`axes.titlepad`\n    The offset of the title from the top of the Axes, in points.\n\nReturns\n-------\n`.Text`\n    The matplotlib text instance representing the title\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    Other keyword arguments are text properties, see `.Text` for a list\n    of valid text properties.", "parameters": {"type": "object", "properties": {"label": {}, "fontdict": {"type": "NoneType", "default": null}, "loc": {"type": "NoneType", "default": null}, "pad": {"type": "NoneType", "default": null}, "y": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/418", "data": {"name": "matplotlib.pyplot.subplots[1].plot", "type": "method", "signature": "(*args, scalex=True, scaley=True, data=None, **kwargs)", "description": "Plot y versus x as lines and/or markers.\n\nCall signatures::\n\n    plot([x], y, [fmt], *, data=None, **kwargs)\n    plot([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n\nThe coordinates of the points or line nodes are given by *x*, *y*.\n\nThe optional parameter *fmt* is a convenient way for defining basic\nformatting like color, marker and linestyle. It's a shortcut string\nnotation described in the *Notes* section below.\n\n>>> plot(x, y)        # plot x and y using default line style and color\n>>> plot(x, y, 'bo')  # plot x and y using blue circle markers\n>>> plot(y)           # plot y using x as index array 0..N-1\n>>> plot(y, 'r+')     # ditto, but with red plusses\n\nYou can use `.Line2D` properties as keyword arguments for more\ncontrol on the appearance. Line properties and *fmt* can be mixed.\nThe following two calls yield identical results:\n\n>>> plot(x, y, 'go--', linewidth=2, markersize=12)\n>>> plot(x, y, color='green', marker='o', linestyle='dashed',\n...      linewidth=2, markersize=12)\n\nWhen conflicting with *fmt*, keyword arguments take precedence.\n\n\n**Plotting labelled data**\n\nThere's a convenient way for plotting objects with labelled data (i.e.\ndata that can be accessed by index ``obj['y']``). Instead of giving\nthe data in *x* and *y*, you can provide the object in the *data*\nparameter and just give the labels for *x* and *y*::\n\n>>> plot('xlabel', 'ylabel', data=obj)\n\nAll indexable objects are supported. This could e.g. be a `dict`, a\n`pandas.DataFrame` or a structured numpy array.\n\n\n**Plotting multiple sets of data**\n\nThere are various ways to plot multiple sets of data.\n\n- The most straight forward way is just to call `plot` multiple times.\n  Example:\n\n  >>> plot(x1, y1, 'bo')\n  >>> plot(x2, y2, 'go')\n\n- If *x* and/or *y* are 2D arrays a separate data set will be drawn\n  for every column. If both *x* and *y* are 2D, they must have the\n  same shape. If only one of them is 2D with shape (N, m) the other\n  must have length N and will be used for every data set m.\n\n  Example:\n\n  >>> x = [1, 2, 3]\n  >>> y = np.array([[1, 2], [3, 4], [5, 6]])\n  >>> plot(x, y)\n\n  is equivalent to:\n\n  >>> for col in range(y.shape[1]):\n  ...     plot(x, y[:, col])\n\n- The third way is to specify multiple sets of *[x]*, *y*, *[fmt]*\n  groups::\n\n  >>> plot(x1, y1, 'g^', x2, y2, 'g-')\n\n  In this case, any additional keyword argument applies to all\n  datasets. Also, this syntax cannot be combined with the *data*\n  parameter.\n\nBy default, each line is assigned a different style specified by a\n'style cycle'. The *fmt* and line property parameters are only\nnecessary if you want explicit deviations from these defaults.\nAlternatively, you can also change the style cycle using\n:rc:`axes.prop_cycle`.\n\n\nParameters\n----------\nx, y : array-like or scalar\n    The horizontal / vertical coordinates of the data points.\n    *x* values are optional and default to ``range(len(y))``.\n\n    Commonly, these parameters are 1D arrays.\n\n    They can also be scalars, or two-dimensional (in that case, the\n    columns represent separate data sets).\n\n    These arguments cannot be passed as keywords.\n\nfmt : str, optional\n    A format string, e.g. 'ro' for red circles. See the *Notes*\n    section for a full description of the format strings.\n\n    Format strings are just an abbreviation for quickly setting\n    basic line properties. All of these and more can also be\n    controlled by keyword arguments.\n\n    This argument cannot be passed as keyword.\n\ndata : indexable object, optional\n    An object with labelled data. If given, provide the label names to\n    plot in *x* and *y*.\n\n    .. note::\n        Technically there's a slight ambiguity in calls where the\n        second label is a valid *fmt*. ``plot('n', 'o', data=obj)``\n        could be ``plt(x, y)`` or ``plt(y, fmt)``. In such cases,\n        the former interpretation is chosen, but a warning is issued.\n        You may suppress the warning by adding an empty format string\n        ``plot('n', 'o', '', data=obj)``.\n\nReturns\n-------\nlist of `.Line2D`\n    A list of lines representing the plotted data.\n\nOther Parameters\n----------------\nscalex, scaley : bool, default: True\n    These parameters determine if the view limits are adapted to the\n    data limits. The values are passed on to\n    `~.axes.Axes.autoscale_view`.\n\n**kwargs : `~matplotlib.lines.Line2D` properties, optional\n    *kwargs* are used to specify properties like a line label (for\n    auto legends), linewidth, antialiasing, marker face color.\n    Example::\n\n    >>> plot([1, 2, 3], [1, 2, 3], 'go-', label='line 1', linewidth=2)\n    >>> plot([1, 2, 3], [1, 4, 9], 'rs', label='line 2')\n\n    If you specify multiple lines with one plot call, the kwargs apply\n    to all those lines. In case the label object is iterable, each\n    element is used as labels for each set of data.\n\n    Here is a list of available `.Line2D` properties:\n\n    Properties:\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: scalar or None\n    animated: bool\n    antialiased or aa: bool\n    clip_box: `~matplotlib.transforms.BboxBase` or None\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    color or c: color\n    dash_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    dash_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    dashes: sequence of floats (on/off ink in points) or (None, None)\n    data: (2, N) array or two 1D arrays\n    drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n    figure: `~matplotlib.figure.Figure`\n    fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n    gapcolor: color or None\n    gid: str\n    in_layout: bool\n    label: object\n    linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n    linewidth or lw: float\n    marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n    markeredgecolor or mec: color\n    markeredgewidth or mew: float\n    markerfacecolor or mfc: color\n    markerfacecoloralt or mfcalt: color\n    markersize or ms: float\n    markevery: None or int or (int, int) or slice or list[int] or float or (float, float) or list[bool]\n    mouseover: bool\n    path_effects: list of `.AbstractPathEffect`\n    picker: float or callable[[Artist, Event], tuple[bool, dict]]\n    pickradius: float\n    rasterized: bool\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    solid_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    solid_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    transform: unknown\n    url: str\n    visible: bool\n    xdata: 1D array\n    ydata: 1D array\n    zorder: float\n\nSee Also\n--------\nscatter : XY scatter plot with markers of varying size and/or color (\n    sometimes also called bubble chart).\n\nNotes\n-----\n**Format Strings**\n\nA format string consists of a part for color, marker and line::\n\n    fmt = '[marker][line][color]'\n\nEach of them is optional. If not provided, the value from the style\ncycle is used. Exception: If ``line`` is given, but no ``marker``,\nthe data will be a line without markers.\n\nOther combinations such as ``[color][marker][line]`` are also\nsupported, but note that their parsing may be ambiguous.\n\n**Markers**\n\n=============   ===============================\ncharacter       description\n=============   ===============================\n``'.'``         point marker\n``','``         pixel marker\n``'o'``         circle marker\n``'v'``         triangle_down marker\n``'^'``         triangle_up marker\n``'<'``         triangle_left marker\n``'>'``         triangle_right marker\n``'1'``         tri_down marker\n``'2'``         tri_up marker\n``'3'``         tri_left marker\n``'4'``         tri_right marker\n``'8'``         octagon marker\n``'s'``         square marker\n``'p'``         pentagon marker\n``'P'``         plus (filled) marker\n``'*'``         star marker\n``'h'``         hexagon1 marker\n``'H'``         hexagon2 marker\n``'+'``         plus marker\n``'x'``         x marker\n``'X'``         x (filled) marker\n``'D'``         diamond marker\n``'d'``         thin_diamond marker\n``'|'``         vline marker\n``'_'``         hline marker\n=============   ===============================\n\n**Line Styles**\n\n=============    ===============================\ncharacter        description\n=============    ===============================\n``'-'``          solid line style\n``'--'``         dashed line style\n``'-.'``         dash-dot line style\n``':'``          dotted line style\n=============    ===============================\n\nExample format strings::\n\n    'b'    # blue markers with default shape\n    'or'   # red circles\n    '-g'   # green solid line\n    '--'   # dashed line with default color\n    '^k:'  # black triangle_up markers connected by a dotted line\n\n**Colors**\n\nThe supported color abbreviations are the single letter codes\n\n=============    ===============================\ncharacter        color\n=============    ===============================\n``'b'``          blue\n``'g'``          green\n``'r'``          red\n``'c'``          cyan\n``'m'``          magenta\n``'y'``          yellow\n``'k'``          black\n``'w'``          white\n=============    ===============================\n\nand the ``'CN'`` colors that index into the default property cycle.\n\nIf the color is the only part of the format string, you can\nadditionally use any  `matplotlib.colors` spec, e.g. full names\n(``'green'``) or hex strings (``'#008000'``).", "parameters": {"type": "object", "properties": {"scalex": {"type": "bool", "default": true}, "scaley": {"type": "bool", "default": true}, "data": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/418", "data": {"name": "matplotlib.pyplot.subplots[1].set_ylabel", "type": "method", "signature": "(ylabel, fontdict=None, labelpad=None, *, loc=None, **kwargs)", "description": "Set the label for the y-axis.\n\nParameters\n----------\nylabel : str\n    The label text.\n\nlabelpad : float, default: :rc:`axes.labelpad`\n    Spacing in points from the Axes bounding box including ticks\n    and tick labels.  If None, the previous value is left as is.\n\nloc : {'bottom', 'center', 'top'}, default: :rc:`yaxis.labellocation`\n    The label position. This is a high-level alternative for passing\n    parameters *y* and *horizontalalignment*.\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    `.Text` properties control the appearance of the label.\n\nSee Also\n--------\ntext : Documents the properties supported by `.Text`.", "parameters": {"type": "object", "properties": {"ylabel": {}, "fontdict": {"type": "NoneType", "default": null}, "labelpad": {"type": "NoneType", "default": null}, "loc": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/418", "data": {"name": "matplotlib.pyplot.subplots[1].set_xlabel", "type": "method", "signature": "(xlabel, fontdict=None, labelpad=None, *, loc=None, **kwargs)", "description": "Set the label for the x-axis.\n\nParameters\n----------\nxlabel : str\n    The label text.\n\nlabelpad : float, default: :rc:`axes.labelpad`\n    Spacing in points from the Axes bounding box including ticks\n    and tick labels.  If None, the previous value is left as is.\n\nloc : {'left', 'center', 'right'}, default: :rc:`xaxis.labellocation`\n    The label position. This is a high-level alternative for passing\n    parameters *x* and *horizontalalignment*.\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    `.Text` properties control the appearance of the label.\n\nSee Also\n--------\ntext : Documents the properties supported by `.Text`.", "parameters": {"type": "object", "properties": {"xlabel": {}, "fontdict": {"type": "NoneType", "default": null}, "labelpad": {"type": "NoneType", "default": null}, "loc": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/418", "data": {"name": "sklearn.metrics.auc", "type": "callable", "signature": "(x, y)", "description": "Compute Area Under the Curve (AUC) using the trapezoidal rule.\n\nThis is a general function, given points on a curve.  For computing the\narea under the ROC-curve, see :func:`roc_auc_score`.  For an alternative\nway to summarize a precision-recall curve, see\n:func:`average_precision_score`.\n\nParameters\n----------\nx : array-like of shape (n,)\n    X coordinates. These must be either monotonic increasing or monotonic\n    decreasing.\ny : array-like of shape (n,)\n    Y coordinates.\n\nReturns\n-------\nauc : float\n    Area Under the Curve.\n\nSee Also\n--------\nroc_auc_score : Compute the area under the ROC curve.\naverage_precision_score : Compute average precision from prediction scores.\nprecision_recall_curve : Compute precision-recall pairs for different\n    probability thresholds.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn import metrics\n>>> y = np.array([1, 1, 2, 2])\n>>> pred = np.array([0.1, 0.4, 0.35, 0.8])\n>>> fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=2)\n>>> metrics.auc(fpr, tpr)\n0.75", "parameters": {"type": "object", "properties": {"x": {}, "y": {}}}}}
{"task_id": "BigCodeBench/418", "data": {"name": "sklearn.metrics.roc_curve", "type": "callable", "signature": "(y_true, y_score)", "description": "Compute Receiver operating characteristic (ROC).\n\nNote: this implementation is restricted to the binary classification task.\n\nRead more in the :ref:`User Guide <roc_metrics>`.\n\nParameters\n----------\ny_true : array-like of shape (n_samples,)\n    True binary labels. If labels are not either {-1, 1} or {0, 1}, then\n    pos_label should be explicitly given.\n\ny_score : array-like of shape (n_samples,)\n    Target scores, can either be probability estimates of the positive\n    class, confidence values, or non-thresholded measure of decisions\n    (as returned by \"decision_function\" on some classifiers).\n\npos_label : int, float, bool or str, default=None\n    The label of the positive class.\n    When ``pos_label=None``, if `y_true` is in {-1, 1} or {0, 1},\n    ``pos_label`` is set to 1, otherwise an error will be raised.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\ndrop_intermediate : bool, default=True\n    Whether to drop some suboptimal thresholds which would not appear\n    on a plotted ROC curve. This is useful in order to create lighter\n    ROC curves.\n\n    .. versionadded:: 0.17\n       parameter *drop_intermediate*.\n\nReturns\n-------\nfpr : ndarray of shape (>2,)\n    Increasing false positive rates such that element i is the false\n    positive rate of predictions with score >= `thresholds[i]`.\n\ntpr : ndarray of shape (>2,)\n    Increasing true positive rates such that element `i` is the true\n    positive rate of predictions with score >= `thresholds[i]`.\n\nthresholds : ndarray of shape (n_thresholds,)\n    Decreasing thresholds on the decision function used to compute\n    fpr and tpr. `thresholds[0]` represents no instances being predicted\n    and is arbitrarily set to `np.inf`.\n\nSee Also\n--------\nRocCurveDisplay.from_estimator : Plot Receiver Operating Characteristic\n    (ROC) curve given an estimator and some data.\nRocCurveDisplay.from_predictions : Plot Receiver Operating Characteristic\n    (ROC) curve given the true and predicted values.\ndet_curve: Compute error rates for different probability thresholds.\nroc_auc_score : Compute the area under the ROC curve.\n\nNotes\n-----\nSince the thresholds are sorted from low to high values, they\nare reversed upon returning them to ensure they correspond to both ``fpr``\nand ``tpr``, which are sorted in reversed order during their calculation.\n\nAn arbitrary threshold is added for the case `tpr=0` and `fpr=0` to\nensure that the curve starts at `(0, 0)`. This threshold corresponds to the\n`np.inf`.\n\nReferences\n----------\n.. [1] `Wikipedia entry for the Receiver operating characteristic\n        <https://en.wikipedia.org/wiki/Receiver_operating_characteristic>`_\n\n.. [2] Fawcett T. An introduction to ROC analysis[J]. Pattern Recognition\n       Letters, 2006, 27(8):861-874.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn import metrics\n>>> y = np.array([1, 1, 2, 2])\n>>> scores = np.array([0.1, 0.4, 0.35, 0.8])\n>>> fpr, tpr, thresholds = metrics.roc_curve(y, scores, pos_label=2)\n>>> fpr\narray([0. , 0. , 0.5, 0.5, 1. ])\n>>> tpr\narray([0. , 0.5, 0.5, 1. , 1. ])\n>>> thresholds\narray([ inf, 0.8 , 0.4 , 0.35, 0.1 ])", "parameters": {"type": "object", "properties": {"y_true": {}, "y_score": {}}}}}
{"task_id": "BigCodeBench/418", "data": {"name": "sklearn.model_selection.train_test_split", "type": "callable", "signature": "(*arrays, test_size=None)", "description": "Split arrays or matrices into random train and test subsets.\n\nQuick utility that wraps input validation,\n``next(ShuffleSplit().split(X, y))``, and application to input data\ninto a single call for splitting (and optionally subsampling) data into a\none-liner.\n\nRead more in the :ref:`User Guide <cross_validation>`.\n\nParameters\n----------\n*arrays : sequence of indexables with same length / shape[0]\n    Allowed inputs are lists, numpy arrays, scipy-sparse\n    matrices or pandas dataframes.\n\ntest_size : float or int, default=None\n    If float, should be between 0.0 and 1.0 and represent the proportion\n    of the dataset to include in the test split. If int, represents the\n    absolute number of test samples. If None, the value is set to the\n    complement of the train size. If ``train_size`` is also None, it will\n    be set to 0.25.\n\ntrain_size : float or int, default=None\n    If float, should be between 0.0 and 1.0 and represent the\n    proportion of the dataset to include in the train split. If\n    int, represents the absolute number of train samples. If None,\n    the value is automatically set to the complement of the test size.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the shuffling applied to the data before applying the split.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nshuffle : bool, default=True\n    Whether or not to shuffle the data before splitting. If shuffle=False\n    then stratify must be None.\n\nstratify : array-like, default=None\n    If not None, data is split in a stratified fashion, using this as\n    the class labels.\n    Read more in the :ref:`User Guide <stratification>`.\n\nReturns\n-------\nsplitting : list, length=2 * len(arrays)\n    List containing train-test split of inputs.\n\n    .. versionadded:: 0.16\n        If the input is sparse, the output will be a\n        ``scipy.sparse.csr_matrix``. Else, output type is the same as the\n        input type.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.model_selection import train_test_split\n>>> X, y = np.arange(10).reshape((5, 2)), range(5)\n>>> X\narray([[0, 1],\n       [2, 3],\n       [4, 5],\n       [6, 7],\n       [8, 9]])\n>>> list(y)\n[0, 1, 2, 3, 4]\n\n>>> X_train, X_test, y_train, y_test = train_test_split(\n...     X, y, test_size=0.33, random_state=42)\n...\n>>> X_train\narray([[4, 5],\n       [0, 1],\n       [6, 7]])\n>>> y_train\n[2, 0, 3]\n>>> X_test\narray([[2, 3],\n       [8, 9]])\n>>> y_test\n[1, 4]\n\n>>> train_test_split(y, shuffle=False)\n[[0, 1, 2], [3, 4]]", "parameters": {"type": "object", "properties": {"test_size": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/418", "data": {"name": "tensorflow.keras.layers"}}
{"task_id": "BigCodeBench/418", "data": {"name": "tensorflow.keras.layers.Dense(input_dim=2, units=1, activation='sigmoid')"}}
{"task_id": "BigCodeBench/418", "data": {"name": "tensorflow.keras.optimizers"}}
{"task_id": "BigCodeBench/418", "data": {"name": "tensorflow.keras.optimizers.SGD(learning_rate=0.1)"}}
{"task_id": "BigCodeBench/418", "data": {"name": "tensorflow.keras.Sequential([keras.layers.Dense(input_dim=2, units=1, activation='sigmoid')])"}}
{"task_id": "BigCodeBench/418", "data": {"name": "tensorflow.keras.Sequential.predict(X_test, verbose=0)"}}
{"task_id": "BigCodeBench/418", "data": {"name": "tensorflow.keras.Sequential.fit(X_train, Y_train, epochs=200, batch_size=1, verbose=0)"}}
{"task_id": "BigCodeBench/418", "data": {"name": "tensorflow.keras.Sequential.compile(loss='binary_crossentropy', optimizer=keras.optimizers.SGD(learning_rate=0.1))"}}
{"task_id": "BigCodeBench/424", "data": {"name": "cv2.imread", "type": "callable", "signature": "(*args, **kwargs)", "description": "imread(filename[, flags]) -> retval\n.   @brief Loads an image from a file.\n.   \n.   @anchor imread\n.   \n.   The function imread loads an image from the specified file and returns it. If the image cannot be\n.   read (because of missing file, improper permissions, unsupported or invalid format), the function\n.   returns an empty matrix ( Mat::data==NULL ).\n.   \n.   Currently, the following file formats are supported:\n.   \n.   -   Windows bitmaps - \\*.bmp, \\*.dib (always supported)\n.   -   JPEG files - \\*.jpeg, \\*.jpg, \\*.jpe (see the *Note* section)\n.   -   JPEG 2000 files - \\*.jp2 (see the *Note* section)\n.   -   Portable Network Graphics - \\*.png (see the *Note* section)\n.   -   WebP - \\*.webp (see the *Note* section)\n.   -   AVIF - \\*.avif (see the *Note* section)\n.   -   Portable image format - \\*.pbm, \\*.pgm, \\*.ppm \\*.pxm, \\*.pnm (always supported)\n.   -   PFM files - \\*.pfm (see the *Note* section)\n.   -   Sun rasters - \\*.sr, \\*.ras (always supported)\n.   -   TIFF files - \\*.tiff, \\*.tif (see the *Note* section)\n.   -   OpenEXR Image files - \\*.exr (see the *Note* section)\n.   -   Radiance HDR - \\*.hdr, \\*.pic (always supported)\n.   -   Raster and Vector geospatial data supported by GDAL (see the *Note* section)\n.   \n.   @note\n.   -   The function determines the type of an image by the content, not by the file extension.\n.   -   In the case of color images, the decoded images will have the channels stored in **B G R** order.\n.   -   When using IMREAD_GRAYSCALE, the codec's internal grayscale conversion will be used, if available.\n.       Results may differ to the output of cvtColor()\n.   -   On Microsoft Windows\\* OS and MacOSX\\*, the codecs shipped with an OpenCV image (libjpeg,\n.       libpng, libtiff, and libjasper) are used by default. So, OpenCV can always read JPEGs, PNGs,\n.       and TIFFs. On MacOSX, there is also an option to use native MacOSX image readers. But beware\n.       that currently these native image loaders give images with different pixel values because of\n.       the color management embedded into MacOSX.\n.   -   On Linux\\*, BSD flavors and other Unix-like open-source operating systems, OpenCV looks for\n.       codecs supplied with an OS image. Install the relevant packages (do not forget the development\n.       files, for example, \"libjpeg-dev\", in Debian\\* and Ubuntu\\*) to get the codec support or turn\n.       on the OPENCV_BUILD_3RDPARTY_LIBS flag in CMake.\n.   -   In the case you set *WITH_GDAL* flag to true in CMake and @ref IMREAD_LOAD_GDAL to load the image,\n.       then the [GDAL](http://www.gdal.org) driver will be used in order to decode the image, supporting\n.       the following formats: [Raster](http://www.gdal.org/formats_list.html),\n.       [Vector](http://www.gdal.org/ogr_formats.html).\n.   -   If EXIF information is embedded in the image file, the EXIF orientation will be taken into account\n.       and thus the image will be rotated accordingly except if the flags @ref IMREAD_IGNORE_ORIENTATION\n.       or @ref IMREAD_UNCHANGED are passed.\n.   -   Use the IMREAD_UNCHANGED flag to keep the floating point values from PFM image.\n.   -   By default number of pixels must be less than 2^30. Limit can be set using system\n.       variable OPENCV_IO_MAX_IMAGE_PIXELS\n.   \n.   @param filename Name of file to be loaded.\n.   @param flags Flag that can take values of cv::ImreadModes", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/424", "data": {"name": "cv2.imread.shape", "type": "callable", "signature": "(*args, **kwargs)", "description": "imread(filename[, flags]) -> retval\n.   @brief Loads an image from a file.\n.   \n.   @anchor imread\n.   \n.   The function imread loads an image from the specified file and returns it. If the image cannot be\n.   read (because of missing file, improper permissions, unsupported or invalid format), the function\n.   returns an empty matrix ( Mat::data==NULL ).\n.   \n.   Currently, the following file formats are supported:\n.   \n.   -   Windows bitmaps - \\*.bmp, \\*.dib (always supported)\n.   -   JPEG files - \\*.jpeg, \\*.jpg, \\*.jpe (see the *Note* section)\n.   -   JPEG 2000 files - \\*.jp2 (see the *Note* section)\n.   -   Portable Network Graphics - \\*.png (see the *Note* section)\n.   -   WebP - \\*.webp (see the *Note* section)\n.   -   AVIF - \\*.avif (see the *Note* section)\n.   -   Portable image format - \\*.pbm, \\*.pgm, \\*.ppm \\*.pxm, \\*.pnm (always supported)\n.   -   PFM files - \\*.pfm (see the *Note* section)\n.   -   Sun rasters - \\*.sr, \\*.ras (always supported)\n.   -   TIFF files - \\*.tiff, \\*.tif (see the *Note* section)\n.   -   OpenEXR Image files - \\*.exr (see the *Note* section)\n.   -   Radiance HDR - \\*.hdr, \\*.pic (always supported)\n.   -   Raster and Vector geospatial data supported by GDAL (see the *Note* section)\n.   \n.   @note\n.   -   The function determines the type of an image by the content, not by the file extension.\n.   -   In the case of color images, the decoded images will have the channels stored in **B G R** order.\n.   -   When using IMREAD_GRAYSCALE, the codec's internal grayscale conversion will be used, if available.\n.       Results may differ to the output of cvtColor()\n.   -   On Microsoft Windows\\* OS and MacOSX\\*, the codecs shipped with an OpenCV image (libjpeg,\n.       libpng, libtiff, and libjasper) are used by default. So, OpenCV can always read JPEGs, PNGs,\n.       and TIFFs. On MacOSX, there is also an option to use native MacOSX image readers. But beware\n.       that currently these native image loaders give images with different pixel values because of\n.       the color management embedded into MacOSX.\n.   -   On Linux\\*, BSD flavors and other Unix-like open-source operating systems, OpenCV looks for\n.       codecs supplied with an OS image. Install the relevant packages (do not forget the development\n.       files, for example, \"libjpeg-dev\", in Debian\\* and Ubuntu\\*) to get the codec support or turn\n.       on the OPENCV_BUILD_3RDPARTY_LIBS flag in CMake.\n.   -   In the case you set *WITH_GDAL* flag to true in CMake and @ref IMREAD_LOAD_GDAL to load the image,\n.       then the [GDAL](http://www.gdal.org) driver will be used in order to decode the image, supporting\n.       the following formats: [Raster](http://www.gdal.org/formats_list.html),\n.       [Vector](http://www.gdal.org/ogr_formats.html).\n.   -   If EXIF information is embedded in the image file, the EXIF orientation will be taken into account\n.       and thus the image will be rotated accordingly except if the flags @ref IMREAD_IGNORE_ORIENTATION\n.       or @ref IMREAD_UNCHANGED are passed.\n.   -   Use the IMREAD_UNCHANGED flag to keep the floating point values from PFM image.\n.   -   By default number of pixels must be less than 2^30. Limit can be set using system\n.       variable OPENCV_IO_MAX_IMAGE_PIXELS\n.   \n.   @param filename Name of file to be loaded.\n.   @param flags Flag that can take values of cv::ImreadModes", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/424", "data": {"name": "cv2.imread.copy", "type": "callable", "signature": "()", "description": "imread(filename[, flags]) -> retval\n.   @brief Loads an image from a file.\n.   \n.   @anchor imread\n.   \n.   The function imread loads an image from the specified file and returns it. If the image cannot be\n.   read (because of missing file, improper permissions, unsupported or invalid format), the function\n.   returns an empty matrix ( Mat::data==NULL ).\n.   \n.   Currently, the following file formats are supported:\n.   \n.   -   Windows bitmaps - \\*.bmp, \\*.dib (always supported)\n.   -   JPEG files - \\*.jpeg, \\*.jpg, \\*.jpe (see the *Note* section)\n.   -   JPEG 2000 files - \\*.jp2 (see the *Note* section)\n.   -   Portable Network Graphics - \\*.png (see the *Note* section)\n.   -   WebP - \\*.webp (see the *Note* section)\n.   -   AVIF - \\*.avif (see the *Note* section)\n.   -   Portable image format - \\*.pbm, \\*.pgm, \\*.ppm \\*.pxm, \\*.pnm (always supported)\n.   -   PFM files - \\*.pfm (see the *Note* section)\n.   -   Sun rasters - \\*.sr, \\*.ras (always supported)\n.   -   TIFF files - \\*.tiff, \\*.tif (see the *Note* section)\n.   -   OpenEXR Image files - \\*.exr (see the *Note* section)\n.   -   Radiance HDR - \\*.hdr, \\*.pic (always supported)\n.   -   Raster and Vector geospatial data supported by GDAL (see the *Note* section)\n.   \n.   @note\n.   -   The function determines the type of an image by the content, not by the file extension.\n.   -   In the case of color images, the decoded images will have the channels stored in **B G R** order.\n.   -   When using IMREAD_GRAYSCALE, the codec's internal grayscale conversion will be used, if available.\n.       Results may differ to the output of cvtColor()\n.   -   On Microsoft Windows\\* OS and MacOSX\\*, the codecs shipped with an OpenCV image (libjpeg,\n.       libpng, libtiff, and libjasper) are used by default. So, OpenCV can always read JPEGs, PNGs,\n.       and TIFFs. On MacOSX, there is also an option to use native MacOSX image readers. But beware\n.       that currently these native image loaders give images with different pixel values because of\n.       the color management embedded into MacOSX.\n.   -   On Linux\\*, BSD flavors and other Unix-like open-source operating systems, OpenCV looks for\n.       codecs supplied with an OS image. Install the relevant packages (do not forget the development\n.       files, for example, \"libjpeg-dev\", in Debian\\* and Ubuntu\\*) to get the codec support or turn\n.       on the OPENCV_BUILD_3RDPARTY_LIBS flag in CMake.\n.   -   In the case you set *WITH_GDAL* flag to true in CMake and @ref IMREAD_LOAD_GDAL to load the image,\n.       then the [GDAL](http://www.gdal.org) driver will be used in order to decode the image, supporting\n.       the following formats: [Raster](http://www.gdal.org/formats_list.html),\n.       [Vector](http://www.gdal.org/ogr_formats.html).\n.   -   If EXIF information is embedded in the image file, the EXIF orientation will be taken into account\n.       and thus the image will be rotated accordingly except if the flags @ref IMREAD_IGNORE_ORIENTATION\n.       or @ref IMREAD_UNCHANGED are passed.\n.   -   Use the IMREAD_UNCHANGED flag to keep the floating point values from PFM image.\n.   -   By default number of pixels must be less than 2^30. Limit can be set using system\n.       variable OPENCV_IO_MAX_IMAGE_PIXELS\n.   \n.   @param filename Name of file to be loaded.\n.   @param flags Flag that can take values of cv::ImreadModes", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/424", "data": {"name": "cv2.imread.reshape", "type": "callable", "signature": "(*args, **kwargs)", "description": "imread(filename[, flags]) -> retval\n.   @brief Loads an image from a file.\n.   \n.   @anchor imread\n.   \n.   The function imread loads an image from the specified file and returns it. If the image cannot be\n.   read (because of missing file, improper permissions, unsupported or invalid format), the function\n.   returns an empty matrix ( Mat::data==NULL ).\n.   \n.   Currently, the following file formats are supported:\n.   \n.   -   Windows bitmaps - \\*.bmp, \\*.dib (always supported)\n.   -   JPEG files - \\*.jpeg, \\*.jpg, \\*.jpe (see the *Note* section)\n.   -   JPEG 2000 files - \\*.jp2 (see the *Note* section)\n.   -   Portable Network Graphics - \\*.png (see the *Note* section)\n.   -   WebP - \\*.webp (see the *Note* section)\n.   -   AVIF - \\*.avif (see the *Note* section)\n.   -   Portable image format - \\*.pbm, \\*.pgm, \\*.ppm \\*.pxm, \\*.pnm (always supported)\n.   -   PFM files - \\*.pfm (see the *Note* section)\n.   -   Sun rasters - \\*.sr, \\*.ras (always supported)\n.   -   TIFF files - \\*.tiff, \\*.tif (see the *Note* section)\n.   -   OpenEXR Image files - \\*.exr (see the *Note* section)\n.   -   Radiance HDR - \\*.hdr, \\*.pic (always supported)\n.   -   Raster and Vector geospatial data supported by GDAL (see the *Note* section)\n.   \n.   @note\n.   -   The function determines the type of an image by the content, not by the file extension.\n.   -   In the case of color images, the decoded images will have the channels stored in **B G R** order.\n.   -   When using IMREAD_GRAYSCALE, the codec's internal grayscale conversion will be used, if available.\n.       Results may differ to the output of cvtColor()\n.   -   On Microsoft Windows\\* OS and MacOSX\\*, the codecs shipped with an OpenCV image (libjpeg,\n.       libpng, libtiff, and libjasper) are used by default. So, OpenCV can always read JPEGs, PNGs,\n.       and TIFFs. On MacOSX, there is also an option to use native MacOSX image readers. But beware\n.       that currently these native image loaders give images with different pixel values because of\n.       the color management embedded into MacOSX.\n.   -   On Linux\\*, BSD flavors and other Unix-like open-source operating systems, OpenCV looks for\n.       codecs supplied with an OS image. Install the relevant packages (do not forget the development\n.       files, for example, \"libjpeg-dev\", in Debian\\* and Ubuntu\\*) to get the codec support or turn\n.       on the OPENCV_BUILD_3RDPARTY_LIBS flag in CMake.\n.   -   In the case you set *WITH_GDAL* flag to true in CMake and @ref IMREAD_LOAD_GDAL to load the image,\n.       then the [GDAL](http://www.gdal.org) driver will be used in order to decode the image, supporting\n.       the following formats: [Raster](http://www.gdal.org/formats_list.html),\n.       [Vector](http://www.gdal.org/ogr_formats.html).\n.   -   If EXIF information is embedded in the image file, the EXIF orientation will be taken into account\n.       and thus the image will be rotated accordingly except if the flags @ref IMREAD_IGNORE_ORIENTATION\n.       or @ref IMREAD_UNCHANGED are passed.\n.   -   Use the IMREAD_UNCHANGED flag to keep the floating point values from PFM image.\n.   -   By default number of pixels must be less than 2^30. Limit can be set using system\n.       variable OPENCV_IO_MAX_IMAGE_PIXELS\n.   \n.   @param filename Name of file to be loaded.\n.   @param flags Flag that can take values of cv::ImreadModes", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/424", "data": {"name": "cv2.imwrite", "type": "callable", "signature": "(*args, **kwargs)", "description": "imwrite(filename, img[, params]) -> retval\n.   @brief Saves an image to a specified file.\n.   \n.   The function imwrite saves the image to the specified file. The image format is chosen based on the\n.   filename extension (see cv::imread for the list of extensions). In general, only 8-bit unsigned (CV_8U)\n.   single-channel or 3-channel (with 'BGR' channel order) images\n.   can be saved using this function, with these exceptions:\n.   \n.   - With OpenEXR encoder, only 32-bit float (CV_32F) images can be saved.\n.     - 8-bit unsigned (CV_8U) images are not supported.\n.   - With Radiance HDR encoder, non 64-bit float (CV_64F) images can be saved.\n.     - All images will be converted to 32-bit float (CV_32F).\n.   - With JPEG 2000 encoder, 8-bit unsigned (CV_8U) and 16-bit unsigned (CV_16U) images can be saved.\n.   - With PAM encoder, 8-bit unsigned (CV_8U) and 16-bit unsigned (CV_16U) images can be saved.\n.   - With PNG encoder, 8-bit unsigned (CV_8U) and 16-bit unsigned (CV_16U) images can be saved.\n.     - PNG images with an alpha channel can be saved using this function. To do this, create\n.       8-bit (or 16-bit) 4-channel image BGRA, where the alpha channel goes last. Fully transparent pixels\n.       should have alpha set to 0, fully opaque pixels should have alpha set to 255/65535 (see the code sample below).\n.   - With PGM/PPM encoder, 8-bit unsigned (CV_8U) and 16-bit unsigned (CV_16U) images can be saved.\n.   - With TIFF encoder, 8-bit unsigned (CV_8U), 16-bit unsigned (CV_16U),\n.                        32-bit float (CV_32F) and 64-bit float (CV_64F) images can be saved.\n.     - Multiple images (vector of Mat) can be saved in TIFF format (see the code sample below).\n.     - 32-bit float 3-channel (CV_32FC3) TIFF images will be saved\n.       using the LogLuv high dynamic range encoding (4 bytes per pixel)\n.   \n.   If the image format is not supported, the image will be converted to 8-bit unsigned (CV_8U) and saved that way.\n.   \n.   If the format, depth or channel order is different, use\n.   Mat::convertTo and cv::cvtColor to convert it before saving. Or, use the universal FileStorage I/O\n.   functions to save the image to XML or YAML format.\n.   \n.   The sample below shows how to create a BGRA image, how to set custom compression parameters and save it to a PNG file.\n.   It also demonstrates how to save multiple images in a TIFF file:\n.   @include snippets/imgcodecs_imwrite.cpp\n.   @param filename Name of the file.\n.   @param img (Mat or vector of Mat) Image or Images to be saved.\n.   @param params Format-specific parameters encoded as pairs (paramId_1, paramValue_1, paramId_2, paramValue_2, ... .) see cv::ImwriteFlags", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/424", "data": {"name": "numpy.array", "type": "callable", "signature": "(*args, **kwargs)", "description": "array(object, dtype=None, *, copy=True, order='K', subok=False, ndmin=0,\n      like=None)\n\nCreate an array.\n\nParameters\n----------\nobject : array_like\n    An array, any object exposing the array interface, an object whose\n    __array__ method returns an array, or any (nested) sequence.\n    If object is a scalar, a 0-dimensional array containing object is\n    returned.\ndtype : data-type, optional\n    The desired data-type for the array.  If not given, then the type will\n    be determined as the minimum type required to hold the objects in the\n    sequence.\ncopy : bool, optional\n    If true (default), then the object is copied.  Otherwise, a copy will\n    only be made if __array__ returns a copy, if obj is a nested sequence,\n    or if a copy is needed to satisfy any of the other requirements\n    (`dtype`, `order`, etc.).\norder : {'K', 'A', 'C', 'F'}, optional\n    Specify the memory layout of the array. If object is not an array, the\n    newly created array will be in C order (row major) unless 'F' is\n    specified, in which case it will be in Fortran order (column major).\n    If object is an array the following holds.\n\n    ===== ========= ===================================================\n    order  no copy                     copy=True\n    ===== ========= ===================================================\n    'K'   unchanged F & C order preserved, otherwise most similar order\n    'A'   unchanged F order if input is F and not C, otherwise C order\n    'C'   C order   C order\n    'F'   F order   F order\n    ===== ========= ===================================================\n\n    When ``copy=False`` and a copy is made for other reasons, the result is\n    the same as if ``copy=True``, with some exceptions for 'A', see the\n    Notes section. The default order is 'K'.\nsubok : bool, optional\n    If True, then sub-classes will be passed-through, otherwise\n    the returned array will be forced to be a base-class array (default).\nndmin : int, optional\n    Specifies the minimum number of dimensions that the resulting\n    array should have.  Ones will be prepended to the shape as\n    needed to meet this requirement.\nlike : array_like, optional\n    Reference object to allow the creation of arrays which are not\n    NumPy arrays. If an array-like passed in as ``like`` supports\n    the ``__array_function__`` protocol, the result will be defined\n    by it. In this case, it ensures the creation of an array object\n    compatible with that passed in via this argument.\n\n    .. versionadded:: 1.20.0\n\nReturns\n-------\nout : ndarray\n    An array object satisfying the specified requirements.\n\nSee Also\n--------\nempty_like : Return an empty array with shape and type of input.\nones_like : Return an array of ones with shape and type of input.\nzeros_like : Return an array of zeros with shape and type of input.\nfull_like : Return a new array with shape of input filled with value.\nempty : Return a new uninitialized array.\nones : Return a new array setting values to one.\nzeros : Return a new array setting values to zero.\nfull : Return a new array of given shape filled with value.\n\n\nNotes\n-----\nWhen order is 'A' and `object` is an array in neither 'C' nor 'F' order,\nand a copy is forced by a change in dtype, then the order of the result is\nnot necessarily 'C' as expected. This is likely a bug.\n\nExamples\n--------\n>>> np.array([1, 2, 3])\narray([1, 2, 3])\n\nUpcasting:\n\n>>> np.array([1, 2, 3.0])\narray([ 1.,  2.,  3.])\n\nMore than one dimension:\n\n>>> np.array([[1, 2], [3, 4]])\narray([[1, 2],\n       [3, 4]])\n\nMinimum dimensions 2:\n\n>>> np.array([1, 2, 3], ndmin=2)\narray([[1, 2, 3]])\n\nType provided:\n\n>>> np.array([1, 2, 3], dtype=complex)\narray([ 1.+0.j,  2.+0.j,  3.+0.j])\n\nData-type consisting of more than one element:\n\n>>> x = np.array([(1,2),(3,4)],dtype=[('a','<i4'),('b','<i4')])\n>>> x['a']\narray([1, 3])\n\nCreating an array from sub-classes:\n\n>>> np.array(np.mat('1 2; 3 4'))\narray([[1, 2],\n       [3, 4]])\n\n>>> np.array(np.mat('1 2; 3 4'), subok=True)\nmatrix([[1, 2],\n        [3, 4]])", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/424", "data": {"name": "numpy.stack", "type": "callable", "signature": "(arrays, axis=0)", "description": "Join a sequence of arrays along a new axis.\n\nThe ``axis`` parameter specifies the index of the new axis in the\ndimensions of the result. For example, if ``axis=0`` it will be the first\ndimension and if ``axis=-1`` it will be the last dimension.\n\n.. versionadded:: 1.10.0\n\nParameters\n----------\narrays : sequence of array_like\n    Each array must have the same shape.\n\naxis : int, optional\n    The axis in the result array along which the input arrays are stacked.\n\nout : ndarray, optional\n    If provided, the destination to place the result. The shape must be\n    correct, matching that of what stack would have returned if no\n    out argument were specified.\n\nReturns\n-------\nstacked : ndarray\n    The stacked array has one more dimension than the input arrays.\n\nSee Also\n--------\nconcatenate : Join a sequence of arrays along an existing axis.\nblock : Assemble an nd-array from nested lists of blocks.\nsplit : Split array into a list of multiple sub-arrays of equal size.\n\nExamples\n--------\n>>> arrays = [np.random.randn(3, 4) for _ in range(10)]\n>>> np.stack(arrays, axis=0).shape\n(10, 3, 4)\n\n>>> np.stack(arrays, axis=1).shape\n(3, 10, 4)\n\n>>> np.stack(arrays, axis=2).shape\n(3, 4, 10)\n\n>>> a = np.array([1, 2, 3])\n>>> b = np.array([4, 5, 6])\n>>> np.stack((a, b))\narray([[1, 2, 3],\n       [4, 5, 6]])\n\n>>> np.stack((a, b), axis=-1)\narray([[1, 4],\n       [2, 5],\n       [3, 6]])", "parameters": {"type": "object", "properties": {"arrays": {}, "axis": {"type": "integer", "default": 0}}}}}
{"task_id": "BigCodeBench/424", "data": {"name": "numpy.where", "type": "callable", "signature": "(*args, **kwargs)", "description": "where(condition, [x, y], /)\n\nReturn elements chosen from `x` or `y` depending on `condition`.\n\n.. note::\n    When only `condition` is provided, this function is a shorthand for\n    ``np.asarray(condition).nonzero()``. Using `nonzero` directly should be\n    preferred, as it behaves correctly for subclasses. The rest of this\n    documentation covers only the case where all three arguments are\n    provided.\n\nParameters\n----------\ncondition : array_like, bool\n    Where True, yield `x`, otherwise yield `y`.\nx, y : array_like\n    Values from which to choose. `x`, `y` and `condition` need to be\n    broadcastable to some shape.\n\nReturns\n-------\nout : ndarray\n    An array with elements from `x` where `condition` is True, and elements\n    from `y` elsewhere.\n\nSee Also\n--------\nchoose\nnonzero : The function that is called when x and y are omitted\n\nNotes\n-----\nIf all the arrays are 1-D, `where` is equivalent to::\n\n    [xv if c else yv\n     for c, xv, yv in zip(condition, x, y)]\n\nExamples\n--------\n>>> a = np.arange(10)\n>>> a\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n>>> np.where(a < 5, a, 10*a)\narray([ 0,  1,  2,  3,  4, 50, 60, 70, 80, 90])\n\nThis can be used on multidimensional arrays too:\n\n>>> np.where([[True, False], [True, True]],\n...          [[1, 2], [3, 4]],\n...          [[9, 8], [7, 6]])\narray([[1, 8],\n       [3, 4]])\n\nThe shapes of x, y, and the condition are broadcast together:\n\n>>> x, y = np.ogrid[:3, :4]\n>>> np.where(x < y, x, 10 + y)  # both x and 10+y are broadcast\narray([[10,  0,  0,  0],\n       [10, 11,  1,  1],\n       [10, 11, 12,  2]])\n\n>>> a = np.array([[0, 1, 2],\n...               [0, 2, 4],\n...               [0, 3, 6]])\n>>> np.where(a < 4, a, -1)  # -1 is broadcast\narray([[ 0,  1,  2],\n       [ 0,  2, -1],\n       [ 0,  3, -1]])", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/424", "data": {"name": "os.path.exists", "type": "callable", "signature": "(path)", "description": "Test whether a path exists.  Returns False for broken symbolic links", "parameters": {"type": "object", "properties": {"path": {}}}}}
{"task_id": "BigCodeBench/424", "data": {"name": "segmented_image.reshape(img.shape).astype('uint8').reshape(img.shape)"}}
{"task_id": "BigCodeBench/424", "data": {"name": "sklearn.cluster.KMeans", "type": "class", "signature": "(n_clusters=8, random_state=None)", "description": "K-Means clustering.\n\nRead more in the :ref:`User Guide <k_means>`.\n\nParameters\n----------\n\nn_clusters : int, default=8\n    The number of clusters to form as well as the number of\n    centroids to generate.\n\n    For an example of how to choose an optimal value for `n_clusters` refer to\n    :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_silhouette_analysis.py`.\n\ninit : {'k-means++', 'random'}, callable or array-like of shape             (n_clusters, n_features), default='k-means++'\n    Method for initialization:\n\n    * 'k-means++' : selects initial cluster centroids using sampling             based on an empirical probability distribution of the points'             contribution to the overall inertia. This technique speeds up             convergence. The algorithm implemented is \"greedy k-means++\". It             differs from the vanilla k-means++ by making several trials at             each sampling step and choosing the best centroid among them.\n\n    * 'random': choose `n_clusters` observations (rows) at random from         data for the initial centroids.\n\n    * If an array is passed, it should be of shape (n_clusters, n_features)        and gives the initial centers.\n\n    * If a callable is passed, it should take arguments X, n_clusters and a        random state and return an initialization.\n\n    For an example of how to use the different `init` strategy, see the example\n    entitled :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_digits.py`.\n\nn_init : 'auto' or int, default='auto'\n    Number of times the k-means algorithm is run with different centroid\n    seeds. The final results is the best output of `n_init` consecutive runs\n    in terms of inertia. Several runs are recommended for sparse\n    high-dimensional problems (see :ref:`kmeans_sparse_high_dim`).\n\n    When `n_init='auto'`, the number of runs depends on the value of init:\n    10 if using `init='random'` or `init` is a callable;\n    1 if using `init='k-means++'` or `init` is an array-like.\n\n    .. versionadded:: 1.2\n       Added 'auto' option for `n_init`.\n\n    .. versionchanged:: 1.4\n       Default value for `n_init` changed to `'auto'`.\n\nmax_iter : int, default=300\n    Maximum number of iterations of the k-means algorithm for a\n    single run.\n\ntol : float, default=1e-4\n    Relative tolerance with regards to Frobenius norm of the difference\n    in the cluster centers of two consecutive iterations to declare\n    convergence.\n\nverbose : int, default=0\n    Verbosity mode.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for centroid initialization. Use\n    an int to make the randomness deterministic.\n    See :term:`Glossary <random_state>`.\n\ncopy_x : bool, default=True\n    When pre-computing distances it is more numerically accurate to center\n    the data first. If copy_x is True (default), then the original data is\n    not modified. If False, the original data is modified, and put back\n    before the function returns, but small numerical differences may be\n    introduced by subtracting and then adding the data mean. Note that if\n    the original data is not C-contiguous, a copy will be made even if\n    copy_x is False. If the original data is sparse, but not in CSR format,\n    a copy will be made even if copy_x is False.\n\nalgorithm : {\"lloyd\", \"elkan\"}, default=\"lloyd\"\n    K-means algorithm to use. The classical EM-style algorithm is `\"lloyd\"`.\n    The `\"elkan\"` variation can be more efficient on some datasets with\n    well-defined clusters, by using the triangle inequality. However it's\n    more memory intensive due to the allocation of an extra array of shape\n    `(n_samples, n_clusters)`.\n\n    .. versionchanged:: 0.18\n        Added Elkan algorithm\n\n    .. versionchanged:: 1.1\n        Renamed \"full\" to \"lloyd\", and deprecated \"auto\" and \"full\".\n        Changed \"auto\" to use \"lloyd\" instead of \"elkan\".\n\nAttributes\n----------\ncluster_centers_ : ndarray of shape (n_clusters, n_features)\n    Coordinates of cluster centers. If the algorithm stops before fully\n    converging (see ``tol`` and ``max_iter``), these will not be\n    consistent with ``labels_``.\n\nlabels_ : ndarray of shape (n_samples,)\n    Labels of each point\n\ninertia_ : float\n    Sum of squared distances of samples to their closest cluster center,\n    weighted by the sample weights if provided.\n\nn_iter_ : int\n    Number of iterations run.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nMiniBatchKMeans : Alternative online implementation that does incremental\n    updates of the centers positions using mini-batches.\n    For large scale learning (say n_samples > 10k) MiniBatchKMeans is\n    probably much faster than the default batch implementation.\n\nNotes\n-----\nThe k-means problem is solved using either Lloyd's or Elkan's algorithm.\n\nThe average complexity is given by O(k n T), where n is the number of\nsamples and T is the number of iteration.\n\nThe worst case complexity is given by O(n^(k+2/p)) with\nn = n_samples, p = n_features.\nRefer to :doi:`\"How slow is the k-means method?\" D. Arthur and S. Vassilvitskii -\nSoCG2006.<10.1145/1137856.1137880>` for more details.\n\nIn practice, the k-means algorithm is very fast (one of the fastest\nclustering algorithms available), but it falls in local minima. That's why\nit can be useful to restart it several times.\n\nIf the algorithm stops before fully converging (because of ``tol`` or\n``max_iter``), ``labels_`` and ``cluster_centers_`` will not be consistent,\ni.e. the ``cluster_centers_`` will not be the means of the points in each\ncluster. Also, the estimator will reassign ``labels_`` after the last\niteration to make ``labels_`` consistent with ``predict`` on the training\nset.\n\nExamples\n--------\n\n>>> from sklearn.cluster import KMeans\n>>> import numpy as np\n>>> X = np.array([[1, 2], [1, 4], [1, 0],\n...               [10, 2], [10, 4], [10, 0]])\n>>> kmeans = KMeans(n_clusters=2, random_state=0, n_init=\"auto\").fit(X)\n>>> kmeans.labels_\narray([1, 1, 1, 0, 0, 0], dtype=int32)\n>>> kmeans.predict([[0, 0], [12, 3]])\narray([1, 0], dtype=int32)\n>>> kmeans.cluster_centers_\narray([[10.,  2.],\n       [ 1.,  2.]])\n\nFor a more detailed example of K-Means using the iris dataset see\n:ref:`sphx_glr_auto_examples_cluster_plot_cluster_iris.py`.\n\nFor examples of common problems with K-Means and how to address them see\n:ref:`sphx_glr_auto_examples_cluster_plot_kmeans_assumptions.py`.\n\nFor an example of how to use K-Means to perform color quantization see\n:ref:`sphx_glr_auto_examples_cluster_plot_color_quantization.py`.\n\nFor a demonstration of how K-Means can be used to cluster text documents see\n:ref:`sphx_glr_auto_examples_text_plot_document_clustering.py`.\n\nFor a comparison between K-Means and MiniBatchKMeans refer to example\n:ref:`sphx_glr_auto_examples_cluster_plot_mini_batch_kmeans.py`.", "parameters": {"type": "object", "properties": {"n_clusters": {"type": "integer", "default": 8}, "random_state": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/424", "data": {"name": "sklearn.cluster.KMeans.labels_", "type": "class", "signature": "(n_clusters=8, *, init='k-means++', n_init='auto', max_iter=300, tol=0.0001, verbose=0, random_state=None, copy_x=True, algorithm='lloyd')", "description": "K-Means clustering.\n\nRead more in the :ref:`User Guide <k_means>`.\n\nParameters\n----------\n\nn_clusters : int, default=8\n    The number of clusters to form as well as the number of\n    centroids to generate.\n\n    For an example of how to choose an optimal value for `n_clusters` refer to\n    :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_silhouette_analysis.py`.\n\ninit : {'k-means++', 'random'}, callable or array-like of shape             (n_clusters, n_features), default='k-means++'\n    Method for initialization:\n\n    * 'k-means++' : selects initial cluster centroids using sampling             based on an empirical probability distribution of the points'             contribution to the overall inertia. This technique speeds up             convergence. The algorithm implemented is \"greedy k-means++\". It             differs from the vanilla k-means++ by making several trials at             each sampling step and choosing the best centroid among them.\n\n    * 'random': choose `n_clusters` observations (rows) at random from         data for the initial centroids.\n\n    * If an array is passed, it should be of shape (n_clusters, n_features)        and gives the initial centers.\n\n    * If a callable is passed, it should take arguments X, n_clusters and a        random state and return an initialization.\n\n    For an example of how to use the different `init` strategy, see the example\n    entitled :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_digits.py`.\n\nn_init : 'auto' or int, default='auto'\n    Number of times the k-means algorithm is run with different centroid\n    seeds. The final results is the best output of `n_init` consecutive runs\n    in terms of inertia. Several runs are recommended for sparse\n    high-dimensional problems (see :ref:`kmeans_sparse_high_dim`).\n\n    When `n_init='auto'`, the number of runs depends on the value of init:\n    10 if using `init='random'` or `init` is a callable;\n    1 if using `init='k-means++'` or `init` is an array-like.\n\n    .. versionadded:: 1.2\n       Added 'auto' option for `n_init`.\n\n    .. versionchanged:: 1.4\n       Default value for `n_init` changed to `'auto'`.\n\nmax_iter : int, default=300\n    Maximum number of iterations of the k-means algorithm for a\n    single run.\n\ntol : float, default=1e-4\n    Relative tolerance with regards to Frobenius norm of the difference\n    in the cluster centers of two consecutive iterations to declare\n    convergence.\n\nverbose : int, default=0\n    Verbosity mode.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for centroid initialization. Use\n    an int to make the randomness deterministic.\n    See :term:`Glossary <random_state>`.\n\ncopy_x : bool, default=True\n    When pre-computing distances it is more numerically accurate to center\n    the data first. If copy_x is True (default), then the original data is\n    not modified. If False, the original data is modified, and put back\n    before the function returns, but small numerical differences may be\n    introduced by subtracting and then adding the data mean. Note that if\n    the original data is not C-contiguous, a copy will be made even if\n    copy_x is False. If the original data is sparse, but not in CSR format,\n    a copy will be made even if copy_x is False.\n\nalgorithm : {\"lloyd\", \"elkan\"}, default=\"lloyd\"\n    K-means algorithm to use. The classical EM-style algorithm is `\"lloyd\"`.\n    The `\"elkan\"` variation can be more efficient on some datasets with\n    well-defined clusters, by using the triangle inequality. However it's\n    more memory intensive due to the allocation of an extra array of shape\n    `(n_samples, n_clusters)`.\n\n    .. versionchanged:: 0.18\n        Added Elkan algorithm\n\n    .. versionchanged:: 1.1\n        Renamed \"full\" to \"lloyd\", and deprecated \"auto\" and \"full\".\n        Changed \"auto\" to use \"lloyd\" instead of \"elkan\".\n\nAttributes\n----------\ncluster_centers_ : ndarray of shape (n_clusters, n_features)\n    Coordinates of cluster centers. If the algorithm stops before fully\n    converging (see ``tol`` and ``max_iter``), these will not be\n    consistent with ``labels_``.\n\nlabels_ : ndarray of shape (n_samples,)\n    Labels of each point\n\ninertia_ : float\n    Sum of squared distances of samples to their closest cluster center,\n    weighted by the sample weights if provided.\n\nn_iter_ : int\n    Number of iterations run.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nMiniBatchKMeans : Alternative online implementation that does incremental\n    updates of the centers positions using mini-batches.\n    For large scale learning (say n_samples > 10k) MiniBatchKMeans is\n    probably much faster than the default batch implementation.\n\nNotes\n-----\nThe k-means problem is solved using either Lloyd's or Elkan's algorithm.\n\nThe average complexity is given by O(k n T), where n is the number of\nsamples and T is the number of iteration.\n\nThe worst case complexity is given by O(n^(k+2/p)) with\nn = n_samples, p = n_features.\nRefer to :doi:`\"How slow is the k-means method?\" D. Arthur and S. Vassilvitskii -\nSoCG2006.<10.1145/1137856.1137880>` for more details.\n\nIn practice, the k-means algorithm is very fast (one of the fastest\nclustering algorithms available), but it falls in local minima. That's why\nit can be useful to restart it several times.\n\nIf the algorithm stops before fully converging (because of ``tol`` or\n``max_iter``), ``labels_`` and ``cluster_centers_`` will not be consistent,\ni.e. the ``cluster_centers_`` will not be the means of the points in each\ncluster. Also, the estimator will reassign ``labels_`` after the last\niteration to make ``labels_`` consistent with ``predict`` on the training\nset.\n\nExamples\n--------\n\n>>> from sklearn.cluster import KMeans\n>>> import numpy as np\n>>> X = np.array([[1, 2], [1, 4], [1, 0],\n...               [10, 2], [10, 4], [10, 0]])\n>>> kmeans = KMeans(n_clusters=2, random_state=0, n_init=\"auto\").fit(X)\n>>> kmeans.labels_\narray([1, 1, 1, 0, 0, 0], dtype=int32)\n>>> kmeans.predict([[0, 0], [12, 3]])\narray([1, 0], dtype=int32)\n>>> kmeans.cluster_centers_\narray([[10.,  2.],\n       [ 1.,  2.]])\n\nFor a more detailed example of K-Means using the iris dataset see\n:ref:`sphx_glr_auto_examples_cluster_plot_cluster_iris.py`.\n\nFor examples of common problems with K-Means and how to address them see\n:ref:`sphx_glr_auto_examples_cluster_plot_kmeans_assumptions.py`.\n\nFor an example of how to use K-Means to perform color quantization see\n:ref:`sphx_glr_auto_examples_cluster_plot_color_quantization.py`.\n\nFor a demonstration of how K-Means can be used to cluster text documents see\n:ref:`sphx_glr_auto_examples_text_plot_document_clustering.py`.\n\nFor a comparison between K-Means and MiniBatchKMeans refer to example\n:ref:`sphx_glr_auto_examples_cluster_plot_mini_batch_kmeans.py`."}}
{"task_id": "BigCodeBench/424", "data": {"name": "sklearn.cluster.KMeans.labels_.reshape", "type": "class", "signature": "(n_clusters=8)", "description": "K-Means clustering.\n\nRead more in the :ref:`User Guide <k_means>`.\n\nParameters\n----------\n\nn_clusters : int, default=8\n    The number of clusters to form as well as the number of\n    centroids to generate.\n\n    For an example of how to choose an optimal value for `n_clusters` refer to\n    :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_silhouette_analysis.py`.\n\ninit : {'k-means++', 'random'}, callable or array-like of shape             (n_clusters, n_features), default='k-means++'\n    Method for initialization:\n\n    * 'k-means++' : selects initial cluster centroids using sampling             based on an empirical probability distribution of the points'             contribution to the overall inertia. This technique speeds up             convergence. The algorithm implemented is \"greedy k-means++\". It             differs from the vanilla k-means++ by making several trials at             each sampling step and choosing the best centroid among them.\n\n    * 'random': choose `n_clusters` observations (rows) at random from         data for the initial centroids.\n\n    * If an array is passed, it should be of shape (n_clusters, n_features)        and gives the initial centers.\n\n    * If a callable is passed, it should take arguments X, n_clusters and a        random state and return an initialization.\n\n    For an example of how to use the different `init` strategy, see the example\n    entitled :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_digits.py`.\n\nn_init : 'auto' or int, default='auto'\n    Number of times the k-means algorithm is run with different centroid\n    seeds. The final results is the best output of `n_init` consecutive runs\n    in terms of inertia. Several runs are recommended for sparse\n    high-dimensional problems (see :ref:`kmeans_sparse_high_dim`).\n\n    When `n_init='auto'`, the number of runs depends on the value of init:\n    10 if using `init='random'` or `init` is a callable;\n    1 if using `init='k-means++'` or `init` is an array-like.\n\n    .. versionadded:: 1.2\n       Added 'auto' option for `n_init`.\n\n    .. versionchanged:: 1.4\n       Default value for `n_init` changed to `'auto'`.\n\nmax_iter : int, default=300\n    Maximum number of iterations of the k-means algorithm for a\n    single run.\n\ntol : float, default=1e-4\n    Relative tolerance with regards to Frobenius norm of the difference\n    in the cluster centers of two consecutive iterations to declare\n    convergence.\n\nverbose : int, default=0\n    Verbosity mode.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for centroid initialization. Use\n    an int to make the randomness deterministic.\n    See :term:`Glossary <random_state>`.\n\ncopy_x : bool, default=True\n    When pre-computing distances it is more numerically accurate to center\n    the data first. If copy_x is True (default), then the original data is\n    not modified. If False, the original data is modified, and put back\n    before the function returns, but small numerical differences may be\n    introduced by subtracting and then adding the data mean. Note that if\n    the original data is not C-contiguous, a copy will be made even if\n    copy_x is False. If the original data is sparse, but not in CSR format,\n    a copy will be made even if copy_x is False.\n\nalgorithm : {\"lloyd\", \"elkan\"}, default=\"lloyd\"\n    K-means algorithm to use. The classical EM-style algorithm is `\"lloyd\"`.\n    The `\"elkan\"` variation can be more efficient on some datasets with\n    well-defined clusters, by using the triangle inequality. However it's\n    more memory intensive due to the allocation of an extra array of shape\n    `(n_samples, n_clusters)`.\n\n    .. versionchanged:: 0.18\n        Added Elkan algorithm\n\n    .. versionchanged:: 1.1\n        Renamed \"full\" to \"lloyd\", and deprecated \"auto\" and \"full\".\n        Changed \"auto\" to use \"lloyd\" instead of \"elkan\".\n\nAttributes\n----------\ncluster_centers_ : ndarray of shape (n_clusters, n_features)\n    Coordinates of cluster centers. If the algorithm stops before fully\n    converging (see ``tol`` and ``max_iter``), these will not be\n    consistent with ``labels_``.\n\nlabels_ : ndarray of shape (n_samples,)\n    Labels of each point\n\ninertia_ : float\n    Sum of squared distances of samples to their closest cluster center,\n    weighted by the sample weights if provided.\n\nn_iter_ : int\n    Number of iterations run.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nMiniBatchKMeans : Alternative online implementation that does incremental\n    updates of the centers positions using mini-batches.\n    For large scale learning (say n_samples > 10k) MiniBatchKMeans is\n    probably much faster than the default batch implementation.\n\nNotes\n-----\nThe k-means problem is solved using either Lloyd's or Elkan's algorithm.\n\nThe average complexity is given by O(k n T), where n is the number of\nsamples and T is the number of iteration.\n\nThe worst case complexity is given by O(n^(k+2/p)) with\nn = n_samples, p = n_features.\nRefer to :doi:`\"How slow is the k-means method?\" D. Arthur and S. Vassilvitskii -\nSoCG2006.<10.1145/1137856.1137880>` for more details.\n\nIn practice, the k-means algorithm is very fast (one of the fastest\nclustering algorithms available), but it falls in local minima. That's why\nit can be useful to restart it several times.\n\nIf the algorithm stops before fully converging (because of ``tol`` or\n``max_iter``), ``labels_`` and ``cluster_centers_`` will not be consistent,\ni.e. the ``cluster_centers_`` will not be the means of the points in each\ncluster. Also, the estimator will reassign ``labels_`` after the last\niteration to make ``labels_`` consistent with ``predict`` on the training\nset.\n\nExamples\n--------\n\n>>> from sklearn.cluster import KMeans\n>>> import numpy as np\n>>> X = np.array([[1, 2], [1, 4], [1, 0],\n...               [10, 2], [10, 4], [10, 0]])\n>>> kmeans = KMeans(n_clusters=2, random_state=0, n_init=\"auto\").fit(X)\n>>> kmeans.labels_\narray([1, 1, 1, 0, 0, 0], dtype=int32)\n>>> kmeans.predict([[0, 0], [12, 3]])\narray([1, 0], dtype=int32)\n>>> kmeans.cluster_centers_\narray([[10.,  2.],\n       [ 1.,  2.]])\n\nFor a more detailed example of K-Means using the iris dataset see\n:ref:`sphx_glr_auto_examples_cluster_plot_cluster_iris.py`.\n\nFor examples of common problems with K-Means and how to address them see\n:ref:`sphx_glr_auto_examples_cluster_plot_kmeans_assumptions.py`.\n\nFor an example of how to use K-Means to perform color quantization see\n:ref:`sphx_glr_auto_examples_cluster_plot_color_quantization.py`.\n\nFor a demonstration of how K-Means can be used to cluster text documents see\n:ref:`sphx_glr_auto_examples_text_plot_document_clustering.py`.\n\nFor a comparison between K-Means and MiniBatchKMeans refer to example\n:ref:`sphx_glr_auto_examples_cluster_plot_mini_batch_kmeans.py`.", "parameters": {"type": "object", "properties": {"n_clusters": {"type": "integer", "default": 8}}}}}
{"task_id": "BigCodeBench/424", "data": {"name": "sklearn.cluster.KMeans.fit", "type": "callable", "signature": "(self, X)", "description": "Compute k-means clustering.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training instances to cluster. It must be noted that the data\n    will be converted to C ordering, which will cause a memory\n    copy if the given data is not C-contiguous.\n    If a sparse matrix is passed, a copy will be made if it's not in\n    CSR format.\n\ny : Ignored\n    Not used, present here for API consistency by convention.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    The weights for each observation in X. If None, all observations\n    are assigned equal weight. `sample_weight` is not used during\n    initialization if `init` is a callable or a user provided array.\n\n    .. versionadded:: 0.20\n\nReturns\n-------\nself : object\n    Fitted estimator.", "parameters": {"type": "object", "properties": {"X": {}}}}}
{"task_id": "BigCodeBench/424", "data": {"name": "sklearn.cluster.KMeans.cluster_centers_", "type": "class", "signature": "(n_clusters=8, *, init='k-means++', n_init='auto', max_iter=300, tol=0.0001, verbose=0, random_state=None, copy_x=True, algorithm='lloyd')", "description": "K-Means clustering.\n\nRead more in the :ref:`User Guide <k_means>`.\n\nParameters\n----------\n\nn_clusters : int, default=8\n    The number of clusters to form as well as the number of\n    centroids to generate.\n\n    For an example of how to choose an optimal value for `n_clusters` refer to\n    :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_silhouette_analysis.py`.\n\ninit : {'k-means++', 'random'}, callable or array-like of shape             (n_clusters, n_features), default='k-means++'\n    Method for initialization:\n\n    * 'k-means++' : selects initial cluster centroids using sampling             based on an empirical probability distribution of the points'             contribution to the overall inertia. This technique speeds up             convergence. The algorithm implemented is \"greedy k-means++\". It             differs from the vanilla k-means++ by making several trials at             each sampling step and choosing the best centroid among them.\n\n    * 'random': choose `n_clusters` observations (rows) at random from         data for the initial centroids.\n\n    * If an array is passed, it should be of shape (n_clusters, n_features)        and gives the initial centers.\n\n    * If a callable is passed, it should take arguments X, n_clusters and a        random state and return an initialization.\n\n    For an example of how to use the different `init` strategy, see the example\n    entitled :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_digits.py`.\n\nn_init : 'auto' or int, default='auto'\n    Number of times the k-means algorithm is run with different centroid\n    seeds. The final results is the best output of `n_init` consecutive runs\n    in terms of inertia. Several runs are recommended for sparse\n    high-dimensional problems (see :ref:`kmeans_sparse_high_dim`).\n\n    When `n_init='auto'`, the number of runs depends on the value of init:\n    10 if using `init='random'` or `init` is a callable;\n    1 if using `init='k-means++'` or `init` is an array-like.\n\n    .. versionadded:: 1.2\n       Added 'auto' option for `n_init`.\n\n    .. versionchanged:: 1.4\n       Default value for `n_init` changed to `'auto'`.\n\nmax_iter : int, default=300\n    Maximum number of iterations of the k-means algorithm for a\n    single run.\n\ntol : float, default=1e-4\n    Relative tolerance with regards to Frobenius norm of the difference\n    in the cluster centers of two consecutive iterations to declare\n    convergence.\n\nverbose : int, default=0\n    Verbosity mode.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for centroid initialization. Use\n    an int to make the randomness deterministic.\n    See :term:`Glossary <random_state>`.\n\ncopy_x : bool, default=True\n    When pre-computing distances it is more numerically accurate to center\n    the data first. If copy_x is True (default), then the original data is\n    not modified. If False, the original data is modified, and put back\n    before the function returns, but small numerical differences may be\n    introduced by subtracting and then adding the data mean. Note that if\n    the original data is not C-contiguous, a copy will be made even if\n    copy_x is False. If the original data is sparse, but not in CSR format,\n    a copy will be made even if copy_x is False.\n\nalgorithm : {\"lloyd\", \"elkan\"}, default=\"lloyd\"\n    K-means algorithm to use. The classical EM-style algorithm is `\"lloyd\"`.\n    The `\"elkan\"` variation can be more efficient on some datasets with\n    well-defined clusters, by using the triangle inequality. However it's\n    more memory intensive due to the allocation of an extra array of shape\n    `(n_samples, n_clusters)`.\n\n    .. versionchanged:: 0.18\n        Added Elkan algorithm\n\n    .. versionchanged:: 1.1\n        Renamed \"full\" to \"lloyd\", and deprecated \"auto\" and \"full\".\n        Changed \"auto\" to use \"lloyd\" instead of \"elkan\".\n\nAttributes\n----------\ncluster_centers_ : ndarray of shape (n_clusters, n_features)\n    Coordinates of cluster centers. If the algorithm stops before fully\n    converging (see ``tol`` and ``max_iter``), these will not be\n    consistent with ``labels_``.\n\nlabels_ : ndarray of shape (n_samples,)\n    Labels of each point\n\ninertia_ : float\n    Sum of squared distances of samples to their closest cluster center,\n    weighted by the sample weights if provided.\n\nn_iter_ : int\n    Number of iterations run.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nMiniBatchKMeans : Alternative online implementation that does incremental\n    updates of the centers positions using mini-batches.\n    For large scale learning (say n_samples > 10k) MiniBatchKMeans is\n    probably much faster than the default batch implementation.\n\nNotes\n-----\nThe k-means problem is solved using either Lloyd's or Elkan's algorithm.\n\nThe average complexity is given by O(k n T), where n is the number of\nsamples and T is the number of iteration.\n\nThe worst case complexity is given by O(n^(k+2/p)) with\nn = n_samples, p = n_features.\nRefer to :doi:`\"How slow is the k-means method?\" D. Arthur and S. Vassilvitskii -\nSoCG2006.<10.1145/1137856.1137880>` for more details.\n\nIn practice, the k-means algorithm is very fast (one of the fastest\nclustering algorithms available), but it falls in local minima. That's why\nit can be useful to restart it several times.\n\nIf the algorithm stops before fully converging (because of ``tol`` or\n``max_iter``), ``labels_`` and ``cluster_centers_`` will not be consistent,\ni.e. the ``cluster_centers_`` will not be the means of the points in each\ncluster. Also, the estimator will reassign ``labels_`` after the last\niteration to make ``labels_`` consistent with ``predict`` on the training\nset.\n\nExamples\n--------\n\n>>> from sklearn.cluster import KMeans\n>>> import numpy as np\n>>> X = np.array([[1, 2], [1, 4], [1, 0],\n...               [10, 2], [10, 4], [10, 0]])\n>>> kmeans = KMeans(n_clusters=2, random_state=0, n_init=\"auto\").fit(X)\n>>> kmeans.labels_\narray([1, 1, 1, 0, 0, 0], dtype=int32)\n>>> kmeans.predict([[0, 0], [12, 3]])\narray([1, 0], dtype=int32)\n>>> kmeans.cluster_centers_\narray([[10.,  2.],\n       [ 1.,  2.]])\n\nFor a more detailed example of K-Means using the iris dataset see\n:ref:`sphx_glr_auto_examples_cluster_plot_cluster_iris.py`.\n\nFor examples of common problems with K-Means and how to address them see\n:ref:`sphx_glr_auto_examples_cluster_plot_kmeans_assumptions.py`.\n\nFor an example of how to use K-Means to perform color quantization see\n:ref:`sphx_glr_auto_examples_cluster_plot_color_quantization.py`.\n\nFor a demonstration of how K-Means can be used to cluster text documents see\n:ref:`sphx_glr_auto_examples_text_plot_document_clustering.py`.\n\nFor a comparison between K-Means and MiniBatchKMeans refer to example\n:ref:`sphx_glr_auto_examples_cluster_plot_mini_batch_kmeans.py`.", "parameters": {"type": "object", "properties": {"n_clusters": {"type": "integer", "default": 8}, "init": {"type": "str", "default": "k-means++"}, "n_init": {"type": "str", "default": "auto"}, "max_iter": {"type": "integer", "default": 300}, "tol": {"type": "float", "default": 0.0001}, "verbose": {"type": "integer", "default": 0}, "random_state": {"type": "NoneType", "default": null}, "copy_x": {"type": "bool", "default": true}, "algorithm": {"type": "str", "default": "lloyd"}}}}}
{"task_id": "BigCodeBench/443", "data": {"name": "T.shape"}}
{"task_id": "BigCodeBench/443", "data": {"name": "matplotlib.pyplot.Axes", "type": "class", "signature": "(fig, *args, facecolor=None, frameon=True, sharex=None, sharey=None, label='', xscale=None, yscale=None, box_aspect=None, **kwargs)", "description": "An Axes object encapsulates all the elements of an individual (sub-)plot in\na figure.\n\nIt contains most of the (sub-)plot elements: `~.axis.Axis`,\n`~.axis.Tick`, `~.lines.Line2D`, `~.text.Text`, `~.patches.Polygon`, etc.,\nand sets the coordinate system.\n\nLike all visible elements in a figure, Axes is an `.Artist` subclass.\n\nThe `Axes` instance supports callbacks through a callbacks attribute which\nis a `~.cbook.CallbackRegistry` instance.  The events you can connect to\nare 'xlim_changed' and 'ylim_changed' and the callback will be called with\nfunc(*ax*) where *ax* is the `Axes` instance.\n\n.. note::\n\n    As a user, you do not instantiate Axes directly, but use Axes creation\n    methods instead; e.g. from `.pyplot` or `.Figure`:\n    `~.pyplot.subplots`, `~.pyplot.subplot_mosaic` or `.Figure.add_axes`.\n\nAttributes\n----------\ndataLim : `.Bbox`\n    The bounding box enclosing all data displayed in the Axes.\nviewLim : `.Bbox`\n    The view limits in data coordinates.", "parameters": {"type": "object", "properties": {"fig": {}, "facecolor": {"type": "NoneType", "default": null}, "frameon": {"type": "bool", "default": true}, "sharex": {"type": "NoneType", "default": null}, "sharey": {"type": "NoneType", "default": null}, "label": {"type": "str", "default": ""}, "xscale": {"type": "NoneType", "default": null}, "yscale": {"type": "NoneType", "default": null}, "box_aspect": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/443", "data": {"name": "matplotlib.pyplot.subplots", "type": "class", "signature": "()", "description": "Create a figure and a set of subplots.\n\nThis utility wrapper makes it convenient to create common layouts of\nsubplots, including the enclosing figure object, in a single call.\n\nParameters\n----------\nnrows, ncols : int, default: 1\n    Number of rows/columns of the subplot grid.\n\nsharex, sharey : bool or {'none', 'all', 'row', 'col'}, default: False\n    Controls sharing of properties among x (*sharex*) or y (*sharey*)\n    axes:\n\n    - True or 'all': x- or y-axis will be shared among all subplots.\n    - False or 'none': each subplot x- or y-axis will be independent.\n    - 'row': each subplot row will share an x- or y-axis.\n    - 'col': each subplot column will share an x- or y-axis.\n\n    When subplots have a shared x-axis along a column, only the x tick\n    labels of the bottom subplot are created. Similarly, when subplots\n    have a shared y-axis along a row, only the y tick labels of the first\n    column subplot are created. To later turn other subplots' ticklabels\n    on, use `~matplotlib.axes.Axes.tick_params`.\n\n    When subplots have a shared axis that has units, calling\n    `~matplotlib.axis.Axis.set_units` will update each axis with the\n    new units.\n\nsqueeze : bool, default: True\n    - If True, extra dimensions are squeezed out from the returned\n      array of `~matplotlib.axes.Axes`:\n\n      - if only one subplot is constructed (nrows=ncols=1), the\n        resulting single Axes object is returned as a scalar.\n      - for Nx1 or 1xM subplots, the returned object is a 1D numpy\n        object array of Axes objects.\n      - for NxM, subplots with N>1 and M>1 are returned as a 2D array.\n\n    - If False, no squeezing at all is done: the returned Axes object is\n      always a 2D array containing Axes instances, even if it ends up\n      being 1x1.\n\nwidth_ratios : array-like of length *ncols*, optional\n    Defines the relative widths of the columns. Each column gets a\n    relative width of ``width_ratios[i] / sum(width_ratios)``.\n    If not given, all columns will have the same width.  Equivalent\n    to ``gridspec_kw={'width_ratios': [...]}``.\n\nheight_ratios : array-like of length *nrows*, optional\n    Defines the relative heights of the rows. Each row gets a\n    relative height of ``height_ratios[i] / sum(height_ratios)``.\n    If not given, all rows will have the same height. Convenience\n    for ``gridspec_kw={'height_ratios': [...]}``.\n\nsubplot_kw : dict, optional\n    Dict with keywords passed to the\n    `~matplotlib.figure.Figure.add_subplot` call used to create each\n    subplot.\n\ngridspec_kw : dict, optional\n    Dict with keywords passed to the `~matplotlib.gridspec.GridSpec`\n    constructor used to create the grid the subplots are placed on.\n\n**fig_kw\n    All additional keyword arguments are passed to the\n    `.pyplot.figure` call.\n\nReturns\n-------\nfig : `.Figure`\n\nax : `~matplotlib.axes.Axes` or array of Axes\n    *ax* can be either a single `~.axes.Axes` object, or an array of Axes\n    objects if more than one subplot was created.  The dimensions of the\n    resulting array can be controlled with the squeeze keyword, see above.\n\n    Typical idioms for handling the return value are::\n\n        # using the variable ax for single a Axes\n        fig, ax = plt.subplots()\n\n        # using the variable axs for multiple Axes\n        fig, axs = plt.subplots(2, 2)\n\n        # using tuple unpacking for multiple Axes\n        fig, (ax1, ax2) = plt.subplots(1, 2)\n        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n\n    The names ``ax`` and pluralized ``axs`` are preferred over ``axes``\n    because for the latter it's not clear if it refers to a single\n    `~.axes.Axes` instance or a collection of these.\n\nSee Also\n--------\n.pyplot.figure\n.pyplot.subplot\n.pyplot.axes\n.Figure.subplots\n.Figure.add_subplot\n\nExamples\n--------\n::\n\n    # First create some toy data:\n    x = np.linspace(0, 2*np.pi, 400)\n    y = np.sin(x**2)\n\n    # Create just a figure and only one subplot\n    fig, ax = plt.subplots()\n    ax.plot(x, y)\n    ax.set_title('Simple plot')\n\n    # Create two subplots and unpack the output array immediately\n    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n    ax1.plot(x, y)\n    ax1.set_title('Sharing Y axis')\n    ax2.scatter(x, y)\n\n    # Create four polar axes and access them through the returned array\n    fig, axs = plt.subplots(2, 2, subplot_kw=dict(projection=\"polar\"))\n    axs[0, 0].plot(x, y)\n    axs[1, 1].scatter(x, y)\n\n    # Share a X axis with each column of subplots\n    plt.subplots(2, 2, sharex='col')\n\n    # Share a Y axis with each row of subplots\n    plt.subplots(2, 2, sharey='row')\n\n    # Share both X and Y axes with all subplots\n    plt.subplots(2, 2, sharex='all', sharey='all')\n\n    # Note that this is the same as\n    plt.subplots(2, 2, sharex=True, sharey=True)\n\n    # Create figure number 10 with a single subplot\n    # and clears it if it already exists.\n    fig, ax = plt.subplots(num=10, clear=True)", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/443", "data": {"name": "matplotlib.pyplot.subplots[1].set_title", "type": "method", "signature": "(label, fontdict=None, loc=None, pad=None, *, y=None, **kwargs)", "description": "Set a title for the Axes.\n\nSet one of the three available Axes titles. The available titles\nare positioned above the Axes in the center, flush with the left\nedge, and flush with the right edge.\n\nParameters\n----------\nlabel : str\n    Text to use for the title\n\nfontdict : dict\n\n    .. admonition:: Discouraged\n\n       The use of *fontdict* is discouraged. Parameters should be passed as\n       individual keyword arguments or using dictionary-unpacking\n       ``set_title(..., **fontdict)``.\n\n    A dictionary controlling the appearance of the title text,\n    the default *fontdict* is::\n\n       {'fontsize': rcParams['axes.titlesize'],\n        'fontweight': rcParams['axes.titleweight'],\n        'color': rcParams['axes.titlecolor'],\n        'verticalalignment': 'baseline',\n        'horizontalalignment': loc}\n\nloc : {'center', 'left', 'right'}, default: :rc:`axes.titlelocation`\n    Which title to set.\n\ny : float, default: :rc:`axes.titley`\n    Vertical Axes location for the title (1.0 is the top).  If\n    None (the default) and :rc:`axes.titley` is also None, y is\n    determined automatically to avoid decorators on the Axes.\n\npad : float, default: :rc:`axes.titlepad`\n    The offset of the title from the top of the Axes, in points.\n\nReturns\n-------\n`.Text`\n    The matplotlib text instance representing the title\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    Other keyword arguments are text properties, see `.Text` for a list\n    of valid text properties.", "parameters": {"type": "object", "properties": {"label": {}, "fontdict": {"type": "NoneType", "default": null}, "loc": {"type": "NoneType", "default": null}, "pad": {"type": "NoneType", "default": null}, "y": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/443", "data": {"name": "matplotlib.pyplot.subplots[1].scatter", "type": "method", "signature": "(x, y, s=None, c=None, marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=None, linewidths=None, *, edgecolors=None, plotnonfinite=False, data=None, **kwargs)", "description": "A scatter plot of *y* vs. *x* with varying marker size and/or color.\n\nParameters\n----------\nx, y : float or array-like, shape (n, )\n    The data positions.\n\ns : float or array-like, shape (n, ), optional\n    The marker size in points**2 (typographic points are 1/72 in.).\n    Default is ``rcParams['lines.markersize'] ** 2``.\n\n    The linewidth and edgecolor can visually interact with the marker\n    size, and can lead to artifacts if the marker size is smaller than\n    the linewidth.\n\n    If the linewidth is greater than 0 and the edgecolor is anything\n    but *'none'*, then the effective size of the marker will be\n    increased by half the linewidth because the stroke will be centered\n    on the edge of the shape.\n\n    To eliminate the marker edge either set *linewidth=0* or\n    *edgecolor='none'*.\n\nc : array-like or list of colors or color, optional\n    The marker colors. Possible values:\n\n    - A scalar or sequence of n numbers to be mapped to colors using\n      *cmap* and *norm*.\n    - A 2D array in which the rows are RGB or RGBA.\n    - A sequence of colors of length n.\n    - A single color format string.\n\n    Note that *c* should not be a single numeric RGB or RGBA sequence\n    because that is indistinguishable from an array of values to be\n    colormapped. If you want to specify the same RGB or RGBA value for\n    all points, use a 2D array with a single row.  Otherwise,\n    value-matching will have precedence in case of a size matching with\n    *x* and *y*.\n\n    If you wish to specify a single color for all points\n    prefer the *color* keyword argument.\n\n    Defaults to `None`. In that case the marker color is determined\n    by the value of *color*, *facecolor* or *facecolors*. In case\n    those are not specified or `None`, the marker color is determined\n    by the next color of the ``Axes``' current \"shape and fill\" color\n    cycle. This cycle defaults to :rc:`axes.prop_cycle`.\n\nmarker : `~.markers.MarkerStyle`, default: :rc:`scatter.marker`\n    The marker style. *marker* can be either an instance of the class\n    or the text shorthand for a particular marker.\n    See :mod:`matplotlib.markers` for more information about marker\n    styles.\n\ncmap : str or `~matplotlib.colors.Colormap`, default: :rc:`image.cmap`\n    The Colormap instance or registered colormap name used to map scalar data\n    to colors.\n\n    This parameter is ignored if *c* is RGB(A).\n\nnorm : str or `~matplotlib.colors.Normalize`, optional\n    The normalization method used to scale scalar data to the [0, 1] range\n    before mapping to colors using *cmap*. By default, a linear scaling is\n    used, mapping the lowest value to 0 and the highest to 1.\n\n    If given, this can be one of the following:\n\n    - An instance of `.Normalize` or one of its subclasses\n      (see :ref:`colormapnorms`).\n    - A scale name, i.e. one of \"linear\", \"log\", \"symlog\", \"logit\", etc.  For a\n      list of available scales, call `matplotlib.scale.get_scale_names()`.\n      In that case, a suitable `.Normalize` subclass is dynamically generated\n      and instantiated.\n\n    This parameter is ignored if *c* is RGB(A).\n\nvmin, vmax : float, optional\n    When using scalar data and no explicit *norm*, *vmin* and *vmax* define\n    the data range that the colormap covers. By default, the colormap covers\n    the complete value range of the supplied data. It is an error to use\n    *vmin*/*vmax* when a *norm* instance is given (but using a `str` *norm*\n    name together with *vmin*/*vmax* is acceptable).\n\n    This parameter is ignored if *c* is RGB(A).\n\nalpha : float, default: None\n    The alpha blending value, between 0 (transparent) and 1 (opaque).\n\nlinewidths : float or array-like, default: :rc:`lines.linewidth`\n    The linewidth of the marker edges. Note: The default *edgecolors*\n    is 'face'. You may want to change this as well.\n\nedgecolors : {'face', 'none', *None*} or color or sequence of color, default: :rc:`scatter.edgecolors`\n    The edge color of the marker. Possible values:\n\n    - 'face': The edge color will always be the same as the face color.\n    - 'none': No patch boundary will be drawn.\n    - A color or sequence of colors.\n\n    For non-filled markers, *edgecolors* is ignored. Instead, the color\n    is determined like with 'face', i.e. from *c*, *colors*, or\n    *facecolors*.\n\nplotnonfinite : bool, default: False\n    Whether to plot points with nonfinite *c* (i.e. ``inf``, ``-inf``\n    or ``nan``). If ``True`` the points are drawn with the *bad*\n    colormap color (see `.Colormap.set_bad`).\n\nReturns\n-------\n`~matplotlib.collections.PathCollection`\n\nOther Parameters\n----------------\ndata : indexable object, optional\n    If given, the following parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` (unless this raises an exception):\n\n    *x*, *y*, *s*, *linewidths*, *edgecolors*, *c*, *facecolor*, *facecolors*, *color*\n**kwargs : `~matplotlib.collections.Collection` properties\n\nSee Also\n--------\nplot : To plot scatter plots when markers are identical in size and\n    color.\n\nNotes\n-----\n* The `.plot` function will be faster for scatterplots where markers\n  don't vary in size or color.\n\n* Any or all of *x*, *y*, *s*, and *c* may be masked arrays, in which\n  case all masks will be combined and only unmasked points will be\n  plotted.\n\n* Fundamentally, scatter works with 1D arrays; *x*, *y*, *s*, and *c*\n  may be input as N-D arrays, but within scatter they will be\n  flattened. The exception is *c*, which will be flattened only if its\n  size matches the size of *x* and *y*.", "parameters": {"type": "object", "properties": {"x": {}, "y": {}, "s": {"type": "NoneType", "default": null}, "c": {"type": "NoneType", "default": null}, "marker": {"type": "NoneType", "default": null}, "cmap": {"type": "NoneType", "default": null}, "norm": {"type": "NoneType", "default": null}, "vmin": {"type": "NoneType", "default": null}, "vmax": {"type": "NoneType", "default": null}, "alpha": {"type": "NoneType", "default": null}, "linewidths": {"type": "NoneType", "default": null}, "edgecolors": {"type": "NoneType", "default": null}, "plotnonfinite": {"type": "bool", "default": false}, "data": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/443", "data": {"name": "numpy.tensordot", "type": "callable", "signature": "(a, b, axes=2)", "description": "Compute tensor dot product along specified axes.\n\nGiven two tensors, `a` and `b`, and an array_like object containing\ntwo array_like objects, ``(a_axes, b_axes)``, sum the products of\n`a`'s and `b`'s elements (components) over the axes specified by\n``a_axes`` and ``b_axes``. The third argument can be a single non-negative\ninteger_like scalar, ``N``; if it is such, then the last ``N`` dimensions\nof `a` and the first ``N`` dimensions of `b` are summed over.\n\nParameters\n----------\na, b : array_like\n    Tensors to \"dot\".\n\naxes : int or (2,) array_like\n    * integer_like\n      If an int N, sum over the last N axes of `a` and the first N axes\n      of `b` in order. The sizes of the corresponding axes must match.\n    * (2,) array_like\n      Or, a list of axes to be summed over, first sequence applying to `a`,\n      second to `b`. Both elements array_like must be of the same length.\n\nReturns\n-------\noutput : ndarray\n    The tensor dot product of the input.\n\nSee Also\n--------\ndot, einsum\n\nNotes\n-----\nThree common use cases are:\n    * ``axes = 0`` : tensor product :math:`a\\otimes b`\n    * ``axes = 1`` : tensor dot product :math:`a\\cdot b`\n    * ``axes = 2`` : (default) tensor double contraction :math:`a:b`\n\nWhen `axes` is integer_like, the sequence for evaluation will be: first\nthe -Nth axis in `a` and 0th axis in `b`, and the -1th axis in `a` and\nNth axis in `b` last.\n\nWhen there is more than one axis to sum over - and they are not the last\n(first) axes of `a` (`b`) - the argument `axes` should consist of\ntwo sequences of the same length, with the first axis to sum over given\nfirst in both sequences, the second axis second, and so forth.\n\nThe shape of the result consists of the non-contracted axes of the\nfirst tensor, followed by the non-contracted axes of the second.\n\nExamples\n--------\nA \"traditional\" example:\n\n>>> a = np.arange(60.).reshape(3,4,5)\n>>> b = np.arange(24.).reshape(4,3,2)\n>>> c = np.tensordot(a,b, axes=([1,0],[0,1]))\n>>> c.shape\n(5, 2)\n>>> c\narray([[4400., 4730.],\n       [4532., 4874.],\n       [4664., 5018.],\n       [4796., 5162.],\n       [4928., 5306.]])\n>>> # A slower but equivalent way of computing the same...\n>>> d = np.zeros((5,2))\n>>> for i in range(5):\n...   for j in range(2):\n...     for k in range(3):\n...       for n in range(4):\n...         d[i,j] += a[k,n,i] * b[n,k,j]\n>>> c == d\narray([[ True,  True],\n       [ True,  True],\n       [ True,  True],\n       [ True,  True],\n       [ True,  True]])\n\nAn extended example taking advantage of the overloading of + and \\*:\n\n>>> a = np.array(range(1, 9))\n>>> a.shape = (2, 2, 2)\n>>> A = np.array(('a', 'b', 'c', 'd'), dtype=object)\n>>> A.shape = (2, 2)\n>>> a; A\narray([[[1, 2],\n        [3, 4]],\n       [[5, 6],\n        [7, 8]]])\narray([['a', 'b'],\n       ['c', 'd']], dtype=object)\n\n>>> np.tensordot(a, A) # third argument default is 2 for double-contraction\narray(['abbcccdddd', 'aaaaabbbbbbcccccccdddddddd'], dtype=object)\n\n>>> np.tensordot(a, A, 1)\narray([[['acc', 'bdd'],\n        ['aaacccc', 'bbbdddd']],\n       [['aaaaacccccc', 'bbbbbdddddd'],\n        ['aaaaaaacccccccc', 'bbbbbbbdddddddd']]], dtype=object)\n\n>>> np.tensordot(a, A, 0) # tensor product (result too long to incl.)\narray([[[[['a', 'b'],\n          ['c', 'd']],\n          ...\n\n>>> np.tensordot(a, A, (0, 1))\narray([[['abbbbb', 'cddddd'],\n        ['aabbbbbb', 'ccdddddd']],\n       [['aaabbbbbbb', 'cccddddddd'],\n        ['aaaabbbbbbbb', 'ccccdddddddd']]], dtype=object)\n\n>>> np.tensordot(a, A, (2, 1))\narray([[['abb', 'cdd'],\n        ['aaabbbb', 'cccdddd']],\n       [['aaaaabbbbbb', 'cccccdddddd'],\n        ['aaaaaaabbbbbbbb', 'cccccccdddddddd']]], dtype=object)\n\n>>> np.tensordot(a, A, ((0, 1), (0, 1)))\narray(['abbbcccccddddddd', 'aabbbbccccccdddddddd'], dtype=object)\n\n>>> np.tensordot(a, A, ((2, 1), (1, 0)))\narray(['acccbbdddd', 'aaaaacccccccbbbbbbdddddddd'], dtype=object)", "parameters": {"type": "object", "properties": {"a": {}, "b": {}, "axes": {"type": "integer", "default": 2}}}}}
{"task_id": "BigCodeBench/443", "data": {"name": "numpy.tensordot.swapaxes.reshape", "type": "callable", "signature": "(a, b)", "description": "Compute tensor dot product along specified axes.\n\nGiven two tensors, `a` and `b`, and an array_like object containing\ntwo array_like objects, ``(a_axes, b_axes)``, sum the products of\n`a`'s and `b`'s elements (components) over the axes specified by\n``a_axes`` and ``b_axes``. The third argument can be a single non-negative\ninteger_like scalar, ``N``; if it is such, then the last ``N`` dimensions\nof `a` and the first ``N`` dimensions of `b` are summed over.\n\nParameters\n----------\na, b : array_like\n    Tensors to \"dot\".\n\naxes : int or (2,) array_like\n    * integer_like\n      If an int N, sum over the last N axes of `a` and the first N axes\n      of `b` in order. The sizes of the corresponding axes must match.\n    * (2,) array_like\n      Or, a list of axes to be summed over, first sequence applying to `a`,\n      second to `b`. Both elements array_like must be of the same length.\n\nReturns\n-------\noutput : ndarray\n    The tensor dot product of the input.\n\nSee Also\n--------\ndot, einsum\n\nNotes\n-----\nThree common use cases are:\n    * ``axes = 0`` : tensor product :math:`a\\otimes b`\n    * ``axes = 1`` : tensor dot product :math:`a\\cdot b`\n    * ``axes = 2`` : (default) tensor double contraction :math:`a:b`\n\nWhen `axes` is integer_like, the sequence for evaluation will be: first\nthe -Nth axis in `a` and 0th axis in `b`, and the -1th axis in `a` and\nNth axis in `b` last.\n\nWhen there is more than one axis to sum over - and they are not the last\n(first) axes of `a` (`b`) - the argument `axes` should consist of\ntwo sequences of the same length, with the first axis to sum over given\nfirst in both sequences, the second axis second, and so forth.\n\nThe shape of the result consists of the non-contracted axes of the\nfirst tensor, followed by the non-contracted axes of the second.\n\nExamples\n--------\nA \"traditional\" example:\n\n>>> a = np.arange(60.).reshape(3,4,5)\n>>> b = np.arange(24.).reshape(4,3,2)\n>>> c = np.tensordot(a,b, axes=([1,0],[0,1]))\n>>> c.shape\n(5, 2)\n>>> c\narray([[4400., 4730.],\n       [4532., 4874.],\n       [4664., 5018.],\n       [4796., 5162.],\n       [4928., 5306.]])\n>>> # A slower but equivalent way of computing the same...\n>>> d = np.zeros((5,2))\n>>> for i in range(5):\n...   for j in range(2):\n...     for k in range(3):\n...       for n in range(4):\n...         d[i,j] += a[k,n,i] * b[n,k,j]\n>>> c == d\narray([[ True,  True],\n       [ True,  True],\n       [ True,  True],\n       [ True,  True],\n       [ True,  True]])\n\nAn extended example taking advantage of the overloading of + and \\*:\n\n>>> a = np.array(range(1, 9))\n>>> a.shape = (2, 2, 2)\n>>> A = np.array(('a', 'b', 'c', 'd'), dtype=object)\n>>> A.shape = (2, 2)\n>>> a; A\narray([[[1, 2],\n        [3, 4]],\n       [[5, 6],\n        [7, 8]]])\narray([['a', 'b'],\n       ['c', 'd']], dtype=object)\n\n>>> np.tensordot(a, A) # third argument default is 2 for double-contraction\narray(['abbcccdddd', 'aaaaabbbbbbcccccccdddddddd'], dtype=object)\n\n>>> np.tensordot(a, A, 1)\narray([[['acc', 'bdd'],\n        ['aaacccc', 'bbbdddd']],\n       [['aaaaacccccc', 'bbbbbdddddd'],\n        ['aaaaaaacccccccc', 'bbbbbbbdddddddd']]], dtype=object)\n\n>>> np.tensordot(a, A, 0) # tensor product (result too long to incl.)\narray([[[[['a', 'b'],\n          ['c', 'd']],\n          ...\n\n>>> np.tensordot(a, A, (0, 1))\narray([[['abbbbb', 'cddddd'],\n        ['aabbbbbb', 'ccdddddd']],\n       [['aaabbbbbbb', 'cccddddddd'],\n        ['aaaabbbbbbbb', 'ccccdddddddd']]], dtype=object)\n\n>>> np.tensordot(a, A, (2, 1))\narray([[['abb', 'cdd'],\n        ['aaabbbb', 'cccdddd']],\n       [['aaaaabbbbbb', 'cccccdddddd'],\n        ['aaaaaaabbbbbbbb', 'cccccccdddddddd']]], dtype=object)\n\n>>> np.tensordot(a, A, ((0, 1), (0, 1)))\narray(['abbbcccccddddddd', 'aabbbbccccccdddddddd'], dtype=object)\n\n>>> np.tensordot(a, A, ((2, 1), (1, 0)))\narray(['acccbbdddd', 'aaaaacccccccbbbbbbdddddddd'], dtype=object)", "parameters": {"type": "object", "properties": {"a": {}, "b": {}}}}}
{"task_id": "BigCodeBench/443", "data": {"name": "sklearn.cluster.KMeans", "type": "class", "signature": "(n_clusters=8, n_init='auto', random_state=None)", "description": "K-Means clustering.\n\nRead more in the :ref:`User Guide <k_means>`.\n\nParameters\n----------\n\nn_clusters : int, default=8\n    The number of clusters to form as well as the number of\n    centroids to generate.\n\n    For an example of how to choose an optimal value for `n_clusters` refer to\n    :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_silhouette_analysis.py`.\n\ninit : {'k-means++', 'random'}, callable or array-like of shape             (n_clusters, n_features), default='k-means++'\n    Method for initialization:\n\n    * 'k-means++' : selects initial cluster centroids using sampling             based on an empirical probability distribution of the points'             contribution to the overall inertia. This technique speeds up             convergence. The algorithm implemented is \"greedy k-means++\". It             differs from the vanilla k-means++ by making several trials at             each sampling step and choosing the best centroid among them.\n\n    * 'random': choose `n_clusters` observations (rows) at random from         data for the initial centroids.\n\n    * If an array is passed, it should be of shape (n_clusters, n_features)        and gives the initial centers.\n\n    * If a callable is passed, it should take arguments X, n_clusters and a        random state and return an initialization.\n\n    For an example of how to use the different `init` strategy, see the example\n    entitled :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_digits.py`.\n\nn_init : 'auto' or int, default='auto'\n    Number of times the k-means algorithm is run with different centroid\n    seeds. The final results is the best output of `n_init` consecutive runs\n    in terms of inertia. Several runs are recommended for sparse\n    high-dimensional problems (see :ref:`kmeans_sparse_high_dim`).\n\n    When `n_init='auto'`, the number of runs depends on the value of init:\n    10 if using `init='random'` or `init` is a callable;\n    1 if using `init='k-means++'` or `init` is an array-like.\n\n    .. versionadded:: 1.2\n       Added 'auto' option for `n_init`.\n\n    .. versionchanged:: 1.4\n       Default value for `n_init` changed to `'auto'`.\n\nmax_iter : int, default=300\n    Maximum number of iterations of the k-means algorithm for a\n    single run.\n\ntol : float, default=1e-4\n    Relative tolerance with regards to Frobenius norm of the difference\n    in the cluster centers of two consecutive iterations to declare\n    convergence.\n\nverbose : int, default=0\n    Verbosity mode.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for centroid initialization. Use\n    an int to make the randomness deterministic.\n    See :term:`Glossary <random_state>`.\n\ncopy_x : bool, default=True\n    When pre-computing distances it is more numerically accurate to center\n    the data first. If copy_x is True (default), then the original data is\n    not modified. If False, the original data is modified, and put back\n    before the function returns, but small numerical differences may be\n    introduced by subtracting and then adding the data mean. Note that if\n    the original data is not C-contiguous, a copy will be made even if\n    copy_x is False. If the original data is sparse, but not in CSR format,\n    a copy will be made even if copy_x is False.\n\nalgorithm : {\"lloyd\", \"elkan\"}, default=\"lloyd\"\n    K-means algorithm to use. The classical EM-style algorithm is `\"lloyd\"`.\n    The `\"elkan\"` variation can be more efficient on some datasets with\n    well-defined clusters, by using the triangle inequality. However it's\n    more memory intensive due to the allocation of an extra array of shape\n    `(n_samples, n_clusters)`.\n\n    .. versionchanged:: 0.18\n        Added Elkan algorithm\n\n    .. versionchanged:: 1.1\n        Renamed \"full\" to \"lloyd\", and deprecated \"auto\" and \"full\".\n        Changed \"auto\" to use \"lloyd\" instead of \"elkan\".\n\nAttributes\n----------\ncluster_centers_ : ndarray of shape (n_clusters, n_features)\n    Coordinates of cluster centers. If the algorithm stops before fully\n    converging (see ``tol`` and ``max_iter``), these will not be\n    consistent with ``labels_``.\n\nlabels_ : ndarray of shape (n_samples,)\n    Labels of each point\n\ninertia_ : float\n    Sum of squared distances of samples to their closest cluster center,\n    weighted by the sample weights if provided.\n\nn_iter_ : int\n    Number of iterations run.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nMiniBatchKMeans : Alternative online implementation that does incremental\n    updates of the centers positions using mini-batches.\n    For large scale learning (say n_samples > 10k) MiniBatchKMeans is\n    probably much faster than the default batch implementation.\n\nNotes\n-----\nThe k-means problem is solved using either Lloyd's or Elkan's algorithm.\n\nThe average complexity is given by O(k n T), where n is the number of\nsamples and T is the number of iteration.\n\nThe worst case complexity is given by O(n^(k+2/p)) with\nn = n_samples, p = n_features.\nRefer to :doi:`\"How slow is the k-means method?\" D. Arthur and S. Vassilvitskii -\nSoCG2006.<10.1145/1137856.1137880>` for more details.\n\nIn practice, the k-means algorithm is very fast (one of the fastest\nclustering algorithms available), but it falls in local minima. That's why\nit can be useful to restart it several times.\n\nIf the algorithm stops before fully converging (because of ``tol`` or\n``max_iter``), ``labels_`` and ``cluster_centers_`` will not be consistent,\ni.e. the ``cluster_centers_`` will not be the means of the points in each\ncluster. Also, the estimator will reassign ``labels_`` after the last\niteration to make ``labels_`` consistent with ``predict`` on the training\nset.\n\nExamples\n--------\n\n>>> from sklearn.cluster import KMeans\n>>> import numpy as np\n>>> X = np.array([[1, 2], [1, 4], [1, 0],\n...               [10, 2], [10, 4], [10, 0]])\n>>> kmeans = KMeans(n_clusters=2, random_state=0, n_init=\"auto\").fit(X)\n>>> kmeans.labels_\narray([1, 1, 1, 0, 0, 0], dtype=int32)\n>>> kmeans.predict([[0, 0], [12, 3]])\narray([1, 0], dtype=int32)\n>>> kmeans.cluster_centers_\narray([[10.,  2.],\n       [ 1.,  2.]])\n\nFor a more detailed example of K-Means using the iris dataset see\n:ref:`sphx_glr_auto_examples_cluster_plot_cluster_iris.py`.\n\nFor examples of common problems with K-Means and how to address them see\n:ref:`sphx_glr_auto_examples_cluster_plot_kmeans_assumptions.py`.\n\nFor an example of how to use K-Means to perform color quantization see\n:ref:`sphx_glr_auto_examples_cluster_plot_color_quantization.py`.\n\nFor a demonstration of how K-Means can be used to cluster text documents see\n:ref:`sphx_glr_auto_examples_text_plot_document_clustering.py`.\n\nFor a comparison between K-Means and MiniBatchKMeans refer to example\n:ref:`sphx_glr_auto_examples_cluster_plot_mini_batch_kmeans.py`.", "parameters": {"type": "object", "properties": {"n_clusters": {"type": "integer", "default": 8}, "n_init": {"type": "str", "default": "auto"}, "random_state": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/443", "data": {"name": "sklearn.cluster.KMeans.fit_predict", "type": "callable", "signature": "(self, X)", "description": "Compute cluster centers and predict cluster index for each sample.\n\nConvenience method; equivalent to calling fit(X) followed by\npredict(X).\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    New data to transform.\n\ny : Ignored\n    Not used, present here for API consistency by convention.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    The weights for each observation in X. If None, all observations\n    are assigned equal weight.\n\nReturns\n-------\nlabels : ndarray of shape (n_samples,)\n    Index of the cluster each sample belongs to.", "parameters": {"type": "object", "properties": {"X": {}}}}}
{"task_id": "BigCodeBench/445", "data": {"name": "matplotlib.pyplot.subplots", "type": "callable", "signature": "()", "description": "Create a figure and a set of subplots.\n\nThis utility wrapper makes it convenient to create common layouts of\nsubplots, including the enclosing figure object, in a single call.\n\nParameters\n----------\nnrows, ncols : int, default: 1\n    Number of rows/columns of the subplot grid.\n\nsharex, sharey : bool or {'none', 'all', 'row', 'col'}, default: False\n    Controls sharing of properties among x (*sharex*) or y (*sharey*)\n    axes:\n\n    - True or 'all': x- or y-axis will be shared among all subplots.\n    - False or 'none': each subplot x- or y-axis will be independent.\n    - 'row': each subplot row will share an x- or y-axis.\n    - 'col': each subplot column will share an x- or y-axis.\n\n    When subplots have a shared x-axis along a column, only the x tick\n    labels of the bottom subplot are created. Similarly, when subplots\n    have a shared y-axis along a row, only the y tick labels of the first\n    column subplot are created. To later turn other subplots' ticklabels\n    on, use `~matplotlib.axes.Axes.tick_params`.\n\n    When subplots have a shared axis that has units, calling\n    `~matplotlib.axis.Axis.set_units` will update each axis with the\n    new units.\n\nsqueeze : bool, default: True\n    - If True, extra dimensions are squeezed out from the returned\n      array of `~matplotlib.axes.Axes`:\n\n      - if only one subplot is constructed (nrows=ncols=1), the\n        resulting single Axes object is returned as a scalar.\n      - for Nx1 or 1xM subplots, the returned object is a 1D numpy\n        object array of Axes objects.\n      - for NxM, subplots with N>1 and M>1 are returned as a 2D array.\n\n    - If False, no squeezing at all is done: the returned Axes object is\n      always a 2D array containing Axes instances, even if it ends up\n      being 1x1.\n\nwidth_ratios : array-like of length *ncols*, optional\n    Defines the relative widths of the columns. Each column gets a\n    relative width of ``width_ratios[i] / sum(width_ratios)``.\n    If not given, all columns will have the same width.  Equivalent\n    to ``gridspec_kw={'width_ratios': [...]}``.\n\nheight_ratios : array-like of length *nrows*, optional\n    Defines the relative heights of the rows. Each row gets a\n    relative height of ``height_ratios[i] / sum(height_ratios)``.\n    If not given, all rows will have the same height. Convenience\n    for ``gridspec_kw={'height_ratios': [...]}``.\n\nsubplot_kw : dict, optional\n    Dict with keywords passed to the\n    `~matplotlib.figure.Figure.add_subplot` call used to create each\n    subplot.\n\ngridspec_kw : dict, optional\n    Dict with keywords passed to the `~matplotlib.gridspec.GridSpec`\n    constructor used to create the grid the subplots are placed on.\n\n**fig_kw\n    All additional keyword arguments are passed to the\n    `.pyplot.figure` call.\n\nReturns\n-------\nfig : `.Figure`\n\nax : `~matplotlib.axes.Axes` or array of Axes\n    *ax* can be either a single `~.axes.Axes` object, or an array of Axes\n    objects if more than one subplot was created.  The dimensions of the\n    resulting array can be controlled with the squeeze keyword, see above.\n\n    Typical idioms for handling the return value are::\n\n        # using the variable ax for single a Axes\n        fig, ax = plt.subplots()\n\n        # using the variable axs for multiple Axes\n        fig, axs = plt.subplots(2, 2)\n\n        # using tuple unpacking for multiple Axes\n        fig, (ax1, ax2) = plt.subplots(1, 2)\n        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n\n    The names ``ax`` and pluralized ``axs`` are preferred over ``axes``\n    because for the latter it's not clear if it refers to a single\n    `~.axes.Axes` instance or a collection of these.\n\nSee Also\n--------\n.pyplot.figure\n.pyplot.subplot\n.pyplot.axes\n.Figure.subplots\n.Figure.add_subplot\n\nExamples\n--------\n::\n\n    # First create some toy data:\n    x = np.linspace(0, 2*np.pi, 400)\n    y = np.sin(x**2)\n\n    # Create just a figure and only one subplot\n    fig, ax = plt.subplots()\n    ax.plot(x, y)\n    ax.set_title('Simple plot')\n\n    # Create two subplots and unpack the output array immediately\n    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n    ax1.plot(x, y)\n    ax1.set_title('Sharing Y axis')\n    ax2.scatter(x, y)\n\n    # Create four polar axes and access them through the returned array\n    fig, axs = plt.subplots(2, 2, subplot_kw=dict(projection=\"polar\"))\n    axs[0, 0].plot(x, y)\n    axs[1, 1].scatter(x, y)\n\n    # Share a X axis with each column of subplots\n    plt.subplots(2, 2, sharex='col')\n\n    # Share a Y axis with each row of subplots\n    plt.subplots(2, 2, sharey='row')\n\n    # Share both X and Y axes with all subplots\n    plt.subplots(2, 2, sharex='all', sharey='all')\n\n    # Note that this is the same as\n    plt.subplots(2, 2, sharex=True, sharey=True)\n\n    # Create figure number 10 with a single subplot\n    # and clears it if it already exists.\n    fig, ax = plt.subplots(num=10, clear=True)", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/445", "data": {"name": "numpy.random.seed", "type": "callable", "signature": "(*args, **kwargs)", "description": "seed(self, seed=None)\n\nReseed a legacy MT19937 BitGenerator\n\nNotes\n-----\nThis is a convenience, legacy function.\n\nThe best practice is to **not** reseed a BitGenerator, rather to\nrecreate a new one. This method is here for legacy reasons.\nThis example demonstrates best practice.\n\n>>> from numpy.random import MT19937\n>>> from numpy.random import RandomState, SeedSequence\n>>> rs = RandomState(MT19937(SeedSequence(123456789)))\n# Later, you want to restart the stream\n>>> rs = RandomState(MT19937(SeedSequence(987654321)))", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/445", "data": {"name": "numpy.random.normal", "type": "callable", "signature": "(*args, **kwargs)", "description": "normal(loc=0.0, scale=1.0, size=None)\n\nDraw random samples from a normal (Gaussian) distribution.\n\nThe probability density function of the normal distribution, first\nderived by De Moivre and 200 years later by both Gauss and Laplace\nindependently [2]_, is often called the bell curve because of\nits characteristic shape (see the example below).\n\nThe normal distributions occurs often in nature.  For example, it\ndescribes the commonly occurring distribution of samples influenced\nby a large number of tiny, random disturbances, each with its own\nunique distribution [2]_.\n\n.. note::\n    New code should use the ``normal`` method of a ``default_rng()``\n    instance instead; please see the :ref:`random-quick-start`.\n\nParameters\n----------\nloc : float or array_like of floats\n    Mean (\"centre\") of the distribution.\nscale : float or array_like of floats\n    Standard deviation (spread or \"width\") of the distribution. Must be\n    non-negative.\nsize : int or tuple of ints, optional\n    Output shape.  If the given shape is, e.g., ``(m, n, k)``, then\n    ``m * n * k`` samples are drawn.  If size is ``None`` (default),\n    a single value is returned if ``loc`` and ``scale`` are both scalars.\n    Otherwise, ``np.broadcast(loc, scale).size`` samples are drawn.\n\nReturns\n-------\nout : ndarray or scalar\n    Drawn samples from the parameterized normal distribution.\n\nSee Also\n--------\nscipy.stats.norm : probability density function, distribution or\n    cumulative density function, etc.\nrandom.Generator.normal: which should be used for new code.\n\nNotes\n-----\nThe probability density for the Gaussian distribution is\n\n.. math:: p(x) = \\frac{1}{\\sqrt{ 2 \\pi \\sigma^2 }}\n                 e^{ - \\frac{ (x - \\mu)^2 } {2 \\sigma^2} },\n\nwhere :math:`\\mu` is the mean and :math:`\\sigma` the standard\ndeviation. The square of the standard deviation, :math:`\\sigma^2`,\nis called the variance.\n\nThe function has its peak at the mean, and its \"spread\" increases with\nthe standard deviation (the function reaches 0.607 times its maximum at\n:math:`x + \\sigma` and :math:`x - \\sigma` [2]_).  This implies that\nnormal is more likely to return samples lying close to the mean, rather\nthan those far away.\n\nReferences\n----------\n.. [1] Wikipedia, \"Normal distribution\",\n       https://en.wikipedia.org/wiki/Normal_distribution\n.. [2] P. R. Peebles Jr., \"Central Limit Theorem\" in \"Probability,\n       Random Variables and Random Signal Principles\", 4th ed., 2001,\n       pp. 51, 51, 125.\n\nExamples\n--------\nDraw samples from the distribution:\n\n>>> mu, sigma = 0, 0.1 # mean and standard deviation\n>>> s = np.random.normal(mu, sigma, 1000)\n\nVerify the mean and the variance:\n\n>>> abs(mu - np.mean(s))\n0.0  # may vary\n\n>>> abs(sigma - np.std(s, ddof=1))\n0.1  # may vary\n\nDisplay the histogram of the samples, along with\nthe probability density function:\n\n>>> import matplotlib.pyplot as plt\n>>> count, bins, ignored = plt.hist(s, 30, density=True)\n>>> plt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) *\n...                np.exp( - (bins - mu)**2 / (2 * sigma**2) ),\n...          linewidth=2, color='r')\n>>> plt.show()\n\nTwo-by-four array of samples from N(3, 6.25):\n\n>>> np.random.normal(3, 2.5, size=(2, 4))\narray([[-4.49401501,  4.00950034, -1.81814867,  7.29718677],   # random\n       [ 0.39924804,  4.68456316,  4.99394529,  4.84057254]])  # random", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/445", "data": {"name": "points.shape"}}
{"task_id": "BigCodeBench/445", "data": {"name": "scipy.spatial.Voronoi", "type": "class", "signature": "(points)", "description": "Voronoi(points, furthest_site=False, incremental=False, qhull_options=None)\n\nVoronoi diagrams in N dimensions.\n\n.. versionadded:: 0.12.0\n\nParameters\n----------\npoints : ndarray of floats, shape (npoints, ndim)\n    Coordinates of points to construct a Voronoi diagram from\nfurthest_site : bool, optional\n    Whether to compute a furthest-site Voronoi diagram. Default: False\nincremental : bool, optional\n    Allow adding new points incrementally. This takes up some additional\n    resources.\nqhull_options : str, optional\n    Additional options to pass to Qhull. See Qhull manual\n    for details. (Default: \"Qbb Qc Qz Qx\" for ndim > 4 and\n    \"Qbb Qc Qz\" otherwise. Incremental mode omits \"Qz\".)\n\nAttributes\n----------\npoints : ndarray of double, shape (npoints, ndim)\n    Coordinates of input points.\nvertices : ndarray of double, shape (nvertices, ndim)\n    Coordinates of the Voronoi vertices.\nridge_points : ndarray of ints, shape ``(nridges, 2)``\n    Indices of the points between which each Voronoi ridge lies.\nridge_vertices : list of list of ints, shape ``(nridges, *)``\n    Indices of the Voronoi vertices forming each Voronoi ridge.\nregions : list of list of ints, shape ``(nregions, *)``\n    Indices of the Voronoi vertices forming each Voronoi region.\n    -1 indicates vertex outside the Voronoi diagram.\n    When qhull option \"Qz\" was specified, an empty sublist\n    represents the Voronoi region for a point at infinity that\n    was added internally.\npoint_region : array of ints, shape (npoints)\n    Index of the Voronoi region for each input point.\n    If qhull option \"Qc\" was not specified, the list will contain -1\n    for points that are not associated with a Voronoi region.\n    If qhull option \"Qz\" was specified, there will be one less\n    element than the number of regions because an extra point\n    at infinity is added internally to facilitate computation.\nfurthest_site\n    True if this was a furthest site triangulation and False if not.\n\n    .. versionadded:: 1.4.0\n\nRaises\n------\nQhullError\n    Raised when Qhull encounters an error condition, such as\n    geometrical degeneracy when options to resolve are not enabled.\nValueError\n    Raised if an incompatible array is given as input.\n\nNotes\n-----\nThe Voronoi diagram is computed using the\n`Qhull library <http://www.qhull.org/>`__.\n\nExamples\n--------\nVoronoi diagram for a set of point:\n\n>>> import numpy as np\n>>> points = np.array([[0, 0], [0, 1], [0, 2], [1, 0], [1, 1], [1, 2],\n...                    [2, 0], [2, 1], [2, 2]])\n>>> from scipy.spatial import Voronoi, voronoi_plot_2d\n>>> vor = Voronoi(points)\n\nPlot it:\n\n>>> import matplotlib.pyplot as plt\n>>> fig = voronoi_plot_2d(vor)\n>>> plt.show()\n\nThe Voronoi vertices:\n\n>>> vor.vertices\narray([[0.5, 0.5],\n       [0.5, 1.5],\n       [1.5, 0.5],\n       [1.5, 1.5]])\n\nThere is a single finite Voronoi region, and four finite Voronoi\nridges:\n\n>>> vor.regions\n[[], [-1, 0], [-1, 1], [1, -1, 0], [3, -1, 2], [-1, 3], [-1, 2], [0, 1, 3, 2], [2, -1, 0], [3, -1, 1]]\n>>> vor.ridge_vertices\n[[-1, 0], [-1, 0], [-1, 1], [-1, 1], [0, 1], [-1, 3], [-1, 2], [2, 3], [-1, 3], [-1, 2], [1, 3], [0, 2]]\n\nThe ridges are perpendicular between lines drawn between the following\ninput points:\n\n>>> vor.ridge_points\narray([[0, 3],\n       [0, 1],\n       [2, 5],\n       [2, 1],\n       [1, 4],\n       [7, 8],\n       [7, 6],\n       [7, 4],\n       [8, 5],\n       [6, 3],\n       [4, 5],\n       [4, 3]], dtype=int32)", "parameters": {"type": "object", "properties": {"points": {}}}}}
{"task_id": "BigCodeBench/445", "data": {"name": "scipy.spatial.voronoi_plot_2d", "type": "callable", "signature": "(vor, ax=None, **kw)", "description": "Plot the given Voronoi diagram in 2-D\n\nParameters\n----------\nvor : scipy.spatial.Voronoi instance\n    Diagram to plot\nax : matplotlib.axes.Axes instance, optional\n    Axes to plot on\nshow_points : bool, optional\n    Add the Voronoi points to the plot.\nshow_vertices : bool, optional\n    Add the Voronoi vertices to the plot.\nline_colors : string, optional\n    Specifies the line color for polygon boundaries\nline_width : float, optional\n    Specifies the line width for polygon boundaries\nline_alpha : float, optional\n    Specifies the line alpha for polygon boundaries\npoint_size : float, optional\n    Specifies the size of points\n\nReturns\n-------\nfig : matplotlib.figure.Figure instance\n    Figure for the plot\n\nSee Also\n--------\nVoronoi\n\nNotes\n-----\nRequires Matplotlib.\n\nExamples\n--------\n>>> import numpy as np\n>>> import matplotlib.pyplot as plt\n>>> from scipy.spatial import Voronoi, voronoi_plot_2d\n\nCreate a set of points for the example:\n\n>>> rng = np.random.default_rng()\n>>> points = rng.random((10,2))\n\nGenerate the Voronoi diagram for the points:\n\n>>> vor = Voronoi(points)\n\nUse `voronoi_plot_2d` to plot the diagram:\n\n>>> fig = voronoi_plot_2d(vor)\n\nUse `voronoi_plot_2d` to plot the diagram again, with some settings\ncustomized:\n\n>>> fig = voronoi_plot_2d(vor, show_vertices=False, line_colors='orange',\n...                       line_width=2, line_alpha=0.6, point_size=2)\n>>> plt.show()", "parameters": {"type": "object", "properties": {"vor": {}, "ax": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/454", "data": {"name": "files_moved.append(dest_file_path)"}}
{"task_id": "BigCodeBench/454", "data": {"name": "glob.glob", "type": "callable", "signature": "(pathname)", "description": "Return a list of paths matching a pathname pattern.\n\nThe pattern may contain simple shell-style wildcards a la\nfnmatch. However, unlike fnmatch, filenames starting with a\ndot are special cases that are not matched by '*' and '?'\npatterns.\n\nIf recursive is true, the pattern '**' will match any files and\nzero or more directories and subdirectories.", "parameters": {"type": "object", "properties": {"pathname": {}}}}}
{"task_id": "BigCodeBench/454", "data": {"name": "os.path.basename", "type": "callable", "signature": "(p)", "description": "Returns the final component of a pathname", "parameters": {"type": "object", "properties": {"p": {}}}}}
{"task_id": "BigCodeBench/454", "data": {"name": "os.path.exists", "type": "callable", "signature": "(path)", "description": "Test whether a path exists.  Returns False for broken symbolic links", "parameters": {"type": "object", "properties": {"path": {}}}}}
{"task_id": "BigCodeBench/454", "data": {"name": "os.path.join", "type": "callable", "signature": "(a, *p)", "description": "Join two or more pathname components, inserting '/' as needed.\nIf any component is an absolute path, all previous path components\nwill be discarded.  An empty last part will result in a path that\nends with a separator.", "parameters": {"type": "object", "properties": {"a": {}}}}}
{"task_id": "BigCodeBench/454", "data": {"name": "shutil.move", "type": "callable", "signature": "(src, dst)", "description": "Recursively move a file or directory to another location. This is\nsimilar to the Unix \"mv\" command. Return the file or directory's\ndestination.\n\nIf the destination is a directory or a symlink to a directory, the source\nis moved inside the directory. The destination path must not already\nexist.\n\nIf the destination already exists but is not a directory, it may be\noverwritten depending on os.rename() semantics.\n\nIf the destination is on our current filesystem, then rename() is used.\nOtherwise, src is copied to the destination and then removed. Symlinks are\nrecreated under the new name if os.rename() fails because of cross\nfilesystem renames.\n\nThe optional `copy_function` argument is a callable that will be used\nto copy the source or it will be delegated to `copytree`.\nBy default, copy2() is used, but any function that supports the same\nsignature (like copy()) can be used.\n\nA lot more could be done here...  A look at a mv.c shows a lot of\nthe issues this implementation glosses over.", "parameters": {"type": "object", "properties": {"src": {}, "dst": {}}}}}
{"task_id": "BigCodeBench/458", "data": {"name": "pandas.DataFrame", "type": "class", "signature": "(data=None, index: 'Axes | None' = None, columns: 'Axes | None' = None, dtype: 'Dtype | None' = None, copy: 'bool | None' = None) -> 'None'", "description": "Two-dimensional, size-mutable, potentially heterogeneous tabular data.\n\nData structure also contains labeled axes (rows and columns).\nArithmetic operations align on both row and column labels. Can be\nthought of as a dict-like container for Series objects. The primary\npandas data structure.\n\nParameters\n----------\ndata : ndarray (structured or homogeneous), Iterable, dict, or DataFrame\n    Dict can contain Series, arrays, constants, dataclass or list-like objects. If\n    data is a dict, column order follows insertion-order. If a dict contains Series\n    which have an index defined, it is aligned by its index. This alignment also\n    occurs if data is a Series or a DataFrame itself. Alignment is done on\n    Series/DataFrame inputs.\n\n    If data is a list of dicts, column order follows insertion-order.\n\nindex : Index or array-like\n    Index to use for resulting frame. Will default to RangeIndex if\n    no indexing information part of input data and no index provided.\ncolumns : Index or array-like\n    Column labels to use for resulting frame when data does not have them,\n    defaulting to RangeIndex(0, 1, 2, ..., n). If data contains column labels,\n    will perform column selection instead.\ndtype : dtype, default None\n    Data type to force. Only a single dtype is allowed. If None, infer.\ncopy : bool or None, default None\n    Copy data from inputs.\n    For dict data, the default of None behaves like ``copy=True``.  For DataFrame\n    or 2d ndarray input, the default of None behaves like ``copy=False``.\n    If data is a dict containing one or more Series (possibly of different dtypes),\n    ``copy=False`` will ensure that these inputs are not copied.\n\n    .. versionchanged:: 1.3.0\n\nSee Also\n--------\nDataFrame.from_records : Constructor from tuples, also record arrays.\nDataFrame.from_dict : From dicts of Series, arrays, or dicts.\nread_csv : Read a comma-separated values (csv) file into DataFrame.\nread_table : Read general delimited file into DataFrame.\nread_clipboard : Read text from clipboard into DataFrame.\n\nNotes\n-----\nPlease reference the :ref:`User Guide <basics.dataframe>` for more information.\n\nExamples\n--------\nConstructing DataFrame from a dictionary.\n\n>>> d = {'col1': [1, 2], 'col2': [3, 4]}\n>>> df = pd.DataFrame(data=d)\n>>> df\n   col1  col2\n0     1     3\n1     2     4\n\nNotice that the inferred dtype is int64.\n\n>>> df.dtypes\ncol1    int64\ncol2    int64\ndtype: object\n\nTo enforce a single dtype:\n\n>>> df = pd.DataFrame(data=d, dtype=np.int8)\n>>> df.dtypes\ncol1    int8\ncol2    int8\ndtype: object\n\nConstructing DataFrame from a dictionary including Series:\n\n>>> d = {'col1': [0, 1, 2, 3], 'col2': pd.Series([2, 3], index=[2, 3])}\n>>> pd.DataFrame(data=d, index=[0, 1, 2, 3])\n   col1  col2\n0     0   NaN\n1     1   NaN\n2     2   2.0\n3     3   3.0\n\nConstructing DataFrame from numpy ndarray:\n\n>>> df2 = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),\n...                    columns=['a', 'b', 'c'])\n>>> df2\n   a  b  c\n0  1  2  3\n1  4  5  6\n2  7  8  9\n\nConstructing DataFrame from a numpy ndarray that has labeled columns:\n\n>>> data = np.array([(1, 2, 3), (4, 5, 6), (7, 8, 9)],\n...                 dtype=[(\"a\", \"i4\"), (\"b\", \"i4\"), (\"c\", \"i4\")])\n>>> df3 = pd.DataFrame(data, columns=['c', 'a'])\n...\n>>> df3\n   c  a\n0  3  1\n1  6  4\n2  9  7\n\nConstructing DataFrame from dataclass:\n\n>>> from dataclasses import make_dataclass\n>>> Point = make_dataclass(\"Point\", [(\"x\", int), (\"y\", int)])\n>>> pd.DataFrame([Point(0, 0), Point(0, 3), Point(2, 3)])\n   x  y\n0  0  0\n1  0  3\n2  2  3\n\nConstructing DataFrame from Series/DataFrame:\n\n>>> ser = pd.Series([1, 2, 3], index=[\"a\", \"b\", \"c\"])\n>>> df = pd.DataFrame(data=ser, index=[\"a\", \"c\"])\n>>> df\n   0\na  1\nc  3\n\n>>> df1 = pd.DataFrame([1, 2, 3], index=[\"a\", \"b\", \"c\"], columns=[\"x\"])\n>>> df2 = pd.DataFrame(data=df1, index=[\"a\", \"c\"])\n>>> df2\n   x\na  1\nc  3", "parameters": {"type": "object", "properties": {"data": {"type": "NoneType", "default": null}, "index": {"type": ["axes", "null"], "default": null}, "columns": {"type": ["axes", "null"], "default": null}, "dtype": {"type": ["dtype", "null"], "default": null}, "copy": {"type": ["bool", "null"], "default": null}}}}}
{"task_id": "BigCodeBench/458", "data": {"name": "pandas.DataFrame.columns", "type": "constant", "signature": null, "description": "The column labels of the DataFrame.\n\nExamples\n--------\n>>> df = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})\n>>> df\n     A  B\n0    1  3\n1    2  4\n>>> df.columns\nIndex(['A', 'B'], dtype='object')", "value": "<pandas._libs.properties.AxisProperty object at 0x7fa4c568abf0>", "parameters": {"type": "constant"}}}
{"task_id": "BigCodeBench/458", "data": {"name": "json.loads", "type": "callable", "signature": "(s)", "description": "Deserialize ``s`` (a ``str``, ``bytes`` or ``bytearray`` instance\ncontaining a JSON document) to a Python object.\n\n``object_hook`` is an optional function that will be called with the\nresult of any object literal decode (a ``dict``). The return value of\n``object_hook`` will be used instead of the ``dict``. This feature\ncan be used to implement custom decoders (e.g. JSON-RPC class hinting).\n\n``object_pairs_hook`` is an optional function that will be called with the\nresult of any object literal decoded with an ordered list of pairs.  The\nreturn value of ``object_pairs_hook`` will be used instead of the ``dict``.\nThis feature can be used to implement custom decoders.  If ``object_hook``\nis also defined, the ``object_pairs_hook`` takes priority.\n\n``parse_float``, if specified, will be called with the string\nof every JSON float to be decoded. By default this is equivalent to\nfloat(num_str). This can be used to use another datatype or parser\nfor JSON floats (e.g. decimal.Decimal).\n\n``parse_int``, if specified, will be called with the string\nof every JSON int to be decoded. By default this is equivalent to\nint(num_str). This can be used to use another datatype or parser\nfor JSON integers (e.g. float).\n\n``parse_constant``, if specified, will be called with one of the\nfollowing strings: -Infinity, Infinity, NaN.\nThis can be used to raise an exception if invalid JSON numbers\nare encountered.\n\nTo use a custom ``JSONDecoder`` subclass, specify it with the ``cls``\nkwarg; otherwise ``JSONDecoder`` is used.", "parameters": {"type": "object", "properties": {"s": {}}}}}
{"task_id": "BigCodeBench/458", "data": {"name": "json.loads.items", "type": "callable", "signature": "()", "description": "Deserialize ``s`` (a ``str``, ``bytes`` or ``bytearray`` instance\ncontaining a JSON document) to a Python object.\n\n``object_hook`` is an optional function that will be called with the\nresult of any object literal decode (a ``dict``). The return value of\n``object_hook`` will be used instead of the ``dict``. This feature\ncan be used to implement custom decoders (e.g. JSON-RPC class hinting).\n\n``object_pairs_hook`` is an optional function that will be called with the\nresult of any object literal decoded with an ordered list of pairs.  The\nreturn value of ``object_pairs_hook`` will be used instead of the ``dict``.\nThis feature can be used to implement custom decoders.  If ``object_hook``\nis also defined, the ``object_pairs_hook`` takes priority.\n\n``parse_float``, if specified, will be called with the string\nof every JSON float to be decoded. By default this is equivalent to\nfloat(num_str). This can be used to use another datatype or parser\nfor JSON floats (e.g. decimal.Decimal).\n\n``parse_int``, if specified, will be called with the string\nof every JSON int to be decoded. By default this is equivalent to\nint(num_str). This can be used to use another datatype or parser\nfor JSON integers (e.g. float).\n\n``parse_constant``, if specified, will be called with one of the\nfollowing strings: -Infinity, Infinity, NaN.\nThis can be used to raise an exception if invalid JSON numbers\nare encountered.\n\nTo use a custom ``JSONDecoder`` subclass, specify it with the ``cls``\nkwarg; otherwise ``JSONDecoder`` is used.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/458", "data": {"name": "json.loads.values", "type": "callable", "signature": "()", "description": "Deserialize ``s`` (a ``str``, ``bytes`` or ``bytearray`` instance\ncontaining a JSON document) to a Python object.\n\n``object_hook`` is an optional function that will be called with the\nresult of any object literal decode (a ``dict``). The return value of\n``object_hook`` will be used instead of the ``dict``. This feature\ncan be used to implement custom decoders (e.g. JSON-RPC class hinting).\n\n``object_pairs_hook`` is an optional function that will be called with the\nresult of any object literal decoded with an ordered list of pairs.  The\nreturn value of ``object_pairs_hook`` will be used instead of the ``dict``.\nThis feature can be used to implement custom decoders.  If ``object_hook``\nis also defined, the ``object_pairs_hook`` takes priority.\n\n``parse_float``, if specified, will be called with the string\nof every JSON float to be decoded. By default this is equivalent to\nfloat(num_str). This can be used to use another datatype or parser\nfor JSON floats (e.g. decimal.Decimal).\n\n``parse_int``, if specified, will be called with the string\nof every JSON int to be decoded. By default this is equivalent to\nint(num_str). This can be used to use another datatype or parser\nfor JSON integers (e.g. float).\n\n``parse_constant``, if specified, will be called with one of the\nfollowing strings: -Infinity, Infinity, NaN.\nThis can be used to raise an exception if invalid JSON numbers\nare encountered.\n\nTo use a custom ``JSONDecoder`` subclass, specify it with the ``cls``\nkwarg; otherwise ``JSONDecoder`` is used.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/458", "data": {"name": "pandas.to_numeric", "type": "callable", "signature": "(arg, errors: 'DateTimeErrorChoices' = 'raise')", "description": "Convert argument to a numeric type.\n\nThe default return dtype is `float64` or `int64`\ndepending on the data supplied. Use the `downcast` parameter\nto obtain other dtypes.\n\nPlease note that precision loss may occur if really large numbers\nare passed in. Due to the internal limitations of `ndarray`, if\nnumbers smaller than `-9223372036854775808` (np.iinfo(np.int64).min)\nor larger than `18446744073709551615` (np.iinfo(np.uint64).max) are\npassed in, it is very likely they will be converted to float so that\nthey can be stored in an `ndarray`. These warnings apply similarly to\n`Series` since it internally leverages `ndarray`.\n\nParameters\n----------\narg : scalar, list, tuple, 1-d array, or Series\n    Argument to be converted.\nerrors : {'ignore', 'raise', 'coerce'}, default 'raise'\n    - If 'raise', then invalid parsing will raise an exception.\n    - If 'coerce', then invalid parsing will be set as NaN.\n    - If 'ignore', then invalid parsing will return the input.\n\n    .. versionchanged:: 2.2\n\n    \"ignore\" is deprecated. Catch exceptions explicitly instead.\n\ndowncast : str, default None\n    Can be 'integer', 'signed', 'unsigned', or 'float'.\n    If not None, and if the data has been successfully cast to a\n    numerical dtype (or if the data was numeric to begin with),\n    downcast that resulting data to the smallest numerical dtype\n    possible according to the following rules:\n\n    - 'integer' or 'signed': smallest signed int dtype (min.: np.int8)\n    - 'unsigned': smallest unsigned int dtype (min.: np.uint8)\n    - 'float': smallest float dtype (min.: np.float32)\n\n    As this behaviour is separate from the core conversion to\n    numeric values, any errors raised during the downcasting\n    will be surfaced regardless of the value of the 'errors' input.\n\n    In addition, downcasting will only occur if the size\n    of the resulting data's dtype is strictly larger than\n    the dtype it is to be cast to, so if none of the dtypes\n    checked satisfy that specification, no downcasting will be\n    performed on the data.\ndtype_backend : {'numpy_nullable', 'pyarrow'}, default 'numpy_nullable'\n    Back-end data type applied to the resultant :class:`DataFrame`\n    (still experimental). Behaviour is as follows:\n\n    * ``\"numpy_nullable\"``: returns nullable-dtype-backed :class:`DataFrame`\n      (default).\n    * ``\"pyarrow\"``: returns pyarrow-backed nullable :class:`ArrowDtype`\n      DataFrame.\n\n    .. versionadded:: 2.0\n\nReturns\n-------\nret\n    Numeric if parsing succeeded.\n    Return type depends on input.  Series if Series, otherwise ndarray.\n\nSee Also\n--------\nDataFrame.astype : Cast argument to a specified dtype.\nto_datetime : Convert argument to datetime.\nto_timedelta : Convert argument to timedelta.\nnumpy.ndarray.astype : Cast a numpy array to a specified type.\nDataFrame.convert_dtypes : Convert dtypes.\n\nExamples\n--------\nTake separate series and convert to numeric, coercing when told to\n\n>>> s = pd.Series(['1.0', '2', -3])\n>>> pd.to_numeric(s)\n0    1.0\n1    2.0\n2   -3.0\ndtype: float64\n>>> pd.to_numeric(s, downcast='float')\n0    1.0\n1    2.0\n2   -3.0\ndtype: float32\n>>> pd.to_numeric(s, downcast='signed')\n0    1\n1    2\n2   -3\ndtype: int8\n>>> s = pd.Series(['apple', '1.0', '2', -3])\n>>> pd.to_numeric(s, errors='coerce')\n0    NaN\n1    1.0\n2    2.0\n3   -3.0\ndtype: float64\n\nDowncasting of nullable integer and floating dtypes is supported:\n\n>>> s = pd.Series([1, 2, 3], dtype=\"Int64\")\n>>> pd.to_numeric(s, downcast=\"integer\")\n0    1\n1    2\n2    3\ndtype: Int8\n>>> s = pd.Series([1.0, 2.1, 3.0], dtype=\"Float64\")\n>>> pd.to_numeric(s, downcast=\"float\")\n0    1.0\n1    2.1\n2    3.0\ndtype: Float32", "parameters": {"type": "object", "properties": {"arg": {}, "errors": {"type": "datetimeerrorchoices", "default": "raise"}}}}}
{"task_id": "BigCodeBench/458", "data": {"name": "pandas.to_numeric.isnull", "type": "callable", "signature": "()", "description": "Convert argument to a numeric type.\n\nThe default return dtype is `float64` or `int64`\ndepending on the data supplied. Use the `downcast` parameter\nto obtain other dtypes.\n\nPlease note that precision loss may occur if really large numbers\nare passed in. Due to the internal limitations of `ndarray`, if\nnumbers smaller than `-9223372036854775808` (np.iinfo(np.int64).min)\nor larger than `18446744073709551615` (np.iinfo(np.uint64).max) are\npassed in, it is very likely they will be converted to float so that\nthey can be stored in an `ndarray`. These warnings apply similarly to\n`Series` since it internally leverages `ndarray`.\n\nParameters\n----------\narg : scalar, list, tuple, 1-d array, or Series\n    Argument to be converted.\nerrors : {'ignore', 'raise', 'coerce'}, default 'raise'\n    - If 'raise', then invalid parsing will raise an exception.\n    - If 'coerce', then invalid parsing will be set as NaN.\n    - If 'ignore', then invalid parsing will return the input.\n\n    .. versionchanged:: 2.2\n\n    \"ignore\" is deprecated. Catch exceptions explicitly instead.\n\ndowncast : str, default None\n    Can be 'integer', 'signed', 'unsigned', or 'float'.\n    If not None, and if the data has been successfully cast to a\n    numerical dtype (or if the data was numeric to begin with),\n    downcast that resulting data to the smallest numerical dtype\n    possible according to the following rules:\n\n    - 'integer' or 'signed': smallest signed int dtype (min.: np.int8)\n    - 'unsigned': smallest unsigned int dtype (min.: np.uint8)\n    - 'float': smallest float dtype (min.: np.float32)\n\n    As this behaviour is separate from the core conversion to\n    numeric values, any errors raised during the downcasting\n    will be surfaced regardless of the value of the 'errors' input.\n\n    In addition, downcasting will only occur if the size\n    of the resulting data's dtype is strictly larger than\n    the dtype it is to be cast to, so if none of the dtypes\n    checked satisfy that specification, no downcasting will be\n    performed on the data.\ndtype_backend : {'numpy_nullable', 'pyarrow'}, default 'numpy_nullable'\n    Back-end data type applied to the resultant :class:`DataFrame`\n    (still experimental). Behaviour is as follows:\n\n    * ``\"numpy_nullable\"``: returns nullable-dtype-backed :class:`DataFrame`\n      (default).\n    * ``\"pyarrow\"``: returns pyarrow-backed nullable :class:`ArrowDtype`\n      DataFrame.\n\n    .. versionadded:: 2.0\n\nReturns\n-------\nret\n    Numeric if parsing succeeded.\n    Return type depends on input.  Series if Series, otherwise ndarray.\n\nSee Also\n--------\nDataFrame.astype : Cast argument to a specified dtype.\nto_datetime : Convert argument to datetime.\nto_timedelta : Convert argument to timedelta.\nnumpy.ndarray.astype : Cast a numpy array to a specified type.\nDataFrame.convert_dtypes : Convert dtypes.\n\nExamples\n--------\nTake separate series and convert to numeric, coercing when told to\n\n>>> s = pd.Series(['1.0', '2', -3])\n>>> pd.to_numeric(s)\n0    1.0\n1    2.0\n2   -3.0\ndtype: float64\n>>> pd.to_numeric(s, downcast='float')\n0    1.0\n1    2.0\n2   -3.0\ndtype: float32\n>>> pd.to_numeric(s, downcast='signed')\n0    1\n1    2\n2   -3\ndtype: int8\n>>> s = pd.Series(['apple', '1.0', '2', -3])\n>>> pd.to_numeric(s, errors='coerce')\n0    NaN\n1    1.0\n2    2.0\n3   -3.0\ndtype: float64\n\nDowncasting of nullable integer and floating dtypes is supported:\n\n>>> s = pd.Series([1, 2, 3], dtype=\"Int64\")\n>>> pd.to_numeric(s, downcast=\"integer\")\n0    1\n1    2\n2    3\ndtype: Int8\n>>> s = pd.Series([1.0, 2.1, 3.0], dtype=\"Float64\")\n>>> pd.to_numeric(s, downcast=\"float\")\n0    1.0\n1    2.1\n2    3.0\ndtype: Float32", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/458", "data": {"name": "re.compile", "type": "callable", "signature": "(pattern)", "description": "Compile a regular expression pattern, returning a Pattern object.", "parameters": {"type": "object", "properties": {"pattern": {}}}}}
{"task_id": "BigCodeBench/458", "data": {"name": "re.compile.match", "type": "callable", "signature": "(pattern)", "description": "Compile a regular expression pattern, returning a Pattern object.", "parameters": {"type": "object", "properties": {"pattern": {}}}}}
{"task_id": "BigCodeBench/461", "data": {"name": "psutil.Process", "type": "class", "signature": "(pid=None)", "description": "Represents an OS process with the given PID.\nIf PID is omitted current process PID (os.getpid()) is used.\nRaise NoSuchProcess if PID does not exist.\n\nNote that most of the methods of this class do not make sure\nthe PID of the process being queried has been reused over time.\nThat means you might end up retrieving an information referring\nto another process in case the original one this instance\nrefers to is gone in the meantime.\n\nThe only exceptions for which process identity is pre-emptively\nchecked and guaranteed are:\n\n - parent()\n - children()\n - nice() (set)\n - ionice() (set)\n - rlimit() (set)\n - cpu_affinity (set)\n - suspend()\n - resume()\n - send_signal()\n - terminate()\n - kill()\n\nTo prevent this problem for all other methods you can:\n - use is_running() before querying the process\n - if you're continuously iterating over a set of Process\n   instances use process_iter() which pre-emptively checks\n   process identity for every yielded instance", "parameters": {"type": "object", "properties": {"pid": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/461", "data": {"name": "psutil.Process.is_running", "type": "callable", "signature": "(self)", "description": "Return whether this process is running.\nIt also checks if PID has been reused by another process in\nwhich case return False.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/461", "data": {"name": "psutil.Process.memory_info", "type": "callable", "signature": "(self)", "description": "Return a namedtuple with variable fields depending on the\nplatform, representing memory information about the process.\n\nThe \"portable\" fields available on all platforms are `rss` and `vms`.\n\nAll numbers are expressed in bytes.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/461", "data": {"name": "psutil.Process.cpu_percent", "type": "callable", "signature": "(self, interval=None)", "description": "Return a float representing the current process CPU\nutilization as a percentage.\n\nWhen *interval* is 0.0 or None (default) compares process times\nto system CPU times elapsed since last call, returning\nimmediately (non-blocking). That means that the first time\nthis is called it will return a meaningful 0.0 value.\n\nWhen *interval* is > 0.0 compares process times to system CPU\ntimes elapsed before and after the interval (blocking).\n\nIn this case is recommended for accuracy that this function\nbe called with at least 0.1 seconds between calls.\n\nA value > 100.0 can be returned in case of processes running\nmultiple threads on different CPU cores.\n\nThe returned value is explicitly NOT split evenly between\nall available logical CPUs. This means that a busy loop process\nrunning on a system with 2 logical CPUs will be reported as\nhaving 100% CPU utilization instead of 50%.\n\nExamples:\n\n  >>> import psutil\n  >>> p = psutil.Process(os.getpid())\n  >>> # blocking\n  >>> p.cpu_percent(interval=1)\n  2.0\n  >>> # non-blocking (percentage since last call)\n  >>> p.cpu_percent(interval=None)\n  2.9\n  >>>", "parameters": {"type": "object", "properties": {"interval": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/461", "data": {"name": "subprocess.Popen", "type": "class", "signature": "(args, bufsize=-1, executable=None, stdin=None, stdout=None, stderr=None, preexec_fn=None, close_fds=True, shell=False, cwd=None, env=None, universal_newlines=None, startupinfo=None, creationflags=0, restore_signals=True, start_new_session=False, pass_fds=(), *, user=None, group=None, extra_groups=None, encoding=None, errors=None, text=None, umask=-1, pipesize=-1)", "description": "Execute a child program in a new process.\n\nFor a complete description of the arguments see the Python documentation.\n\nArguments:\n  args: A string, or a sequence of program arguments.\n\n  bufsize: supplied as the buffering argument to the open() function when\n      creating the stdin/stdout/stderr pipe file objects\n\n  executable: A replacement program to execute.\n\n  stdin, stdout and stderr: These specify the executed programs' standard\n      input, standard output and standard error file handles, respectively.\n\n  preexec_fn: (POSIX only) An object to be called in the child process\n      just before the child is executed.\n\n  close_fds: Controls closing or inheriting of file descriptors.\n\n  shell: If true, the command will be executed through the shell.\n\n  cwd: Sets the current directory before the child is executed.\n\n  env: Defines the environment variables for the new process.\n\n  text: If true, decode stdin, stdout and stderr using the given encoding\n      (if set) or the system default otherwise.\n\n  universal_newlines: Alias of text, provided for backwards compatibility.\n\n  startupinfo and creationflags (Windows only)\n\n  restore_signals (POSIX only)\n\n  start_new_session (POSIX only)\n\n  group (POSIX only)\n\n  extra_groups (POSIX only)\n\n  user (POSIX only)\n\n  umask (POSIX only)\n\n  pass_fds (POSIX only)\n\n  encoding and errors: Text mode encoding and error handling to use for\n      file objects stdin, stdout and stderr.\n\nAttributes:\n    stdin, stdout, stderr, pid, returncode", "parameters": {"type": "object", "properties": {"args": {}, "bufsize": {"type": "integer", "default": -1}, "executable": {"type": "NoneType", "default": null}, "stdin": {"type": "NoneType", "default": null}, "stdout": {"type": "NoneType", "default": null}, "stderr": {"type": "NoneType", "default": null}, "preexec_fn": {"type": "NoneType", "default": null}, "close_fds": {"type": "bool", "default": true}, "shell": {"type": "bool", "default": false}, "cwd": {"type": "NoneType", "default": null}, "env": {"type": "NoneType", "default": null}, "universal_newlines": {"type": "NoneType", "default": null}, "startupinfo": {"type": "NoneType", "default": null}, "creationflags": {"type": "integer", "default": 0}, "restore_signals": {"type": "bool", "default": true}, "start_new_session": {"type": "bool", "default": false}, "pass_fds": {"type": "str", "default": "()"}, "user": {"type": "NoneType", "default": null}, "group": {"type": "NoneType", "default": null}, "extra_groups": {"type": "NoneType", "default": null}, "encoding": {"type": "NoneType", "default": null}, "errors": {"type": "NoneType", "default": null}, "text": {"type": "NoneType", "default": null}, "umask": {"type": "integer", "default": -1}, "pipesize": {"type": "integer", "default": -1}}}}}
{"task_id": "BigCodeBench/461", "data": {"name": "subprocess.Popen.pid", "type": "class", "signature": "(args, bufsize=-1, executable=None, stdin=None, stdout=None, stderr=None, preexec_fn=None, close_fds=True, shell=False, cwd=None, env=None, universal_newlines=None, startupinfo=None, creationflags=0, restore_signals=True, start_new_session=False, pass_fds=(), *, user=None, group=None, extra_groups=None, encoding=None, errors=None, text=None, umask=-1, pipesize=-1)", "description": "Execute a child program in a new process.\n\nFor a complete description of the arguments see the Python documentation.\n\nArguments:\n  args: A string, or a sequence of program arguments.\n\n  bufsize: supplied as the buffering argument to the open() function when\n      creating the stdin/stdout/stderr pipe file objects\n\n  executable: A replacement program to execute.\n\n  stdin, stdout and stderr: These specify the executed programs' standard\n      input, standard output and standard error file handles, respectively.\n\n  preexec_fn: (POSIX only) An object to be called in the child process\n      just before the child is executed.\n\n  close_fds: Controls closing or inheriting of file descriptors.\n\n  shell: If true, the command will be executed through the shell.\n\n  cwd: Sets the current directory before the child is executed.\n\n  env: Defines the environment variables for the new process.\n\n  text: If true, decode stdin, stdout and stderr using the given encoding\n      (if set) or the system default otherwise.\n\n  universal_newlines: Alias of text, provided for backwards compatibility.\n\n  startupinfo and creationflags (Windows only)\n\n  restore_signals (POSIX only)\n\n  start_new_session (POSIX only)\n\n  group (POSIX only)\n\n  extra_groups (POSIX only)\n\n  user (POSIX only)\n\n  umask (POSIX only)\n\n  pass_fds (POSIX only)\n\n  encoding and errors: Text mode encoding and error handling to use for\n      file objects stdin, stdout and stderr.\n\nAttributes:\n    stdin, stdout, stderr, pid, returncode", "parameters": {"type": "object", "properties": {"args": {}, "bufsize": {"type": "integer", "default": -1}, "executable": {"type": "NoneType", "default": null}, "stdin": {"type": "NoneType", "default": null}, "stdout": {"type": "NoneType", "default": null}, "stderr": {"type": "NoneType", "default": null}, "preexec_fn": {"type": "NoneType", "default": null}, "close_fds": {"type": "bool", "default": true}, "shell": {"type": "bool", "default": false}, "cwd": {"type": "NoneType", "default": null}, "env": {"type": "NoneType", "default": null}, "universal_newlines": {"type": "NoneType", "default": null}, "startupinfo": {"type": "NoneType", "default": null}, "creationflags": {"type": "integer", "default": 0}, "restore_signals": {"type": "bool", "default": true}, "start_new_session": {"type": "bool", "default": false}, "pass_fds": {"type": "str", "default": "()"}, "user": {"type": "NoneType", "default": null}, "group": {"type": "NoneType", "default": null}, "extra_groups": {"type": "NoneType", "default": null}, "encoding": {"type": "NoneType", "default": null}, "errors": {"type": "NoneType", "default": null}, "text": {"type": "NoneType", "default": null}, "umask": {"type": "integer", "default": -1}, "pipesize": {"type": "integer", "default": -1}}}}}
{"task_id": "BigCodeBench/461", "data": {"name": "subprocess.Popen.wait", "type": "callable", "signature": "(self)", "description": "Wait for child process to terminate; returns self.returncode.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/461", "data": {"name": "subprocess.Popen.poll", "type": "callable", "signature": "(self)", "description": "Check if child process has terminated. Set and return returncode\nattribute.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/461", "data": {"name": "subprocess.Popen.terminate", "type": "callable", "signature": "(self)", "description": "Terminate the process with SIGTERM\n            ", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/461", "data": {"name": "os.path.exists", "type": "callable", "signature": "(path)", "description": "Test whether a path exists.  Returns False for broken symbolic links", "parameters": {"type": "object", "properties": {"path": {}}}}}
{"task_id": "BigCodeBench/461", "data": {"name": "time.time", "type": "callable", "signature": "()", "description": "time() -> floating point number\n\nReturn the current time in seconds since the Epoch.\nFractions of a second may be present if the system clock provides them.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/461", "data": {"name": "time.sleep", "type": "callable", "signature": "(*args, **kwargs)", "description": "sleep(seconds)\n\nDelay execution for a given number of seconds.  The argument may be\na floating point number for subsecond precision.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/477", "data": {"name": "matplotlib.pyplot.subplots", "type": "class", "signature": "()", "description": "Create a figure and a set of subplots.\n\nThis utility wrapper makes it convenient to create common layouts of\nsubplots, including the enclosing figure object, in a single call.\n\nParameters\n----------\nnrows, ncols : int, default: 1\n    Number of rows/columns of the subplot grid.\n\nsharex, sharey : bool or {'none', 'all', 'row', 'col'}, default: False\n    Controls sharing of properties among x (*sharex*) or y (*sharey*)\n    axes:\n\n    - True or 'all': x- or y-axis will be shared among all subplots.\n    - False or 'none': each subplot x- or y-axis will be independent.\n    - 'row': each subplot row will share an x- or y-axis.\n    - 'col': each subplot column will share an x- or y-axis.\n\n    When subplots have a shared x-axis along a column, only the x tick\n    labels of the bottom subplot are created. Similarly, when subplots\n    have a shared y-axis along a row, only the y tick labels of the first\n    column subplot are created. To later turn other subplots' ticklabels\n    on, use `~matplotlib.axes.Axes.tick_params`.\n\n    When subplots have a shared axis that has units, calling\n    `~matplotlib.axis.Axis.set_units` will update each axis with the\n    new units.\n\nsqueeze : bool, default: True\n    - If True, extra dimensions are squeezed out from the returned\n      array of `~matplotlib.axes.Axes`:\n\n      - if only one subplot is constructed (nrows=ncols=1), the\n        resulting single Axes object is returned as a scalar.\n      - for Nx1 or 1xM subplots, the returned object is a 1D numpy\n        object array of Axes objects.\n      - for NxM, subplots with N>1 and M>1 are returned as a 2D array.\n\n    - If False, no squeezing at all is done: the returned Axes object is\n      always a 2D array containing Axes instances, even if it ends up\n      being 1x1.\n\nwidth_ratios : array-like of length *ncols*, optional\n    Defines the relative widths of the columns. Each column gets a\n    relative width of ``width_ratios[i] / sum(width_ratios)``.\n    If not given, all columns will have the same width.  Equivalent\n    to ``gridspec_kw={'width_ratios': [...]}``.\n\nheight_ratios : array-like of length *nrows*, optional\n    Defines the relative heights of the rows. Each row gets a\n    relative height of ``height_ratios[i] / sum(height_ratios)``.\n    If not given, all rows will have the same height. Convenience\n    for ``gridspec_kw={'height_ratios': [...]}``.\n\nsubplot_kw : dict, optional\n    Dict with keywords passed to the\n    `~matplotlib.figure.Figure.add_subplot` call used to create each\n    subplot.\n\ngridspec_kw : dict, optional\n    Dict with keywords passed to the `~matplotlib.gridspec.GridSpec`\n    constructor used to create the grid the subplots are placed on.\n\n**fig_kw\n    All additional keyword arguments are passed to the\n    `.pyplot.figure` call.\n\nReturns\n-------\nfig : `.Figure`\n\nax : `~matplotlib.axes.Axes` or array of Axes\n    *ax* can be either a single `~.axes.Axes` object, or an array of Axes\n    objects if more than one subplot was created.  The dimensions of the\n    resulting array can be controlled with the squeeze keyword, see above.\n\n    Typical idioms for handling the return value are::\n\n        # using the variable ax for single a Axes\n        fig, ax = plt.subplots()\n\n        # using the variable axs for multiple Axes\n        fig, axs = plt.subplots(2, 2)\n\n        # using tuple unpacking for multiple Axes\n        fig, (ax1, ax2) = plt.subplots(1, 2)\n        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n\n    The names ``ax`` and pluralized ``axs`` are preferred over ``axes``\n    because for the latter it's not clear if it refers to a single\n    `~.axes.Axes` instance or a collection of these.\n\nSee Also\n--------\n.pyplot.figure\n.pyplot.subplot\n.pyplot.axes\n.Figure.subplots\n.Figure.add_subplot\n\nExamples\n--------\n::\n\n    # First create some toy data:\n    x = np.linspace(0, 2*np.pi, 400)\n    y = np.sin(x**2)\n\n    # Create just a figure and only one subplot\n    fig, ax = plt.subplots()\n    ax.plot(x, y)\n    ax.set_title('Simple plot')\n\n    # Create two subplots and unpack the output array immediately\n    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n    ax1.plot(x, y)\n    ax1.set_title('Sharing Y axis')\n    ax2.scatter(x, y)\n\n    # Create four polar axes and access them through the returned array\n    fig, axs = plt.subplots(2, 2, subplot_kw=dict(projection=\"polar\"))\n    axs[0, 0].plot(x, y)\n    axs[1, 1].scatter(x, y)\n\n    # Share a X axis with each column of subplots\n    plt.subplots(2, 2, sharex='col')\n\n    # Share a Y axis with each row of subplots\n    plt.subplots(2, 2, sharey='row')\n\n    # Share both X and Y axes with all subplots\n    plt.subplots(2, 2, sharex='all', sharey='all')\n\n    # Note that this is the same as\n    plt.subplots(2, 2, sharex=True, sharey=True)\n\n    # Create figure number 10 with a single subplot\n    # and clears it if it already exists.\n    fig, ax = plt.subplots(num=10, clear=True)", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/477", "data": {"name": "matplotlib.pyplot.subplots[1].scatter", "type": "method", "signature": "(x, y, s=None, c=None, marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=None, linewidths=None, *, edgecolors=None, plotnonfinite=False, data=None, **kwargs)", "description": "A scatter plot of *y* vs. *x* with varying marker size and/or color.\n\nParameters\n----------\nx, y : float or array-like, shape (n, )\n    The data positions.\n\ns : float or array-like, shape (n, ), optional\n    The marker size in points**2 (typographic points are 1/72 in.).\n    Default is ``rcParams['lines.markersize'] ** 2``.\n\n    The linewidth and edgecolor can visually interact with the marker\n    size, and can lead to artifacts if the marker size is smaller than\n    the linewidth.\n\n    If the linewidth is greater than 0 and the edgecolor is anything\n    but *'none'*, then the effective size of the marker will be\n    increased by half the linewidth because the stroke will be centered\n    on the edge of the shape.\n\n    To eliminate the marker edge either set *linewidth=0* or\n    *edgecolor='none'*.\n\nc : array-like or list of colors or color, optional\n    The marker colors. Possible values:\n\n    - A scalar or sequence of n numbers to be mapped to colors using\n      *cmap* and *norm*.\n    - A 2D array in which the rows are RGB or RGBA.\n    - A sequence of colors of length n.\n    - A single color format string.\n\n    Note that *c* should not be a single numeric RGB or RGBA sequence\n    because that is indistinguishable from an array of values to be\n    colormapped. If you want to specify the same RGB or RGBA value for\n    all points, use a 2D array with a single row.  Otherwise,\n    value-matching will have precedence in case of a size matching with\n    *x* and *y*.\n\n    If you wish to specify a single color for all points\n    prefer the *color* keyword argument.\n\n    Defaults to `None`. In that case the marker color is determined\n    by the value of *color*, *facecolor* or *facecolors*. In case\n    those are not specified or `None`, the marker color is determined\n    by the next color of the ``Axes``' current \"shape and fill\" color\n    cycle. This cycle defaults to :rc:`axes.prop_cycle`.\n\nmarker : `~.markers.MarkerStyle`, default: :rc:`scatter.marker`\n    The marker style. *marker* can be either an instance of the class\n    or the text shorthand for a particular marker.\n    See :mod:`matplotlib.markers` for more information about marker\n    styles.\n\ncmap : str or `~matplotlib.colors.Colormap`, default: :rc:`image.cmap`\n    The Colormap instance or registered colormap name used to map scalar data\n    to colors.\n\n    This parameter is ignored if *c* is RGB(A).\n\nnorm : str or `~matplotlib.colors.Normalize`, optional\n    The normalization method used to scale scalar data to the [0, 1] range\n    before mapping to colors using *cmap*. By default, a linear scaling is\n    used, mapping the lowest value to 0 and the highest to 1.\n\n    If given, this can be one of the following:\n\n    - An instance of `.Normalize` or one of its subclasses\n      (see :ref:`colormapnorms`).\n    - A scale name, i.e. one of \"linear\", \"log\", \"symlog\", \"logit\", etc.  For a\n      list of available scales, call `matplotlib.scale.get_scale_names()`.\n      In that case, a suitable `.Normalize` subclass is dynamically generated\n      and instantiated.\n\n    This parameter is ignored if *c* is RGB(A).\n\nvmin, vmax : float, optional\n    When using scalar data and no explicit *norm*, *vmin* and *vmax* define\n    the data range that the colormap covers. By default, the colormap covers\n    the complete value range of the supplied data. It is an error to use\n    *vmin*/*vmax* when a *norm* instance is given (but using a `str` *norm*\n    name together with *vmin*/*vmax* is acceptable).\n\n    This parameter is ignored if *c* is RGB(A).\n\nalpha : float, default: None\n    The alpha blending value, between 0 (transparent) and 1 (opaque).\n\nlinewidths : float or array-like, default: :rc:`lines.linewidth`\n    The linewidth of the marker edges. Note: The default *edgecolors*\n    is 'face'. You may want to change this as well.\n\nedgecolors : {'face', 'none', *None*} or color or sequence of color, default: :rc:`scatter.edgecolors`\n    The edge color of the marker. Possible values:\n\n    - 'face': The edge color will always be the same as the face color.\n    - 'none': No patch boundary will be drawn.\n    - A color or sequence of colors.\n\n    For non-filled markers, *edgecolors* is ignored. Instead, the color\n    is determined like with 'face', i.e. from *c*, *colors*, or\n    *facecolors*.\n\nplotnonfinite : bool, default: False\n    Whether to plot points with nonfinite *c* (i.e. ``inf``, ``-inf``\n    or ``nan``). If ``True`` the points are drawn with the *bad*\n    colormap color (see `.Colormap.set_bad`).\n\nReturns\n-------\n`~matplotlib.collections.PathCollection`\n\nOther Parameters\n----------------\ndata : indexable object, optional\n    If given, the following parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` (unless this raises an exception):\n\n    *x*, *y*, *s*, *linewidths*, *edgecolors*, *c*, *facecolor*, *facecolors*, *color*\n**kwargs : `~matplotlib.collections.Collection` properties\n\nSee Also\n--------\nplot : To plot scatter plots when markers are identical in size and\n    color.\n\nNotes\n-----\n* The `.plot` function will be faster for scatterplots where markers\n  don't vary in size or color.\n\n* Any or all of *x*, *y*, *s*, and *c* may be masked arrays, in which\n  case all masks will be combined and only unmasked points will be\n  plotted.\n\n* Fundamentally, scatter works with 1D arrays; *x*, *y*, *s*, and *c*\n  may be input as N-D arrays, but within scatter they will be\n  flattened. The exception is *c*, which will be flattened only if its\n  size matches the size of *x* and *y*.", "parameters": {"type": "object", "properties": {"x": {}, "y": {}, "s": {"type": "NoneType", "default": null}, "c": {"type": "NoneType", "default": null}, "marker": {"type": "NoneType", "default": null}, "cmap": {"type": "NoneType", "default": null}, "norm": {"type": "NoneType", "default": null}, "vmin": {"type": "NoneType", "default": null}, "vmax": {"type": "NoneType", "default": null}, "alpha": {"type": "NoneType", "default": null}, "linewidths": {"type": "NoneType", "default": null}, "edgecolors": {"type": "NoneType", "default": null}, "plotnonfinite": {"type": "bool", "default": false}, "data": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/477", "data": {"name": "numpy.random.rand", "type": "callable", "signature": "(*args, **kwargs)", "description": "rand(d0, d1, ..., dn)\n\nRandom values in a given shape.\n\n.. note::\n    This is a convenience function for users porting code from Matlab,\n    and wraps `random_sample`. That function takes a\n    tuple to specify the size of the output, which is consistent with\n    other NumPy functions like `numpy.zeros` and `numpy.ones`.\n\nCreate an array of the given shape and populate it with\nrandom samples from a uniform distribution\nover ``[0, 1)``.\n\nParameters\n----------\nd0, d1, ..., dn : int, optional\n    The dimensions of the returned array, must be non-negative.\n    If no argument is given a single Python float is returned.\n\nReturns\n-------\nout : ndarray, shape ``(d0, d1, ..., dn)``\n    Random values.\n\nSee Also\n--------\nrandom\n\nExamples\n--------\n>>> np.random.rand(3,2)\narray([[ 0.14022471,  0.96360618],  #random\n       [ 0.37601032,  0.25528411],  #random\n       [ 0.49313049,  0.94909878]]) #random", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/477", "data": {"name": "numpy.array", "type": "callable", "signature": "(*args, **kwargs)", "description": "array(object, dtype=None, *, copy=True, order='K', subok=False, ndmin=0,\n      like=None)\n\nCreate an array.\n\nParameters\n----------\nobject : array_like\n    An array, any object exposing the array interface, an object whose\n    __array__ method returns an array, or any (nested) sequence.\n    If object is a scalar, a 0-dimensional array containing object is\n    returned.\ndtype : data-type, optional\n    The desired data-type for the array.  If not given, then the type will\n    be determined as the minimum type required to hold the objects in the\n    sequence.\ncopy : bool, optional\n    If true (default), then the object is copied.  Otherwise, a copy will\n    only be made if __array__ returns a copy, if obj is a nested sequence,\n    or if a copy is needed to satisfy any of the other requirements\n    (`dtype`, `order`, etc.).\norder : {'K', 'A', 'C', 'F'}, optional\n    Specify the memory layout of the array. If object is not an array, the\n    newly created array will be in C order (row major) unless 'F' is\n    specified, in which case it will be in Fortran order (column major).\n    If object is an array the following holds.\n\n    ===== ========= ===================================================\n    order  no copy                     copy=True\n    ===== ========= ===================================================\n    'K'   unchanged F & C order preserved, otherwise most similar order\n    'A'   unchanged F order if input is F and not C, otherwise C order\n    'C'   C order   C order\n    'F'   F order   F order\n    ===== ========= ===================================================\n\n    When ``copy=False`` and a copy is made for other reasons, the result is\n    the same as if ``copy=True``, with some exceptions for 'A', see the\n    Notes section. The default order is 'K'.\nsubok : bool, optional\n    If True, then sub-classes will be passed-through, otherwise\n    the returned array will be forced to be a base-class array (default).\nndmin : int, optional\n    Specifies the minimum number of dimensions that the resulting\n    array should have.  Ones will be prepended to the shape as\n    needed to meet this requirement.\nlike : array_like, optional\n    Reference object to allow the creation of arrays which are not\n    NumPy arrays. If an array-like passed in as ``like`` supports\n    the ``__array_function__`` protocol, the result will be defined\n    by it. In this case, it ensures the creation of an array object\n    compatible with that passed in via this argument.\n\n    .. versionadded:: 1.20.0\n\nReturns\n-------\nout : ndarray\n    An array object satisfying the specified requirements.\n\nSee Also\n--------\nempty_like : Return an empty array with shape and type of input.\nones_like : Return an array of ones with shape and type of input.\nzeros_like : Return an array of zeros with shape and type of input.\nfull_like : Return a new array with shape of input filled with value.\nempty : Return a new uninitialized array.\nones : Return a new array setting values to one.\nzeros : Return a new array setting values to zero.\nfull : Return a new array of given shape filled with value.\n\n\nNotes\n-----\nWhen order is 'A' and `object` is an array in neither 'C' nor 'F' order,\nand a copy is forced by a change in dtype, then the order of the result is\nnot necessarily 'C' as expected. This is likely a bug.\n\nExamples\n--------\n>>> np.array([1, 2, 3])\narray([1, 2, 3])\n\nUpcasting:\n\n>>> np.array([1, 2, 3.0])\narray([ 1.,  2.,  3.])\n\nMore than one dimension:\n\n>>> np.array([[1, 2], [3, 4]])\narray([[1, 2],\n       [3, 4]])\n\nMinimum dimensions 2:\n\n>>> np.array([1, 2, 3], ndmin=2)\narray([[1, 2, 3]])\n\nType provided:\n\n>>> np.array([1, 2, 3], dtype=complex)\narray([ 1.+0.j,  2.+0.j,  3.+0.j])\n\nData-type consisting of more than one element:\n\n>>> x = np.array([(1,2),(3,4)],dtype=[('a','<i4'),('b','<i4')])\n>>> x['a']\narray([1, 3])\n\nCreating an array from sub-classes:\n\n>>> np.array(np.mat('1 2; 3 4'))\narray([[1, 2],\n       [3, 4]])\n\n>>> np.array(np.mat('1 2; 3 4'), subok=True)\nmatrix([[1, 2],\n        [3, 4]])", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/477", "data": {"name": "numpy.random.seed", "type": "callable", "signature": "(*args, **kwargs)", "description": "seed(self, seed=None)\n\nReseed a legacy MT19937 BitGenerator\n\nNotes\n-----\nThis is a convenience, legacy function.\n\nThe best practice is to **not** reseed a BitGenerator, rather to\nrecreate a new one. This method is here for legacy reasons.\nThis example demonstrates best practice.\n\n>>> from numpy.random import MT19937\n>>> from numpy.random import RandomState, SeedSequence\n>>> rs = RandomState(MT19937(SeedSequence(123456789)))\n# Later, you want to restart the stream\n>>> rs = RandomState(MT19937(SeedSequence(987654321)))", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/477", "data": {"name": "numpy.random.shuffle", "type": "callable", "signature": "(*args, **kwargs)", "description": "shuffle(x)\n\nModify a sequence in-place by shuffling its contents.\n\nThis function only shuffles the array along the first axis of a\nmulti-dimensional array. The order of sub-arrays is changed but\ntheir contents remains the same.\n\n.. note::\n    New code should use the ``shuffle`` method of a ``default_rng()``\n    instance instead; please see the :ref:`random-quick-start`.\n\nParameters\n----------\nx : ndarray or MutableSequence\n    The array, list or mutable sequence to be shuffled.\n\nReturns\n-------\nNone\n\nSee Also\n--------\nrandom.Generator.shuffle: which should be used for new code.\n\nExamples\n--------\n>>> arr = np.arange(10)\n>>> np.random.shuffle(arr)\n>>> arr\n[1 7 5 2 9 4 3 6 0 8] # random\n\nMulti-dimensional arrays are only shuffled along the first axis:\n\n>>> arr = np.arange(9).reshape((3, 3))\n>>> np.random.shuffle(arr)\n>>> arr\narray([[3, 4, 5], # random\n       [6, 7, 8],\n       [0, 1, 2]])", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/477", "data": {"name": "numpy.random.choice", "type": "callable", "signature": "(*args, **kwargs)", "description": "choice(a, size=None, replace=True, p=None)\n\nGenerates a random sample from a given 1-D array\n\n.. versionadded:: 1.7.0\n\n.. note::\n    New code should use the ``choice`` method of a ``default_rng()``\n    instance instead; please see the :ref:`random-quick-start`.\n\nParameters\n----------\na : 1-D array-like or int\n    If an ndarray, a random sample is generated from its elements.\n    If an int, the random sample is generated as if it were ``np.arange(a)``\nsize : int or tuple of ints, optional\n    Output shape.  If the given shape is, e.g., ``(m, n, k)``, then\n    ``m * n * k`` samples are drawn.  Default is None, in which case a\n    single value is returned.\nreplace : boolean, optional\n    Whether the sample is with or without replacement. Default is True,\n    meaning that a value of ``a`` can be selected multiple times.\np : 1-D array-like, optional\n    The probabilities associated with each entry in a.\n    If not given, the sample assumes a uniform distribution over all\n    entries in ``a``.\n\nReturns\n-------\nsamples : single item or ndarray\n    The generated random samples\n\nRaises\n------\nValueError\n    If a is an int and less than zero, if a or p are not 1-dimensional,\n    if a is an array-like of size 0, if p is not a vector of\n    probabilities, if a and p have different lengths, or if\n    replace=False and the sample size is greater than the population\n    size\n\nSee Also\n--------\nrandint, shuffle, permutation\nrandom.Generator.choice: which should be used in new code\n\nNotes\n-----\nSetting user-specified probabilities through ``p`` uses a more general but less\nefficient sampler than the default. The general sampler produces a different sample\nthan the optimized sampler even if each element of ``p`` is 1 / len(a).\n\nSampling random rows from a 2-D array is not possible with this function,\nbut is possible with `Generator.choice` through its ``axis`` keyword.\n\nExamples\n--------\nGenerate a uniform random sample from np.arange(5) of size 3:\n\n>>> np.random.choice(5, 3)\narray([0, 3, 4]) # random\n>>> #This is equivalent to np.random.randint(0,5,3)\n\nGenerate a non-uniform random sample from np.arange(5) of size 3:\n\n>>> np.random.choice(5, 3, p=[0.1, 0, 0.3, 0.6, 0])\narray([3, 3, 0]) # random\n\nGenerate a uniform random sample from np.arange(5) of size 3 without\nreplacement:\n\n>>> np.random.choice(5, 3, replace=False)\narray([3,1,0]) # random\n>>> #This is equivalent to np.random.permutation(np.arange(5))[:3]\n\nGenerate a non-uniform random sample from np.arange(5) of size\n3 without replacement:\n\n>>> np.random.choice(5, 3, replace=False, p=[0.1, 0, 0.3, 0.6, 0])\narray([2, 3, 0]) # random\n\nAny of the above can be repeated with an arbitrary array-like\ninstead of just integers. For instance:\n\n>>> aa_milne_arr = ['pooh', 'rabbit', 'piglet', 'Christopher']\n>>> np.random.choice(aa_milne_arr, 5, p=[0.5, 0.1, 0.1, 0.3])\narray(['pooh', 'pooh', 'pooh', 'Christopher', 'piglet'], # random\n      dtype='<U11')", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/477", "data": {"name": "numpy.concatenate", "type": "callable", "signature": "(*args, **kwargs)", "description": "concatenate((a1, a2, ...), axis=0, out=None, dtype=None, casting=\"same_kind\")\n\nJoin a sequence of arrays along an existing axis.\n\nParameters\n----------\na1, a2, ... : sequence of array_like\n    The arrays must have the same shape, except in the dimension\n    corresponding to `axis` (the first, by default).\naxis : int, optional\n    The axis along which the arrays will be joined.  If axis is None,\n    arrays are flattened before use.  Default is 0.\nout : ndarray, optional\n    If provided, the destination to place the result. The shape must be\n    correct, matching that of what concatenate would have returned if no\n    out argument were specified.\ndtype : str or dtype\n    If provided, the destination array will have this dtype. Cannot be\n    provided together with `out`.\n\n    .. versionadded:: 1.20.0\n\ncasting : {'no', 'equiv', 'safe', 'same_kind', 'unsafe'}, optional\n    Controls what kind of data casting may occur. Defaults to 'same_kind'.\n\n    .. versionadded:: 1.20.0\n\nReturns\n-------\nres : ndarray\n    The concatenated array.\n\nSee Also\n--------\nma.concatenate : Concatenate function that preserves input masks.\narray_split : Split an array into multiple sub-arrays of equal or\n              near-equal size.\nsplit : Split array into a list of multiple sub-arrays of equal size.\nhsplit : Split array into multiple sub-arrays horizontally (column wise).\nvsplit : Split array into multiple sub-arrays vertically (row wise).\ndsplit : Split array into multiple sub-arrays along the 3rd axis (depth).\nstack : Stack a sequence of arrays along a new axis.\nblock : Assemble arrays from blocks.\nhstack : Stack arrays in sequence horizontally (column wise).\nvstack : Stack arrays in sequence vertically (row wise).\ndstack : Stack arrays in sequence depth wise (along third dimension).\ncolumn_stack : Stack 1-D arrays as columns into a 2-D array.\n\nNotes\n-----\nWhen one or more of the arrays to be concatenated is a MaskedArray,\nthis function will return a MaskedArray object instead of an ndarray,\nbut the input masks are *not* preserved. In cases where a MaskedArray\nis expected as input, use the ma.concatenate function from the masked\narray module instead.\n\nExamples\n--------\n>>> a = np.array([[1, 2], [3, 4]])\n>>> b = np.array([[5, 6]])\n>>> np.concatenate((a, b), axis=0)\narray([[1, 2],\n       [3, 4],\n       [5, 6]])\n>>> np.concatenate((a, b.T), axis=1)\narray([[1, 2, 5],\n       [3, 4, 6]])\n>>> np.concatenate((a, b), axis=None)\narray([1, 2, 3, 4, 5, 6])\n\nThis function will not preserve masking of MaskedArray inputs.\n\n>>> a = np.ma.arange(3)\n>>> a[1] = np.ma.masked\n>>> b = np.arange(2, 5)\n>>> a\nmasked_array(data=[0, --, 2],\n             mask=[False,  True, False],\n       fill_value=999999)\n>>> b\narray([2, 3, 4])\n>>> np.concatenate([a, b])\nmasked_array(data=[0, 1, 2, 2, 3, 4],\n             mask=False,\n       fill_value=999999)\n>>> np.ma.concatenate([a, b])\nmasked_array(data=[0, --, 2, 2, 3, 4],\n             mask=[False,  True, False, False, False, False],\n       fill_value=999999)", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/486", "data": {"name": "datetime.datetime", "type": "class", "signature": "(self, /, *args, **kwargs)", "description": "datetime(year, month, day[, hour[, minute[, second[, microsecond[,tzinfo]]]]])\n\nThe year, month and day arguments are required. tzinfo may be None, or an\ninstance of a tzinfo subclass. The remaining arguments may be ints.", "parameters": {"type": "object", "properties": {"/": {}}}}}
{"task_id": "BigCodeBench/486", "data": {"name": "datetime.datetime.fromtimestamp", "type": "callable", "signature": "(*args, **kwargs)", "description": "timestamp[, tz] -> tz's local time from POSIX timestamp.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/486", "data": {"name": "pandas.DataFrame", "type": "class", "signature": "(data=None, index: 'Axes | None' = None, columns: 'Axes | None' = None, dtype: 'Dtype | None' = None, copy: 'bool | None' = None) -> 'None'", "description": "Two-dimensional, size-mutable, potentially heterogeneous tabular data.\n\nData structure also contains labeled axes (rows and columns).\nArithmetic operations align on both row and column labels. Can be\nthought of as a dict-like container for Series objects. The primary\npandas data structure.\n\nParameters\n----------\ndata : ndarray (structured or homogeneous), Iterable, dict, or DataFrame\n    Dict can contain Series, arrays, constants, dataclass or list-like objects. If\n    data is a dict, column order follows insertion-order. If a dict contains Series\n    which have an index defined, it is aligned by its index. This alignment also\n    occurs if data is a Series or a DataFrame itself. Alignment is done on\n    Series/DataFrame inputs.\n\n    If data is a list of dicts, column order follows insertion-order.\n\nindex : Index or array-like\n    Index to use for resulting frame. Will default to RangeIndex if\n    no indexing information part of input data and no index provided.\ncolumns : Index or array-like\n    Column labels to use for resulting frame when data does not have them,\n    defaulting to RangeIndex(0, 1, 2, ..., n). If data contains column labels,\n    will perform column selection instead.\ndtype : dtype, default None\n    Data type to force. Only a single dtype is allowed. If None, infer.\ncopy : bool or None, default None\n    Copy data from inputs.\n    For dict data, the default of None behaves like ``copy=True``.  For DataFrame\n    or 2d ndarray input, the default of None behaves like ``copy=False``.\n    If data is a dict containing one or more Series (possibly of different dtypes),\n    ``copy=False`` will ensure that these inputs are not copied.\n\n    .. versionchanged:: 1.3.0\n\nSee Also\n--------\nDataFrame.from_records : Constructor from tuples, also record arrays.\nDataFrame.from_dict : From dicts of Series, arrays, or dicts.\nread_csv : Read a comma-separated values (csv) file into DataFrame.\nread_table : Read general delimited file into DataFrame.\nread_clipboard : Read text from clipboard into DataFrame.\n\nNotes\n-----\nPlease reference the :ref:`User Guide <basics.dataframe>` for more information.\n\nExamples\n--------\nConstructing DataFrame from a dictionary.\n\n>>> d = {'col1': [1, 2], 'col2': [3, 4]}\n>>> df = pd.DataFrame(data=d)\n>>> df\n   col1  col2\n0     1     3\n1     2     4\n\nNotice that the inferred dtype is int64.\n\n>>> df.dtypes\ncol1    int64\ncol2    int64\ndtype: object\n\nTo enforce a single dtype:\n\n>>> df = pd.DataFrame(data=d, dtype=np.int8)\n>>> df.dtypes\ncol1    int8\ncol2    int8\ndtype: object\n\nConstructing DataFrame from a dictionary including Series:\n\n>>> d = {'col1': [0, 1, 2, 3], 'col2': pd.Series([2, 3], index=[2, 3])}\n>>> pd.DataFrame(data=d, index=[0, 1, 2, 3])\n   col1  col2\n0     0   NaN\n1     1   NaN\n2     2   2.0\n3     3   3.0\n\nConstructing DataFrame from numpy ndarray:\n\n>>> df2 = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),\n...                    columns=['a', 'b', 'c'])\n>>> df2\n   a  b  c\n0  1  2  3\n1  4  5  6\n2  7  8  9\n\nConstructing DataFrame from a numpy ndarray that has labeled columns:\n\n>>> data = np.array([(1, 2, 3), (4, 5, 6), (7, 8, 9)],\n...                 dtype=[(\"a\", \"i4\"), (\"b\", \"i4\"), (\"c\", \"i4\")])\n>>> df3 = pd.DataFrame(data, columns=['c', 'a'])\n...\n>>> df3\n   c  a\n0  3  1\n1  6  4\n2  9  7\n\nConstructing DataFrame from dataclass:\n\n>>> from dataclasses import make_dataclass\n>>> Point = make_dataclass(\"Point\", [(\"x\", int), (\"y\", int)])\n>>> pd.DataFrame([Point(0, 0), Point(0, 3), Point(2, 3)])\n   x  y\n0  0  0\n1  0  3\n2  2  3\n\nConstructing DataFrame from Series/DataFrame:\n\n>>> ser = pd.Series([1, 2, 3], index=[\"a\", \"b\", \"c\"])\n>>> df = pd.DataFrame(data=ser, index=[\"a\", \"c\"])\n>>> df\n   0\na  1\nc  3\n\n>>> df1 = pd.DataFrame([1, 2, 3], index=[\"a\", \"b\", \"c\"], columns=[\"x\"])\n>>> df2 = pd.DataFrame(data=df1, index=[\"a\", \"c\"])\n>>> df2\n   x\na  1\nc  3", "parameters": {"type": "object", "properties": {"data": {"type": "NoneType", "default": null}, "index": {"type": ["axes", "null"], "default": null}, "columns": {"type": ["axes", "null"], "default": null}, "dtype": {"type": ["dtype", "null"], "default": null}, "copy": {"type": ["bool", "null"], "default": null}}}}}
{"task_id": "BigCodeBench/486", "data": {"name": "pandas.DataFrame.loc", "type": "constant", "signature": null, "description": "Access a group of rows and columns by label(s) or a boolean array.\n\n``.loc[]`` is primarily label based, but may also be used with a\nboolean array.\n\nAllowed inputs are:\n\n- A single label, e.g. ``5`` or ``'a'``, (note that ``5`` is\n  interpreted as a *label* of the index, and **never** as an\n  integer position along the index).\n- A list or array of labels, e.g. ``['a', 'b', 'c']``.\n- A slice object with labels, e.g. ``'a':'f'``.\n\n  .. warning:: Note that contrary to usual python slices, **both** the\n      start and the stop are included\n\n- A boolean array of the same length as the axis being sliced,\n  e.g. ``[True, False, True]``.\n- An alignable boolean Series. The index of the key will be aligned before\n  masking.\n- An alignable Index. The Index of the returned selection will be the input.\n- A ``callable`` function with one argument (the calling Series or\n  DataFrame) and that returns valid output for indexing (one of the above)\n\nSee more at :ref:`Selection by Label <indexing.label>`.\n\nRaises\n------\nKeyError\n    If any items are not found.\nIndexingError\n    If an indexed key is passed and its index is unalignable to the frame index.\n\nSee Also\n--------\nDataFrame.at : Access a single value for a row/column label pair.\nDataFrame.iloc : Access group of rows and columns by integer position(s).\nDataFrame.xs : Returns a cross-section (row(s) or column(s)) from the\n               Series/DataFrame.\nSeries.loc : Access group of values using labels.\n\nExamples\n--------\n**Getting values**\n\n>>> df = pd.DataFrame([[1, 2], [4, 5], [7, 8]],\n...                   index=['cobra', 'viper', 'sidewinder'],\n...                   columns=['max_speed', 'shield'])\n>>> df\n            max_speed  shield\ncobra               1       2\nviper               4       5\nsidewinder          7       8\n\nSingle label. Note this returns the row as a Series.\n\n>>> df.loc['viper']\nmax_speed    4\nshield       5\nName: viper, dtype: int64\n\nList of labels. Note using ``[[]]`` returns a DataFrame.\n\n>>> df.loc[['viper', 'sidewinder']]\n            max_speed  shield\nviper               4       5\nsidewinder          7       8\n\nSingle label for row and column\n\n>>> df.loc['cobra', 'shield']\n2\n\nSlice with labels for row and single label for column. As mentioned\nabove, note that both the start and stop of the slice are included.\n\n>>> df.loc['cobra':'viper', 'max_speed']\ncobra    1\nviper    4\nName: max_speed, dtype: int64\n\nBoolean list with the same length as the row axis\n\n>>> df.loc[[False, False, True]]\n            max_speed  shield\nsidewinder          7       8\n\nAlignable boolean Series:\n\n>>> df.loc[pd.Series([False, True, False],\n...                  index=['viper', 'sidewinder', 'cobra'])]\n                     max_speed  shield\nsidewinder          7       8\n\nIndex (same behavior as ``df.reindex``)\n\n>>> df.loc[pd.Index([\"cobra\", \"viper\"], name=\"foo\")]\n       max_speed  shield\nfoo\ncobra          1       2\nviper          4       5\n\nConditional that returns a boolean Series\n\n>>> df.loc[df['shield'] > 6]\n            max_speed  shield\nsidewinder          7       8\n\nConditional that returns a boolean Series with column labels specified\n\n>>> df.loc[df['shield'] > 6, ['max_speed']]\n            max_speed\nsidewinder          7\n\nMultiple conditional using ``&`` that returns a boolean Series\n\n>>> df.loc[(df['max_speed'] > 1) & (df['shield'] < 8)]\n            max_speed  shield\nviper          4       5\n\nMultiple conditional using ``|`` that returns a boolean Series\n\n>>> df.loc[(df['max_speed'] > 4) | (df['shield'] < 5)]\n            max_speed  shield\ncobra               1       2\nsidewinder          7       8\n\nPlease ensure that each condition is wrapped in parentheses ``()``.\nSee the :ref:`user guide<indexing.boolean>`\nfor more details and explanations of Boolean indexing.\n\n.. note::\n    If you find yourself using 3 or more conditionals in ``.loc[]``,\n    consider using :ref:`advanced indexing<advanced.advanced_hierarchical>`.\n\n    See below for using ``.loc[]`` on MultiIndex DataFrames.\n\nCallable that returns a boolean Series\n\n>>> df.loc[lambda df: df['shield'] == 8]\n            max_speed  shield\nsidewinder          7       8\n\n**Setting values**\n\nSet value for all items matching the list of labels\n\n>>> df.loc[['viper', 'sidewinder'], ['shield']] = 50\n>>> df\n            max_speed  shield\ncobra               1       2\nviper               4      50\nsidewinder          7      50\n\nSet value for an entire row\n\n>>> df.loc['cobra'] = 10\n>>> df\n            max_speed  shield\ncobra              10      10\nviper               4      50\nsidewinder          7      50\n\nSet value for an entire column\n\n>>> df.loc[:, 'max_speed'] = 30\n>>> df\n            max_speed  shield\ncobra              30      10\nviper              30      50\nsidewinder         30      50\n\nSet value for rows matching callable condition\n\n>>> df.loc[df['shield'] > 35] = 0\n>>> df\n            max_speed  shield\ncobra              30      10\nviper               0       0\nsidewinder          0       0\n\nAdd value matching location\n\n>>> df.loc[\"viper\", \"shield\"] += 5\n>>> df\n            max_speed  shield\ncobra              30      10\nviper               0       5\nsidewinder          0       0\n\nSetting using a ``Series`` or a ``DataFrame`` sets the values matching the\nindex labels, not the index positions.\n\n>>> shuffled_df = df.loc[[\"viper\", \"cobra\", \"sidewinder\"]]\n>>> df.loc[:] += shuffled_df\n>>> df\n            max_speed  shield\ncobra              60      20\nviper               0      10\nsidewinder          0       0\n\n**Getting values on a DataFrame with an index that has integer labels**\n\nAnother example using integers for the index\n\n>>> df = pd.DataFrame([[1, 2], [4, 5], [7, 8]],\n...                   index=[7, 8, 9], columns=['max_speed', 'shield'])\n>>> df\n   max_speed  shield\n7          1       2\n8          4       5\n9          7       8\n\nSlice with integer labels for rows. As mentioned above, note that both\nthe start and stop of the slice are included.\n\n>>> df.loc[7:9]\n   max_speed  shield\n7          1       2\n8          4       5\n9          7       8\n\n**Getting values with a MultiIndex**\n\nA number of examples using a DataFrame with a MultiIndex\n\n>>> tuples = [\n...     ('cobra', 'mark i'), ('cobra', 'mark ii'),\n...     ('sidewinder', 'mark i'), ('sidewinder', 'mark ii'),\n...     ('viper', 'mark ii'), ('viper', 'mark iii')\n... ]\n>>> index = pd.MultiIndex.from_tuples(tuples)\n>>> values = [[12, 2], [0, 4], [10, 20],\n...           [1, 4], [7, 1], [16, 36]]\n>>> df = pd.DataFrame(values, columns=['max_speed', 'shield'], index=index)\n>>> df\n                     max_speed  shield\ncobra      mark i           12       2\n           mark ii           0       4\nsidewinder mark i           10      20\n           mark ii           1       4\nviper      mark ii           7       1\n           mark iii         16      36\n\nSingle label. Note this returns a DataFrame with a single index.\n\n>>> df.loc['cobra']\n         max_speed  shield\nmark i          12       2\nmark ii          0       4\n\nSingle index tuple. Note this returns a Series.\n\n>>> df.loc[('cobra', 'mark ii')]\nmax_speed    0\nshield       4\nName: (cobra, mark ii), dtype: int64\n\nSingle label for row and column. Similar to passing in a tuple, this\nreturns a Series.\n\n>>> df.loc['cobra', 'mark i']\nmax_speed    12\nshield        2\nName: (cobra, mark i), dtype: int64\n\nSingle tuple. Note using ``[[]]`` returns a DataFrame.\n\n>>> df.loc[[('cobra', 'mark ii')]]\n               max_speed  shield\ncobra mark ii          0       4\n\nSingle tuple for the index with a single label for the column\n\n>>> df.loc[('cobra', 'mark i'), 'shield']\n2\n\nSlice from index tuple to single label\n\n>>> df.loc[('cobra', 'mark i'):'viper']\n                     max_speed  shield\ncobra      mark i           12       2\n           mark ii           0       4\nsidewinder mark i           10      20\n           mark ii           1       4\nviper      mark ii           7       1\n           mark iii         16      36\n\nSlice from index tuple to index tuple\n\n>>> df.loc[('cobra', 'mark i'):('viper', 'mark ii')]\n                    max_speed  shield\ncobra      mark i          12       2\n           mark ii          0       4\nsidewinder mark i          10      20\n           mark ii          1       4\nviper      mark ii          7       1\n\nPlease see the :ref:`user guide<advanced.advanced_hierarchical>`\nfor more details and explanations of advanced indexing.", "value": "<property object at 0x7fa4c5d0da80>", "parameters": {"type": "constant"}}}
{"task_id": "BigCodeBench/486", "data": {"name": "pandas.DataFrame.plot", "type": "class", "signature": "()", "description": "Make plots of Series or DataFrame.\n\nUses the backend specified by the\noption ``plotting.backend``. By default, matplotlib is used.\n\nParameters\n----------\ndata : Series or DataFrame\n    The object for which the method is called.\nx : label or position, default None\n    Only used if data is a DataFrame.\ny : label, position or list of label, positions, default None\n    Allows plotting of one column versus another. Only used if data is a\n    DataFrame.\nkind : str\n    The kind of plot to produce:\n\n    - 'line' : line plot (default)\n    - 'bar' : vertical bar plot\n    - 'barh' : horizontal bar plot\n    - 'hist' : histogram\n    - 'box' : boxplot\n    - 'kde' : Kernel Density Estimation plot\n    - 'density' : same as 'kde'\n    - 'area' : area plot\n    - 'pie' : pie plot\n    - 'scatter' : scatter plot (DataFrame only)\n    - 'hexbin' : hexbin plot (DataFrame only)\nax : matplotlib axes object, default None\n    An axes of the current figure.\nsubplots : bool or sequence of iterables, default False\n    Whether to group columns into subplots:\n\n    - ``False`` : No subplots will be used\n    - ``True`` : Make separate subplots for each column.\n    - sequence of iterables of column labels: Create a subplot for each\n      group of columns. For example `[('a', 'c'), ('b', 'd')]` will\n      create 2 subplots: one with columns 'a' and 'c', and one\n      with columns 'b' and 'd'. Remaining columns that aren't specified\n      will be plotted in additional subplots (one per column).\n\n      .. versionadded:: 1.5.0\n\nsharex : bool, default True if ax is None else False\n    In case ``subplots=True``, share x axis and set some x axis labels\n    to invisible; defaults to True if ax is None otherwise False if\n    an ax is passed in; Be aware, that passing in both an ax and\n    ``sharex=True`` will alter all x axis labels for all axis in a figure.\nsharey : bool, default False\n    In case ``subplots=True``, share y axis and set some y axis labels to invisible.\nlayout : tuple, optional\n    (rows, columns) for the layout of subplots.\nfigsize : a tuple (width, height) in inches\n    Size of a figure object.\nuse_index : bool, default True\n    Use index as ticks for x axis.\ntitle : str or list\n    Title to use for the plot. If a string is passed, print the string\n    at the top of the figure. If a list is passed and `subplots` is\n    True, print each item in the list above the corresponding subplot.\ngrid : bool, default None (matlab style default)\n    Axis grid lines.\nlegend : bool or {'reverse'}\n    Place legend on axis subplots.\nstyle : list or dict\n    The matplotlib line style per column.\nlogx : bool or 'sym', default False\n    Use log scaling or symlog scaling on x axis.\n\nlogy : bool or 'sym' default False\n    Use log scaling or symlog scaling on y axis.\n\nloglog : bool or 'sym', default False\n    Use log scaling or symlog scaling on both x and y axes.\n\nxticks : sequence\n    Values to use for the xticks.\nyticks : sequence\n    Values to use for the yticks.\nxlim : 2-tuple/list\n    Set the x limits of the current axes.\nylim : 2-tuple/list\n    Set the y limits of the current axes.\nxlabel : label, optional\n    Name to use for the xlabel on x-axis. Default uses index name as xlabel, or the\n    x-column name for planar plots.\n\n    .. versionchanged:: 2.0.0\n\n        Now applicable to histograms.\n\nylabel : label, optional\n    Name to use for the ylabel on y-axis. Default will show no ylabel, or the\n    y-column name for planar plots.\n\n    .. versionchanged:: 2.0.0\n\n        Now applicable to histograms.\n\nrot : float, default None\n    Rotation for ticks (xticks for vertical, yticks for horizontal\n    plots).\nfontsize : float, default None\n    Font size for xticks and yticks.\ncolormap : str or matplotlib colormap object, default None\n    Colormap to select colors from. If string, load colormap with that\n    name from matplotlib.\ncolorbar : bool, optional\n    If True, plot colorbar (only relevant for 'scatter' and 'hexbin'\n    plots).\nposition : float\n    Specify relative alignments for bar plot layout.\n    From 0 (left/bottom-end) to 1 (right/top-end). Default is 0.5\n    (center).\ntable : bool, Series or DataFrame, default False\n    If True, draw a table using the data in the DataFrame and the data\n    will be transposed to meet matplotlib's default layout.\n    If a Series or DataFrame is passed, use passed data to draw a\n    table.\nyerr : DataFrame, Series, array-like, dict and str\n    See :ref:`Plotting with Error Bars <visualization.errorbars>` for\n    detail.\nxerr : DataFrame, Series, array-like, dict and str\n    Equivalent to yerr.\nstacked : bool, default False in line and bar plots, and True in area plot\n    If True, create stacked plot.\nsecondary_y : bool or sequence, default False\n    Whether to plot on the secondary y-axis if a list/tuple, which\n    columns to plot on secondary y-axis.\nmark_right : bool, default True\n    When using a secondary_y axis, automatically mark the column\n    labels with \"(right)\" in the legend.\ninclude_bool : bool, default is False\n    If True, boolean values can be plotted.\nbackend : str, default None\n    Backend to use instead of the backend specified in the option\n    ``plotting.backend``. For instance, 'matplotlib'. Alternatively, to\n    specify the ``plotting.backend`` for the whole session, set\n    ``pd.options.plotting.backend``.\n**kwargs\n    Options to pass to matplotlib plotting method.\n\nReturns\n-------\n:class:`matplotlib.axes.Axes` or numpy.ndarray of them\n    If the backend is not the default matplotlib one, the return value\n    will be the object returned by the backend.\n\nNotes\n-----\n- See matplotlib documentation online for more on this subject\n- If `kind` = 'bar' or 'barh', you can specify relative alignments\n  for bar plot layout by `position` keyword.\n  From 0 (left/bottom-end) to 1 (right/top-end). Default is 0.5\n  (center)\n\nExamples\n--------\nFor Series:\n\n.. plot::\n    :context: close-figs\n\n    >>> ser = pd.Series([1, 2, 3, 3])\n    >>> plot = ser.plot(kind='hist', title=\"My plot\")\n\nFor DataFrame:\n\n.. plot::\n    :context: close-figs\n\n    >>> df = pd.DataFrame({'length': [1.5, 0.5, 1.2, 0.9, 3],\n    ...                   'width': [0.7, 0.2, 0.15, 0.2, 1.1]},\n    ...                   index=['pig', 'rabbit', 'duck', 'chicken', 'horse'])\n    >>> plot = df.plot(title=\"DataFrame Plot\")\n\nFor SeriesGroupBy:\n\n.. plot::\n    :context: close-figs\n\n    >>> lst = [-1, -2, -3, 1, 2, 3]\n    >>> ser = pd.Series([1, 2, 2, 4, 6, 6], index=lst)\n    >>> plot = ser.groupby(lambda x: x > 0).plot(title=\"SeriesGroupBy Plot\")\n\nFor DataFrameGroupBy:\n\n.. plot::\n    :context: close-figs\n\n    >>> df = pd.DataFrame({\"col1\" : [1, 2, 3, 4],\n    ...                   \"col2\" : [\"A\", \"B\", \"A\", \"B\"]})\n    >>> plot = df.groupby(\"col2\").plot(kind=\"bar\", title=\"DataFrameGroupBy Plot\")", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/486", "data": {"name": "df.plot(x='Time', y='Value').set_ylabel('Value')"}}
{"task_id": "BigCodeBench/486", "data": {"name": "numpy.random.seed", "type": "callable", "signature": "(*args, **kwargs)", "description": "seed(self, seed=None)\n\nReseed a legacy MT19937 BitGenerator\n\nNotes\n-----\nThis is a convenience, legacy function.\n\nThe best practice is to **not** reseed a BitGenerator, rather to\nrecreate a new one. This method is here for legacy reasons.\nThis example demonstrates best practice.\n\n>>> from numpy.random import MT19937\n>>> from numpy.random import RandomState, SeedSequence\n>>> rs = RandomState(MT19937(SeedSequence(123456789)))\n# Later, you want to restart the stream\n>>> rs = RandomState(MT19937(SeedSequence(987654321)))", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/486", "data": {"name": "numpy.arange", "type": "callable", "signature": "(*args, **kwargs)", "description": "arange([start,] stop[, step,], dtype=None, *, like=None)\n\nReturn evenly spaced values within a given interval.\n\n``arange`` can be called with a varying number of positional arguments:\n\n* ``arange(stop)``: Values are generated within the half-open interval\n  ``[0, stop)`` (in other words, the interval including `start` but\n  excluding `stop`).\n* ``arange(start, stop)``: Values are generated within the half-open\n  interval ``[start, stop)``.\n* ``arange(start, stop, step)`` Values are generated within the half-open\n  interval ``[start, stop)``, with spacing between values given by\n  ``step``.\n\nFor integer arguments the function is roughly equivalent to the Python\nbuilt-in :py:class:`range`, but returns an ndarray rather than a ``range``\ninstance.\n\nWhen using a non-integer step, such as 0.1, it is often better to use\n`numpy.linspace`.\n\nSee the Warning sections below for more information.\n\nParameters\n----------\nstart : integer or real, optional\n    Start of interval.  The interval includes this value.  The default\n    start value is 0.\nstop : integer or real\n    End of interval.  The interval does not include this value, except\n    in some cases where `step` is not an integer and floating point\n    round-off affects the length of `out`.\nstep : integer or real, optional\n    Spacing between values.  For any output `out`, this is the distance\n    between two adjacent values, ``out[i+1] - out[i]``.  The default\n    step size is 1.  If `step` is specified as a position argument,\n    `start` must also be given.\ndtype : dtype, optional\n    The type of the output array.  If `dtype` is not given, infer the data\n    type from the other input arguments.\nlike : array_like, optional\n    Reference object to allow the creation of arrays which are not\n    NumPy arrays. If an array-like passed in as ``like`` supports\n    the ``__array_function__`` protocol, the result will be defined\n    by it. In this case, it ensures the creation of an array object\n    compatible with that passed in via this argument.\n\n    .. versionadded:: 1.20.0\n\nReturns\n-------\narange : ndarray\n    Array of evenly spaced values.\n\n    For floating point arguments, the length of the result is\n    ``ceil((stop - start)/step)``.  Because of floating point overflow,\n    this rule may result in the last element of `out` being greater\n    than `stop`.\n\nWarnings\n--------\nThe length of the output might not be numerically stable.\n\nAnother stability issue is due to the internal implementation of\n`numpy.arange`.\nThe actual step value used to populate the array is\n``dtype(start + step) - dtype(start)`` and not `step`. Precision loss\ncan occur here, due to casting or due to using floating points when\n`start` is much larger than `step`. This can lead to unexpected\nbehaviour. For example::\n\n  >>> np.arange(0, 5, 0.5, dtype=int)\n  array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n  >>> np.arange(-3, 3, 0.5, dtype=int)\n  array([-3, -2, -1,  0,  1,  2,  3,  4,  5,  6,  7,  8])\n\nIn such cases, the use of `numpy.linspace` should be preferred.\n\nThe built-in :py:class:`range` generates :std:doc:`Python built-in integers\nthat have arbitrary size <c-api/long>`, while `numpy.arange` produces\n`numpy.int32` or `numpy.int64` numbers. This may result in incorrect\nresults for large integer values::\n\n  >>> power = 40\n  >>> modulo = 10000\n  >>> x1 = [(n ** power) % modulo for n in range(8)]\n  >>> x2 = [(n ** power) % modulo for n in np.arange(8)]\n  >>> print(x1)\n  [0, 1, 7776, 8801, 6176, 625, 6576, 4001]  # correct\n  >>> print(x2)\n  [0, 1, 7776, 7185, 0, 5969, 4816, 3361]  # incorrect\n\nSee Also\n--------\nnumpy.linspace : Evenly spaced numbers with careful handling of endpoints.\nnumpy.ogrid: Arrays of evenly spaced numbers in N-dimensions.\nnumpy.mgrid: Grid-shaped arrays of evenly spaced numbers in N-dimensions.\n\nExamples\n--------\n>>> np.arange(3)\narray([0, 1, 2])\n>>> np.arange(3.0)\narray([ 0.,  1.,  2.])\n>>> np.arange(3,7)\narray([3, 4, 5, 6])\n>>> np.arange(3,7,2)\narray([3, 5])", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/486", "data": {"name": "numpy.random.normal", "type": "callable", "signature": "(**kwargs)", "description": "normal(loc=0.0, scale=1.0, size=None)\n\nDraw random samples from a normal (Gaussian) distribution.\n\nThe probability density function of the normal distribution, first\nderived by De Moivre and 200 years later by both Gauss and Laplace\nindependently [2]_, is often called the bell curve because of\nits characteristic shape (see the example below).\n\nThe normal distributions occurs often in nature.  For example, it\ndescribes the commonly occurring distribution of samples influenced\nby a large number of tiny, random disturbances, each with its own\nunique distribution [2]_.\n\n.. note::\n    New code should use the ``normal`` method of a ``default_rng()``\n    instance instead; please see the :ref:`random-quick-start`.\n\nParameters\n----------\nloc : float or array_like of floats\n    Mean (\"centre\") of the distribution.\nscale : float or array_like of floats\n    Standard deviation (spread or \"width\") of the distribution. Must be\n    non-negative.\nsize : int or tuple of ints, optional\n    Output shape.  If the given shape is, e.g., ``(m, n, k)``, then\n    ``m * n * k`` samples are drawn.  If size is ``None`` (default),\n    a single value is returned if ``loc`` and ``scale`` are both scalars.\n    Otherwise, ``np.broadcast(loc, scale).size`` samples are drawn.\n\nReturns\n-------\nout : ndarray or scalar\n    Drawn samples from the parameterized normal distribution.\n\nSee Also\n--------\nscipy.stats.norm : probability density function, distribution or\n    cumulative density function, etc.\nrandom.Generator.normal: which should be used for new code.\n\nNotes\n-----\nThe probability density for the Gaussian distribution is\n\n.. math:: p(x) = \\frac{1}{\\sqrt{ 2 \\pi \\sigma^2 }}\n                 e^{ - \\frac{ (x - \\mu)^2 } {2 \\sigma^2} },\n\nwhere :math:`\\mu` is the mean and :math:`\\sigma` the standard\ndeviation. The square of the standard deviation, :math:`\\sigma^2`,\nis called the variance.\n\nThe function has its peak at the mean, and its \"spread\" increases with\nthe standard deviation (the function reaches 0.607 times its maximum at\n:math:`x + \\sigma` and :math:`x - \\sigma` [2]_).  This implies that\nnormal is more likely to return samples lying close to the mean, rather\nthan those far away.\n\nReferences\n----------\n.. [1] Wikipedia, \"Normal distribution\",\n       https://en.wikipedia.org/wiki/Normal_distribution\n.. [2] P. R. Peebles Jr., \"Central Limit Theorem\" in \"Probability,\n       Random Variables and Random Signal Principles\", 4th ed., 2001,\n       pp. 51, 51, 125.\n\nExamples\n--------\nDraw samples from the distribution:\n\n>>> mu, sigma = 0, 0.1 # mean and standard deviation\n>>> s = np.random.normal(mu, sigma, 1000)\n\nVerify the mean and the variance:\n\n>>> abs(mu - np.mean(s))\n0.0  # may vary\n\n>>> abs(sigma - np.std(s, ddof=1))\n0.1  # may vary\n\nDisplay the histogram of the samples, along with\nthe probability density function:\n\n>>> import matplotlib.pyplot as plt\n>>> count, bins, ignored = plt.hist(s, 30, density=True)\n>>> plt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) *\n...                np.exp( - (bins - mu)**2 / (2 * sigma**2) ),\n...          linewidth=2, color='r')\n>>> plt.show()\n\nTwo-by-four array of samples from N(3, 6.25):\n\n>>> np.random.normal(3, 2.5, size=(2, 4))\narray([[-4.49401501,  4.00950034, -1.81814867,  7.29718677],   # random\n       [ 0.39924804,  4.68456316,  4.99394529,  4.84057254]])  # random", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/492", "data": {"name": "datetime.datetime", "type": "class", "signature": "(self, /, *args, **kwargs)", "description": "datetime(year, month, day[, hour[, minute[, second[, microsecond[,tzinfo]]]]])\n\nThe year, month and day arguments are required. tzinfo may be None, or an\ninstance of a tzinfo subclass. The remaining arguments may be ints.", "parameters": {"type": "object", "properties": {"/": {}}}}}
{"task_id": "BigCodeBench/492", "data": {"name": "datetime.datetime.now", "type": "callable", "signature": "()", "description": "Returns new datetime object representing current time local to tz.\n\n  tz\n    Timezone object.\n\nIf no tz is specified, uses local timezone.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/492", "data": {"name": "datetime.datetime.fromtimestamp", "type": "callable", "signature": "(*args, **kwargs)", "description": "timestamp[, tz] -> tz's local time from POSIX timestamp.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/492", "data": {"name": "pandas.date_range", "type": "callable", "signature": "(start=None, end=None, freq=None, **kwargs) -> 'DatetimeIndex)", "description": "Return a fixed frequency DatetimeIndex.\n\nReturns the range of equally spaced time points (where the difference between any\ntwo adjacent points is specified by the given frequency) such that they all\nsatisfy `start <[=] x <[=] end`, where the first one and the last one are, resp.,\nthe first and last time points in that range that fall on the boundary of ``freq``\n(if given as a frequency string) or that are valid for ``freq`` (if given as a\n:class:`pandas.tseries.offsets.DateOffset`). (If exactly one of ``start``,\n``end``, or ``freq`` is *not* specified, this missing parameter can be computed\ngiven ``periods``, the number of timesteps in the range. See the note below.)\n\nParameters\n----------\nstart : str or datetime-like, optional\n    Left bound for generating dates.\nend : str or datetime-like, optional\n    Right bound for generating dates.\nperiods : int, optional\n    Number of periods to generate.\nfreq : str, Timedelta, datetime.timedelta, or DateOffset, default 'D'\n    Frequency strings can have multiples, e.g. '5h'. See\n    :ref:`here <timeseries.offset_aliases>` for a list of\n    frequency aliases.\ntz : str or tzinfo, optional\n    Time zone name for returning localized DatetimeIndex, for example\n    'Asia/Hong_Kong'. By default, the resulting DatetimeIndex is\n    timezone-naive unless timezone-aware datetime-likes are passed.\nnormalize : bool, default False\n    Normalize start/end dates to midnight before generating date range.\nname : str, default None\n    Name of the resulting DatetimeIndex.\ninclusive : {\"both\", \"neither\", \"left\", \"right\"}, default \"both\"\n    Include boundaries; Whether to set each bound as closed or open.\n\n    .. versionadded:: 1.4.0\nunit : str, default None\n    Specify the desired resolution of the result.\n\n    .. versionadded:: 2.0.0\n**kwargs\n    For compatibility. Has no effect on the result.\n\nReturns\n-------\nDatetimeIndex\n\nSee Also\n--------\nDatetimeIndex : An immutable container for datetimes.\ntimedelta_range : Return a fixed frequency TimedeltaIndex.\nperiod_range : Return a fixed frequency PeriodIndex.\ninterval_range : Return a fixed frequency IntervalIndex.\n\nNotes\n-----\nOf the four parameters ``start``, ``end``, ``periods``, and ``freq``,\nexactly three must be specified. If ``freq`` is omitted, the resulting\n``DatetimeIndex`` will have ``periods`` linearly spaced elements between\n``start`` and ``end`` (closed on both sides).\n\nTo learn more about the frequency strings, please see `this link\n<https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases>`__.\n\nExamples\n--------\n**Specifying the values**\n\nThe next four examples generate the same `DatetimeIndex`, but vary\nthe combination of `start`, `end` and `periods`.\n\nSpecify `start` and `end`, with the default daily frequency.\n\n>>> pd.date_range(start='1/1/2018', end='1/08/2018')\nDatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03', '2018-01-04',\n               '2018-01-05', '2018-01-06', '2018-01-07', '2018-01-08'],\n              dtype='datetime64[ns]', freq='D')\n\nSpecify timezone-aware `start` and `end`, with the default daily frequency.\n\n>>> pd.date_range(\n...     start=pd.to_datetime(\"1/1/2018\").tz_localize(\"Europe/Berlin\"),\n...     end=pd.to_datetime(\"1/08/2018\").tz_localize(\"Europe/Berlin\"),\n... )\nDatetimeIndex(['2018-01-01 00:00:00+01:00', '2018-01-02 00:00:00+01:00',\n               '2018-01-03 00:00:00+01:00', '2018-01-04 00:00:00+01:00',\n               '2018-01-05 00:00:00+01:00', '2018-01-06 00:00:00+01:00',\n               '2018-01-07 00:00:00+01:00', '2018-01-08 00:00:00+01:00'],\n              dtype='datetime64[ns, Europe/Berlin]', freq='D')\n\nSpecify `start` and `periods`, the number of periods (days).\n\n>>> pd.date_range(start='1/1/2018', periods=8)\nDatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03', '2018-01-04',\n               '2018-01-05', '2018-01-06', '2018-01-07', '2018-01-08'],\n              dtype='datetime64[ns]', freq='D')\n\nSpecify `end` and `periods`, the number of periods (days).\n\n>>> pd.date_range(end='1/1/2018', periods=8)\nDatetimeIndex(['2017-12-25', '2017-12-26', '2017-12-27', '2017-12-28',\n               '2017-12-29', '2017-12-30', '2017-12-31', '2018-01-01'],\n              dtype='datetime64[ns]', freq='D')\n\nSpecify `start`, `end`, and `periods`; the frequency is generated\nautomatically (linearly spaced).\n\n>>> pd.date_range(start='2018-04-24', end='2018-04-27', periods=3)\nDatetimeIndex(['2018-04-24 00:00:00', '2018-04-25 12:00:00',\n               '2018-04-27 00:00:00'],\n              dtype='datetime64[ns]', freq=None)\n\n**Other Parameters**\n\nChanged the `freq` (frequency) to ``'ME'`` (month end frequency).\n\n>>> pd.date_range(start='1/1/2018', periods=5, freq='ME')\nDatetimeIndex(['2018-01-31', '2018-02-28', '2018-03-31', '2018-04-30',\n               '2018-05-31'],\n              dtype='datetime64[ns]', freq='ME')\n\nMultiples are allowed\n\n>>> pd.date_range(start='1/1/2018', periods=5, freq='3ME')\nDatetimeIndex(['2018-01-31', '2018-04-30', '2018-07-31', '2018-10-31',\n               '2019-01-31'],\n              dtype='datetime64[ns]', freq='3ME')\n\n`freq` can also be specified as an Offset object.\n\n>>> pd.date_range(start='1/1/2018', periods=5, freq=pd.offsets.MonthEnd(3))\nDatetimeIndex(['2018-01-31', '2018-04-30', '2018-07-31', '2018-10-31',\n               '2019-01-31'],\n              dtype='datetime64[ns]', freq='3ME')\n\nSpecify `tz` to set the timezone.\n\n>>> pd.date_range(start='1/1/2018', periods=5, tz='Asia/Tokyo')\nDatetimeIndex(['2018-01-01 00:00:00+09:00', '2018-01-02 00:00:00+09:00',\n               '2018-01-03 00:00:00+09:00', '2018-01-04 00:00:00+09:00',\n               '2018-01-05 00:00:00+09:00'],\n              dtype='datetime64[ns, Asia/Tokyo]', freq='D')\n\n`inclusive` controls whether to include `start` and `end` that are on the\nboundary. The default, \"both\", includes boundary points on either end.\n\n>>> pd.date_range(start='2017-01-01', end='2017-01-04', inclusive=\"both\")\nDatetimeIndex(['2017-01-01', '2017-01-02', '2017-01-03', '2017-01-04'],\n              dtype='datetime64[ns]', freq='D')\n\nUse ``inclusive='left'`` to exclude `end` if it falls on the boundary.\n\n>>> pd.date_range(start='2017-01-01', end='2017-01-04', inclusive='left')\nDatetimeIndex(['2017-01-01', '2017-01-02', '2017-01-03'],\n              dtype='datetime64[ns]', freq='D')\n\nUse ``inclusive='right'`` to exclude `start` if it falls on the boundary, and\nsimilarly ``inclusive='neither'`` will exclude both `start` and `end`.\n\n>>> pd.date_range(start='2017-01-01', end='2017-01-04', inclusive='right')\nDatetimeIndex(['2017-01-02', '2017-01-03', '2017-01-04'],\n              dtype='datetime64[ns]', freq='D')\n\n**Specify a unit**\n\n>>> pd.date_range(start=\"2017-01-01\", periods=10, freq=\"100YS\", unit=\"s\")\nDatetimeIndex(['2017-01-01', '2117-01-01', '2217-01-01', '2317-01-01',\n               '2417-01-01', '2517-01-01', '2617-01-01', '2717-01-01',\n               '2817-01-01', '2917-01-01'],\n              dtype='datetime64[s]', freq='100YS-JAN')", "parameters": {"type": "object", "properties": {"start": {"type": "NoneType", "default": null}, "end": {"type": "NoneType", "default": null}, "freq": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/492", "data": {"name": "random.randint", "type": "callable", "signature": "(a, b)", "description": "Return random integer in range [a, b], including both end points.\n        ", "parameters": {"type": "object", "properties": {"a": {}, "b": {}}}}}
{"task_id": "BigCodeBench/492", "data": {"name": "random.seed", "type": "callable", "signature": "(a=None)", "description": "Initialize internal state from a seed.\n\nThe only supported seed types are None, int, float,\nstr, bytes, and bytearray.\n\nNone or no argument seeds from current time or from an operating\nsystem specific randomness source if available.\n\nIf *a* is an int, all bits are used.\n\nFor version 2 (the default), all of the bits are used if *a* is a str,\nbytes, or bytearray.  For version 1 (provided for reproducing random\nsequences from older versions of Python), the algorithm for str and\nbytes generates a narrower range of seeds.", "parameters": {"type": "object", "properties": {"a": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/492", "data": {"name": "sales_data.append([product, date, sales])"}}
{"task_id": "BigCodeBench/501", "data": {"name": "book.add_sheet(sheet_name).write(0, col_index, col)"}}
{"task_id": "BigCodeBench/501", "data": {"name": "book.add_sheet(sheet_name).write(row_index + 1, col_index, row[col])"}}
{"task_id": "BigCodeBench/501", "data": {"name": "os.path.abspath", "type": "callable", "signature": "(path)", "description": "Return an absolute path.", "parameters": {"type": "object", "properties": {"path": {}}}}}
{"task_id": "BigCodeBench/501", "data": {"name": "pandas.read_json", "type": "callable", "signature": "(path_or_buf: 'FilePath | ReadBuffer[str] | ReadBuffer[bytes]')", "description": "Convert a JSON string to pandas object.\n\nParameters\n----------\npath_or_buf : a valid JSON str, path object or file-like object\n    Any valid string path is acceptable. The string could be a URL. Valid\n    URL schemes include http, ftp, s3, and file. For file URLs, a host is\n    expected. A local file could be:\n    ``file://localhost/path/to/table.json``.\n\n    If you want to pass in a path object, pandas accepts any\n    ``os.PathLike``.\n\n    By file-like object, we refer to objects with a ``read()`` method,\n    such as a file handle (e.g. via builtin ``open`` function)\n    or ``StringIO``.\n\n    .. deprecated:: 2.1.0\n        Passing json literal strings is deprecated.\n\norient : str, optional\n    Indication of expected JSON string format.\n    Compatible JSON strings can be produced by ``to_json()`` with a\n    corresponding orient value.\n    The set of possible orients is:\n\n    - ``'split'`` : dict like\n      ``{index -> [index], columns -> [columns], data -> [values]}``\n    - ``'records'`` : list like\n      ``[{column -> value}, ... , {column -> value}]``\n    - ``'index'`` : dict like ``{index -> {column -> value}}``\n    - ``'columns'`` : dict like ``{column -> {index -> value}}``\n    - ``'values'`` : just the values array\n    - ``'table'`` : dict like ``{'schema': {schema}, 'data': {data}}``\n\n    The allowed and default values depend on the value\n    of the `typ` parameter.\n\n    * when ``typ == 'series'``,\n\n      - allowed orients are ``{'split','records','index'}``\n      - default is ``'index'``\n      - The Series index must be unique for orient ``'index'``.\n\n    * when ``typ == 'frame'``,\n\n      - allowed orients are ``{'split','records','index',\n        'columns','values', 'table'}``\n      - default is ``'columns'``\n      - The DataFrame index must be unique for orients ``'index'`` and\n        ``'columns'``.\n      - The DataFrame columns must be unique for orients ``'index'``,\n        ``'columns'``, and ``'records'``.\n\ntyp : {'frame', 'series'}, default 'frame'\n    The type of object to recover.\n\ndtype : bool or dict, default None\n    If True, infer dtypes; if a dict of column to dtype, then use those;\n    if False, then don't infer dtypes at all, applies only to the data.\n\n    For all ``orient`` values except ``'table'``, default is True.\n\nconvert_axes : bool, default None\n    Try to convert the axes to the proper dtypes.\n\n    For all ``orient`` values except ``'table'``, default is True.\n\nconvert_dates : bool or list of str, default True\n    If True then default datelike columns may be converted (depending on\n    keep_default_dates).\n    If False, no dates will be converted.\n    If a list of column names, then those columns will be converted and\n    default datelike columns may also be converted (depending on\n    keep_default_dates).\n\nkeep_default_dates : bool, default True\n    If parsing dates (convert_dates is not False), then try to parse the\n    default datelike columns.\n    A column label is datelike if\n\n    * it ends with ``'_at'``,\n\n    * it ends with ``'_time'``,\n\n    * it begins with ``'timestamp'``,\n\n    * it is ``'modified'``, or\n\n    * it is ``'date'``.\n\nprecise_float : bool, default False\n    Set to enable usage of higher precision (strtod) function when\n    decoding string to double values. Default (False) is to use fast but\n    less precise builtin functionality.\n\ndate_unit : str, default None\n    The timestamp unit to detect if converting dates. The default behaviour\n    is to try and detect the correct precision, but if this is not desired\n    then pass one of 's', 'ms', 'us' or 'ns' to force parsing only seconds,\n    milliseconds, microseconds or nanoseconds respectively.\n\nencoding : str, default is 'utf-8'\n    The encoding to use to decode py3 bytes.\n\nencoding_errors : str, optional, default \"strict\"\n    How encoding errors are treated. `List of possible values\n    <https://docs.python.org/3/library/codecs.html#error-handlers>`_ .\n\n    .. versionadded:: 1.3.0\n\nlines : bool, default False\n    Read the file as a json object per line.\n\nchunksize : int, optional\n    Return JsonReader object for iteration.\n    See the `line-delimited json docs\n    <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#line-delimited-json>`_\n    for more information on ``chunksize``.\n    This can only be passed if `lines=True`.\n    If this is None, the file will be read into memory all at once.\ncompression : str or dict, default 'infer'\n    For on-the-fly decompression of on-disk data. If 'infer' and 'path_or_buf' is\n    path-like, then detect compression from the following extensions: '.gz',\n    '.bz2', '.zip', '.xz', '.zst', '.tar', '.tar.gz', '.tar.xz' or '.tar.bz2'\n    (otherwise no compression).\n    If using 'zip' or 'tar', the ZIP file must contain only one data file to be read in.\n    Set to ``None`` for no decompression.\n    Can also be a dict with key ``'method'`` set\n    to one of {``'zip'``, ``'gzip'``, ``'bz2'``, ``'zstd'``, ``'xz'``, ``'tar'``} and\n    other key-value pairs are forwarded to\n    ``zipfile.ZipFile``, ``gzip.GzipFile``,\n    ``bz2.BZ2File``, ``zstandard.ZstdDecompressor``, ``lzma.LZMAFile`` or\n    ``tarfile.TarFile``, respectively.\n    As an example, the following could be passed for Zstandard decompression using a\n    custom compression dictionary:\n    ``compression={'method': 'zstd', 'dict_data': my_compression_dict}``.\n\n    .. versionadded:: 1.5.0\n        Added support for `.tar` files.\n\n    .. versionchanged:: 1.4.0 Zstandard support.\n\nnrows : int, optional\n    The number of lines from the line-delimited jsonfile that has to be read.\n    This can only be passed if `lines=True`.\n    If this is None, all the rows will be returned.\n\nstorage_options : dict, optional\n    Extra options that make sense for a particular storage connection, e.g.\n    host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n    are forwarded to ``urllib.request.Request`` as header options. For other\n    URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n    forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n    details, and for more examples on storage options refer `here\n    <https://pandas.pydata.org/docs/user_guide/io.html?\n    highlight=storage_options#reading-writing-remote-files>`_.\n\ndtype_backend : {'numpy_nullable', 'pyarrow'}, default 'numpy_nullable'\n    Back-end data type applied to the resultant :class:`DataFrame`\n    (still experimental). Behaviour is as follows:\n\n    * ``\"numpy_nullable\"``: returns nullable-dtype-backed :class:`DataFrame`\n      (default).\n    * ``\"pyarrow\"``: returns pyarrow-backed nullable :class:`ArrowDtype`\n      DataFrame.\n\n    .. versionadded:: 2.0\n\nengine : {\"ujson\", \"pyarrow\"}, default \"ujson\"\n    Parser engine to use. The ``\"pyarrow\"`` engine is only available when\n    ``lines=True``.\n\n    .. versionadded:: 2.0\n\nReturns\n-------\nSeries, DataFrame, or pandas.api.typing.JsonReader\n    A JsonReader is returned when ``chunksize`` is not ``0`` or ``None``.\n    Otherwise, the type returned depends on the value of ``typ``.\n\nSee Also\n--------\nDataFrame.to_json : Convert a DataFrame to a JSON string.\nSeries.to_json : Convert a Series to a JSON string.\njson_normalize : Normalize semi-structured JSON data into a flat table.\n\nNotes\n-----\nSpecific to ``orient='table'``, if a :class:`DataFrame` with a literal\n:class:`Index` name of `index` gets written with :func:`to_json`, the\nsubsequent read operation will incorrectly set the :class:`Index` name to\n``None``. This is because `index` is also used by :func:`DataFrame.to_json`\nto denote a missing :class:`Index` name, and the subsequent\n:func:`read_json` operation cannot distinguish between the two. The same\nlimitation is encountered with a :class:`MultiIndex` and any names\nbeginning with ``'level_'``.\n\nExamples\n--------\n>>> from io import StringIO\n>>> df = pd.DataFrame([['a', 'b'], ['c', 'd']],\n...                   index=['row 1', 'row 2'],\n...                   columns=['col 1', 'col 2'])\n\nEncoding/decoding a Dataframe using ``'split'`` formatted JSON:\n\n>>> df.to_json(orient='split')\n    '{\"columns\":[\"col 1\",\"col 2\"],\"index\":[\"row 1\",\"row 2\"],\"data\":[[\"a\",\"b\"],[\"c\",\"d\"]]}'\n>>> pd.read_json(StringIO(_), orient='split')\n      col 1 col 2\nrow 1     a     b\nrow 2     c     d\n\nEncoding/decoding a Dataframe using ``'index'`` formatted JSON:\n\n>>> df.to_json(orient='index')\n'{\"row 1\":{\"col 1\":\"a\",\"col 2\":\"b\"},\"row 2\":{\"col 1\":\"c\",\"col 2\":\"d\"}}'\n\n>>> pd.read_json(StringIO(_), orient='index')\n      col 1 col 2\nrow 1     a     b\nrow 2     c     d\n\nEncoding/decoding a Dataframe using ``'records'`` formatted JSON.\nNote that index labels are not preserved with this encoding.\n\n>>> df.to_json(orient='records')\n'[{\"col 1\":\"a\",\"col 2\":\"b\"},{\"col 1\":\"c\",\"col 2\":\"d\"}]'\n>>> pd.read_json(StringIO(_), orient='records')\n  col 1 col 2\n0     a     b\n1     c     d\n\nEncoding with Table Schema\n\n>>> df.to_json(orient='table')\n    '{\"schema\":{\"fields\":[{\"name\":\"index\",\"type\":\"string\"},{\"name\":\"col 1\",\"type\":\"string\"},{\"name\":\"col 2\",\"type\":\"string\"}],\"primaryKey\":[\"index\"],\"pandas_version\":\"1.4.0\"},\"data\":[{\"index\":\"row 1\",\"col 1\":\"a\",\"col 2\":\"b\"},{\"index\":\"row 2\",\"col 1\":\"c\",\"col 2\":\"d\"}]}'\n\nThe following example uses ``dtype_backend=\"numpy_nullable\"``\n\n>>> data = '''{\"index\": {\"0\": 0, \"1\": 1},\n...        \"a\": {\"0\": 1, \"1\": null},\n...        \"b\": {\"0\": 2.5, \"1\": 4.5},\n...        \"c\": {\"0\": true, \"1\": false},\n...        \"d\": {\"0\": \"a\", \"1\": \"b\"},\n...        \"e\": {\"0\": 1577.2, \"1\": 1577.1}}'''\n>>> pd.read_json(StringIO(data), dtype_backend=\"numpy_nullable\")\n   index     a    b      c  d       e\n0      0     1  2.5   True  a  1577.2\n1      1  <NA>  4.5  False  b  1577.1", "parameters": {"type": "object", "properties": {"path_or_buf": {"type": ["readbuffer[str]", "readbuffer[bytes]", "filepath"]}}}}}
{"task_id": "BigCodeBench/501", "data": {"name": "pandas.read_json.empty", "type": "callable", "signature": "(path_or_buf: 'FilePath | ReadBuffer[str] | ReadBuffer[bytes]', *, orient: 'str | None' = None, typ: \"Literal['frame', 'series']\" = 'frame', dtype: 'DtypeArg | None' = None, convert_axes: 'bool | None' = None, convert_dates: 'bool | list[str]' = True, keep_default_dates: 'bool' = True, precise_float: 'bool' = False, date_unit: 'str | None' = None, encoding: 'str | None' = None, encoding_errors: 'str | None' = 'strict', lines: 'bool' = False, chunksize: 'int | None' = None, compression: 'CompressionOptions' = 'infer', nrows: 'int | None' = None, storage_options: 'StorageOptions | None' = None, dtype_backend: 'DtypeBackend | lib.NoDefault' = <no_default>, engine: 'JSONEngine' = 'ujson') -> 'DataFrame | Series | JsonReader'", "description": "Convert a JSON string to pandas object.\n\nParameters\n----------\npath_or_buf : a valid JSON str, path object or file-like object\n    Any valid string path is acceptable. The string could be a URL. Valid\n    URL schemes include http, ftp, s3, and file. For file URLs, a host is\n    expected. A local file could be:\n    ``file://localhost/path/to/table.json``.\n\n    If you want to pass in a path object, pandas accepts any\n    ``os.PathLike``.\n\n    By file-like object, we refer to objects with a ``read()`` method,\n    such as a file handle (e.g. via builtin ``open`` function)\n    or ``StringIO``.\n\n    .. deprecated:: 2.1.0\n        Passing json literal strings is deprecated.\n\norient : str, optional\n    Indication of expected JSON string format.\n    Compatible JSON strings can be produced by ``to_json()`` with a\n    corresponding orient value.\n    The set of possible orients is:\n\n    - ``'split'`` : dict like\n      ``{index -> [index], columns -> [columns], data -> [values]}``\n    - ``'records'`` : list like\n      ``[{column -> value}, ... , {column -> value}]``\n    - ``'index'`` : dict like ``{index -> {column -> value}}``\n    - ``'columns'`` : dict like ``{column -> {index -> value}}``\n    - ``'values'`` : just the values array\n    - ``'table'`` : dict like ``{'schema': {schema}, 'data': {data}}``\n\n    The allowed and default values depend on the value\n    of the `typ` parameter.\n\n    * when ``typ == 'series'``,\n\n      - allowed orients are ``{'split','records','index'}``\n      - default is ``'index'``\n      - The Series index must be unique for orient ``'index'``.\n\n    * when ``typ == 'frame'``,\n\n      - allowed orients are ``{'split','records','index',\n        'columns','values', 'table'}``\n      - default is ``'columns'``\n      - The DataFrame index must be unique for orients ``'index'`` and\n        ``'columns'``.\n      - The DataFrame columns must be unique for orients ``'index'``,\n        ``'columns'``, and ``'records'``.\n\ntyp : {'frame', 'series'}, default 'frame'\n    The type of object to recover.\n\ndtype : bool or dict, default None\n    If True, infer dtypes; if a dict of column to dtype, then use those;\n    if False, then don't infer dtypes at all, applies only to the data.\n\n    For all ``orient`` values except ``'table'``, default is True.\n\nconvert_axes : bool, default None\n    Try to convert the axes to the proper dtypes.\n\n    For all ``orient`` values except ``'table'``, default is True.\n\nconvert_dates : bool or list of str, default True\n    If True then default datelike columns may be converted (depending on\n    keep_default_dates).\n    If False, no dates will be converted.\n    If a list of column names, then those columns will be converted and\n    default datelike columns may also be converted (depending on\n    keep_default_dates).\n\nkeep_default_dates : bool, default True\n    If parsing dates (convert_dates is not False), then try to parse the\n    default datelike columns.\n    A column label is datelike if\n\n    * it ends with ``'_at'``,\n\n    * it ends with ``'_time'``,\n\n    * it begins with ``'timestamp'``,\n\n    * it is ``'modified'``, or\n\n    * it is ``'date'``.\n\nprecise_float : bool, default False\n    Set to enable usage of higher precision (strtod) function when\n    decoding string to double values. Default (False) is to use fast but\n    less precise builtin functionality.\n\ndate_unit : str, default None\n    The timestamp unit to detect if converting dates. The default behaviour\n    is to try and detect the correct precision, but if this is not desired\n    then pass one of 's', 'ms', 'us' or 'ns' to force parsing only seconds,\n    milliseconds, microseconds or nanoseconds respectively.\n\nencoding : str, default is 'utf-8'\n    The encoding to use to decode py3 bytes.\n\nencoding_errors : str, optional, default \"strict\"\n    How encoding errors are treated. `List of possible values\n    <https://docs.python.org/3/library/codecs.html#error-handlers>`_ .\n\n    .. versionadded:: 1.3.0\n\nlines : bool, default False\n    Read the file as a json object per line.\n\nchunksize : int, optional\n    Return JsonReader object for iteration.\n    See the `line-delimited json docs\n    <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#line-delimited-json>`_\n    for more information on ``chunksize``.\n    This can only be passed if `lines=True`.\n    If this is None, the file will be read into memory all at once.\ncompression : str or dict, default 'infer'\n    For on-the-fly decompression of on-disk data. If 'infer' and 'path_or_buf' is\n    path-like, then detect compression from the following extensions: '.gz',\n    '.bz2', '.zip', '.xz', '.zst', '.tar', '.tar.gz', '.tar.xz' or '.tar.bz2'\n    (otherwise no compression).\n    If using 'zip' or 'tar', the ZIP file must contain only one data file to be read in.\n    Set to ``None`` for no decompression.\n    Can also be a dict with key ``'method'`` set\n    to one of {``'zip'``, ``'gzip'``, ``'bz2'``, ``'zstd'``, ``'xz'``, ``'tar'``} and\n    other key-value pairs are forwarded to\n    ``zipfile.ZipFile``, ``gzip.GzipFile``,\n    ``bz2.BZ2File``, ``zstandard.ZstdDecompressor``, ``lzma.LZMAFile`` or\n    ``tarfile.TarFile``, respectively.\n    As an example, the following could be passed for Zstandard decompression using a\n    custom compression dictionary:\n    ``compression={'method': 'zstd', 'dict_data': my_compression_dict}``.\n\n    .. versionadded:: 1.5.0\n        Added support for `.tar` files.\n\n    .. versionchanged:: 1.4.0 Zstandard support.\n\nnrows : int, optional\n    The number of lines from the line-delimited jsonfile that has to be read.\n    This can only be passed if `lines=True`.\n    If this is None, all the rows will be returned.\n\nstorage_options : dict, optional\n    Extra options that make sense for a particular storage connection, e.g.\n    host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n    are forwarded to ``urllib.request.Request`` as header options. For other\n    URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n    forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n    details, and for more examples on storage options refer `here\n    <https://pandas.pydata.org/docs/user_guide/io.html?\n    highlight=storage_options#reading-writing-remote-files>`_.\n\ndtype_backend : {'numpy_nullable', 'pyarrow'}, default 'numpy_nullable'\n    Back-end data type applied to the resultant :class:`DataFrame`\n    (still experimental). Behaviour is as follows:\n\n    * ``\"numpy_nullable\"``: returns nullable-dtype-backed :class:`DataFrame`\n      (default).\n    * ``\"pyarrow\"``: returns pyarrow-backed nullable :class:`ArrowDtype`\n      DataFrame.\n\n    .. versionadded:: 2.0\n\nengine : {\"ujson\", \"pyarrow\"}, default \"ujson\"\n    Parser engine to use. The ``\"pyarrow\"`` engine is only available when\n    ``lines=True``.\n\n    .. versionadded:: 2.0\n\nReturns\n-------\nSeries, DataFrame, or pandas.api.typing.JsonReader\n    A JsonReader is returned when ``chunksize`` is not ``0`` or ``None``.\n    Otherwise, the type returned depends on the value of ``typ``.\n\nSee Also\n--------\nDataFrame.to_json : Convert a DataFrame to a JSON string.\nSeries.to_json : Convert a Series to a JSON string.\njson_normalize : Normalize semi-structured JSON data into a flat table.\n\nNotes\n-----\nSpecific to ``orient='table'``, if a :class:`DataFrame` with a literal\n:class:`Index` name of `index` gets written with :func:`to_json`, the\nsubsequent read operation will incorrectly set the :class:`Index` name to\n``None``. This is because `index` is also used by :func:`DataFrame.to_json`\nto denote a missing :class:`Index` name, and the subsequent\n:func:`read_json` operation cannot distinguish between the two. The same\nlimitation is encountered with a :class:`MultiIndex` and any names\nbeginning with ``'level_'``.\n\nExamples\n--------\n>>> from io import StringIO\n>>> df = pd.DataFrame([['a', 'b'], ['c', 'd']],\n...                   index=['row 1', 'row 2'],\n...                   columns=['col 1', 'col 2'])\n\nEncoding/decoding a Dataframe using ``'split'`` formatted JSON:\n\n>>> df.to_json(orient='split')\n    '{\"columns\":[\"col 1\",\"col 2\"],\"index\":[\"row 1\",\"row 2\"],\"data\":[[\"a\",\"b\"],[\"c\",\"d\"]]}'\n>>> pd.read_json(StringIO(_), orient='split')\n      col 1 col 2\nrow 1     a     b\nrow 2     c     d\n\nEncoding/decoding a Dataframe using ``'index'`` formatted JSON:\n\n>>> df.to_json(orient='index')\n'{\"row 1\":{\"col 1\":\"a\",\"col 2\":\"b\"},\"row 2\":{\"col 1\":\"c\",\"col 2\":\"d\"}}'\n\n>>> pd.read_json(StringIO(_), orient='index')\n      col 1 col 2\nrow 1     a     b\nrow 2     c     d\n\nEncoding/decoding a Dataframe using ``'records'`` formatted JSON.\nNote that index labels are not preserved with this encoding.\n\n>>> df.to_json(orient='records')\n'[{\"col 1\":\"a\",\"col 2\":\"b\"},{\"col 1\":\"c\",\"col 2\":\"d\"}]'\n>>> pd.read_json(StringIO(_), orient='records')\n  col 1 col 2\n0     a     b\n1     c     d\n\nEncoding with Table Schema\n\n>>> df.to_json(orient='table')\n    '{\"schema\":{\"fields\":[{\"name\":\"index\",\"type\":\"string\"},{\"name\":\"col 1\",\"type\":\"string\"},{\"name\":\"col 2\",\"type\":\"string\"}],\"primaryKey\":[\"index\"],\"pandas_version\":\"1.4.0\"},\"data\":[{\"index\":\"row 1\",\"col 1\":\"a\",\"col 2\":\"b\"},{\"index\":\"row 2\",\"col 1\":\"c\",\"col 2\":\"d\"}]}'\n\nThe following example uses ``dtype_backend=\"numpy_nullable\"``\n\n>>> data = '''{\"index\": {\"0\": 0, \"1\": 1},\n...        \"a\": {\"0\": 1, \"1\": null},\n...        \"b\": {\"0\": 2.5, \"1\": 4.5},\n...        \"c\": {\"0\": true, \"1\": false},\n...        \"d\": {\"0\": \"a\", \"1\": \"b\"},\n...        \"e\": {\"0\": 1577.2, \"1\": 1577.1}}'''\n>>> pd.read_json(StringIO(data), dtype_backend=\"numpy_nullable\")\n   index     a    b      c  d       e\n0      0     1  2.5   True  a  1577.2\n1      1  <NA>  4.5  False  b  1577.1", "parameters": {"type": "object", "properties": {"path_or_buf": {"type": ["readbuffer[str]", "readbuffer[bytes]", "filepath"]}, "orient": {"type": ["str", "null"], "default": null}, "typ": {"type": "\"literal['frame', 'series']\"", "default": "frame"}, "dtype": {"type": ["dtypearg", "null"], "default": null}, "convert_axes": {"type": ["bool", "null"], "default": null}, "convert_dates": {"type": ["list[str]", "bool"], "default": true}, "keep_default_dates": {"type": "bool", "default": true}, "precise_float": {"type": "bool", "default": false}, "date_unit": {"type": ["str", "null"], "default": null}, "encoding": {"type": ["str", "null"], "default": null}, "encoding_errors": {"type": ["str", "null"], "default": "strict"}, "lines": {"type": "bool", "default": false}, "chunksize": {"type": ["integer", "null"], "default": null}, "compression": {"type": "compressionoptions", "default": "infer"}, "nrows": {"type": ["integer", "null"], "default": null}, "storage_options": {"type": ["storageoptions", "null"], "default": null}, "dtype_backend": {"type": ["dtypebackend", "lib.nodefault"], "default": "<no_default>"}, "engine": {"type": "jsonengine", "default": "ujson"}}}}}
{"task_id": "BigCodeBench/501", "data": {"name": "pandas.read_json.columns", "type": "callable", "signature": "(path_or_buf: 'FilePath | ReadBuffer[str] | ReadBuffer[bytes]', *, orient: 'str | None' = None, typ: \"Literal['frame', 'series']\" = 'frame', dtype: 'DtypeArg | None' = None, convert_axes: 'bool | None' = None, convert_dates: 'bool | list[str]' = True, keep_default_dates: 'bool' = True, precise_float: 'bool' = False, date_unit: 'str | None' = None, encoding: 'str | None' = None, encoding_errors: 'str | None' = 'strict', lines: 'bool' = False, chunksize: 'int | None' = None, compression: 'CompressionOptions' = 'infer', nrows: 'int | None' = None, storage_options: 'StorageOptions | None' = None, dtype_backend: 'DtypeBackend | lib.NoDefault' = <no_default>, engine: 'JSONEngine' = 'ujson') -> 'DataFrame | Series | JsonReader'", "description": "Convert a JSON string to pandas object.\n\nParameters\n----------\npath_or_buf : a valid JSON str, path object or file-like object\n    Any valid string path is acceptable. The string could be a URL. Valid\n    URL schemes include http, ftp, s3, and file. For file URLs, a host is\n    expected. A local file could be:\n    ``file://localhost/path/to/table.json``.\n\n    If you want to pass in a path object, pandas accepts any\n    ``os.PathLike``.\n\n    By file-like object, we refer to objects with a ``read()`` method,\n    such as a file handle (e.g. via builtin ``open`` function)\n    or ``StringIO``.\n\n    .. deprecated:: 2.1.0\n        Passing json literal strings is deprecated.\n\norient : str, optional\n    Indication of expected JSON string format.\n    Compatible JSON strings can be produced by ``to_json()`` with a\n    corresponding orient value.\n    The set of possible orients is:\n\n    - ``'split'`` : dict like\n      ``{index -> [index], columns -> [columns], data -> [values]}``\n    - ``'records'`` : list like\n      ``[{column -> value}, ... , {column -> value}]``\n    - ``'index'`` : dict like ``{index -> {column -> value}}``\n    - ``'columns'`` : dict like ``{column -> {index -> value}}``\n    - ``'values'`` : just the values array\n    - ``'table'`` : dict like ``{'schema': {schema}, 'data': {data}}``\n\n    The allowed and default values depend on the value\n    of the `typ` parameter.\n\n    * when ``typ == 'series'``,\n\n      - allowed orients are ``{'split','records','index'}``\n      - default is ``'index'``\n      - The Series index must be unique for orient ``'index'``.\n\n    * when ``typ == 'frame'``,\n\n      - allowed orients are ``{'split','records','index',\n        'columns','values', 'table'}``\n      - default is ``'columns'``\n      - The DataFrame index must be unique for orients ``'index'`` and\n        ``'columns'``.\n      - The DataFrame columns must be unique for orients ``'index'``,\n        ``'columns'``, and ``'records'``.\n\ntyp : {'frame', 'series'}, default 'frame'\n    The type of object to recover.\n\ndtype : bool or dict, default None\n    If True, infer dtypes; if a dict of column to dtype, then use those;\n    if False, then don't infer dtypes at all, applies only to the data.\n\n    For all ``orient`` values except ``'table'``, default is True.\n\nconvert_axes : bool, default None\n    Try to convert the axes to the proper dtypes.\n\n    For all ``orient`` values except ``'table'``, default is True.\n\nconvert_dates : bool or list of str, default True\n    If True then default datelike columns may be converted (depending on\n    keep_default_dates).\n    If False, no dates will be converted.\n    If a list of column names, then those columns will be converted and\n    default datelike columns may also be converted (depending on\n    keep_default_dates).\n\nkeep_default_dates : bool, default True\n    If parsing dates (convert_dates is not False), then try to parse the\n    default datelike columns.\n    A column label is datelike if\n\n    * it ends with ``'_at'``,\n\n    * it ends with ``'_time'``,\n\n    * it begins with ``'timestamp'``,\n\n    * it is ``'modified'``, or\n\n    * it is ``'date'``.\n\nprecise_float : bool, default False\n    Set to enable usage of higher precision (strtod) function when\n    decoding string to double values. Default (False) is to use fast but\n    less precise builtin functionality.\n\ndate_unit : str, default None\n    The timestamp unit to detect if converting dates. The default behaviour\n    is to try and detect the correct precision, but if this is not desired\n    then pass one of 's', 'ms', 'us' or 'ns' to force parsing only seconds,\n    milliseconds, microseconds or nanoseconds respectively.\n\nencoding : str, default is 'utf-8'\n    The encoding to use to decode py3 bytes.\n\nencoding_errors : str, optional, default \"strict\"\n    How encoding errors are treated. `List of possible values\n    <https://docs.python.org/3/library/codecs.html#error-handlers>`_ .\n\n    .. versionadded:: 1.3.0\n\nlines : bool, default False\n    Read the file as a json object per line.\n\nchunksize : int, optional\n    Return JsonReader object for iteration.\n    See the `line-delimited json docs\n    <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#line-delimited-json>`_\n    for more information on ``chunksize``.\n    This can only be passed if `lines=True`.\n    If this is None, the file will be read into memory all at once.\ncompression : str or dict, default 'infer'\n    For on-the-fly decompression of on-disk data. If 'infer' and 'path_or_buf' is\n    path-like, then detect compression from the following extensions: '.gz',\n    '.bz2', '.zip', '.xz', '.zst', '.tar', '.tar.gz', '.tar.xz' or '.tar.bz2'\n    (otherwise no compression).\n    If using 'zip' or 'tar', the ZIP file must contain only one data file to be read in.\n    Set to ``None`` for no decompression.\n    Can also be a dict with key ``'method'`` set\n    to one of {``'zip'``, ``'gzip'``, ``'bz2'``, ``'zstd'``, ``'xz'``, ``'tar'``} and\n    other key-value pairs are forwarded to\n    ``zipfile.ZipFile``, ``gzip.GzipFile``,\n    ``bz2.BZ2File``, ``zstandard.ZstdDecompressor``, ``lzma.LZMAFile`` or\n    ``tarfile.TarFile``, respectively.\n    As an example, the following could be passed for Zstandard decompression using a\n    custom compression dictionary:\n    ``compression={'method': 'zstd', 'dict_data': my_compression_dict}``.\n\n    .. versionadded:: 1.5.0\n        Added support for `.tar` files.\n\n    .. versionchanged:: 1.4.0 Zstandard support.\n\nnrows : int, optional\n    The number of lines from the line-delimited jsonfile that has to be read.\n    This can only be passed if `lines=True`.\n    If this is None, all the rows will be returned.\n\nstorage_options : dict, optional\n    Extra options that make sense for a particular storage connection, e.g.\n    host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n    are forwarded to ``urllib.request.Request`` as header options. For other\n    URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n    forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n    details, and for more examples on storage options refer `here\n    <https://pandas.pydata.org/docs/user_guide/io.html?\n    highlight=storage_options#reading-writing-remote-files>`_.\n\ndtype_backend : {'numpy_nullable', 'pyarrow'}, default 'numpy_nullable'\n    Back-end data type applied to the resultant :class:`DataFrame`\n    (still experimental). Behaviour is as follows:\n\n    * ``\"numpy_nullable\"``: returns nullable-dtype-backed :class:`DataFrame`\n      (default).\n    * ``\"pyarrow\"``: returns pyarrow-backed nullable :class:`ArrowDtype`\n      DataFrame.\n\n    .. versionadded:: 2.0\n\nengine : {\"ujson\", \"pyarrow\"}, default \"ujson\"\n    Parser engine to use. The ``\"pyarrow\"`` engine is only available when\n    ``lines=True``.\n\n    .. versionadded:: 2.0\n\nReturns\n-------\nSeries, DataFrame, or pandas.api.typing.JsonReader\n    A JsonReader is returned when ``chunksize`` is not ``0`` or ``None``.\n    Otherwise, the type returned depends on the value of ``typ``.\n\nSee Also\n--------\nDataFrame.to_json : Convert a DataFrame to a JSON string.\nSeries.to_json : Convert a Series to a JSON string.\njson_normalize : Normalize semi-structured JSON data into a flat table.\n\nNotes\n-----\nSpecific to ``orient='table'``, if a :class:`DataFrame` with a literal\n:class:`Index` name of `index` gets written with :func:`to_json`, the\nsubsequent read operation will incorrectly set the :class:`Index` name to\n``None``. This is because `index` is also used by :func:`DataFrame.to_json`\nto denote a missing :class:`Index` name, and the subsequent\n:func:`read_json` operation cannot distinguish between the two. The same\nlimitation is encountered with a :class:`MultiIndex` and any names\nbeginning with ``'level_'``.\n\nExamples\n--------\n>>> from io import StringIO\n>>> df = pd.DataFrame([['a', 'b'], ['c', 'd']],\n...                   index=['row 1', 'row 2'],\n...                   columns=['col 1', 'col 2'])\n\nEncoding/decoding a Dataframe using ``'split'`` formatted JSON:\n\n>>> df.to_json(orient='split')\n    '{\"columns\":[\"col 1\",\"col 2\"],\"index\":[\"row 1\",\"row 2\"],\"data\":[[\"a\",\"b\"],[\"c\",\"d\"]]}'\n>>> pd.read_json(StringIO(_), orient='split')\n      col 1 col 2\nrow 1     a     b\nrow 2     c     d\n\nEncoding/decoding a Dataframe using ``'index'`` formatted JSON:\n\n>>> df.to_json(orient='index')\n'{\"row 1\":{\"col 1\":\"a\",\"col 2\":\"b\"},\"row 2\":{\"col 1\":\"c\",\"col 2\":\"d\"}}'\n\n>>> pd.read_json(StringIO(_), orient='index')\n      col 1 col 2\nrow 1     a     b\nrow 2     c     d\n\nEncoding/decoding a Dataframe using ``'records'`` formatted JSON.\nNote that index labels are not preserved with this encoding.\n\n>>> df.to_json(orient='records')\n'[{\"col 1\":\"a\",\"col 2\":\"b\"},{\"col 1\":\"c\",\"col 2\":\"d\"}]'\n>>> pd.read_json(StringIO(_), orient='records')\n  col 1 col 2\n0     a     b\n1     c     d\n\nEncoding with Table Schema\n\n>>> df.to_json(orient='table')\n    '{\"schema\":{\"fields\":[{\"name\":\"index\",\"type\":\"string\"},{\"name\":\"col 1\",\"type\":\"string\"},{\"name\":\"col 2\",\"type\":\"string\"}],\"primaryKey\":[\"index\"],\"pandas_version\":\"1.4.0\"},\"data\":[{\"index\":\"row 1\",\"col 1\":\"a\",\"col 2\":\"b\"},{\"index\":\"row 2\",\"col 1\":\"c\",\"col 2\":\"d\"}]}'\n\nThe following example uses ``dtype_backend=\"numpy_nullable\"``\n\n>>> data = '''{\"index\": {\"0\": 0, \"1\": 1},\n...        \"a\": {\"0\": 1, \"1\": null},\n...        \"b\": {\"0\": 2.5, \"1\": 4.5},\n...        \"c\": {\"0\": true, \"1\": false},\n...        \"d\": {\"0\": \"a\", \"1\": \"b\"},\n...        \"e\": {\"0\": 1577.2, \"1\": 1577.1}}'''\n>>> pd.read_json(StringIO(data), dtype_backend=\"numpy_nullable\")\n   index     a    b      c  d       e\n0      0     1  2.5   True  a  1577.2\n1      1  <NA>  4.5  False  b  1577.1", "parameters": {"type": "object", "properties": {"path_or_buf": {"type": ["readbuffer[str]", "readbuffer[bytes]", "filepath"]}, "orient": {"type": ["str", "null"], "default": null}, "typ": {"type": "\"literal['frame', 'series']\"", "default": "frame"}, "dtype": {"type": ["dtypearg", "null"], "default": null}, "convert_axes": {"type": ["bool", "null"], "default": null}, "convert_dates": {"type": ["list[str]", "bool"], "default": true}, "keep_default_dates": {"type": "bool", "default": true}, "precise_float": {"type": "bool", "default": false}, "date_unit": {"type": ["str", "null"], "default": null}, "encoding": {"type": ["str", "null"], "default": null}, "encoding_errors": {"type": ["str", "null"], "default": "strict"}, "lines": {"type": "bool", "default": false}, "chunksize": {"type": ["integer", "null"], "default": null}, "compression": {"type": "compressionoptions", "default": "infer"}, "nrows": {"type": ["integer", "null"], "default": null}, "storage_options": {"type": ["storageoptions", "null"], "default": null}, "dtype_backend": {"type": ["dtypebackend", "lib.nodefault"], "default": "<no_default>"}, "engine": {"type": "jsonengine", "default": "ujson"}}}}}
{"task_id": "BigCodeBench/501", "data": {"name": "pandas.read_json.iterrows", "type": "callable", "signature": "()", "description": "Convert a JSON string to pandas object.\n\nParameters\n----------\npath_or_buf : a valid JSON str, path object or file-like object\n    Any valid string path is acceptable. The string could be a URL. Valid\n    URL schemes include http, ftp, s3, and file. For file URLs, a host is\n    expected. A local file could be:\n    ``file://localhost/path/to/table.json``.\n\n    If you want to pass in a path object, pandas accepts any\n    ``os.PathLike``.\n\n    By file-like object, we refer to objects with a ``read()`` method,\n    such as a file handle (e.g. via builtin ``open`` function)\n    or ``StringIO``.\n\n    .. deprecated:: 2.1.0\n        Passing json literal strings is deprecated.\n\norient : str, optional\n    Indication of expected JSON string format.\n    Compatible JSON strings can be produced by ``to_json()`` with a\n    corresponding orient value.\n    The set of possible orients is:\n\n    - ``'split'`` : dict like\n      ``{index -> [index], columns -> [columns], data -> [values]}``\n    - ``'records'`` : list like\n      ``[{column -> value}, ... , {column -> value}]``\n    - ``'index'`` : dict like ``{index -> {column -> value}}``\n    - ``'columns'`` : dict like ``{column -> {index -> value}}``\n    - ``'values'`` : just the values array\n    - ``'table'`` : dict like ``{'schema': {schema}, 'data': {data}}``\n\n    The allowed and default values depend on the value\n    of the `typ` parameter.\n\n    * when ``typ == 'series'``,\n\n      - allowed orients are ``{'split','records','index'}``\n      - default is ``'index'``\n      - The Series index must be unique for orient ``'index'``.\n\n    * when ``typ == 'frame'``,\n\n      - allowed orients are ``{'split','records','index',\n        'columns','values', 'table'}``\n      - default is ``'columns'``\n      - The DataFrame index must be unique for orients ``'index'`` and\n        ``'columns'``.\n      - The DataFrame columns must be unique for orients ``'index'``,\n        ``'columns'``, and ``'records'``.\n\ntyp : {'frame', 'series'}, default 'frame'\n    The type of object to recover.\n\ndtype : bool or dict, default None\n    If True, infer dtypes; if a dict of column to dtype, then use those;\n    if False, then don't infer dtypes at all, applies only to the data.\n\n    For all ``orient`` values except ``'table'``, default is True.\n\nconvert_axes : bool, default None\n    Try to convert the axes to the proper dtypes.\n\n    For all ``orient`` values except ``'table'``, default is True.\n\nconvert_dates : bool or list of str, default True\n    If True then default datelike columns may be converted (depending on\n    keep_default_dates).\n    If False, no dates will be converted.\n    If a list of column names, then those columns will be converted and\n    default datelike columns may also be converted (depending on\n    keep_default_dates).\n\nkeep_default_dates : bool, default True\n    If parsing dates (convert_dates is not False), then try to parse the\n    default datelike columns.\n    A column label is datelike if\n\n    * it ends with ``'_at'``,\n\n    * it ends with ``'_time'``,\n\n    * it begins with ``'timestamp'``,\n\n    * it is ``'modified'``, or\n\n    * it is ``'date'``.\n\nprecise_float : bool, default False\n    Set to enable usage of higher precision (strtod) function when\n    decoding string to double values. Default (False) is to use fast but\n    less precise builtin functionality.\n\ndate_unit : str, default None\n    The timestamp unit to detect if converting dates. The default behaviour\n    is to try and detect the correct precision, but if this is not desired\n    then pass one of 's', 'ms', 'us' or 'ns' to force parsing only seconds,\n    milliseconds, microseconds or nanoseconds respectively.\n\nencoding : str, default is 'utf-8'\n    The encoding to use to decode py3 bytes.\n\nencoding_errors : str, optional, default \"strict\"\n    How encoding errors are treated. `List of possible values\n    <https://docs.python.org/3/library/codecs.html#error-handlers>`_ .\n\n    .. versionadded:: 1.3.0\n\nlines : bool, default False\n    Read the file as a json object per line.\n\nchunksize : int, optional\n    Return JsonReader object for iteration.\n    See the `line-delimited json docs\n    <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#line-delimited-json>`_\n    for more information on ``chunksize``.\n    This can only be passed if `lines=True`.\n    If this is None, the file will be read into memory all at once.\ncompression : str or dict, default 'infer'\n    For on-the-fly decompression of on-disk data. If 'infer' and 'path_or_buf' is\n    path-like, then detect compression from the following extensions: '.gz',\n    '.bz2', '.zip', '.xz', '.zst', '.tar', '.tar.gz', '.tar.xz' or '.tar.bz2'\n    (otherwise no compression).\n    If using 'zip' or 'tar', the ZIP file must contain only one data file to be read in.\n    Set to ``None`` for no decompression.\n    Can also be a dict with key ``'method'`` set\n    to one of {``'zip'``, ``'gzip'``, ``'bz2'``, ``'zstd'``, ``'xz'``, ``'tar'``} and\n    other key-value pairs are forwarded to\n    ``zipfile.ZipFile``, ``gzip.GzipFile``,\n    ``bz2.BZ2File``, ``zstandard.ZstdDecompressor``, ``lzma.LZMAFile`` or\n    ``tarfile.TarFile``, respectively.\n    As an example, the following could be passed for Zstandard decompression using a\n    custom compression dictionary:\n    ``compression={'method': 'zstd', 'dict_data': my_compression_dict}``.\n\n    .. versionadded:: 1.5.0\n        Added support for `.tar` files.\n\n    .. versionchanged:: 1.4.0 Zstandard support.\n\nnrows : int, optional\n    The number of lines from the line-delimited jsonfile that has to be read.\n    This can only be passed if `lines=True`.\n    If this is None, all the rows will be returned.\n\nstorage_options : dict, optional\n    Extra options that make sense for a particular storage connection, e.g.\n    host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n    are forwarded to ``urllib.request.Request`` as header options. For other\n    URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n    forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n    details, and for more examples on storage options refer `here\n    <https://pandas.pydata.org/docs/user_guide/io.html?\n    highlight=storage_options#reading-writing-remote-files>`_.\n\ndtype_backend : {'numpy_nullable', 'pyarrow'}, default 'numpy_nullable'\n    Back-end data type applied to the resultant :class:`DataFrame`\n    (still experimental). Behaviour is as follows:\n\n    * ``\"numpy_nullable\"``: returns nullable-dtype-backed :class:`DataFrame`\n      (default).\n    * ``\"pyarrow\"``: returns pyarrow-backed nullable :class:`ArrowDtype`\n      DataFrame.\n\n    .. versionadded:: 2.0\n\nengine : {\"ujson\", \"pyarrow\"}, default \"ujson\"\n    Parser engine to use. The ``\"pyarrow\"`` engine is only available when\n    ``lines=True``.\n\n    .. versionadded:: 2.0\n\nReturns\n-------\nSeries, DataFrame, or pandas.api.typing.JsonReader\n    A JsonReader is returned when ``chunksize`` is not ``0`` or ``None``.\n    Otherwise, the type returned depends on the value of ``typ``.\n\nSee Also\n--------\nDataFrame.to_json : Convert a DataFrame to a JSON string.\nSeries.to_json : Convert a Series to a JSON string.\njson_normalize : Normalize semi-structured JSON data into a flat table.\n\nNotes\n-----\nSpecific to ``orient='table'``, if a :class:`DataFrame` with a literal\n:class:`Index` name of `index` gets written with :func:`to_json`, the\nsubsequent read operation will incorrectly set the :class:`Index` name to\n``None``. This is because `index` is also used by :func:`DataFrame.to_json`\nto denote a missing :class:`Index` name, and the subsequent\n:func:`read_json` operation cannot distinguish between the two. The same\nlimitation is encountered with a :class:`MultiIndex` and any names\nbeginning with ``'level_'``.\n\nExamples\n--------\n>>> from io import StringIO\n>>> df = pd.DataFrame([['a', 'b'], ['c', 'd']],\n...                   index=['row 1', 'row 2'],\n...                   columns=['col 1', 'col 2'])\n\nEncoding/decoding a Dataframe using ``'split'`` formatted JSON:\n\n>>> df.to_json(orient='split')\n    '{\"columns\":[\"col 1\",\"col 2\"],\"index\":[\"row 1\",\"row 2\"],\"data\":[[\"a\",\"b\"],[\"c\",\"d\"]]}'\n>>> pd.read_json(StringIO(_), orient='split')\n      col 1 col 2\nrow 1     a     b\nrow 2     c     d\n\nEncoding/decoding a Dataframe using ``'index'`` formatted JSON:\n\n>>> df.to_json(orient='index')\n'{\"row 1\":{\"col 1\":\"a\",\"col 2\":\"b\"},\"row 2\":{\"col 1\":\"c\",\"col 2\":\"d\"}}'\n\n>>> pd.read_json(StringIO(_), orient='index')\n      col 1 col 2\nrow 1     a     b\nrow 2     c     d\n\nEncoding/decoding a Dataframe using ``'records'`` formatted JSON.\nNote that index labels are not preserved with this encoding.\n\n>>> df.to_json(orient='records')\n'[{\"col 1\":\"a\",\"col 2\":\"b\"},{\"col 1\":\"c\",\"col 2\":\"d\"}]'\n>>> pd.read_json(StringIO(_), orient='records')\n  col 1 col 2\n0     a     b\n1     c     d\n\nEncoding with Table Schema\n\n>>> df.to_json(orient='table')\n    '{\"schema\":{\"fields\":[{\"name\":\"index\",\"type\":\"string\"},{\"name\":\"col 1\",\"type\":\"string\"},{\"name\":\"col 2\",\"type\":\"string\"}],\"primaryKey\":[\"index\"],\"pandas_version\":\"1.4.0\"},\"data\":[{\"index\":\"row 1\",\"col 1\":\"a\",\"col 2\":\"b\"},{\"index\":\"row 2\",\"col 1\":\"c\",\"col 2\":\"d\"}]}'\n\nThe following example uses ``dtype_backend=\"numpy_nullable\"``\n\n>>> data = '''{\"index\": {\"0\": 0, \"1\": 1},\n...        \"a\": {\"0\": 1, \"1\": null},\n...        \"b\": {\"0\": 2.5, \"1\": 4.5},\n...        \"c\": {\"0\": true, \"1\": false},\n...        \"d\": {\"0\": \"a\", \"1\": \"b\"},\n...        \"e\": {\"0\": 1577.2, \"1\": 1577.1}}'''\n>>> pd.read_json(StringIO(data), dtype_backend=\"numpy_nullable\")\n   index     a    b      c  d       e\n0      0     1  2.5   True  a  1577.2\n1      1  <NA>  4.5  False  b  1577.1", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/502", "data": {"name": "datetime.datetime", "type": "class", "signature": "(self, /, *args, **kwargs)", "description": "datetime(year, month, day[, hour[, minute[, second[, microsecond[,tzinfo]]]]])\n\nThe year, month and day arguments are required. tzinfo may be None, or an\ninstance of a tzinfo subclass. The remaining arguments may be ints.", "parameters": {"type": "object", "properties": {"/": {}}}}}
{"task_id": "BigCodeBench/502", "data": {"name": "datetime.datetime.now", "type": "callable", "signature": "()", "description": "Returns new datetime object representing current time local to tz.\n\n  tz\n    Timezone object.\n\nIf no tz is specified, uses local timezone.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/502", "data": {"name": "data.append([date, activity, duration])"}}
{"task_id": "BigCodeBench/502", "data": {"name": "random.randint", "type": "callable", "signature": "(a, b)", "description": "Return random integer in range [a, b], including both end points.\n        ", "parameters": {"type": "object", "properties": {"a": {}, "b": {}}}}}
{"task_id": "BigCodeBench/502", "data": {"name": "random.seed", "type": "callable", "signature": "(a=None)", "description": "Initialize internal state from a seed.\n\nThe only supported seed types are None, int, float,\nstr, bytes, and bytearray.\n\nNone or no argument seeds from current time or from an operating\nsystem specific randomness source if available.\n\nIf *a* is an int, all bits are used.\n\nFor version 2 (the default), all of the bits are used if *a* is a str,\nbytes, or bytearray.  For version 1 (provided for reproducing random\nsequences from older versions of Python), the algorithm for str and\nbytes generates a narrower range of seeds.", "parameters": {"type": "object", "properties": {"a": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/502", "data": {"name": "seaborn.lineplot", "type": "callable", "signature": "(data=None, x=None, y=None, hue=None, **kwargs)", "description": "Draw a line plot with possibility of several semantic groupings.\n\nThe relationship between `x` and `y` can be shown for different subsets\nof the data using the `hue`, `size`, and `style` parameters. These\nparameters control what visual semantics are used to identify the different\nsubsets. It is possible to show up to three dimensions independently by\nusing all three semantic types, but this style of plot can be hard to\ninterpret and is often ineffective. Using redundant semantics (i.e. both\n`hue` and `style` for the same variable) can be helpful for making\ngraphics more accessible.\n\nSee the :ref:`tutorial <relational_tutorial>` for more information.\n\nThe default treatment of the `hue` (and to a lesser extent, `size`)\nsemantic, if present, depends on whether the variable is inferred to\nrepresent \"numeric\" or \"categorical\" data. In particular, numeric variables\nare represented with a sequential colormap by default, and the legend\nentries show regular \"ticks\" with values that may or may not exist in the\ndata. This behavior can be controlled through various parameters, as\ndescribed and illustrated below.\n\nBy default, the plot aggregates over multiple `y` values at each value of\n`x` and shows an estimate of the central tendency and a confidence\ninterval for that estimate.\n\nParameters\n----------\ndata : :class:`pandas.DataFrame`, :class:`numpy.ndarray`, mapping, or sequence\n    Input data structure. Either a long-form collection of vectors that can be\n    assigned to named variables or a wide-form dataset that will be internally\n    reshaped.\nx, y : vectors or keys in ``data``\n    Variables that specify positions on the x and y axes.\nhue : vector or key in `data`\n    Grouping variable that will produce lines with different colors.\n    Can be either categorical or numeric, although color mapping will\n    behave differently in latter case.\nsize : vector or key in `data`\n    Grouping variable that will produce lines with different widths.\n    Can be either categorical or numeric, although size mapping will\n    behave differently in latter case.\nstyle : vector or key in `data`\n    Grouping variable that will produce lines with different dashes\n    and/or markers. Can have a numeric dtype but will always be treated\n    as categorical.\nunits : vector or key in `data`\n    Grouping variable identifying sampling units. When used, a separate\n    line will be drawn for each unit with appropriate semantics, but no\n    legend entry will be added. Useful for showing distribution of\n    experimental replicates when exact identities are not needed.\nweights : vector or key in `data`\n    Data values or column used to compute weighted estimation.\n    Note that use of weights currently limits the choice of statistics\n    to a 'mean' estimator and 'ci' errorbar.\npalette : string, list, dict, or :class:`matplotlib.colors.Colormap`\n    Method for choosing the colors to use when mapping the ``hue`` semantic.\n    String values are passed to :func:`color_palette`. List or dict values\n    imply categorical mapping, while a colormap object implies numeric mapping.\nhue_order : vector of strings\n    Specify the order of processing and plotting for categorical levels of the\n    ``hue`` semantic.\nhue_norm : tuple or :class:`matplotlib.colors.Normalize`\n    Either a pair of values that set the normalization range in data units\n    or an object that will map from data units into a [0, 1] interval. Usage\n    implies numeric mapping.\nsizes : list, dict, or tuple\n    An object that determines how sizes are chosen when `size` is used.\n    List or dict arguments should provide a size for each unique data value,\n    which forces a categorical interpretation. The argument may also be a\n    min, max tuple.\nsize_order : list\n    Specified order for appearance of the `size` variable levels,\n    otherwise they are determined from the data. Not relevant when the\n    `size` variable is numeric.\nsize_norm : tuple or Normalize object\n    Normalization in data units for scaling plot objects when the\n    `size` variable is numeric.\ndashes : boolean, list, or dictionary\n    Object determining how to draw the lines for different levels of the\n    `style` variable. Setting to `True` will use default dash codes, or\n    you can pass a list of dash codes or a dictionary mapping levels of the\n    `style` variable to dash codes. Setting to `False` will use solid\n    lines for all subsets. Dashes are specified as in matplotlib: a tuple\n    of `(segment, gap)` lengths, or an empty string to draw a solid line.\nmarkers : boolean, list, or dictionary\n    Object determining how to draw the markers for different levels of the\n    `style` variable. Setting to `True` will use default markers, or\n    you can pass a list of markers or a dictionary mapping levels of the\n    `style` variable to markers. Setting to `False` will draw\n    marker-less lines.  Markers are specified as in matplotlib.\nstyle_order : list\n    Specified order for appearance of the `style` variable levels\n    otherwise they are determined from the data. Not relevant when the\n    `style` variable is numeric.\nestimator : name of pandas method or callable or None\n    Method for aggregating across multiple observations of the `y`\n    variable at the same `x` level. If `None`, all observations will\n    be drawn.\nerrorbar : string, (string, number) tuple, or callable\n    Name of errorbar method (either \"ci\", \"pi\", \"se\", or \"sd\"), or a tuple\n    with a method name and a level parameter, or a function that maps from a\n    vector to a (min, max) interval, or None to hide errorbar. See the\n    :doc:`errorbar tutorial </tutorial/error_bars>` for more information.\nn_boot : int\n    Number of bootstraps to use for computing the confidence interval.\nseed : int, numpy.random.Generator, or numpy.random.RandomState\n    Seed or random number generator for reproducible bootstrapping.\norient : \"x\" or \"y\"\n    Dimension along which the data are sorted / aggregated. Equivalently,\n    the \"independent variable\" of the resulting function.\nsort : boolean\n    If True, the data will be sorted by the x and y variables, otherwise\n    lines will connect points in the order they appear in the dataset.\nerr_style : \"band\" or \"bars\"\n    Whether to draw the confidence intervals with translucent error bands\n    or discrete error bars.\nerr_kws : dict of keyword arguments\n    Additional parameters to control the aesthetics of the error bars. The\n    kwargs are passed either to :meth:`matplotlib.axes.Axes.fill_between`\n    or :meth:`matplotlib.axes.Axes.errorbar`, depending on `err_style`.\nlegend : \"auto\", \"brief\", \"full\", or False\n    How to draw the legend. If \"brief\", numeric `hue` and `size`\n    variables will be represented with a sample of evenly spaced values.\n    If \"full\", every group will get an entry in the legend. If \"auto\",\n    choose between brief or full representation based on number of levels.\n    If `False`, no legend data is added and no legend is drawn.\nci : int or \"sd\" or None\n    Size of the confidence interval to draw when aggregating.\n\n    .. deprecated:: 0.12.0\n        Use the new `errorbar` parameter for more flexibility.\n\nax : :class:`matplotlib.axes.Axes`\n    Pre-existing axes for the plot. Otherwise, call :func:`matplotlib.pyplot.gca`\n    internally.\nkwargs : key, value mappings\n    Other keyword arguments are passed down to\n    :meth:`matplotlib.axes.Axes.plot`.\n\nReturns\n-------\n:class:`matplotlib.axes.Axes`\n    The matplotlib axes containing the plot.\n\nSee Also\n--------\nscatterplot : Plot data using points.\npointplot : Plot point estimates and CIs using markers and lines.\n\nExamples\n--------\n\n.. include:: ../docstrings/lineplot.rst", "parameters": {"type": "object", "properties": {"data": {"type": "NoneType", "default": null}, "x": {"type": "NoneType", "default": null}, "y": {"type": "NoneType", "default": null}, "hue": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/503", "data": {"name": "datetime.datetime", "type": "class", "signature": "(self, /, *args, **kwargs)", "description": "datetime(year, month, day[, hour[, minute[, second[, microsecond[,tzinfo]]]]])\n\nThe year, month and day arguments are required. tzinfo may be None, or an\ninstance of a tzinfo subclass. The remaining arguments may be ints.", "parameters": {"type": "object", "properties": {"/": {}}}}}
{"task_id": "BigCodeBench/503", "data": {"name": "datetime.datetime.now", "type": "callable", "signature": "()", "description": "Returns new datetime object representing current time local to tz.\n\n  tz\n    Timezone object.\n\nIf no tz is specified, uses local timezone.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/503", "data": {"name": "numpy.random.seed", "type": "callable", "signature": "(*args, **kwargs)", "description": "seed(self, seed=None)\n\nReseed a legacy MT19937 BitGenerator\n\nNotes\n-----\nThis is a convenience, legacy function.\n\nThe best practice is to **not** reseed a BitGenerator, rather to\nrecreate a new one. This method is here for legacy reasons.\nThis example demonstrates best practice.\n\n>>> from numpy.random import MT19937\n>>> from numpy.random import RandomState, SeedSequence\n>>> rs = RandomState(MT19937(SeedSequence(123456789)))\n# Later, you want to restart the stream\n>>> rs = RandomState(MT19937(SeedSequence(987654321)))", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/503", "data": {"name": "numpy.random.rand", "type": "callable", "signature": "(*args, **kwargs)", "description": "rand(d0, d1, ..., dn)\n\nRandom values in a given shape.\n\n.. note::\n    This is a convenience function for users porting code from Matlab,\n    and wraps `random_sample`. That function takes a\n    tuple to specify the size of the output, which is consistent with\n    other NumPy functions like `numpy.zeros` and `numpy.ones`.\n\nCreate an array of the given shape and populate it with\nrandom samples from a uniform distribution\nover ``[0, 1)``.\n\nParameters\n----------\nd0, d1, ..., dn : int, optional\n    The dimensions of the returned array, must be non-negative.\n    If no argument is given a single Python float is returned.\n\nReturns\n-------\nout : ndarray, shape ``(d0, d1, ..., dn)``\n    Random values.\n\nSee Also\n--------\nrandom\n\nExamples\n--------\n>>> np.random.rand(3,2)\narray([[ 0.14022471,  0.96360618],  #random\n       [ 0.37601032,  0.25528411],  #random\n       [ 0.49313049,  0.94909878]]) #random", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/503", "data": {"name": "pandas.date_range", "type": "callable", "signature": "(end=None, periods=None, **kwargs) -> 'DatetimeIndex)", "description": "Return a fixed frequency DatetimeIndex.\n\nReturns the range of equally spaced time points (where the difference between any\ntwo adjacent points is specified by the given frequency) such that they all\nsatisfy `start <[=] x <[=] end`, where the first one and the last one are, resp.,\nthe first and last time points in that range that fall on the boundary of ``freq``\n(if given as a frequency string) or that are valid for ``freq`` (if given as a\n:class:`pandas.tseries.offsets.DateOffset`). (If exactly one of ``start``,\n``end``, or ``freq`` is *not* specified, this missing parameter can be computed\ngiven ``periods``, the number of timesteps in the range. See the note below.)\n\nParameters\n----------\nstart : str or datetime-like, optional\n    Left bound for generating dates.\nend : str or datetime-like, optional\n    Right bound for generating dates.\nperiods : int, optional\n    Number of periods to generate.\nfreq : str, Timedelta, datetime.timedelta, or DateOffset, default 'D'\n    Frequency strings can have multiples, e.g. '5h'. See\n    :ref:`here <timeseries.offset_aliases>` for a list of\n    frequency aliases.\ntz : str or tzinfo, optional\n    Time zone name for returning localized DatetimeIndex, for example\n    'Asia/Hong_Kong'. By default, the resulting DatetimeIndex is\n    timezone-naive unless timezone-aware datetime-likes are passed.\nnormalize : bool, default False\n    Normalize start/end dates to midnight before generating date range.\nname : str, default None\n    Name of the resulting DatetimeIndex.\ninclusive : {\"both\", \"neither\", \"left\", \"right\"}, default \"both\"\n    Include boundaries; Whether to set each bound as closed or open.\n\n    .. versionadded:: 1.4.0\nunit : str, default None\n    Specify the desired resolution of the result.\n\n    .. versionadded:: 2.0.0\n**kwargs\n    For compatibility. Has no effect on the result.\n\nReturns\n-------\nDatetimeIndex\n\nSee Also\n--------\nDatetimeIndex : An immutable container for datetimes.\ntimedelta_range : Return a fixed frequency TimedeltaIndex.\nperiod_range : Return a fixed frequency PeriodIndex.\ninterval_range : Return a fixed frequency IntervalIndex.\n\nNotes\n-----\nOf the four parameters ``start``, ``end``, ``periods``, and ``freq``,\nexactly three must be specified. If ``freq`` is omitted, the resulting\n``DatetimeIndex`` will have ``periods`` linearly spaced elements between\n``start`` and ``end`` (closed on both sides).\n\nTo learn more about the frequency strings, please see `this link\n<https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases>`__.\n\nExamples\n--------\n**Specifying the values**\n\nThe next four examples generate the same `DatetimeIndex`, but vary\nthe combination of `start`, `end` and `periods`.\n\nSpecify `start` and `end`, with the default daily frequency.\n\n>>> pd.date_range(start='1/1/2018', end='1/08/2018')\nDatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03', '2018-01-04',\n               '2018-01-05', '2018-01-06', '2018-01-07', '2018-01-08'],\n              dtype='datetime64[ns]', freq='D')\n\nSpecify timezone-aware `start` and `end`, with the default daily frequency.\n\n>>> pd.date_range(\n...     start=pd.to_datetime(\"1/1/2018\").tz_localize(\"Europe/Berlin\"),\n...     end=pd.to_datetime(\"1/08/2018\").tz_localize(\"Europe/Berlin\"),\n... )\nDatetimeIndex(['2018-01-01 00:00:00+01:00', '2018-01-02 00:00:00+01:00',\n               '2018-01-03 00:00:00+01:00', '2018-01-04 00:00:00+01:00',\n               '2018-01-05 00:00:00+01:00', '2018-01-06 00:00:00+01:00',\n               '2018-01-07 00:00:00+01:00', '2018-01-08 00:00:00+01:00'],\n              dtype='datetime64[ns, Europe/Berlin]', freq='D')\n\nSpecify `start` and `periods`, the number of periods (days).\n\n>>> pd.date_range(start='1/1/2018', periods=8)\nDatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03', '2018-01-04',\n               '2018-01-05', '2018-01-06', '2018-01-07', '2018-01-08'],\n              dtype='datetime64[ns]', freq='D')\n\nSpecify `end` and `periods`, the number of periods (days).\n\n>>> pd.date_range(end='1/1/2018', periods=8)\nDatetimeIndex(['2017-12-25', '2017-12-26', '2017-12-27', '2017-12-28',\n               '2017-12-29', '2017-12-30', '2017-12-31', '2018-01-01'],\n              dtype='datetime64[ns]', freq='D')\n\nSpecify `start`, `end`, and `periods`; the frequency is generated\nautomatically (linearly spaced).\n\n>>> pd.date_range(start='2018-04-24', end='2018-04-27', periods=3)\nDatetimeIndex(['2018-04-24 00:00:00', '2018-04-25 12:00:00',\n               '2018-04-27 00:00:00'],\n              dtype='datetime64[ns]', freq=None)\n\n**Other Parameters**\n\nChanged the `freq` (frequency) to ``'ME'`` (month end frequency).\n\n>>> pd.date_range(start='1/1/2018', periods=5, freq='ME')\nDatetimeIndex(['2018-01-31', '2018-02-28', '2018-03-31', '2018-04-30',\n               '2018-05-31'],\n              dtype='datetime64[ns]', freq='ME')\n\nMultiples are allowed\n\n>>> pd.date_range(start='1/1/2018', periods=5, freq='3ME')\nDatetimeIndex(['2018-01-31', '2018-04-30', '2018-07-31', '2018-10-31',\n               '2019-01-31'],\n              dtype='datetime64[ns]', freq='3ME')\n\n`freq` can also be specified as an Offset object.\n\n>>> pd.date_range(start='1/1/2018', periods=5, freq=pd.offsets.MonthEnd(3))\nDatetimeIndex(['2018-01-31', '2018-04-30', '2018-07-31', '2018-10-31',\n               '2019-01-31'],\n              dtype='datetime64[ns]', freq='3ME')\n\nSpecify `tz` to set the timezone.\n\n>>> pd.date_range(start='1/1/2018', periods=5, tz='Asia/Tokyo')\nDatetimeIndex(['2018-01-01 00:00:00+09:00', '2018-01-02 00:00:00+09:00',\n               '2018-01-03 00:00:00+09:00', '2018-01-04 00:00:00+09:00',\n               '2018-01-05 00:00:00+09:00'],\n              dtype='datetime64[ns, Asia/Tokyo]', freq='D')\n\n`inclusive` controls whether to include `start` and `end` that are on the\nboundary. The default, \"both\", includes boundary points on either end.\n\n>>> pd.date_range(start='2017-01-01', end='2017-01-04', inclusive=\"both\")\nDatetimeIndex(['2017-01-01', '2017-01-02', '2017-01-03', '2017-01-04'],\n              dtype='datetime64[ns]', freq='D')\n\nUse ``inclusive='left'`` to exclude `end` if it falls on the boundary.\n\n>>> pd.date_range(start='2017-01-01', end='2017-01-04', inclusive='left')\nDatetimeIndex(['2017-01-01', '2017-01-02', '2017-01-03'],\n              dtype='datetime64[ns]', freq='D')\n\nUse ``inclusive='right'`` to exclude `start` if it falls on the boundary, and\nsimilarly ``inclusive='neither'`` will exclude both `start` and `end`.\n\n>>> pd.date_range(start='2017-01-01', end='2017-01-04', inclusive='right')\nDatetimeIndex(['2017-01-02', '2017-01-03', '2017-01-04'],\n              dtype='datetime64[ns]', freq='D')\n\n**Specify a unit**\n\n>>> pd.date_range(start=\"2017-01-01\", periods=10, freq=\"100YS\", unit=\"s\")\nDatetimeIndex(['2017-01-01', '2117-01-01', '2217-01-01', '2317-01-01',\n               '2417-01-01', '2517-01-01', '2617-01-01', '2717-01-01',\n               '2817-01-01', '2917-01-01'],\n              dtype='datetime64[s]', freq='100YS-JAN')", "parameters": {"type": "object", "properties": {"end": {"type": "NoneType", "default": null}, "periods": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/509", "data": {"name": "csv.reader", "type": "callable", "signature": "(*args, **kwargs)", "description": "csv_reader = reader(iterable [, dialect='excel']\n                        [optional keyword args])\n    for row in csv_reader:\n        process(row)\n\nThe \"iterable\" argument can be any object that returns a line\nof input for each iteration, such as a file object or a list.  The\noptional \"dialect\" parameter is discussed below.  The function\nalso accepts optional keyword arguments which override settings\nprovided by the dialect.\n\nThe returned object is an iterator.  Each iteration returns a row\nof the CSV file (which can span multiple input lines).", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/509", "data": {"name": "data.append([i + 1, status, content])"}}
{"task_id": "BigCodeBench/509", "data": {"name": "difflib.ndiff", "type": "callable", "signature": "(a, b)", "description": "Compare `a` and `b` (lists of strings); return a `Differ`-style delta.\n\nOptional keyword parameters `linejunk` and `charjunk` are for filter\nfunctions, or can be None:\n\n- linejunk: A function that should accept a single string argument and\n  return true iff the string is junk.  The default is None, and is\n  recommended; the underlying SequenceMatcher class has an adaptive\n  notion of \"noise\" lines.\n\n- charjunk: A function that accepts a character (string of length\n  1), and returns true iff the character is junk. The default is\n  the module-level function IS_CHARACTER_JUNK, which filters out\n  whitespace characters (a blank or tab; note: it's a bad idea to\n  include newline in this!).\n\nTools/scripts/ndiff.py is a command-line front-end to this function.\n\nExample:\n\n>>> diff = ndiff('one\\ntwo\\nthree\\n'.splitlines(keepends=True),\n...              'ore\\ntree\\nemu\\n'.splitlines(keepends=True))\n>>> print(''.join(diff), end=\"\")\n- one\n?  ^\n+ ore\n?  ^\n- two\n- three\n?  -\n+ tree\n+ emu", "parameters": {"type": "object", "properties": {"a": {}, "b": {}}}}}
{"task_id": "BigCodeBench/511", "data": {"name": "pandas.DataFrame", "type": "class", "signature": "(data=None, index: 'Axes | None' = None, columns: 'Axes | None' = None, dtype: 'Dtype | None' = None, copy: 'bool | None' = None) -> 'None'", "description": "Two-dimensional, size-mutable, potentially heterogeneous tabular data.\n\nData structure also contains labeled axes (rows and columns).\nArithmetic operations align on both row and column labels. Can be\nthought of as a dict-like container for Series objects. The primary\npandas data structure.\n\nParameters\n----------\ndata : ndarray (structured or homogeneous), Iterable, dict, or DataFrame\n    Dict can contain Series, arrays, constants, dataclass or list-like objects. If\n    data is a dict, column order follows insertion-order. If a dict contains Series\n    which have an index defined, it is aligned by its index. This alignment also\n    occurs if data is a Series or a DataFrame itself. Alignment is done on\n    Series/DataFrame inputs.\n\n    If data is a list of dicts, column order follows insertion-order.\n\nindex : Index or array-like\n    Index to use for resulting frame. Will default to RangeIndex if\n    no indexing information part of input data and no index provided.\ncolumns : Index or array-like\n    Column labels to use for resulting frame when data does not have them,\n    defaulting to RangeIndex(0, 1, 2, ..., n). If data contains column labels,\n    will perform column selection instead.\ndtype : dtype, default None\n    Data type to force. Only a single dtype is allowed. If None, infer.\ncopy : bool or None, default None\n    Copy data from inputs.\n    For dict data, the default of None behaves like ``copy=True``.  For DataFrame\n    or 2d ndarray input, the default of None behaves like ``copy=False``.\n    If data is a dict containing one or more Series (possibly of different dtypes),\n    ``copy=False`` will ensure that these inputs are not copied.\n\n    .. versionchanged:: 1.3.0\n\nSee Also\n--------\nDataFrame.from_records : Constructor from tuples, also record arrays.\nDataFrame.from_dict : From dicts of Series, arrays, or dicts.\nread_csv : Read a comma-separated values (csv) file into DataFrame.\nread_table : Read general delimited file into DataFrame.\nread_clipboard : Read text from clipboard into DataFrame.\n\nNotes\n-----\nPlease reference the :ref:`User Guide <basics.dataframe>` for more information.\n\nExamples\n--------\nConstructing DataFrame from a dictionary.\n\n>>> d = {'col1': [1, 2], 'col2': [3, 4]}\n>>> df = pd.DataFrame(data=d)\n>>> df\n   col1  col2\n0     1     3\n1     2     4\n\nNotice that the inferred dtype is int64.\n\n>>> df.dtypes\ncol1    int64\ncol2    int64\ndtype: object\n\nTo enforce a single dtype:\n\n>>> df = pd.DataFrame(data=d, dtype=np.int8)\n>>> df.dtypes\ncol1    int8\ncol2    int8\ndtype: object\n\nConstructing DataFrame from a dictionary including Series:\n\n>>> d = {'col1': [0, 1, 2, 3], 'col2': pd.Series([2, 3], index=[2, 3])}\n>>> pd.DataFrame(data=d, index=[0, 1, 2, 3])\n   col1  col2\n0     0   NaN\n1     1   NaN\n2     2   2.0\n3     3   3.0\n\nConstructing DataFrame from numpy ndarray:\n\n>>> df2 = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),\n...                    columns=['a', 'b', 'c'])\n>>> df2\n   a  b  c\n0  1  2  3\n1  4  5  6\n2  7  8  9\n\nConstructing DataFrame from a numpy ndarray that has labeled columns:\n\n>>> data = np.array([(1, 2, 3), (4, 5, 6), (7, 8, 9)],\n...                 dtype=[(\"a\", \"i4\"), (\"b\", \"i4\"), (\"c\", \"i4\")])\n>>> df3 = pd.DataFrame(data, columns=['c', 'a'])\n...\n>>> df3\n   c  a\n0  3  1\n1  6  4\n2  9  7\n\nConstructing DataFrame from dataclass:\n\n>>> from dataclasses import make_dataclass\n>>> Point = make_dataclass(\"Point\", [(\"x\", int), (\"y\", int)])\n>>> pd.DataFrame([Point(0, 0), Point(0, 3), Point(2, 3)])\n   x  y\n0  0  0\n1  0  3\n2  2  3\n\nConstructing DataFrame from Series/DataFrame:\n\n>>> ser = pd.Series([1, 2, 3], index=[\"a\", \"b\", \"c\"])\n>>> df = pd.DataFrame(data=ser, index=[\"a\", \"c\"])\n>>> df\n   0\na  1\nc  3\n\n>>> df1 = pd.DataFrame([1, 2, 3], index=[\"a\", \"b\", \"c\"], columns=[\"x\"])\n>>> df2 = pd.DataFrame(data=df1, index=[\"a\", \"c\"])\n>>> df2\n   x\na  1\nc  3", "parameters": {"type": "object", "properties": {"data": {"type": "NoneType", "default": null}, "index": {"type": ["axes", "null"], "default": null}, "columns": {"type": ["axes", "null"], "default": null}, "dtype": {"type": ["dtype", "null"], "default": null}, "copy": {"type": ["bool", "null"], "default": null}}}}}
{"task_id": "BigCodeBench/511", "data": {"name": "pandas.DataFrame.empty", "type": "constant", "signature": null, "description": "Indicator whether Series/DataFrame is empty.\n\nTrue if Series/DataFrame is entirely empty (no items), meaning any of the\naxes are of length 0.\n\nReturns\n-------\nbool\n    If Series/DataFrame is empty, return True, if not return False.\n\nSee Also\n--------\nSeries.dropna : Return series without null values.\nDataFrame.dropna : Return DataFrame with labels on given axis omitted\n    where (all or any) data are missing.\n\nNotes\n-----\nIf Series/DataFrame contains only NaNs, it is still not considered empty. See\nthe example below.\n\nExamples\n--------\nAn example of an actual empty DataFrame. Notice the index is empty:\n\n>>> df_empty = pd.DataFrame({'A' : []})\n>>> df_empty\nEmpty DataFrame\nColumns: [A]\nIndex: []\n>>> df_empty.empty\nTrue\n\nIf we only have NaNs in our DataFrame, it is not considered empty! We\nwill need to drop the NaNs to make the DataFrame empty:\n\n>>> df = pd.DataFrame({'A' : [np.nan]})\n>>> df\n    A\n0 NaN\n>>> df.empty\nFalse\n>>> df.dropna().empty\nTrue\n\n>>> ser_empty = pd.Series({'A' : []})\n>>> ser_empty\nA    []\ndtype: object\n>>> ser_empty.empty\nFalse\n>>> ser_empty = pd.Series()\n>>> ser_empty.empty\nTrue", "value": "<property object at 0x7fa4c5615210>", "parameters": {"type": "constant"}}}
{"task_id": "BigCodeBench/511", "data": {"name": "matplotlib.pyplot.subplots", "type": "class", "signature": "()", "description": "Create a figure and a set of subplots.\n\nThis utility wrapper makes it convenient to create common layouts of\nsubplots, including the enclosing figure object, in a single call.\n\nParameters\n----------\nnrows, ncols : int, default: 1\n    Number of rows/columns of the subplot grid.\n\nsharex, sharey : bool or {'none', 'all', 'row', 'col'}, default: False\n    Controls sharing of properties among x (*sharex*) or y (*sharey*)\n    axes:\n\n    - True or 'all': x- or y-axis will be shared among all subplots.\n    - False or 'none': each subplot x- or y-axis will be independent.\n    - 'row': each subplot row will share an x- or y-axis.\n    - 'col': each subplot column will share an x- or y-axis.\n\n    When subplots have a shared x-axis along a column, only the x tick\n    labels of the bottom subplot are created. Similarly, when subplots\n    have a shared y-axis along a row, only the y tick labels of the first\n    column subplot are created. To later turn other subplots' ticklabels\n    on, use `~matplotlib.axes.Axes.tick_params`.\n\n    When subplots have a shared axis that has units, calling\n    `~matplotlib.axis.Axis.set_units` will update each axis with the\n    new units.\n\nsqueeze : bool, default: True\n    - If True, extra dimensions are squeezed out from the returned\n      array of `~matplotlib.axes.Axes`:\n\n      - if only one subplot is constructed (nrows=ncols=1), the\n        resulting single Axes object is returned as a scalar.\n      - for Nx1 or 1xM subplots, the returned object is a 1D numpy\n        object array of Axes objects.\n      - for NxM, subplots with N>1 and M>1 are returned as a 2D array.\n\n    - If False, no squeezing at all is done: the returned Axes object is\n      always a 2D array containing Axes instances, even if it ends up\n      being 1x1.\n\nwidth_ratios : array-like of length *ncols*, optional\n    Defines the relative widths of the columns. Each column gets a\n    relative width of ``width_ratios[i] / sum(width_ratios)``.\n    If not given, all columns will have the same width.  Equivalent\n    to ``gridspec_kw={'width_ratios': [...]}``.\n\nheight_ratios : array-like of length *nrows*, optional\n    Defines the relative heights of the rows. Each row gets a\n    relative height of ``height_ratios[i] / sum(height_ratios)``.\n    If not given, all rows will have the same height. Convenience\n    for ``gridspec_kw={'height_ratios': [...]}``.\n\nsubplot_kw : dict, optional\n    Dict with keywords passed to the\n    `~matplotlib.figure.Figure.add_subplot` call used to create each\n    subplot.\n\ngridspec_kw : dict, optional\n    Dict with keywords passed to the `~matplotlib.gridspec.GridSpec`\n    constructor used to create the grid the subplots are placed on.\n\n**fig_kw\n    All additional keyword arguments are passed to the\n    `.pyplot.figure` call.\n\nReturns\n-------\nfig : `.Figure`\n\nax : `~matplotlib.axes.Axes` or array of Axes\n    *ax* can be either a single `~.axes.Axes` object, or an array of Axes\n    objects if more than one subplot was created.  The dimensions of the\n    resulting array can be controlled with the squeeze keyword, see above.\n\n    Typical idioms for handling the return value are::\n\n        # using the variable ax for single a Axes\n        fig, ax = plt.subplots()\n\n        # using the variable axs for multiple Axes\n        fig, axs = plt.subplots(2, 2)\n\n        # using tuple unpacking for multiple Axes\n        fig, (ax1, ax2) = plt.subplots(1, 2)\n        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n\n    The names ``ax`` and pluralized ``axs`` are preferred over ``axes``\n    because for the latter it's not clear if it refers to a single\n    `~.axes.Axes` instance or a collection of these.\n\nSee Also\n--------\n.pyplot.figure\n.pyplot.subplot\n.pyplot.axes\n.Figure.subplots\n.Figure.add_subplot\n\nExamples\n--------\n::\n\n    # First create some toy data:\n    x = np.linspace(0, 2*np.pi, 400)\n    y = np.sin(x**2)\n\n    # Create just a figure and only one subplot\n    fig, ax = plt.subplots()\n    ax.plot(x, y)\n    ax.set_title('Simple plot')\n\n    # Create two subplots and unpack the output array immediately\n    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n    ax1.plot(x, y)\n    ax1.set_title('Sharing Y axis')\n    ax2.scatter(x, y)\n\n    # Create four polar axes and access them through the returned array\n    fig, axs = plt.subplots(2, 2, subplot_kw=dict(projection=\"polar\"))\n    axs[0, 0].plot(x, y)\n    axs[1, 1].scatter(x, y)\n\n    # Share a X axis with each column of subplots\n    plt.subplots(2, 2, sharex='col')\n\n    # Share a Y axis with each row of subplots\n    plt.subplots(2, 2, sharey='row')\n\n    # Share both X and Y axes with all subplots\n    plt.subplots(2, 2, sharex='all', sharey='all')\n\n    # Note that this is the same as\n    plt.subplots(2, 2, sharex=True, sharey=True)\n\n    # Create figure number 10 with a single subplot\n    # and clears it if it already exists.\n    fig, ax = plt.subplots(num=10, clear=True)", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/511", "data": {"name": "matplotlib.pyplot.subplots[1].set_title", "type": "method", "signature": "(label, fontdict=None, loc=None, pad=None, *, y=None, **kwargs)", "description": "Set a title for the Axes.\n\nSet one of the three available Axes titles. The available titles\nare positioned above the Axes in the center, flush with the left\nedge, and flush with the right edge.\n\nParameters\n----------\nlabel : str\n    Text to use for the title\n\nfontdict : dict\n\n    .. admonition:: Discouraged\n\n       The use of *fontdict* is discouraged. Parameters should be passed as\n       individual keyword arguments or using dictionary-unpacking\n       ``set_title(..., **fontdict)``.\n\n    A dictionary controlling the appearance of the title text,\n    the default *fontdict* is::\n\n       {'fontsize': rcParams['axes.titlesize'],\n        'fontweight': rcParams['axes.titleweight'],\n        'color': rcParams['axes.titlecolor'],\n        'verticalalignment': 'baseline',\n        'horizontalalignment': loc}\n\nloc : {'center', 'left', 'right'}, default: :rc:`axes.titlelocation`\n    Which title to set.\n\ny : float, default: :rc:`axes.titley`\n    Vertical Axes location for the title (1.0 is the top).  If\n    None (the default) and :rc:`axes.titley` is also None, y is\n    determined automatically to avoid decorators on the Axes.\n\npad : float, default: :rc:`axes.titlepad`\n    The offset of the title from the top of the Axes, in points.\n\nReturns\n-------\n`.Text`\n    The matplotlib text instance representing the title\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    Other keyword arguments are text properties, see `.Text` for a list\n    of valid text properties.", "parameters": {"type": "object", "properties": {"label": {}, "fontdict": {"type": "NoneType", "default": null}, "loc": {"type": "NoneType", "default": null}, "pad": {"type": "NoneType", "default": null}, "y": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/511", "data": {"name": "matplotlib.pyplot.subplots[1].pie", "type": "method", "signature": "(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, shadow=False, labeldistance=1.1, startangle=0, radius=1, counterclock=True, wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, *, normalize=True, hatch=None, data=None)", "description": "Plot a pie chart.\n\nMake a pie chart of array *x*.  The fractional area of each wedge is\ngiven by ``x/sum(x)``.\n\nThe wedges are plotted counterclockwise, by default starting from the\nx-axis.\n\nParameters\n----------\nx : 1D array-like\n    The wedge sizes.\n\nexplode : array-like, default: None\n    If not *None*, is a ``len(x)`` array which specifies the fraction\n    of the radius with which to offset each wedge.\n\nlabels : list, default: None\n    A sequence of strings providing the labels for each wedge\n\ncolors : color or array-like of color, default: None\n    A sequence of colors through which the pie chart will cycle.  If\n    *None*, will use the colors in the currently active cycle.\n\nhatch : str or list, default: None\n    Hatching pattern applied to all pie wedges or sequence of patterns\n    through which the chart will cycle. For a list of valid patterns,\n    see :doc:`/gallery/shapes_and_collections/hatch_style_reference`.\n\n    .. versionadded:: 3.7\n\nautopct : None or str or callable, default: None\n    If not *None*, *autopct* is a string or function used to label the\n    wedges with their numeric value. The label will be placed inside\n    the wedge. If *autopct* is a format string, the label will be\n    ``fmt % pct``. If *autopct* is a function, then it will be called.\n\npctdistance : float, default: 0.6\n    The relative distance along the radius at which the text\n    generated by *autopct* is drawn. To draw the text outside the pie,\n    set *pctdistance* > 1. This parameter is ignored if *autopct* is\n    ``None``.\n\nlabeldistance : float or None, default: 1.1\n    The relative distance along the radius at which the labels are\n    drawn. To draw the labels inside the pie, set  *labeldistance* < 1.\n    If set to ``None``, labels are not drawn but are still stored for\n    use in `.legend`.\n\nshadow : bool or dict, default: False\n    If bool, whether to draw a shadow beneath the pie. If dict, draw a shadow\n    passing the properties in the dict to `.Shadow`.\n\n    .. versionadded:: 3.8\n        *shadow* can be a dict.\n\nstartangle : float, default: 0 degrees\n    The angle by which the start of the pie is rotated,\n    counterclockwise from the x-axis.\n\nradius : float, default: 1\n    The radius of the pie.\n\ncounterclock : bool, default: True\n    Specify fractions direction, clockwise or counterclockwise.\n\nwedgeprops : dict, default: None\n    Dict of arguments passed to each `.patches.Wedge` of the pie.\n    For example, ``wedgeprops = {'linewidth': 3}`` sets the width of\n    the wedge border lines equal to 3. By default, ``clip_on=False``.\n    When there is a conflict between these properties and other\n    keywords, properties passed to *wedgeprops* take precedence.\n\ntextprops : dict, default: None\n    Dict of arguments to pass to the text objects.\n\ncenter : (float, float), default: (0, 0)\n    The coordinates of the center of the chart.\n\nframe : bool, default: False\n    Plot Axes frame with the chart if true.\n\nrotatelabels : bool, default: False\n    Rotate each label to the angle of the corresponding slice if true.\n\nnormalize : bool, default: True\n    When *True*, always make a full pie by normalizing x so that\n    ``sum(x) == 1``. *False* makes a partial pie if ``sum(x) <= 1``\n    and raises a `ValueError` for ``sum(x) > 1``.\n\ndata : indexable object, optional\n    If given, the following parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` (unless this raises an exception):\n\n    *x*, *explode*, *labels*, *colors*\n\nReturns\n-------\npatches : list\n    A sequence of `matplotlib.patches.Wedge` instances\n\ntexts : list\n    A list of the label `.Text` instances.\n\nautotexts : list\n    A list of `.Text` instances for the numeric labels. This will only\n    be returned if the parameter *autopct* is not *None*.\n\nNotes\n-----\nThe pie chart will probably look best if the figure and Axes are\nsquare, or the Axes aspect is equal.\nThis method sets the aspect ratio of the axis to \"equal\".\nThe Axes aspect ratio can be controlled with `.Axes.set_aspect`.", "parameters": {"type": "object", "properties": {"x": {}, "explode": {"type": "NoneType", "default": null}, "labels": {"type": "NoneType", "default": null}, "colors": {"type": "NoneType", "default": null}, "autopct": {"type": "NoneType", "default": null}, "pctdistance": {"type": "float", "default": 0.6}, "shadow": {"type": "bool", "default": false}, "labeldistance": {"type": "float", "default": 1.1}, "startangle": {"type": "integer", "default": 0}, "radius": {"type": "integer", "default": 1}, "counterclock": {"type": "bool", "default": true}, "wedgeprops": {"type": "NoneType", "default": null}, "textprops": {"type": "NoneType", "default": null}, "center": {"type": "str", "default": "(0"}, "0)": {}, "frame": {"type": "bool", "default": false}, "rotatelabels": {"type": "bool", "default": false}, "normalize": {"type": "bool", "default": true}, "hatch": {"type": "NoneType", "default": null}, "data": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/511", "data": {"name": "numpy.nan", "type": "constant", "signature": null, "description": "Convert a string or number to a floating point number, if possible.", "value": "nan", "parameters": {"type": "constant"}}}
{"task_id": "BigCodeBench/511", "data": {"name": "numpy.max", "type": "callable", "signature": "(a)", "description": "Return the maximum of an array or maximum along an axis.\n\nParameters\n----------\na : array_like\n    Input data.\naxis : None or int or tuple of ints, optional\n    Axis or axes along which to operate.  By default, flattened input is\n    used.\n\n    .. versionadded:: 1.7.0\n\n    If this is a tuple of ints, the maximum is selected over multiple axes,\n    instead of a single axis or all the axes as before.\nout : ndarray, optional\n    Alternative output array in which to place the result.  Must\n    be of the same shape and buffer length as the expected output.\n    See :ref:`ufuncs-output-type` for more details.\n\nkeepdims : bool, optional\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\n    If the default value is passed, then `keepdims` will not be\n    passed through to the `amax` method of sub-classes of\n    `ndarray`, however any non-default value will be.  If the\n    sub-class' method does not implement `keepdims` any\n    exceptions will be raised.\n\ninitial : scalar, optional\n    The minimum value of an output element. Must be present to allow\n    computation on empty slice. See `~numpy.ufunc.reduce` for details.\n\n    .. versionadded:: 1.15.0\n\nwhere : array_like of bool, optional\n    Elements to compare for the maximum. See `~numpy.ufunc.reduce`\n    for details.\n\n    .. versionadded:: 1.17.0\n\nReturns\n-------\namax : ndarray or scalar\n    Maximum of `a`. If `axis` is None, the result is a scalar value.\n    If `axis` is given, the result is an array of dimension\n    ``a.ndim - 1``.\n\nSee Also\n--------\namin :\n    The minimum value of an array along a given axis, propagating any NaNs.\nnanmax :\n    The maximum value of an array along a given axis, ignoring any NaNs.\nmaximum :\n    Element-wise maximum of two arrays, propagating any NaNs.\nfmax :\n    Element-wise maximum of two arrays, ignoring any NaNs.\nargmax :\n    Return the indices of the maximum values.\n\nnanmin, minimum, fmin\n\nNotes\n-----\nNaN values are propagated, that is if at least one item is NaN, the\ncorresponding max value will be NaN as well. To ignore NaN values\n(MATLAB behavior), please use nanmax.\n\nDon't use `amax` for element-wise comparison of 2 arrays; when\n``a.shape[0]`` is 2, ``maximum(a[0], a[1])`` is faster than\n``amax(a, axis=0)``.\n\nExamples\n--------\n>>> a = np.arange(4).reshape((2,2))\n>>> a\narray([[0, 1],\n       [2, 3]])\n>>> np.amax(a)           # Maximum of the flattened array\n3\n>>> np.amax(a, axis=0)   # Maxima along the first axis\narray([2, 3])\n>>> np.amax(a, axis=1)   # Maxima along the second axis\narray([1, 3])\n>>> np.amax(a, where=[False, True], initial=-1, axis=0)\narray([-1,  3])\n>>> b = np.arange(5, dtype=float)\n>>> b[2] = np.NaN\n>>> np.amax(b)\nnan\n>>> np.amax(b, where=~np.isnan(b), initial=-1)\n4.0\n>>> np.nanmax(b)\n4.0\n\nYou can use an initial value to compute the maximum of an empty slice, or\nto initialize it to a different value:\n\n>>> np.amax([[-50], [10]], axis=-1, initial=0)\narray([ 0, 10])\n\nNotice that the initial value is used as one of the elements for which the\nmaximum is determined, unlike for the default argument Python's max\nfunction, which is only used for empty iterables.\n\n>>> np.amax([5], initial=6)\n6\n>>> max([5], default=6)\n5", "parameters": {"type": "object", "properties": {"a": {}}}}}
{"task_id": "BigCodeBench/511", "data": {"name": "numpy.min", "type": "callable", "signature": "(a)", "description": "Return the minimum of an array or minimum along an axis.\n\nParameters\n----------\na : array_like\n    Input data.\naxis : None or int or tuple of ints, optional\n    Axis or axes along which to operate.  By default, flattened input is\n    used.\n\n    .. versionadded:: 1.7.0\n\n    If this is a tuple of ints, the minimum is selected over multiple axes,\n    instead of a single axis or all the axes as before.\nout : ndarray, optional\n    Alternative output array in which to place the result.  Must\n    be of the same shape and buffer length as the expected output.\n    See :ref:`ufuncs-output-type` for more details.\n\nkeepdims : bool, optional\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\n    If the default value is passed, then `keepdims` will not be\n    passed through to the `amin` method of sub-classes of\n    `ndarray`, however any non-default value will be.  If the\n    sub-class' method does not implement `keepdims` any\n    exceptions will be raised.\n\ninitial : scalar, optional\n    The maximum value of an output element. Must be present to allow\n    computation on empty slice. See `~numpy.ufunc.reduce` for details.\n\n    .. versionadded:: 1.15.0\n\nwhere : array_like of bool, optional\n    Elements to compare for the minimum. See `~numpy.ufunc.reduce`\n    for details.\n\n    .. versionadded:: 1.17.0\n\nReturns\n-------\namin : ndarray or scalar\n    Minimum of `a`. If `axis` is None, the result is a scalar value.\n    If `axis` is given, the result is an array of dimension\n    ``a.ndim - 1``.\n\nSee Also\n--------\namax :\n    The maximum value of an array along a given axis, propagating any NaNs.\nnanmin :\n    The minimum value of an array along a given axis, ignoring any NaNs.\nminimum :\n    Element-wise minimum of two arrays, propagating any NaNs.\nfmin :\n    Element-wise minimum of two arrays, ignoring any NaNs.\nargmin :\n    Return the indices of the minimum values.\n\nnanmax, maximum, fmax\n\nNotes\n-----\nNaN values are propagated, that is if at least one item is NaN, the\ncorresponding min value will be NaN as well. To ignore NaN values\n(MATLAB behavior), please use nanmin.\n\nDon't use `amin` for element-wise comparison of 2 arrays; when\n``a.shape[0]`` is 2, ``minimum(a[0], a[1])`` is faster than\n``amin(a, axis=0)``.\n\nExamples\n--------\n>>> a = np.arange(4).reshape((2,2))\n>>> a\narray([[0, 1],\n       [2, 3]])\n>>> np.amin(a)           # Minimum of the flattened array\n0\n>>> np.amin(a, axis=0)   # Minima along the first axis\narray([0, 1])\n>>> np.amin(a, axis=1)   # Minima along the second axis\narray([0, 2])\n>>> np.amin(a, where=[False, True], initial=10, axis=0)\narray([10,  1])\n\n>>> b = np.arange(5, dtype=float)\n>>> b[2] = np.NaN\n>>> np.amin(b)\nnan\n>>> np.amin(b, where=~np.isnan(b), initial=10)\n0.0\n>>> np.nanmin(b)\n0.0\n\n>>> np.amin([[-50], [10]], axis=-1, initial=0)\narray([-50,   0])\n\nNotice that the initial value is used as one of the elements for which the\nminimum is determined, unlike for the default argument Python's max\nfunction, which is only used for empty iterables.\n\nNotice that this isn't the same as Python's ``default`` argument.\n\n>>> np.amin([6], initial=5)\n5\n>>> min([6], default=5)\n6", "parameters": {"type": "object", "properties": {"a": {}}}}}
{"task_id": "BigCodeBench/511", "data": {"name": "numpy.sum", "type": "callable", "signature": "(a)", "description": "Sum of array elements over a given axis.\n\nParameters\n----------\na : array_like\n    Elements to sum.\naxis : None or int or tuple of ints, optional\n    Axis or axes along which a sum is performed.  The default,\n    axis=None, will sum all of the elements of the input array.  If\n    axis is negative it counts from the last to the first axis.\n\n    .. versionadded:: 1.7.0\n\n    If axis is a tuple of ints, a sum is performed on all of the axes\n    specified in the tuple instead of a single axis or all the axes as\n    before.\ndtype : dtype, optional\n    The type of the returned array and of the accumulator in which the\n    elements are summed.  The dtype of `a` is used by default unless `a`\n    has an integer dtype of less precision than the default platform\n    integer.  In that case, if `a` is signed then the platform integer\n    is used while if `a` is unsigned then an unsigned integer of the\n    same precision as the platform integer is used.\nout : ndarray, optional\n    Alternative output array in which to place the result. It must have\n    the same shape as the expected output, but the type of the output\n    values will be cast if necessary.\nkeepdims : bool, optional\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\n    If the default value is passed, then `keepdims` will not be\n    passed through to the `sum` method of sub-classes of\n    `ndarray`, however any non-default value will be.  If the\n    sub-class' method does not implement `keepdims` any\n    exceptions will be raised.\ninitial : scalar, optional\n    Starting value for the sum. See `~numpy.ufunc.reduce` for details.\n\n    .. versionadded:: 1.15.0\n\nwhere : array_like of bool, optional\n    Elements to include in the sum. See `~numpy.ufunc.reduce` for details.\n\n    .. versionadded:: 1.17.0\n\nReturns\n-------\nsum_along_axis : ndarray\n    An array with the same shape as `a`, with the specified\n    axis removed.   If `a` is a 0-d array, or if `axis` is None, a scalar\n    is returned.  If an output array is specified, a reference to\n    `out` is returned.\n\nSee Also\n--------\nndarray.sum : Equivalent method.\n\nadd.reduce : Equivalent functionality of `add`.\n\ncumsum : Cumulative sum of array elements.\n\ntrapz : Integration of array values using the composite trapezoidal rule.\n\nmean, average\n\nNotes\n-----\nArithmetic is modular when using integer types, and no error is\nraised on overflow.\n\nThe sum of an empty array is the neutral element 0:\n\n>>> np.sum([])\n0.0\n\nFor floating point numbers the numerical precision of sum (and\n``np.add.reduce``) is in general limited by directly adding each number\nindividually to the result causing rounding errors in every step.\nHowever, often numpy will use a  numerically better approach (partial\npairwise summation) leading to improved precision in many use-cases.\nThis improved precision is always provided when no ``axis`` is given.\nWhen ``axis`` is given, it will depend on which axis is summed.\nTechnically, to provide the best speed possible, the improved precision\nis only used when the summation is along the fast axis in memory.\nNote that the exact precision may vary depending on other parameters.\nIn contrast to NumPy, Python's ``math.fsum`` function uses a slower but\nmore precise approach to summation.\nEspecially when summing a large number of lower precision floating point\nnumbers, such as ``float32``, numerical errors can become significant.\nIn such cases it can be advisable to use `dtype=\"float64\"` to use a higher\nprecision for the output.\n\nExamples\n--------\n>>> np.sum([0.5, 1.5])\n2.0\n>>> np.sum([0.5, 0.7, 0.2, 1.5], dtype=np.int32)\n1\n>>> np.sum([[0, 1], [0, 5]])\n6\n>>> np.sum([[0, 1], [0, 5]], axis=0)\narray([0, 6])\n>>> np.sum([[0, 1], [0, 5]], axis=1)\narray([1, 5])\n>>> np.sum([[0, 1], [np.nan, 5]], where=[False, True], axis=1)\narray([1., 5.])\n\nIf the accumulator is too small, overflow occurs:\n\n>>> np.ones(128, dtype=np.int8).sum(dtype=np.int8)\n-128\n\nYou can also start the sum with a value other than zero:\n\n>>> np.sum([10], initial=5)\n15", "parameters": {"type": "object", "properties": {"a": {}}}}}
{"task_id": "BigCodeBench/511", "data": {"name": "numpy.mean", "type": "callable", "signature": "(a)", "description": "Compute the arithmetic mean along the specified axis.\n\nReturns the average of the array elements.  The average is taken over\nthe flattened array by default, otherwise over the specified axis.\n`float64` intermediate and return values are used for integer inputs.\n\nParameters\n----------\na : array_like\n    Array containing numbers whose mean is desired. If `a` is not an\n    array, a conversion is attempted.\naxis : None or int or tuple of ints, optional\n    Axis or axes along which the means are computed. The default is to\n    compute the mean of the flattened array.\n\n    .. versionadded:: 1.7.0\n\n    If this is a tuple of ints, a mean is performed over multiple axes,\n    instead of a single axis or all the axes as before.\ndtype : data-type, optional\n    Type to use in computing the mean.  For integer inputs, the default\n    is `float64`; for floating point inputs, it is the same as the\n    input dtype.\nout : ndarray, optional\n    Alternate output array in which to place the result.  The default\n    is ``None``; if provided, it must have the same shape as the\n    expected output, but the type will be cast if necessary.\n    See :ref:`ufuncs-output-type` for more details.\n\nkeepdims : bool, optional\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\n    If the default value is passed, then `keepdims` will not be\n    passed through to the `mean` method of sub-classes of\n    `ndarray`, however any non-default value will be.  If the\n    sub-class' method does not implement `keepdims` any\n    exceptions will be raised.\n\nwhere : array_like of bool, optional\n    Elements to include in the mean. See `~numpy.ufunc.reduce` for details.\n\n    .. versionadded:: 1.20.0\n\nReturns\n-------\nm : ndarray, see dtype parameter above\n    If `out=None`, returns a new array containing the mean values,\n    otherwise a reference to the output array is returned.\n\nSee Also\n--------\naverage : Weighted average\nstd, var, nanmean, nanstd, nanvar\n\nNotes\n-----\nThe arithmetic mean is the sum of the elements along the axis divided\nby the number of elements.\n\nNote that for floating-point input, the mean is computed using the\nsame precision the input has.  Depending on the input data, this can\ncause the results to be inaccurate, especially for `float32` (see\nexample below).  Specifying a higher-precision accumulator using the\n`dtype` keyword can alleviate this issue.\n\nBy default, `float16` results are computed using `float32` intermediates\nfor extra precision.\n\nExamples\n--------\n>>> a = np.array([[1, 2], [3, 4]])\n>>> np.mean(a)\n2.5\n>>> np.mean(a, axis=0)\narray([2., 3.])\n>>> np.mean(a, axis=1)\narray([1.5, 3.5])\n\nIn single precision, `mean` can be inaccurate:\n\n>>> a = np.zeros((2, 512*512), dtype=np.float32)\n>>> a[0, :] = 1.0\n>>> a[1, :] = 0.1\n>>> np.mean(a)\n0.54999924\n\nComputing the mean in float64 is more accurate:\n\n>>> np.mean(a, dtype=np.float64)\n0.55000000074505806 # may vary\n\nSpecifying a where argument:\n\n>>> a = np.array([[5, 9, 13], [14, 10, 12], [11, 15, 19]])\n>>> np.mean(a)\n12.0\n>>> np.mean(a, where=[[True], [False], [False]])\n9.0", "parameters": {"type": "object", "properties": {"a": {}}}}}
{"task_id": "BigCodeBench/513", "data": {"name": "pandas.DataFrame", "type": "class", "signature": "(data=None, index: 'Axes | None' = None, columns: 'Axes | None' = None, dtype: 'Dtype | None' = None, copy: 'bool | None' = None) -> 'None'", "description": "Two-dimensional, size-mutable, potentially heterogeneous tabular data.\n\nData structure also contains labeled axes (rows and columns).\nArithmetic operations align on both row and column labels. Can be\nthought of as a dict-like container for Series objects. The primary\npandas data structure.\n\nParameters\n----------\ndata : ndarray (structured or homogeneous), Iterable, dict, or DataFrame\n    Dict can contain Series, arrays, constants, dataclass or list-like objects. If\n    data is a dict, column order follows insertion-order. If a dict contains Series\n    which have an index defined, it is aligned by its index. This alignment also\n    occurs if data is a Series or a DataFrame itself. Alignment is done on\n    Series/DataFrame inputs.\n\n    If data is a list of dicts, column order follows insertion-order.\n\nindex : Index or array-like\n    Index to use for resulting frame. Will default to RangeIndex if\n    no indexing information part of input data and no index provided.\ncolumns : Index or array-like\n    Column labels to use for resulting frame when data does not have them,\n    defaulting to RangeIndex(0, 1, 2, ..., n). If data contains column labels,\n    will perform column selection instead.\ndtype : dtype, default None\n    Data type to force. Only a single dtype is allowed. If None, infer.\ncopy : bool or None, default None\n    Copy data from inputs.\n    For dict data, the default of None behaves like ``copy=True``.  For DataFrame\n    or 2d ndarray input, the default of None behaves like ``copy=False``.\n    If data is a dict containing one or more Series (possibly of different dtypes),\n    ``copy=False`` will ensure that these inputs are not copied.\n\n    .. versionchanged:: 1.3.0\n\nSee Also\n--------\nDataFrame.from_records : Constructor from tuples, also record arrays.\nDataFrame.from_dict : From dicts of Series, arrays, or dicts.\nread_csv : Read a comma-separated values (csv) file into DataFrame.\nread_table : Read general delimited file into DataFrame.\nread_clipboard : Read text from clipboard into DataFrame.\n\nNotes\n-----\nPlease reference the :ref:`User Guide <basics.dataframe>` for more information.\n\nExamples\n--------\nConstructing DataFrame from a dictionary.\n\n>>> d = {'col1': [1, 2], 'col2': [3, 4]}\n>>> df = pd.DataFrame(data=d)\n>>> df\n   col1  col2\n0     1     3\n1     2     4\n\nNotice that the inferred dtype is int64.\n\n>>> df.dtypes\ncol1    int64\ncol2    int64\ndtype: object\n\nTo enforce a single dtype:\n\n>>> df = pd.DataFrame(data=d, dtype=np.int8)\n>>> df.dtypes\ncol1    int8\ncol2    int8\ndtype: object\n\nConstructing DataFrame from a dictionary including Series:\n\n>>> d = {'col1': [0, 1, 2, 3], 'col2': pd.Series([2, 3], index=[2, 3])}\n>>> pd.DataFrame(data=d, index=[0, 1, 2, 3])\n   col1  col2\n0     0   NaN\n1     1   NaN\n2     2   2.0\n3     3   3.0\n\nConstructing DataFrame from numpy ndarray:\n\n>>> df2 = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),\n...                    columns=['a', 'b', 'c'])\n>>> df2\n   a  b  c\n0  1  2  3\n1  4  5  6\n2  7  8  9\n\nConstructing DataFrame from a numpy ndarray that has labeled columns:\n\n>>> data = np.array([(1, 2, 3), (4, 5, 6), (7, 8, 9)],\n...                 dtype=[(\"a\", \"i4\"), (\"b\", \"i4\"), (\"c\", \"i4\")])\n>>> df3 = pd.DataFrame(data, columns=['c', 'a'])\n...\n>>> df3\n   c  a\n0  3  1\n1  6  4\n2  9  7\n\nConstructing DataFrame from dataclass:\n\n>>> from dataclasses import make_dataclass\n>>> Point = make_dataclass(\"Point\", [(\"x\", int), (\"y\", int)])\n>>> pd.DataFrame([Point(0, 0), Point(0, 3), Point(2, 3)])\n   x  y\n0  0  0\n1  0  3\n2  2  3\n\nConstructing DataFrame from Series/DataFrame:\n\n>>> ser = pd.Series([1, 2, 3], index=[\"a\", \"b\", \"c\"])\n>>> df = pd.DataFrame(data=ser, index=[\"a\", \"c\"])\n>>> df\n   0\na  1\nc  3\n\n>>> df1 = pd.DataFrame([1, 2, 3], index=[\"a\", \"b\", \"c\"], columns=[\"x\"])\n>>> df2 = pd.DataFrame(data=df1, index=[\"a\", \"c\"])\n>>> df2\n   x\na  1\nc  3", "parameters": {"type": "object", "properties": {"data": {"type": "NoneType", "default": null}, "index": {"type": ["axes", "null"], "default": null}, "columns": {"type": ["axes", "null"], "default": null}, "dtype": {"type": ["dtype", "null"], "default": null}, "copy": {"type": ["bool", "null"], "default": null}}}}}
{"task_id": "BigCodeBench/513", "data": {"name": "pandas.DataFrame.plot", "type": "class", "signature": "(data: 'Series | DataFrame') -> 'None'", "description": "Make plots of Series or DataFrame.\n\nUses the backend specified by the\noption ``plotting.backend``. By default, matplotlib is used.\n\nParameters\n----------\ndata : Series or DataFrame\n    The object for which the method is called.\nx : label or position, default None\n    Only used if data is a DataFrame.\ny : label, position or list of label, positions, default None\n    Allows plotting of one column versus another. Only used if data is a\n    DataFrame.\nkind : str\n    The kind of plot to produce:\n\n    - 'line' : line plot (default)\n    - 'bar' : vertical bar plot\n    - 'barh' : horizontal bar plot\n    - 'hist' : histogram\n    - 'box' : boxplot\n    - 'kde' : Kernel Density Estimation plot\n    - 'density' : same as 'kde'\n    - 'area' : area plot\n    - 'pie' : pie plot\n    - 'scatter' : scatter plot (DataFrame only)\n    - 'hexbin' : hexbin plot (DataFrame only)\nax : matplotlib axes object, default None\n    An axes of the current figure.\nsubplots : bool or sequence of iterables, default False\n    Whether to group columns into subplots:\n\n    - ``False`` : No subplots will be used\n    - ``True`` : Make separate subplots for each column.\n    - sequence of iterables of column labels: Create a subplot for each\n      group of columns. For example `[('a', 'c'), ('b', 'd')]` will\n      create 2 subplots: one with columns 'a' and 'c', and one\n      with columns 'b' and 'd'. Remaining columns that aren't specified\n      will be plotted in additional subplots (one per column).\n\n      .. versionadded:: 1.5.0\n\nsharex : bool, default True if ax is None else False\n    In case ``subplots=True``, share x axis and set some x axis labels\n    to invisible; defaults to True if ax is None otherwise False if\n    an ax is passed in; Be aware, that passing in both an ax and\n    ``sharex=True`` will alter all x axis labels for all axis in a figure.\nsharey : bool, default False\n    In case ``subplots=True``, share y axis and set some y axis labels to invisible.\nlayout : tuple, optional\n    (rows, columns) for the layout of subplots.\nfigsize : a tuple (width, height) in inches\n    Size of a figure object.\nuse_index : bool, default True\n    Use index as ticks for x axis.\ntitle : str or list\n    Title to use for the plot. If a string is passed, print the string\n    at the top of the figure. If a list is passed and `subplots` is\n    True, print each item in the list above the corresponding subplot.\ngrid : bool, default None (matlab style default)\n    Axis grid lines.\nlegend : bool or {'reverse'}\n    Place legend on axis subplots.\nstyle : list or dict\n    The matplotlib line style per column.\nlogx : bool or 'sym', default False\n    Use log scaling or symlog scaling on x axis.\n\nlogy : bool or 'sym' default False\n    Use log scaling or symlog scaling on y axis.\n\nloglog : bool or 'sym', default False\n    Use log scaling or symlog scaling on both x and y axes.\n\nxticks : sequence\n    Values to use for the xticks.\nyticks : sequence\n    Values to use for the yticks.\nxlim : 2-tuple/list\n    Set the x limits of the current axes.\nylim : 2-tuple/list\n    Set the y limits of the current axes.\nxlabel : label, optional\n    Name to use for the xlabel on x-axis. Default uses index name as xlabel, or the\n    x-column name for planar plots.\n\n    .. versionchanged:: 2.0.0\n\n        Now applicable to histograms.\n\nylabel : label, optional\n    Name to use for the ylabel on y-axis. Default will show no ylabel, or the\n    y-column name for planar plots.\n\n    .. versionchanged:: 2.0.0\n\n        Now applicable to histograms.\n\nrot : float, default None\n    Rotation for ticks (xticks for vertical, yticks for horizontal\n    plots).\nfontsize : float, default None\n    Font size for xticks and yticks.\ncolormap : str or matplotlib colormap object, default None\n    Colormap to select colors from. If string, load colormap with that\n    name from matplotlib.\ncolorbar : bool, optional\n    If True, plot colorbar (only relevant for 'scatter' and 'hexbin'\n    plots).\nposition : float\n    Specify relative alignments for bar plot layout.\n    From 0 (left/bottom-end) to 1 (right/top-end). Default is 0.5\n    (center).\ntable : bool, Series or DataFrame, default False\n    If True, draw a table using the data in the DataFrame and the data\n    will be transposed to meet matplotlib's default layout.\n    If a Series or DataFrame is passed, use passed data to draw a\n    table.\nyerr : DataFrame, Series, array-like, dict and str\n    See :ref:`Plotting with Error Bars <visualization.errorbars>` for\n    detail.\nxerr : DataFrame, Series, array-like, dict and str\n    Equivalent to yerr.\nstacked : bool, default False in line and bar plots, and True in area plot\n    If True, create stacked plot.\nsecondary_y : bool or sequence, default False\n    Whether to plot on the secondary y-axis if a list/tuple, which\n    columns to plot on secondary y-axis.\nmark_right : bool, default True\n    When using a secondary_y axis, automatically mark the column\n    labels with \"(right)\" in the legend.\ninclude_bool : bool, default is False\n    If True, boolean values can be plotted.\nbackend : str, default None\n    Backend to use instead of the backend specified in the option\n    ``plotting.backend``. For instance, 'matplotlib'. Alternatively, to\n    specify the ``plotting.backend`` for the whole session, set\n    ``pd.options.plotting.backend``.\n**kwargs\n    Options to pass to matplotlib plotting method.\n\nReturns\n-------\n:class:`matplotlib.axes.Axes` or numpy.ndarray of them\n    If the backend is not the default matplotlib one, the return value\n    will be the object returned by the backend.\n\nNotes\n-----\n- See matplotlib documentation online for more on this subject\n- If `kind` = 'bar' or 'barh', you can specify relative alignments\n  for bar plot layout by `position` keyword.\n  From 0 (left/bottom-end) to 1 (right/top-end). Default is 0.5\n  (center)\n\nExamples\n--------\nFor Series:\n\n.. plot::\n    :context: close-figs\n\n    >>> ser = pd.Series([1, 2, 3, 3])\n    >>> plot = ser.plot(kind='hist', title=\"My plot\")\n\nFor DataFrame:\n\n.. plot::\n    :context: close-figs\n\n    >>> df = pd.DataFrame({'length': [1.5, 0.5, 1.2, 0.9, 3],\n    ...                   'width': [0.7, 0.2, 0.15, 0.2, 1.1]},\n    ...                   index=['pig', 'rabbit', 'duck', 'chicken', 'horse'])\n    >>> plot = df.plot(title=\"DataFrame Plot\")\n\nFor SeriesGroupBy:\n\n.. plot::\n    :context: close-figs\n\n    >>> lst = [-1, -2, -3, 1, 2, 3]\n    >>> ser = pd.Series([1, 2, 2, 4, 6, 6], index=lst)\n    >>> plot = ser.groupby(lambda x: x > 0).plot(title=\"SeriesGroupBy Plot\")\n\nFor DataFrameGroupBy:\n\n.. plot::\n    :context: close-figs\n\n    >>> df = pd.DataFrame({\"col1\" : [1, 2, 3, 4],\n    ...                   \"col2\" : [\"A\", \"B\", \"A\", \"B\"]})\n    >>> plot = df.groupby(\"col2\").plot(kind=\"bar\", title=\"DataFrameGroupBy Plot\")"}}
{"task_id": "BigCodeBench/513", "data": {"name": "pandas.DataFrame.plot.line", "type": "callable", "signature": "(self, x: 'Hashable | None' = None, y: 'Hashable | None' = None, **kwargs) -> 'PlotAccessor)", "description": "Plot Series or DataFrame as lines.\n\nThis function is useful to plot lines using DataFrame's values\nas coordinates.\n\nParameters\n----------\nx : label or position, optional\n    Allows plotting of one column versus another. If not specified,\n    the index of the DataFrame is used.\ny : label or position, optional\n    Allows plotting of one column versus another. If not specified,\n    all numerical columns are used.\ncolor : str, array-like, or dict, optional\n    The color for each of the DataFrame's columns. Possible values are:\n\n    - A single color string referred to by name, RGB or RGBA code,\n        for instance 'red' or '#a98d19'.\n\n    - A sequence of color strings referred to by name, RGB or RGBA\n        code, which will be used for each column recursively. For\n        instance ['green','yellow'] each column's line will be filled in\n        green or yellow, alternatively. If there is only a single column to\n        be plotted, then only the first color from the color list will be\n        used.\n\n    - A dict of the form {column name : color}, so that each column will be\n        colored accordingly. For example, if your columns are called `a` and\n        `b`, then passing {'a': 'green', 'b': 'red'} will color lines for\n        column `a` in green and lines for column `b` in red.\n\n**kwargs\n    Additional keyword arguments are documented in\n    :meth:`DataFrame.plot`.\n\nReturns\n-------\nmatplotlib.axes.Axes or np.ndarray of them\n    An ndarray is returned with one :class:`matplotlib.axes.Axes`\n    per column when ``subplots=True``.\n\n        See Also\n        --------\n        matplotlib.pyplot.plot : Plot y versus x as lines and/or markers.\n\n        Examples\n        --------\n\n        .. plot::\n            :context: close-figs\n\n            >>> s = pd.Series([1, 3, 2])\n            >>> s.plot.line()  # doctest: +SKIP\n\n        .. plot::\n            :context: close-figs\n\n            The following example shows the populations for some animals\n            over the years.\n\n            >>> df = pd.DataFrame({\n            ...    'pig': [20, 18, 489, 675, 1776],\n            ...    'horse': [4, 25, 281, 600, 1900]\n            ...    }, index=[1990, 1997, 2003, 2009, 2014])\n            >>> lines = df.plot.line()\n\n        .. plot::\n           :context: close-figs\n\n           An example with subplots, so an array of axes is returned.\n\n           >>> axes = df.plot.line(subplots=True)\n           >>> type(axes)\n           <class 'numpy.ndarray'>\n\n        .. plot::\n           :context: close-figs\n\n           Let's repeat the same example, but specifying colors for\n           each column (in this case, for each animal).\n\n           >>> axes = df.plot.line(\n           ...     subplots=True, color={\"pig\": \"pink\", \"horse\": \"#742802\"}\n           ... )\n\n        .. plot::\n            :context: close-figs\n\n            The following example shows the relationship between both\n            populations.\n\n            >>> lines = df.plot.line(x='pig', y='horse')", "parameters": {"type": "object", "properties": {"x": {"type": ["hashable", "null"], "default": null}, "y": {"type": ["hashable", "null"], "default": null}}}}}
{"task_id": "BigCodeBench/513", "data": {"name": "df.plot.line(x='Date', y=column).set_ylabel(column)"}}
{"task_id": "BigCodeBench/513", "data": {"name": "matplotlib.pyplot.title", "type": "callable", "signature": "(label: 'str')", "description": "Set a title for the Axes.\n\nSet one of the three available Axes titles. The available titles\nare positioned above the Axes in the center, flush with the left\nedge, and flush with the right edge.\n\nParameters\n----------\nlabel : str\n    Text to use for the title\n\nfontdict : dict\n\n    .. admonition:: Discouraged\n\n       The use of *fontdict* is discouraged. Parameters should be passed as\n       individual keyword arguments or using dictionary-unpacking\n       ``set_title(..., **fontdict)``.\n\n    A dictionary controlling the appearance of the title text,\n    the default *fontdict* is::\n\n       {'fontsize': rcParams['axes.titlesize'],\n        'fontweight': rcParams['axes.titleweight'],\n        'color': rcParams['axes.titlecolor'],\n        'verticalalignment': 'baseline',\n        'horizontalalignment': loc}\n\nloc : {'center', 'left', 'right'}, default: :rc:`axes.titlelocation`\n    Which title to set.\n\ny : float, default: :rc:`axes.titley`\n    Vertical Axes location for the title (1.0 is the top).  If\n    None (the default) and :rc:`axes.titley` is also None, y is\n    determined automatically to avoid decorators on the Axes.\n\npad : float, default: :rc:`axes.titlepad`\n    The offset of the title from the top of the Axes, in points.\n\nReturns\n-------\n`.Text`\n    The matplotlib text instance representing the title\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    Other keyword arguments are text properties, see `.Text` for a list\n    of valid text properties.", "parameters": {"type": "object", "properties": {"label": {"type": "str"}}}}}
{"task_id": "BigCodeBench/513", "data": {"name": "numpy.max", "type": "callable", "signature": "(a)", "description": "Return the maximum of an array or maximum along an axis.\n\nParameters\n----------\na : array_like\n    Input data.\naxis : None or int or tuple of ints, optional\n    Axis or axes along which to operate.  By default, flattened input is\n    used.\n\n    .. versionadded:: 1.7.0\n\n    If this is a tuple of ints, the maximum is selected over multiple axes,\n    instead of a single axis or all the axes as before.\nout : ndarray, optional\n    Alternative output array in which to place the result.  Must\n    be of the same shape and buffer length as the expected output.\n    See :ref:`ufuncs-output-type` for more details.\n\nkeepdims : bool, optional\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\n    If the default value is passed, then `keepdims` will not be\n    passed through to the `amax` method of sub-classes of\n    `ndarray`, however any non-default value will be.  If the\n    sub-class' method does not implement `keepdims` any\n    exceptions will be raised.\n\ninitial : scalar, optional\n    The minimum value of an output element. Must be present to allow\n    computation on empty slice. See `~numpy.ufunc.reduce` for details.\n\n    .. versionadded:: 1.15.0\n\nwhere : array_like of bool, optional\n    Elements to compare for the maximum. See `~numpy.ufunc.reduce`\n    for details.\n\n    .. versionadded:: 1.17.0\n\nReturns\n-------\namax : ndarray or scalar\n    Maximum of `a`. If `axis` is None, the result is a scalar value.\n    If `axis` is given, the result is an array of dimension\n    ``a.ndim - 1``.\n\nSee Also\n--------\namin :\n    The minimum value of an array along a given axis, propagating any NaNs.\nnanmax :\n    The maximum value of an array along a given axis, ignoring any NaNs.\nmaximum :\n    Element-wise maximum of two arrays, propagating any NaNs.\nfmax :\n    Element-wise maximum of two arrays, ignoring any NaNs.\nargmax :\n    Return the indices of the maximum values.\n\nnanmin, minimum, fmin\n\nNotes\n-----\nNaN values are propagated, that is if at least one item is NaN, the\ncorresponding max value will be NaN as well. To ignore NaN values\n(MATLAB behavior), please use nanmax.\n\nDon't use `amax` for element-wise comparison of 2 arrays; when\n``a.shape[0]`` is 2, ``maximum(a[0], a[1])`` is faster than\n``amax(a, axis=0)``.\n\nExamples\n--------\n>>> a = np.arange(4).reshape((2,2))\n>>> a\narray([[0, 1],\n       [2, 3]])\n>>> np.amax(a)           # Maximum of the flattened array\n3\n>>> np.amax(a, axis=0)   # Maxima along the first axis\narray([2, 3])\n>>> np.amax(a, axis=1)   # Maxima along the second axis\narray([1, 3])\n>>> np.amax(a, where=[False, True], initial=-1, axis=0)\narray([-1,  3])\n>>> b = np.arange(5, dtype=float)\n>>> b[2] = np.NaN\n>>> np.amax(b)\nnan\n>>> np.amax(b, where=~np.isnan(b), initial=-1)\n4.0\n>>> np.nanmax(b)\n4.0\n\nYou can use an initial value to compute the maximum of an empty slice, or\nto initialize it to a different value:\n\n>>> np.amax([[-50], [10]], axis=-1, initial=0)\narray([ 0, 10])\n\nNotice that the initial value is used as one of the elements for which the\nmaximum is determined, unlike for the default argument Python's max\nfunction, which is only used for empty iterables.\n\n>>> np.amax([5], initial=6)\n6\n>>> max([5], default=6)\n5", "parameters": {"type": "object", "properties": {"a": {}}}}}
{"task_id": "BigCodeBench/513", "data": {"name": "numpy.min", "type": "callable", "signature": "(a)", "description": "Return the minimum of an array or minimum along an axis.\n\nParameters\n----------\na : array_like\n    Input data.\naxis : None or int or tuple of ints, optional\n    Axis or axes along which to operate.  By default, flattened input is\n    used.\n\n    .. versionadded:: 1.7.0\n\n    If this is a tuple of ints, the minimum is selected over multiple axes,\n    instead of a single axis or all the axes as before.\nout : ndarray, optional\n    Alternative output array in which to place the result.  Must\n    be of the same shape and buffer length as the expected output.\n    See :ref:`ufuncs-output-type` for more details.\n\nkeepdims : bool, optional\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\n    If the default value is passed, then `keepdims` will not be\n    passed through to the `amin` method of sub-classes of\n    `ndarray`, however any non-default value will be.  If the\n    sub-class' method does not implement `keepdims` any\n    exceptions will be raised.\n\ninitial : scalar, optional\n    The maximum value of an output element. Must be present to allow\n    computation on empty slice. See `~numpy.ufunc.reduce` for details.\n\n    .. versionadded:: 1.15.0\n\nwhere : array_like of bool, optional\n    Elements to compare for the minimum. See `~numpy.ufunc.reduce`\n    for details.\n\n    .. versionadded:: 1.17.0\n\nReturns\n-------\namin : ndarray or scalar\n    Minimum of `a`. If `axis` is None, the result is a scalar value.\n    If `axis` is given, the result is an array of dimension\n    ``a.ndim - 1``.\n\nSee Also\n--------\namax :\n    The maximum value of an array along a given axis, propagating any NaNs.\nnanmin :\n    The minimum value of an array along a given axis, ignoring any NaNs.\nminimum :\n    Element-wise minimum of two arrays, propagating any NaNs.\nfmin :\n    Element-wise minimum of two arrays, ignoring any NaNs.\nargmin :\n    Return the indices of the minimum values.\n\nnanmax, maximum, fmax\n\nNotes\n-----\nNaN values are propagated, that is if at least one item is NaN, the\ncorresponding min value will be NaN as well. To ignore NaN values\n(MATLAB behavior), please use nanmin.\n\nDon't use `amin` for element-wise comparison of 2 arrays; when\n``a.shape[0]`` is 2, ``minimum(a[0], a[1])`` is faster than\n``amin(a, axis=0)``.\n\nExamples\n--------\n>>> a = np.arange(4).reshape((2,2))\n>>> a\narray([[0, 1],\n       [2, 3]])\n>>> np.amin(a)           # Minimum of the flattened array\n0\n>>> np.amin(a, axis=0)   # Minima along the first axis\narray([0, 1])\n>>> np.amin(a, axis=1)   # Minima along the second axis\narray([0, 2])\n>>> np.amin(a, where=[False, True], initial=10, axis=0)\narray([10,  1])\n\n>>> b = np.arange(5, dtype=float)\n>>> b[2] = np.NaN\n>>> np.amin(b)\nnan\n>>> np.amin(b, where=~np.isnan(b), initial=10)\n0.0\n>>> np.nanmin(b)\n0.0\n\n>>> np.amin([[-50], [10]], axis=-1, initial=0)\narray([-50,   0])\n\nNotice that the initial value is used as one of the elements for which the\nminimum is determined, unlike for the default argument Python's max\nfunction, which is only used for empty iterables.\n\nNotice that this isn't the same as Python's ``default`` argument.\n\n>>> np.amin([6], initial=5)\n5\n>>> min([6], default=5)\n6", "parameters": {"type": "object", "properties": {"a": {}}}}}
{"task_id": "BigCodeBench/513", "data": {"name": "numpy.sum", "type": "callable", "signature": "(a)", "description": "Sum of array elements over a given axis.\n\nParameters\n----------\na : array_like\n    Elements to sum.\naxis : None or int or tuple of ints, optional\n    Axis or axes along which a sum is performed.  The default,\n    axis=None, will sum all of the elements of the input array.  If\n    axis is negative it counts from the last to the first axis.\n\n    .. versionadded:: 1.7.0\n\n    If axis is a tuple of ints, a sum is performed on all of the axes\n    specified in the tuple instead of a single axis or all the axes as\n    before.\ndtype : dtype, optional\n    The type of the returned array and of the accumulator in which the\n    elements are summed.  The dtype of `a` is used by default unless `a`\n    has an integer dtype of less precision than the default platform\n    integer.  In that case, if `a` is signed then the platform integer\n    is used while if `a` is unsigned then an unsigned integer of the\n    same precision as the platform integer is used.\nout : ndarray, optional\n    Alternative output array in which to place the result. It must have\n    the same shape as the expected output, but the type of the output\n    values will be cast if necessary.\nkeepdims : bool, optional\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\n    If the default value is passed, then `keepdims` will not be\n    passed through to the `sum` method of sub-classes of\n    `ndarray`, however any non-default value will be.  If the\n    sub-class' method does not implement `keepdims` any\n    exceptions will be raised.\ninitial : scalar, optional\n    Starting value for the sum. See `~numpy.ufunc.reduce` for details.\n\n    .. versionadded:: 1.15.0\n\nwhere : array_like of bool, optional\n    Elements to include in the sum. See `~numpy.ufunc.reduce` for details.\n\n    .. versionadded:: 1.17.0\n\nReturns\n-------\nsum_along_axis : ndarray\n    An array with the same shape as `a`, with the specified\n    axis removed.   If `a` is a 0-d array, or if `axis` is None, a scalar\n    is returned.  If an output array is specified, a reference to\n    `out` is returned.\n\nSee Also\n--------\nndarray.sum : Equivalent method.\n\nadd.reduce : Equivalent functionality of `add`.\n\ncumsum : Cumulative sum of array elements.\n\ntrapz : Integration of array values using the composite trapezoidal rule.\n\nmean, average\n\nNotes\n-----\nArithmetic is modular when using integer types, and no error is\nraised on overflow.\n\nThe sum of an empty array is the neutral element 0:\n\n>>> np.sum([])\n0.0\n\nFor floating point numbers the numerical precision of sum (and\n``np.add.reduce``) is in general limited by directly adding each number\nindividually to the result causing rounding errors in every step.\nHowever, often numpy will use a  numerically better approach (partial\npairwise summation) leading to improved precision in many use-cases.\nThis improved precision is always provided when no ``axis`` is given.\nWhen ``axis`` is given, it will depend on which axis is summed.\nTechnically, to provide the best speed possible, the improved precision\nis only used when the summation is along the fast axis in memory.\nNote that the exact precision may vary depending on other parameters.\nIn contrast to NumPy, Python's ``math.fsum`` function uses a slower but\nmore precise approach to summation.\nEspecially when summing a large number of lower precision floating point\nnumbers, such as ``float32``, numerical errors can become significant.\nIn such cases it can be advisable to use `dtype=\"float64\"` to use a higher\nprecision for the output.\n\nExamples\n--------\n>>> np.sum([0.5, 1.5])\n2.0\n>>> np.sum([0.5, 0.7, 0.2, 1.5], dtype=np.int32)\n1\n>>> np.sum([[0, 1], [0, 5]])\n6\n>>> np.sum([[0, 1], [0, 5]], axis=0)\narray([0, 6])\n>>> np.sum([[0, 1], [0, 5]], axis=1)\narray([1, 5])\n>>> np.sum([[0, 1], [np.nan, 5]], where=[False, True], axis=1)\narray([1., 5.])\n\nIf the accumulator is too small, overflow occurs:\n\n>>> np.ones(128, dtype=np.int8).sum(dtype=np.int8)\n-128\n\nYou can also start the sum with a value other than zero:\n\n>>> np.sum([10], initial=5)\n15", "parameters": {"type": "object", "properties": {"a": {}}}}}
{"task_id": "BigCodeBench/513", "data": {"name": "numpy.mean", "type": "callable", "signature": "(a)", "description": "Compute the arithmetic mean along the specified axis.\n\nReturns the average of the array elements.  The average is taken over\nthe flattened array by default, otherwise over the specified axis.\n`float64` intermediate and return values are used for integer inputs.\n\nParameters\n----------\na : array_like\n    Array containing numbers whose mean is desired. If `a` is not an\n    array, a conversion is attempted.\naxis : None or int or tuple of ints, optional\n    Axis or axes along which the means are computed. The default is to\n    compute the mean of the flattened array.\n\n    .. versionadded:: 1.7.0\n\n    If this is a tuple of ints, a mean is performed over multiple axes,\n    instead of a single axis or all the axes as before.\ndtype : data-type, optional\n    Type to use in computing the mean.  For integer inputs, the default\n    is `float64`; for floating point inputs, it is the same as the\n    input dtype.\nout : ndarray, optional\n    Alternate output array in which to place the result.  The default\n    is ``None``; if provided, it must have the same shape as the\n    expected output, but the type will be cast if necessary.\n    See :ref:`ufuncs-output-type` for more details.\n\nkeepdims : bool, optional\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\n    If the default value is passed, then `keepdims` will not be\n    passed through to the `mean` method of sub-classes of\n    `ndarray`, however any non-default value will be.  If the\n    sub-class' method does not implement `keepdims` any\n    exceptions will be raised.\n\nwhere : array_like of bool, optional\n    Elements to include in the mean. See `~numpy.ufunc.reduce` for details.\n\n    .. versionadded:: 1.20.0\n\nReturns\n-------\nm : ndarray, see dtype parameter above\n    If `out=None`, returns a new array containing the mean values,\n    otherwise a reference to the output array is returned.\n\nSee Also\n--------\naverage : Weighted average\nstd, var, nanmean, nanstd, nanvar\n\nNotes\n-----\nThe arithmetic mean is the sum of the elements along the axis divided\nby the number of elements.\n\nNote that for floating-point input, the mean is computed using the\nsame precision the input has.  Depending on the input data, this can\ncause the results to be inaccurate, especially for `float32` (see\nexample below).  Specifying a higher-precision accumulator using the\n`dtype` keyword can alleviate this issue.\n\nBy default, `float16` results are computed using `float32` intermediates\nfor extra precision.\n\nExamples\n--------\n>>> a = np.array([[1, 2], [3, 4]])\n>>> np.mean(a)\n2.5\n>>> np.mean(a, axis=0)\narray([2., 3.])\n>>> np.mean(a, axis=1)\narray([1.5, 3.5])\n\nIn single precision, `mean` can be inaccurate:\n\n>>> a = np.zeros((2, 512*512), dtype=np.float32)\n>>> a[0, :] = 1.0\n>>> a[1, :] = 0.1\n>>> np.mean(a)\n0.54999924\n\nComputing the mean in float64 is more accurate:\n\n>>> np.mean(a, dtype=np.float64)\n0.55000000074505806 # may vary\n\nSpecifying a where argument:\n\n>>> a = np.array([[5, 9, 13], [14, 10, 12], [11, 15, 19]])\n>>> np.mean(a)\n12.0\n>>> np.mean(a, where=[[True], [False], [False]])\n9.0", "parameters": {"type": "object", "properties": {"a": {}}}}}
{"task_id": "BigCodeBench/526", "data": {"name": "collections.defaultdict", "type": "class", "signature": "(self, /, *args, **kwargs)", "description": "defaultdict(default_factory=None, /, [...]) --> dict with default factory\n\nThe default factory is called without arguments to produce\na new value when a key is not present, in __getitem__ only.\nA defaultdict compares equal to a dict with the same items.\nAll remaining arguments are treated the same as if they were\npassed to the dict constructor, including keyword arguments.", "parameters": {"type": "object", "properties": {"/": {}}}}}
{"task_id": "BigCodeBench/526", "data": {"name": "collections.defaultdict.items", "type": "callable", "signature": "()", "description": "D.items() -> a set-like object providing a view on D's items", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/526", "data": {"name": "d.keys()"}}
{"task_id": "BigCodeBench/526", "data": {"name": "d.get(key, np.nan)"}}
{"task_id": "BigCodeBench/526", "data": {"name": "json.load", "type": "callable", "signature": "(fp)", "description": "Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\na JSON document) to a Python object.\n\n``object_hook`` is an optional function that will be called with the\nresult of any object literal decode (a ``dict``). The return value of\n``object_hook`` will be used instead of the ``dict``. This feature\ncan be used to implement custom decoders (e.g. JSON-RPC class hinting).\n\n``object_pairs_hook`` is an optional function that will be called with the\nresult of any object literal decoded with an ordered list of pairs.  The\nreturn value of ``object_pairs_hook`` will be used instead of the ``dict``.\nThis feature can be used to implement custom decoders.  If ``object_hook``\nis also defined, the ``object_pairs_hook`` takes priority.\n\nTo use a custom ``JSONDecoder`` subclass, specify it with the ``cls``\nkwarg; otherwise ``JSONDecoder`` is used.", "parameters": {"type": "object", "properties": {"fp": {}}}}}
{"task_id": "BigCodeBench/526", "data": {"name": "numpy.nan", "type": "constant", "signature": null, "description": "Convert a string or number to a floating point number, if possible.", "value": "nan", "parameters": {"type": "constant"}}}
{"task_id": "BigCodeBench/526", "data": {"name": "numpy.nanmean", "type": "callable", "signature": "(a)", "description": "Compute the arithmetic mean along the specified axis, ignoring NaNs.\n\nReturns the average of the array elements.  The average is taken over\nthe flattened array by default, otherwise over the specified axis.\n`float64` intermediate and return values are used for integer inputs.\n\nFor all-NaN slices, NaN is returned and a `RuntimeWarning` is raised.\n\n.. versionadded:: 1.8.0\n\nParameters\n----------\na : array_like\n    Array containing numbers whose mean is desired. If `a` is not an\n    array, a conversion is attempted.\naxis : {int, tuple of int, None}, optional\n    Axis or axes along which the means are computed. The default is to compute\n    the mean of the flattened array.\ndtype : data-type, optional\n    Type to use in computing the mean.  For integer inputs, the default\n    is `float64`; for inexact inputs, it is the same as the input\n    dtype.\nout : ndarray, optional\n    Alternate output array in which to place the result.  The default\n    is ``None``; if provided, it must have the same shape as the\n    expected output, but the type will be cast if necessary. See\n    :ref:`ufuncs-output-type` for more details.\nkeepdims : bool, optional\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the original `a`.\n\n    If the value is anything but the default, then\n    `keepdims` will be passed through to the `mean` or `sum` methods\n    of sub-classes of `ndarray`.  If the sub-classes methods\n    does not implement `keepdims` any exceptions will be raised.\nwhere : array_like of bool, optional\n    Elements to include in the mean. See `~numpy.ufunc.reduce` for details.\n\n    .. versionadded:: 1.22.0\n\nReturns\n-------\nm : ndarray, see dtype parameter above\n    If `out=None`, returns a new array containing the mean values,\n    otherwise a reference to the output array is returned. Nan is\n    returned for slices that contain only NaNs.\n\nSee Also\n--------\naverage : Weighted average\nmean : Arithmetic mean taken while not ignoring NaNs\nvar, nanvar\n\nNotes\n-----\nThe arithmetic mean is the sum of the non-NaN elements along the axis\ndivided by the number of non-NaN elements.\n\nNote that for floating-point input, the mean is computed using the same\nprecision the input has.  Depending on the input data, this can cause\nthe results to be inaccurate, especially for `float32`.  Specifying a\nhigher-precision accumulator using the `dtype` keyword can alleviate\nthis issue.\n\nExamples\n--------\n>>> a = np.array([[1, np.nan], [3, 4]])\n>>> np.nanmean(a)\n2.6666666666666665\n>>> np.nanmean(a, axis=0)\narray([2.,  4.])\n>>> np.nanmean(a, axis=1)\narray([1.,  3.5]) # may vary", "parameters": {"type": "object", "properties": {"a": {}}}}}
{"task_id": "BigCodeBench/526", "data": {"name": "numpy.nanmedian", "type": "callable", "signature": "(a)", "description": "Compute the median along the specified axis, while ignoring NaNs.\n\nReturns the median of the array elements.\n\n.. versionadded:: 1.9.0\n\nParameters\n----------\na : array_like\n    Input array or object that can be converted to an array.\naxis : {int, sequence of int, None}, optional\n    Axis or axes along which the medians are computed. The default\n    is to compute the median along a flattened version of the array.\n    A sequence of axes is supported since version 1.9.0.\nout : ndarray, optional\n    Alternative output array in which to place the result. It must\n    have the same shape and buffer length as the expected output,\n    but the type (of the output) will be cast if necessary.\noverwrite_input : bool, optional\n   If True, then allow use of memory of input array `a` for\n   calculations. The input array will be modified by the call to\n   `median`. This will save memory when you do not need to preserve\n   the contents of the input array. Treat the input as undefined,\n   but it will probably be fully or partially sorted. Default is\n   False. If `overwrite_input` is ``True`` and `a` is not already an\n   `ndarray`, an error will be raised.\nkeepdims : bool, optional\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the original `a`.\n\n    If this is anything but the default value it will be passed\n    through (in the special case of an empty array) to the\n    `mean` function of the underlying array.  If the array is\n    a sub-class and `mean` does not have the kwarg `keepdims` this\n    will raise a RuntimeError.\n\nReturns\n-------\nmedian : ndarray\n    A new array holding the result. If the input contains integers\n    or floats smaller than ``float64``, then the output data-type is\n    ``np.float64``.  Otherwise, the data-type of the output is the\n    same as that of the input. If `out` is specified, that array is\n    returned instead.\n\nSee Also\n--------\nmean, median, percentile\n\nNotes\n-----\nGiven a vector ``V`` of length ``N``, the median of ``V`` is the\nmiddle value of a sorted copy of ``V``, ``V_sorted`` - i.e.,\n``V_sorted[(N-1)/2]``, when ``N`` is odd and the average of the two\nmiddle values of ``V_sorted`` when ``N`` is even.\n\nExamples\n--------\n>>> a = np.array([[10.0, 7, 4], [3, 2, 1]])\n>>> a[0, 1] = np.nan\n>>> a\narray([[10., nan,  4.],\n       [ 3.,  2.,  1.]])\n>>> np.median(a)\nnan\n>>> np.nanmedian(a)\n3.0\n>>> np.nanmedian(a, axis=0)\narray([6.5, 2. , 2.5])\n>>> np.median(a, axis=1)\narray([nan,  2.])\n>>> b = a.copy()\n>>> np.nanmedian(b, axis=1, overwrite_input=True)\narray([7.,  2.])\n>>> assert not np.all(a==b)\n>>> b = a.copy()\n>>> np.nanmedian(b, axis=None, overwrite_input=True)\n3.0\n>>> assert not np.all(a==b)", "parameters": {"type": "object", "properties": {"a": {}}}}}
{"task_id": "BigCodeBench/528", "data": {"name": "collections.Counter", "type": "class", "signature": "(iterable=None, /, **kwds)", "description": "Dict subclass for counting hashable items.  Sometimes called a bag\nor multiset.  Elements are stored as dictionary keys and their counts\nare stored as dictionary values.\n\n>>> c = Counter('abcdeabcdabcaba')  # count elements from a string\n\n>>> c.most_common(3)                # three most common elements\n[('a', 5), ('b', 4), ('c', 3)]\n>>> sorted(c)                       # list all unique elements\n['a', 'b', 'c', 'd', 'e']\n>>> ''.join(sorted(c.elements()))   # list elements with repetitions\n'aaaaabbbbcccdde'\n>>> sum(c.values())                 # total of all counts\n15\n\n>>> c['a']                          # count of letter 'a'\n5\n>>> for elem in 'shazam':           # update counts from an iterable\n...     c[elem] += 1                # by adding 1 to each element's count\n>>> c['a']                          # now there are seven 'a'\n7\n>>> del c['b']                      # remove all 'b'\n>>> c['b']                          # now there are zero 'b'\n0\n\n>>> d = Counter('simsalabim')       # make another counter\n>>> c.update(d)                     # add in the second counter\n>>> c['a']                          # now there are nine 'a'\n9\n\n>>> c.clear()                       # empty the counter\n>>> c\nCounter()\n\nNote:  If a count is set to zero or reduced to zero, it will remain\nin the counter until the entry is deleted or the counter is cleared:\n\n>>> c = Counter('aaabbc')\n>>> c['b'] -= 2                     # reduce the count of 'b' by two\n>>> c.most_common()                 # 'b' is still in, but its count is zero\n[('a', 3), ('c', 1), ('b', 0)]", "parameters": {"type": "object", "properties": {"iterable": {"type": "NoneType", "default": null}, "/": {}}}}}
{"task_id": "BigCodeBench/528", "data": {"name": "collections.Counter.keys", "type": "callable", "signature": "()", "description": "D.keys() -> a set-like object providing a view on D's keys", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/528", "data": {"name": "collections.Counter.values", "type": "callable", "signature": "()", "description": "D.values() -> an object providing a view on D's values", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/528", "data": {"name": "pandas.DataFrame", "type": "class", "signature": "(data=None, index: 'Axes | None' = None, columns: 'Axes | None' = None, dtype: 'Dtype | None' = None, copy: 'bool | None' = None) -> 'None'", "description": "Two-dimensional, size-mutable, potentially heterogeneous tabular data.\n\nData structure also contains labeled axes (rows and columns).\nArithmetic operations align on both row and column labels. Can be\nthought of as a dict-like container for Series objects. The primary\npandas data structure.\n\nParameters\n----------\ndata : ndarray (structured or homogeneous), Iterable, dict, or DataFrame\n    Dict can contain Series, arrays, constants, dataclass or list-like objects. If\n    data is a dict, column order follows insertion-order. If a dict contains Series\n    which have an index defined, it is aligned by its index. This alignment also\n    occurs if data is a Series or a DataFrame itself. Alignment is done on\n    Series/DataFrame inputs.\n\n    If data is a list of dicts, column order follows insertion-order.\n\nindex : Index or array-like\n    Index to use for resulting frame. Will default to RangeIndex if\n    no indexing information part of input data and no index provided.\ncolumns : Index or array-like\n    Column labels to use for resulting frame when data does not have them,\n    defaulting to RangeIndex(0, 1, 2, ..., n). If data contains column labels,\n    will perform column selection instead.\ndtype : dtype, default None\n    Data type to force. Only a single dtype is allowed. If None, infer.\ncopy : bool or None, default None\n    Copy data from inputs.\n    For dict data, the default of None behaves like ``copy=True``.  For DataFrame\n    or 2d ndarray input, the default of None behaves like ``copy=False``.\n    If data is a dict containing one or more Series (possibly of different dtypes),\n    ``copy=False`` will ensure that these inputs are not copied.\n\n    .. versionchanged:: 1.3.0\n\nSee Also\n--------\nDataFrame.from_records : Constructor from tuples, also record arrays.\nDataFrame.from_dict : From dicts of Series, arrays, or dicts.\nread_csv : Read a comma-separated values (csv) file into DataFrame.\nread_table : Read general delimited file into DataFrame.\nread_clipboard : Read text from clipboard into DataFrame.\n\nNotes\n-----\nPlease reference the :ref:`User Guide <basics.dataframe>` for more information.\n\nExamples\n--------\nConstructing DataFrame from a dictionary.\n\n>>> d = {'col1': [1, 2], 'col2': [3, 4]}\n>>> df = pd.DataFrame(data=d)\n>>> df\n   col1  col2\n0     1     3\n1     2     4\n\nNotice that the inferred dtype is int64.\n\n>>> df.dtypes\ncol1    int64\ncol2    int64\ndtype: object\n\nTo enforce a single dtype:\n\n>>> df = pd.DataFrame(data=d, dtype=np.int8)\n>>> df.dtypes\ncol1    int8\ncol2    int8\ndtype: object\n\nConstructing DataFrame from a dictionary including Series:\n\n>>> d = {'col1': [0, 1, 2, 3], 'col2': pd.Series([2, 3], index=[2, 3])}\n>>> pd.DataFrame(data=d, index=[0, 1, 2, 3])\n   col1  col2\n0     0   NaN\n1     1   NaN\n2     2   2.0\n3     3   3.0\n\nConstructing DataFrame from numpy ndarray:\n\n>>> df2 = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),\n...                    columns=['a', 'b', 'c'])\n>>> df2\n   a  b  c\n0  1  2  3\n1  4  5  6\n2  7  8  9\n\nConstructing DataFrame from a numpy ndarray that has labeled columns:\n\n>>> data = np.array([(1, 2, 3), (4, 5, 6), (7, 8, 9)],\n...                 dtype=[(\"a\", \"i4\"), (\"b\", \"i4\"), (\"c\", \"i4\")])\n>>> df3 = pd.DataFrame(data, columns=['c', 'a'])\n...\n>>> df3\n   c  a\n0  3  1\n1  6  4\n2  9  7\n\nConstructing DataFrame from dataclass:\n\n>>> from dataclasses import make_dataclass\n>>> Point = make_dataclass(\"Point\", [(\"x\", int), (\"y\", int)])\n>>> pd.DataFrame([Point(0, 0), Point(0, 3), Point(2, 3)])\n   x  y\n0  0  0\n1  0  3\n2  2  3\n\nConstructing DataFrame from Series/DataFrame:\n\n>>> ser = pd.Series([1, 2, 3], index=[\"a\", \"b\", \"c\"])\n>>> df = pd.DataFrame(data=ser, index=[\"a\", \"c\"])\n>>> df\n   0\na  1\nc  3\n\n>>> df1 = pd.DataFrame([1, 2, 3], index=[\"a\", \"b\", \"c\"], columns=[\"x\"])\n>>> df2 = pd.DataFrame(data=df1, index=[\"a\", \"c\"])\n>>> df2\n   x\na  1\nc  3", "parameters": {"type": "object", "properties": {"data": {"type": "NoneType", "default": null}, "index": {"type": ["axes", "null"], "default": null}, "columns": {"type": ["axes", "null"], "default": null}, "dtype": {"type": ["dtype", "null"], "default": null}, "copy": {"type": ["bool", "null"], "default": null}}}}}
{"task_id": "BigCodeBench/528", "data": {"name": "pandas.DataFrame.plot", "type": "class", "signature": "()", "description": "Make plots of Series or DataFrame.\n\nUses the backend specified by the\noption ``plotting.backend``. By default, matplotlib is used.\n\nParameters\n----------\ndata : Series or DataFrame\n    The object for which the method is called.\nx : label or position, default None\n    Only used if data is a DataFrame.\ny : label, position or list of label, positions, default None\n    Allows plotting of one column versus another. Only used if data is a\n    DataFrame.\nkind : str\n    The kind of plot to produce:\n\n    - 'line' : line plot (default)\n    - 'bar' : vertical bar plot\n    - 'barh' : horizontal bar plot\n    - 'hist' : histogram\n    - 'box' : boxplot\n    - 'kde' : Kernel Density Estimation plot\n    - 'density' : same as 'kde'\n    - 'area' : area plot\n    - 'pie' : pie plot\n    - 'scatter' : scatter plot (DataFrame only)\n    - 'hexbin' : hexbin plot (DataFrame only)\nax : matplotlib axes object, default None\n    An axes of the current figure.\nsubplots : bool or sequence of iterables, default False\n    Whether to group columns into subplots:\n\n    - ``False`` : No subplots will be used\n    - ``True`` : Make separate subplots for each column.\n    - sequence of iterables of column labels: Create a subplot for each\n      group of columns. For example `[('a', 'c'), ('b', 'd')]` will\n      create 2 subplots: one with columns 'a' and 'c', and one\n      with columns 'b' and 'd'. Remaining columns that aren't specified\n      will be plotted in additional subplots (one per column).\n\n      .. versionadded:: 1.5.0\n\nsharex : bool, default True if ax is None else False\n    In case ``subplots=True``, share x axis and set some x axis labels\n    to invisible; defaults to True if ax is None otherwise False if\n    an ax is passed in; Be aware, that passing in both an ax and\n    ``sharex=True`` will alter all x axis labels for all axis in a figure.\nsharey : bool, default False\n    In case ``subplots=True``, share y axis and set some y axis labels to invisible.\nlayout : tuple, optional\n    (rows, columns) for the layout of subplots.\nfigsize : a tuple (width, height) in inches\n    Size of a figure object.\nuse_index : bool, default True\n    Use index as ticks for x axis.\ntitle : str or list\n    Title to use for the plot. If a string is passed, print the string\n    at the top of the figure. If a list is passed and `subplots` is\n    True, print each item in the list above the corresponding subplot.\ngrid : bool, default None (matlab style default)\n    Axis grid lines.\nlegend : bool or {'reverse'}\n    Place legend on axis subplots.\nstyle : list or dict\n    The matplotlib line style per column.\nlogx : bool or 'sym', default False\n    Use log scaling or symlog scaling on x axis.\n\nlogy : bool or 'sym' default False\n    Use log scaling or symlog scaling on y axis.\n\nloglog : bool or 'sym', default False\n    Use log scaling or symlog scaling on both x and y axes.\n\nxticks : sequence\n    Values to use for the xticks.\nyticks : sequence\n    Values to use for the yticks.\nxlim : 2-tuple/list\n    Set the x limits of the current axes.\nylim : 2-tuple/list\n    Set the y limits of the current axes.\nxlabel : label, optional\n    Name to use for the xlabel on x-axis. Default uses index name as xlabel, or the\n    x-column name for planar plots.\n\n    .. versionchanged:: 2.0.0\n\n        Now applicable to histograms.\n\nylabel : label, optional\n    Name to use for the ylabel on y-axis. Default will show no ylabel, or the\n    y-column name for planar plots.\n\n    .. versionchanged:: 2.0.0\n\n        Now applicable to histograms.\n\nrot : float, default None\n    Rotation for ticks (xticks for vertical, yticks for horizontal\n    plots).\nfontsize : float, default None\n    Font size for xticks and yticks.\ncolormap : str or matplotlib colormap object, default None\n    Colormap to select colors from. If string, load colormap with that\n    name from matplotlib.\ncolorbar : bool, optional\n    If True, plot colorbar (only relevant for 'scatter' and 'hexbin'\n    plots).\nposition : float\n    Specify relative alignments for bar plot layout.\n    From 0 (left/bottom-end) to 1 (right/top-end). Default is 0.5\n    (center).\ntable : bool, Series or DataFrame, default False\n    If True, draw a table using the data in the DataFrame and the data\n    will be transposed to meet matplotlib's default layout.\n    If a Series or DataFrame is passed, use passed data to draw a\n    table.\nyerr : DataFrame, Series, array-like, dict and str\n    See :ref:`Plotting with Error Bars <visualization.errorbars>` for\n    detail.\nxerr : DataFrame, Series, array-like, dict and str\n    Equivalent to yerr.\nstacked : bool, default False in line and bar plots, and True in area plot\n    If True, create stacked plot.\nsecondary_y : bool or sequence, default False\n    Whether to plot on the secondary y-axis if a list/tuple, which\n    columns to plot on secondary y-axis.\nmark_right : bool, default True\n    When using a secondary_y axis, automatically mark the column\n    labels with \"(right)\" in the legend.\ninclude_bool : bool, default is False\n    If True, boolean values can be plotted.\nbackend : str, default None\n    Backend to use instead of the backend specified in the option\n    ``plotting.backend``. For instance, 'matplotlib'. Alternatively, to\n    specify the ``plotting.backend`` for the whole session, set\n    ``pd.options.plotting.backend``.\n**kwargs\n    Options to pass to matplotlib plotting method.\n\nReturns\n-------\n:class:`matplotlib.axes.Axes` or numpy.ndarray of them\n    If the backend is not the default matplotlib one, the return value\n    will be the object returned by the backend.\n\nNotes\n-----\n- See matplotlib documentation online for more on this subject\n- If `kind` = 'bar' or 'barh', you can specify relative alignments\n  for bar plot layout by `position` keyword.\n  From 0 (left/bottom-end) to 1 (right/top-end). Default is 0.5\n  (center)\n\nExamples\n--------\nFor Series:\n\n.. plot::\n    :context: close-figs\n\n    >>> ser = pd.Series([1, 2, 3, 3])\n    >>> plot = ser.plot(kind='hist', title=\"My plot\")\n\nFor DataFrame:\n\n.. plot::\n    :context: close-figs\n\n    >>> df = pd.DataFrame({'length': [1.5, 0.5, 1.2, 0.9, 3],\n    ...                   'width': [0.7, 0.2, 0.15, 0.2, 1.1]},\n    ...                   index=['pig', 'rabbit', 'duck', 'chicken', 'horse'])\n    >>> plot = df.plot(title=\"DataFrame Plot\")\n\nFor SeriesGroupBy:\n\n.. plot::\n    :context: close-figs\n\n    >>> lst = [-1, -2, -3, 1, 2, 3]\n    >>> ser = pd.Series([1, 2, 2, 4, 6, 6], index=lst)\n    >>> plot = ser.groupby(lambda x: x > 0).plot(title=\"SeriesGroupBy Plot\")\n\nFor DataFrameGroupBy:\n\n.. plot::\n    :context: close-figs\n\n    >>> df = pd.DataFrame({\"col1\" : [1, 2, 3, 4],\n    ...                   \"col2\" : [\"A\", \"B\", \"A\", \"B\"]})\n    >>> plot = df.groupby(\"col2\").plot(kind=\"bar\", title=\"DataFrameGroupBy Plot\")", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/528", "data": {"name": "csv.reader", "type": "callable", "signature": "(*args, **kwargs)", "description": "csv_reader = reader(iterable [, dialect='excel']\n                        [optional keyword args])\n    for row in csv_reader:\n        process(row)\n\nThe \"iterable\" argument can be any object that returns a line\nof input for each iteration, such as a file object or a list.  The\noptional \"dialect\" parameter is discussed below.  The function\nalso accepts optional keyword arguments which override settings\nprovided by the dialect.\n\nThe returned object is an iterator.  Each iteration returns a row\nof the CSV file (which can span multiple input lines).", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/528", "data": {"name": "df.plot(kind='bar', legend=False, title='Duplicate Entries').set_ylabel('Count')"}}
{"task_id": "BigCodeBench/528", "data": {"name": "file_path.strip().strip()"}}
{"task_id": "BigCodeBench/528", "data": {"name": "file_path.strip().lower()"}}
{"task_id": "BigCodeBench/528", "data": {"name": "list(reader).count(row)"}}
{"task_id": "BigCodeBench/528", "data": {"name": "matplotlib.pyplot.tight_layout", "type": "callable", "signature": "()", "description": "Adjust the padding between and around subplots.\n\nTo exclude an artist on the Axes from the bounding box calculation\nthat determines the subplot parameters (i.e. legend, or annotation),\nset ``a.set_in_layout(False)`` for that artist.\n\nParameters\n----------\npad : float, default: 1.08\n    Padding between the figure edge and the edges of subplots,\n    as a fraction of the font size.\nh_pad, w_pad : float, default: *pad*\n    Padding (height/width) between edges of adjacent subplots,\n    as a fraction of the font size.\nrect : tuple (left, bottom, right, top), default: (0, 0, 1, 1)\n    A rectangle in normalized figure coordinates into which the whole\n    subplots area (including labels) will fit.\n\nSee Also\n--------\n.Figure.set_layout_engine\n.pyplot.tight_layout", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/530", "data": {"name": "df.empty"}}
{"task_id": "BigCodeBench/530", "data": {"name": "matplotlib.pyplot.Axes", "type": "class", "signature": "(fig, *args, facecolor=None, frameon=True, sharex=None, sharey=None, label='', xscale=None, yscale=None, box_aspect=None, **kwargs)", "description": "An Axes object encapsulates all the elements of an individual (sub-)plot in\na figure.\n\nIt contains most of the (sub-)plot elements: `~.axis.Axis`,\n`~.axis.Tick`, `~.lines.Line2D`, `~.text.Text`, `~.patches.Polygon`, etc.,\nand sets the coordinate system.\n\nLike all visible elements in a figure, Axes is an `.Artist` subclass.\n\nThe `Axes` instance supports callbacks through a callbacks attribute which\nis a `~.cbook.CallbackRegistry` instance.  The events you can connect to\nare 'xlim_changed' and 'ylim_changed' and the callback will be called with\nfunc(*ax*) where *ax* is the `Axes` instance.\n\n.. note::\n\n    As a user, you do not instantiate Axes directly, but use Axes creation\n    methods instead; e.g. from `.pyplot` or `.Figure`:\n    `~.pyplot.subplots`, `~.pyplot.subplot_mosaic` or `.Figure.add_axes`.\n\nAttributes\n----------\ndataLim : `.Bbox`\n    The bounding box enclosing all data displayed in the Axes.\nviewLim : `.Bbox`\n    The view limits in data coordinates.", "parameters": {"type": "object", "properties": {"fig": {}, "facecolor": {"type": "NoneType", "default": null}, "frameon": {"type": "bool", "default": true}, "sharex": {"type": "NoneType", "default": null}, "sharey": {"type": "NoneType", "default": null}, "label": {"type": "str", "default": ""}, "xscale": {"type": "NoneType", "default": null}, "yscale": {"type": "NoneType", "default": null}, "box_aspect": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/530", "data": {"name": "matplotlib.pyplot.xlabel", "type": "callable", "signature": "(xlabel: 'str')", "description": "Set the label for the x-axis.\n\nParameters\n----------\nxlabel : str\n    The label text.\n\nlabelpad : float, default: :rc:`axes.labelpad`\n    Spacing in points from the Axes bounding box including ticks\n    and tick labels.  If None, the previous value is left as is.\n\nloc : {'left', 'center', 'right'}, default: :rc:`xaxis.labellocation`\n    The label position. This is a high-level alternative for passing\n    parameters *x* and *horizontalalignment*.\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    `.Text` properties control the appearance of the label.\n\nSee Also\n--------\ntext : Documents the properties supported by `.Text`.", "parameters": {"type": "object", "properties": {"xlabel": {"type": "str"}}}}}
{"task_id": "BigCodeBench/530", "data": {"name": "matplotlib.pyplot.ylabel", "type": "callable", "signature": "(ylabel: 'str')", "description": "Set the label for the y-axis.\n\nParameters\n----------\nylabel : str\n    The label text.\n\nlabelpad : float, default: :rc:`axes.labelpad`\n    Spacing in points from the Axes bounding box including ticks\n    and tick labels.  If None, the previous value is left as is.\n\nloc : {'bottom', 'center', 'top'}, default: :rc:`yaxis.labellocation`\n    The label position. This is a high-level alternative for passing\n    parameters *y* and *horizontalalignment*.\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    `.Text` properties control the appearance of the label.\n\nSee Also\n--------\ntext : Documents the properties supported by `.Text`.", "parameters": {"type": "object", "properties": {"ylabel": {"type": "str"}}}}}
{"task_id": "BigCodeBench/530", "data": {"name": "matplotlib.pyplot.title", "type": "callable", "signature": "(label: 'str')", "description": "Set a title for the Axes.\n\nSet one of the three available Axes titles. The available titles\nare positioned above the Axes in the center, flush with the left\nedge, and flush with the right edge.\n\nParameters\n----------\nlabel : str\n    Text to use for the title\n\nfontdict : dict\n\n    .. admonition:: Discouraged\n\n       The use of *fontdict* is discouraged. Parameters should be passed as\n       individual keyword arguments or using dictionary-unpacking\n       ``set_title(..., **fontdict)``.\n\n    A dictionary controlling the appearance of the title text,\n    the default *fontdict* is::\n\n       {'fontsize': rcParams['axes.titlesize'],\n        'fontweight': rcParams['axes.titleweight'],\n        'color': rcParams['axes.titlecolor'],\n        'verticalalignment': 'baseline',\n        'horizontalalignment': loc}\n\nloc : {'center', 'left', 'right'}, default: :rc:`axes.titlelocation`\n    Which title to set.\n\ny : float, default: :rc:`axes.titley`\n    Vertical Axes location for the title (1.0 is the top).  If\n    None (the default) and :rc:`axes.titley` is also None, y is\n    determined automatically to avoid decorators on the Axes.\n\npad : float, default: :rc:`axes.titlepad`\n    The offset of the title from the top of the Axes, in points.\n\nReturns\n-------\n`.Text`\n    The matplotlib text instance representing the title\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    Other keyword arguments are text properties, see `.Text` for a list\n    of valid text properties.", "parameters": {"type": "object", "properties": {"label": {"type": "str"}}}}}
{"task_id": "BigCodeBench/530", "data": {"name": "numpy.floor", "type": "callable", "signature": "(*args, **kwargs)", "description": "floor(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n\nReturn the floor of the input, element-wise.\n\nThe floor of the scalar `x` is the largest integer `i`, such that\n`i <= x`.  It is often denoted as :math:`\\lfloor x \\rfloor`.\n\nParameters\n----------\nx : array_like\n    Input data.\nout : ndarray, None, or tuple of ndarray and None, optional\n    A location into which the result is stored. If provided, it must have\n    a shape that the inputs broadcast to. If not provided or None,\n    a freshly-allocated array is returned. A tuple (possible only as a\n    keyword argument) must have length equal to the number of outputs.\nwhere : array_like, optional\n    This condition is broadcast over the input. At locations where the\n    condition is True, the `out` array will be set to the ufunc result.\n    Elsewhere, the `out` array will retain its original value.\n    Note that if an uninitialized `out` array is created via the default\n    ``out=None``, locations within it where the condition is False will\n    remain uninitialized.\n**kwargs\n    For other keyword-only arguments, see the\n    :ref:`ufunc docs <ufuncs.kwargs>`.\n\nReturns\n-------\ny : ndarray or scalar\n    The floor of each element in `x`.\n    This is a scalar if `x` is a scalar.\n\nSee Also\n--------\nceil, trunc, rint, fix\n\nNotes\n-----\nSome spreadsheet programs calculate the \"floor-towards-zero\", where\n``floor(-2.5) == -2``.  NumPy instead uses the definition of\n`floor` where `floor(-2.5) == -3`. The \"floor-towards-zero\"\nfunction is called ``fix`` in NumPy.\n\nExamples\n--------\n>>> a = np.array([-1.7, -1.5, -0.2, 0.2, 1.5, 1.7, 2.0])\n>>> np.floor(a)\narray([-2., -2., -1.,  0.,  1.,  1.,  2.])", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/530", "data": {"name": "numpy.arange", "type": "callable", "signature": "(*args, **kwargs)", "description": "arange([start,] stop[, step,], dtype=None, *, like=None)\n\nReturn evenly spaced values within a given interval.\n\n``arange`` can be called with a varying number of positional arguments:\n\n* ``arange(stop)``: Values are generated within the half-open interval\n  ``[0, stop)`` (in other words, the interval including `start` but\n  excluding `stop`).\n* ``arange(start, stop)``: Values are generated within the half-open\n  interval ``[start, stop)``.\n* ``arange(start, stop, step)`` Values are generated within the half-open\n  interval ``[start, stop)``, with spacing between values given by\n  ``step``.\n\nFor integer arguments the function is roughly equivalent to the Python\nbuilt-in :py:class:`range`, but returns an ndarray rather than a ``range``\ninstance.\n\nWhen using a non-integer step, such as 0.1, it is often better to use\n`numpy.linspace`.\n\nSee the Warning sections below for more information.\n\nParameters\n----------\nstart : integer or real, optional\n    Start of interval.  The interval includes this value.  The default\n    start value is 0.\nstop : integer or real\n    End of interval.  The interval does not include this value, except\n    in some cases where `step` is not an integer and floating point\n    round-off affects the length of `out`.\nstep : integer or real, optional\n    Spacing between values.  For any output `out`, this is the distance\n    between two adjacent values, ``out[i+1] - out[i]``.  The default\n    step size is 1.  If `step` is specified as a position argument,\n    `start` must also be given.\ndtype : dtype, optional\n    The type of the output array.  If `dtype` is not given, infer the data\n    type from the other input arguments.\nlike : array_like, optional\n    Reference object to allow the creation of arrays which are not\n    NumPy arrays. If an array-like passed in as ``like`` supports\n    the ``__array_function__`` protocol, the result will be defined\n    by it. In this case, it ensures the creation of an array object\n    compatible with that passed in via this argument.\n\n    .. versionadded:: 1.20.0\n\nReturns\n-------\narange : ndarray\n    Array of evenly spaced values.\n\n    For floating point arguments, the length of the result is\n    ``ceil((stop - start)/step)``.  Because of floating point overflow,\n    this rule may result in the last element of `out` being greater\n    than `stop`.\n\nWarnings\n--------\nThe length of the output might not be numerically stable.\n\nAnother stability issue is due to the internal implementation of\n`numpy.arange`.\nThe actual step value used to populate the array is\n``dtype(start + step) - dtype(start)`` and not `step`. Precision loss\ncan occur here, due to casting or due to using floating points when\n`start` is much larger than `step`. This can lead to unexpected\nbehaviour. For example::\n\n  >>> np.arange(0, 5, 0.5, dtype=int)\n  array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n  >>> np.arange(-3, 3, 0.5, dtype=int)\n  array([-3, -2, -1,  0,  1,  2,  3,  4,  5,  6,  7,  8])\n\nIn such cases, the use of `numpy.linspace` should be preferred.\n\nThe built-in :py:class:`range` generates :std:doc:`Python built-in integers\nthat have arbitrary size <c-api/long>`, while `numpy.arange` produces\n`numpy.int32` or `numpy.int64` numbers. This may result in incorrect\nresults for large integer values::\n\n  >>> power = 40\n  >>> modulo = 10000\n  >>> x1 = [(n ** power) % modulo for n in range(8)]\n  >>> x2 = [(n ** power) % modulo for n in np.arange(8)]\n  >>> print(x1)\n  [0, 1, 7776, 8801, 6176, 625, 6576, 4001]  # correct\n  >>> print(x2)\n  [0, 1, 7776, 7185, 0, 5969, 4816, 3361]  # incorrect\n\nSee Also\n--------\nnumpy.linspace : Evenly spaced numbers with careful handling of endpoints.\nnumpy.ogrid: Arrays of evenly spaced numbers in N-dimensions.\nnumpy.mgrid: Grid-shaped arrays of evenly spaced numbers in N-dimensions.\n\nExamples\n--------\n>>> np.arange(3)\narray([0, 1, 2])\n>>> np.arange(3.0)\narray([ 0.,  1.,  2.])\n>>> np.arange(3,7)\narray([3, 4, 5, 6])\n>>> np.arange(3,7,2)\narray([3, 5])", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/530", "data": {"name": "seaborn.histplot", "type": "callable", "signature": "(data=None, bins='auto', **kwargs)", "description": "Plot univariate or bivariate histograms to show distributions of datasets.\n\nA histogram is a classic visualization tool that represents the distribution\nof one or more variables by counting the number of observations that fall within\ndiscrete bins.\n\nThis function can normalize the statistic computed within each bin to estimate\nfrequency, density or probability mass, and it can add a smooth curve obtained\nusing a kernel density estimate, similar to :func:`kdeplot`.\n\nMore information is provided in the :ref:`user guide <tutorial_hist>`.\n\nParameters\n----------\ndata : :class:`pandas.DataFrame`, :class:`numpy.ndarray`, mapping, or sequence\n    Input data structure. Either a long-form collection of vectors that can be\n    assigned to named variables or a wide-form dataset that will be internally\n    reshaped.\nx, y : vectors or keys in ``data``\n    Variables that specify positions on the x and y axes.\nhue : vector or key in ``data``\n    Semantic variable that is mapped to determine the color of plot elements.\nweights : vector or key in ``data``\n    If provided, weight the contribution of the corresponding data points\n    towards the count in each bin by these factors.\nstat : str\n    Aggregate statistic to compute in each bin.\n    \n    - `count`: show the number of observations in each bin\n    - `frequency`: show the number of observations divided by the bin width\n    - `probability` or `proportion`: normalize such that bar heights sum to 1\n    - `percent`: normalize such that bar heights sum to 100\n    - `density`: normalize such that the total area of the histogram equals 1\nbins : str, number, vector, or a pair of such values\n    Generic bin parameter that can be the name of a reference rule,\n    the number of bins, or the breaks of the bins.\n    Passed to :func:`numpy.histogram_bin_edges`.\nbinwidth : number or pair of numbers\n    Width of each bin, overrides ``bins`` but can be used with\n    ``binrange``.\nbinrange : pair of numbers or a pair of pairs\n    Lowest and highest value for bin edges; can be used either\n    with ``bins`` or ``binwidth``. Defaults to data extremes.\ndiscrete : bool\n    If True, default to ``binwidth=1`` and draw the bars so that they are\n    centered on their corresponding data points. This avoids \"gaps\" that may\n    otherwise appear when using discrete (integer) data.\ncumulative : bool\n    If True, plot the cumulative counts as bins increase.\ncommon_bins : bool\n    If True, use the same bins when semantic variables produce multiple\n    plots. If using a reference rule to determine the bins, it will be computed\n    with the full dataset.\ncommon_norm : bool\n    If True and using a normalized statistic, the normalization will apply over\n    the full dataset. Otherwise, normalize each histogram independently.\nmultiple : {\"layer\", \"dodge\", \"stack\", \"fill\"}\n    Approach to resolving multiple elements when semantic mapping creates subsets.\n    Only relevant with univariate data.\nelement : {\"bars\", \"step\", \"poly\"}\n    Visual representation of the histogram statistic.\n    Only relevant with univariate data.\nfill : bool\n    If True, fill in the space under the histogram.\n    Only relevant with univariate data.\nshrink : number\n    Scale the width of each bar relative to the binwidth by this factor.\n    Only relevant with univariate data.\nkde : bool\n    If True, compute a kernel density estimate to smooth the distribution\n    and show on the plot as (one or more) line(s).\n    Only relevant with univariate data.\nkde_kws : dict\n    Parameters that control the KDE computation, as in :func:`kdeplot`.\nline_kws : dict\n    Parameters that control the KDE visualization, passed to\n    :meth:`matplotlib.axes.Axes.plot`.\nthresh : number or None\n    Cells with a statistic less than or equal to this value will be transparent.\n    Only relevant with bivariate data.\npthresh : number or None\n    Like ``thresh``, but a value in [0, 1] such that cells with aggregate counts\n    (or other statistics, when used) up to this proportion of the total will be\n    transparent.\npmax : number or None\n    A value in [0, 1] that sets that saturation point for the colormap at a value\n    such that cells below constitute this proportion of the total count (or\n    other statistic, when used).\ncbar : bool\n    If True, add a colorbar to annotate the color mapping in a bivariate plot.\n    Note: Does not currently support plots with a ``hue`` variable well.\ncbar_ax : :class:`matplotlib.axes.Axes`\n    Pre-existing axes for the colorbar.\ncbar_kws : dict\n    Additional parameters passed to :meth:`matplotlib.figure.Figure.colorbar`.\npalette : string, list, dict, or :class:`matplotlib.colors.Colormap`\n    Method for choosing the colors to use when mapping the ``hue`` semantic.\n    String values are passed to :func:`color_palette`. List or dict values\n    imply categorical mapping, while a colormap object implies numeric mapping.\nhue_order : vector of strings\n    Specify the order of processing and plotting for categorical levels of the\n    ``hue`` semantic.\nhue_norm : tuple or :class:`matplotlib.colors.Normalize`\n    Either a pair of values that set the normalization range in data units\n    or an object that will map from data units into a [0, 1] interval. Usage\n    implies numeric mapping.\ncolor : :mod:`matplotlib color <matplotlib.colors>`\n    Single color specification for when hue mapping is not used. Otherwise, the\n    plot will try to hook into the matplotlib property cycle.\nlog_scale : bool or number, or pair of bools or numbers\n    Set axis scale(s) to log. A single value sets the data axis for any numeric\n    axes in the plot. A pair of values sets each axis independently.\n    Numeric values are interpreted as the desired base (default 10).\n    When `None` or `False`, seaborn defers to the existing Axes scale.\nlegend : bool\n    If False, suppress the legend for semantic variables.\nax : :class:`matplotlib.axes.Axes`\n    Pre-existing axes for the plot. Otherwise, call :func:`matplotlib.pyplot.gca`\n    internally.\nkwargs\n    Other keyword arguments are passed to one of the following matplotlib\n    functions:\n\n    - :meth:`matplotlib.axes.Axes.bar` (univariate, element=\"bars\")\n    - :meth:`matplotlib.axes.Axes.fill_between` (univariate, other element, fill=True)\n    - :meth:`matplotlib.axes.Axes.plot` (univariate, other element, fill=False)\n    - :meth:`matplotlib.axes.Axes.pcolormesh` (bivariate)\n\nReturns\n-------\n:class:`matplotlib.axes.Axes`\n    The matplotlib axes containing the plot.\n\nSee Also\n--------\ndisplot : Figure-level interface to distribution plot functions.\nkdeplot : Plot univariate or bivariate distributions using kernel density estimation.\nrugplot : Plot a tick at each observation value along the x and/or y axes.\necdfplot : Plot empirical cumulative distribution functions.\njointplot : Draw a bivariate plot with univariate marginal distributions.\n\nNotes\n-----\n\nThe choice of bins for computing and plotting a histogram can exert\nsubstantial influence on the insights that one is able to draw from the\nvisualization. If the bins are too large, they may erase important features.\nOn the other hand, bins that are too small may be dominated by random\nvariability, obscuring the shape of the true underlying distribution. The\ndefault bin size is determined using a reference rule that depends on the\nsample size and variance. This works well in many cases, (i.e., with\n\"well-behaved\" data) but it fails in others. It is always a good to try\ndifferent bin sizes to be sure that you are not missing something important.\nThis function allows you to specify bins in several different ways, such as\nby setting the total number of bins to use, the width of each bin, or the\nspecific locations where the bins should break.\n\nExamples\n--------\n\n.. include:: ../docstrings/histplot.rst", "parameters": {"type": "object", "properties": {"data": {"type": "NoneType", "default": null}, "bins": {"type": "str", "default": "auto"}}}}}
{"task_id": "BigCodeBench/532", "data": {"name": "df.empty"}}
{"task_id": "BigCodeBench/532", "data": {"name": "matplotlib.pyplot.xlim", "type": "callable", "signature": "()", "description": "Get or set the x limits of the current axes.\n\nCall signatures::\n\n    left, right = xlim()  # return the current xlim\n    xlim((left, right))   # set the xlim to left, right\n    xlim(left, right)     # set the xlim to left, right\n\nIf you do not specify args, you can pass *left* or *right* as kwargs,\ni.e.::\n\n    xlim(right=3)  # adjust the right leaving left unchanged\n    xlim(left=1)  # adjust the left leaving right unchanged\n\nSetting limits turns autoscaling off for the x-axis.\n\nReturns\n-------\nleft, right\n    A tuple of the new x-axis limits.\n\nNotes\n-----\nCalling this function with no arguments (e.g. ``xlim()``) is the pyplot\nequivalent of calling `~.Axes.get_xlim` on the current axes.\nCalling this function with arguments is the pyplot equivalent of calling\n`~.Axes.set_xlim` on the current axes. All arguments are passed though.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/532", "data": {"name": "matplotlib.pyplot.subplots", "type": "class", "signature": "()", "description": "Create a figure and a set of subplots.\n\nThis utility wrapper makes it convenient to create common layouts of\nsubplots, including the enclosing figure object, in a single call.\n\nParameters\n----------\nnrows, ncols : int, default: 1\n    Number of rows/columns of the subplot grid.\n\nsharex, sharey : bool or {'none', 'all', 'row', 'col'}, default: False\n    Controls sharing of properties among x (*sharex*) or y (*sharey*)\n    axes:\n\n    - True or 'all': x- or y-axis will be shared among all subplots.\n    - False or 'none': each subplot x- or y-axis will be independent.\n    - 'row': each subplot row will share an x- or y-axis.\n    - 'col': each subplot column will share an x- or y-axis.\n\n    When subplots have a shared x-axis along a column, only the x tick\n    labels of the bottom subplot are created. Similarly, when subplots\n    have a shared y-axis along a row, only the y tick labels of the first\n    column subplot are created. To later turn other subplots' ticklabels\n    on, use `~matplotlib.axes.Axes.tick_params`.\n\n    When subplots have a shared axis that has units, calling\n    `~matplotlib.axis.Axis.set_units` will update each axis with the\n    new units.\n\nsqueeze : bool, default: True\n    - If True, extra dimensions are squeezed out from the returned\n      array of `~matplotlib.axes.Axes`:\n\n      - if only one subplot is constructed (nrows=ncols=1), the\n        resulting single Axes object is returned as a scalar.\n      - for Nx1 or 1xM subplots, the returned object is a 1D numpy\n        object array of Axes objects.\n      - for NxM, subplots with N>1 and M>1 are returned as a 2D array.\n\n    - If False, no squeezing at all is done: the returned Axes object is\n      always a 2D array containing Axes instances, even if it ends up\n      being 1x1.\n\nwidth_ratios : array-like of length *ncols*, optional\n    Defines the relative widths of the columns. Each column gets a\n    relative width of ``width_ratios[i] / sum(width_ratios)``.\n    If not given, all columns will have the same width.  Equivalent\n    to ``gridspec_kw={'width_ratios': [...]}``.\n\nheight_ratios : array-like of length *nrows*, optional\n    Defines the relative heights of the rows. Each row gets a\n    relative height of ``height_ratios[i] / sum(height_ratios)``.\n    If not given, all rows will have the same height. Convenience\n    for ``gridspec_kw={'height_ratios': [...]}``.\n\nsubplot_kw : dict, optional\n    Dict with keywords passed to the\n    `~matplotlib.figure.Figure.add_subplot` call used to create each\n    subplot.\n\ngridspec_kw : dict, optional\n    Dict with keywords passed to the `~matplotlib.gridspec.GridSpec`\n    constructor used to create the grid the subplots are placed on.\n\n**fig_kw\n    All additional keyword arguments are passed to the\n    `.pyplot.figure` call.\n\nReturns\n-------\nfig : `.Figure`\n\nax : `~matplotlib.axes.Axes` or array of Axes\n    *ax* can be either a single `~.axes.Axes` object, or an array of Axes\n    objects if more than one subplot was created.  The dimensions of the\n    resulting array can be controlled with the squeeze keyword, see above.\n\n    Typical idioms for handling the return value are::\n\n        # using the variable ax for single a Axes\n        fig, ax = plt.subplots()\n\n        # using the variable axs for multiple Axes\n        fig, axs = plt.subplots(2, 2)\n\n        # using tuple unpacking for multiple Axes\n        fig, (ax1, ax2) = plt.subplots(1, 2)\n        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n\n    The names ``ax`` and pluralized ``axs`` are preferred over ``axes``\n    because for the latter it's not clear if it refers to a single\n    `~.axes.Axes` instance or a collection of these.\n\nSee Also\n--------\n.pyplot.figure\n.pyplot.subplot\n.pyplot.axes\n.Figure.subplots\n.Figure.add_subplot\n\nExamples\n--------\n::\n\n    # First create some toy data:\n    x = np.linspace(0, 2*np.pi, 400)\n    y = np.sin(x**2)\n\n    # Create just a figure and only one subplot\n    fig, ax = plt.subplots()\n    ax.plot(x, y)\n    ax.set_title('Simple plot')\n\n    # Create two subplots and unpack the output array immediately\n    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n    ax1.plot(x, y)\n    ax1.set_title('Sharing Y axis')\n    ax2.scatter(x, y)\n\n    # Create four polar axes and access them through the returned array\n    fig, axs = plt.subplots(2, 2, subplot_kw=dict(projection=\"polar\"))\n    axs[0, 0].plot(x, y)\n    axs[1, 1].scatter(x, y)\n\n    # Share a X axis with each column of subplots\n    plt.subplots(2, 2, sharex='col')\n\n    # Share a Y axis with each row of subplots\n    plt.subplots(2, 2, sharey='row')\n\n    # Share both X and Y axes with all subplots\n    plt.subplots(2, 2, sharex='all', sharey='all')\n\n    # Note that this is the same as\n    plt.subplots(2, 2, sharex=True, sharey=True)\n\n    # Create figure number 10 with a single subplot\n    # and clears it if it already exists.\n    fig, ax = plt.subplots(num=10, clear=True)", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/532", "data": {"name": "matplotlib.pyplot.subplots[1].set_xlabel", "type": "method", "signature": "(xlabel, fontdict=None, labelpad=None, *, loc=None, **kwargs)", "description": "Set the label for the x-axis.\n\nParameters\n----------\nxlabel : str\n    The label text.\n\nlabelpad : float, default: :rc:`axes.labelpad`\n    Spacing in points from the Axes bounding box including ticks\n    and tick labels.  If None, the previous value is left as is.\n\nloc : {'left', 'center', 'right'}, default: :rc:`xaxis.labellocation`\n    The label position. This is a high-level alternative for passing\n    parameters *x* and *horizontalalignment*.\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    `.Text` properties control the appearance of the label.\n\nSee Also\n--------\ntext : Documents the properties supported by `.Text`.", "parameters": {"type": "object", "properties": {"xlabel": {}, "fontdict": {"type": "NoneType", "default": null}, "labelpad": {"type": "NoneType", "default": null}, "loc": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/532", "data": {"name": "matplotlib.pyplot.subplots[1].set_ylabel", "type": "method", "signature": "(ylabel, fontdict=None, labelpad=None, *, loc=None, **kwargs)", "description": "Set the label for the y-axis.\n\nParameters\n----------\nylabel : str\n    The label text.\n\nlabelpad : float, default: :rc:`axes.labelpad`\n    Spacing in points from the Axes bounding box including ticks\n    and tick labels.  If None, the previous value is left as is.\n\nloc : {'bottom', 'center', 'top'}, default: :rc:`yaxis.labellocation`\n    The label position. This is a high-level alternative for passing\n    parameters *y* and *horizontalalignment*.\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    `.Text` properties control the appearance of the label.\n\nSee Also\n--------\ntext : Documents the properties supported by `.Text`.", "parameters": {"type": "object", "properties": {"ylabel": {}, "fontdict": {"type": "NoneType", "default": null}, "labelpad": {"type": "NoneType", "default": null}, "loc": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/532", "data": {"name": "matplotlib.pyplot.subplots[1].set_title", "type": "method", "signature": "(label, fontdict=None, loc=None, pad=None, *, y=None, **kwargs)", "description": "Set a title for the Axes.\n\nSet one of the three available Axes titles. The available titles\nare positioned above the Axes in the center, flush with the left\nedge, and flush with the right edge.\n\nParameters\n----------\nlabel : str\n    Text to use for the title\n\nfontdict : dict\n\n    .. admonition:: Discouraged\n\n       The use of *fontdict* is discouraged. Parameters should be passed as\n       individual keyword arguments or using dictionary-unpacking\n       ``set_title(..., **fontdict)``.\n\n    A dictionary controlling the appearance of the title text,\n    the default *fontdict* is::\n\n       {'fontsize': rcParams['axes.titlesize'],\n        'fontweight': rcParams['axes.titleweight'],\n        'color': rcParams['axes.titlecolor'],\n        'verticalalignment': 'baseline',\n        'horizontalalignment': loc}\n\nloc : {'center', 'left', 'right'}, default: :rc:`axes.titlelocation`\n    Which title to set.\n\ny : float, default: :rc:`axes.titley`\n    Vertical Axes location for the title (1.0 is the top).  If\n    None (the default) and :rc:`axes.titley` is also None, y is\n    determined automatically to avoid decorators on the Axes.\n\npad : float, default: :rc:`axes.titlepad`\n    The offset of the title from the top of the Axes, in points.\n\nReturns\n-------\n`.Text`\n    The matplotlib text instance representing the title\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    Other keyword arguments are text properties, see `.Text` for a list\n    of valid text properties.", "parameters": {"type": "object", "properties": {"label": {}, "fontdict": {"type": "NoneType", "default": null}, "loc": {"type": "NoneType", "default": null}, "pad": {"type": "NoneType", "default": null}, "y": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/532", "data": {"name": "matplotlib.pyplot.subplots[1].plot", "type": "method", "signature": "(*args, scalex=True, scaley=True, data=None, **kwargs)", "description": "Plot y versus x as lines and/or markers.\n\nCall signatures::\n\n    plot([x], y, [fmt], *, data=None, **kwargs)\n    plot([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n\nThe coordinates of the points or line nodes are given by *x*, *y*.\n\nThe optional parameter *fmt* is a convenient way for defining basic\nformatting like color, marker and linestyle. It's a shortcut string\nnotation described in the *Notes* section below.\n\n>>> plot(x, y)        # plot x and y using default line style and color\n>>> plot(x, y, 'bo')  # plot x and y using blue circle markers\n>>> plot(y)           # plot y using x as index array 0..N-1\n>>> plot(y, 'r+')     # ditto, but with red plusses\n\nYou can use `.Line2D` properties as keyword arguments for more\ncontrol on the appearance. Line properties and *fmt* can be mixed.\nThe following two calls yield identical results:\n\n>>> plot(x, y, 'go--', linewidth=2, markersize=12)\n>>> plot(x, y, color='green', marker='o', linestyle='dashed',\n...      linewidth=2, markersize=12)\n\nWhen conflicting with *fmt*, keyword arguments take precedence.\n\n\n**Plotting labelled data**\n\nThere's a convenient way for plotting objects with labelled data (i.e.\ndata that can be accessed by index ``obj['y']``). Instead of giving\nthe data in *x* and *y*, you can provide the object in the *data*\nparameter and just give the labels for *x* and *y*::\n\n>>> plot('xlabel', 'ylabel', data=obj)\n\nAll indexable objects are supported. This could e.g. be a `dict`, a\n`pandas.DataFrame` or a structured numpy array.\n\n\n**Plotting multiple sets of data**\n\nThere are various ways to plot multiple sets of data.\n\n- The most straight forward way is just to call `plot` multiple times.\n  Example:\n\n  >>> plot(x1, y1, 'bo')\n  >>> plot(x2, y2, 'go')\n\n- If *x* and/or *y* are 2D arrays a separate data set will be drawn\n  for every column. If both *x* and *y* are 2D, they must have the\n  same shape. If only one of them is 2D with shape (N, m) the other\n  must have length N and will be used for every data set m.\n\n  Example:\n\n  >>> x = [1, 2, 3]\n  >>> y = np.array([[1, 2], [3, 4], [5, 6]])\n  >>> plot(x, y)\n\n  is equivalent to:\n\n  >>> for col in range(y.shape[1]):\n  ...     plot(x, y[:, col])\n\n- The third way is to specify multiple sets of *[x]*, *y*, *[fmt]*\n  groups::\n\n  >>> plot(x1, y1, 'g^', x2, y2, 'g-')\n\n  In this case, any additional keyword argument applies to all\n  datasets. Also, this syntax cannot be combined with the *data*\n  parameter.\n\nBy default, each line is assigned a different style specified by a\n'style cycle'. The *fmt* and line property parameters are only\nnecessary if you want explicit deviations from these defaults.\nAlternatively, you can also change the style cycle using\n:rc:`axes.prop_cycle`.\n\n\nParameters\n----------\nx, y : array-like or scalar\n    The horizontal / vertical coordinates of the data points.\n    *x* values are optional and default to ``range(len(y))``.\n\n    Commonly, these parameters are 1D arrays.\n\n    They can also be scalars, or two-dimensional (in that case, the\n    columns represent separate data sets).\n\n    These arguments cannot be passed as keywords.\n\nfmt : str, optional\n    A format string, e.g. 'ro' for red circles. See the *Notes*\n    section for a full description of the format strings.\n\n    Format strings are just an abbreviation for quickly setting\n    basic line properties. All of these and more can also be\n    controlled by keyword arguments.\n\n    This argument cannot be passed as keyword.\n\ndata : indexable object, optional\n    An object with labelled data. If given, provide the label names to\n    plot in *x* and *y*.\n\n    .. note::\n        Technically there's a slight ambiguity in calls where the\n        second label is a valid *fmt*. ``plot('n', 'o', data=obj)``\n        could be ``plt(x, y)`` or ``plt(y, fmt)``. In such cases,\n        the former interpretation is chosen, but a warning is issued.\n        You may suppress the warning by adding an empty format string\n        ``plot('n', 'o', '', data=obj)``.\n\nReturns\n-------\nlist of `.Line2D`\n    A list of lines representing the plotted data.\n\nOther Parameters\n----------------\nscalex, scaley : bool, default: True\n    These parameters determine if the view limits are adapted to the\n    data limits. The values are passed on to\n    `~.axes.Axes.autoscale_view`.\n\n**kwargs : `~matplotlib.lines.Line2D` properties, optional\n    *kwargs* are used to specify properties like a line label (for\n    auto legends), linewidth, antialiasing, marker face color.\n    Example::\n\n    >>> plot([1, 2, 3], [1, 2, 3], 'go-', label='line 1', linewidth=2)\n    >>> plot([1, 2, 3], [1, 4, 9], 'rs', label='line 2')\n\n    If you specify multiple lines with one plot call, the kwargs apply\n    to all those lines. In case the label object is iterable, each\n    element is used as labels for each set of data.\n\n    Here is a list of available `.Line2D` properties:\n\n    Properties:\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: scalar or None\n    animated: bool\n    antialiased or aa: bool\n    clip_box: `~matplotlib.transforms.BboxBase` or None\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    color or c: color\n    dash_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    dash_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    dashes: sequence of floats (on/off ink in points) or (None, None)\n    data: (2, N) array or two 1D arrays\n    drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n    figure: `~matplotlib.figure.Figure`\n    fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n    gapcolor: color or None\n    gid: str\n    in_layout: bool\n    label: object\n    linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n    linewidth or lw: float\n    marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n    markeredgecolor or mec: color\n    markeredgewidth or mew: float\n    markerfacecolor or mfc: color\n    markerfacecoloralt or mfcalt: color\n    markersize or ms: float\n    markevery: None or int or (int, int) or slice or list[int] or float or (float, float) or list[bool]\n    mouseover: bool\n    path_effects: list of `.AbstractPathEffect`\n    picker: float or callable[[Artist, Event], tuple[bool, dict]]\n    pickradius: float\n    rasterized: bool\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    solid_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    solid_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    transform: unknown\n    url: str\n    visible: bool\n    xdata: 1D array\n    ydata: 1D array\n    zorder: float\n\nSee Also\n--------\nscatter : XY scatter plot with markers of varying size and/or color (\n    sometimes also called bubble chart).\n\nNotes\n-----\n**Format Strings**\n\nA format string consists of a part for color, marker and line::\n\n    fmt = '[marker][line][color]'\n\nEach of them is optional. If not provided, the value from the style\ncycle is used. Exception: If ``line`` is given, but no ``marker``,\nthe data will be a line without markers.\n\nOther combinations such as ``[color][marker][line]`` are also\nsupported, but note that their parsing may be ambiguous.\n\n**Markers**\n\n=============   ===============================\ncharacter       description\n=============   ===============================\n``'.'``         point marker\n``','``         pixel marker\n``'o'``         circle marker\n``'v'``         triangle_down marker\n``'^'``         triangle_up marker\n``'<'``         triangle_left marker\n``'>'``         triangle_right marker\n``'1'``         tri_down marker\n``'2'``         tri_up marker\n``'3'``         tri_left marker\n``'4'``         tri_right marker\n``'8'``         octagon marker\n``'s'``         square marker\n``'p'``         pentagon marker\n``'P'``         plus (filled) marker\n``'*'``         star marker\n``'h'``         hexagon1 marker\n``'H'``         hexagon2 marker\n``'+'``         plus marker\n``'x'``         x marker\n``'X'``         x (filled) marker\n``'D'``         diamond marker\n``'d'``         thin_diamond marker\n``'|'``         vline marker\n``'_'``         hline marker\n=============   ===============================\n\n**Line Styles**\n\n=============    ===============================\ncharacter        description\n=============    ===============================\n``'-'``          solid line style\n``'--'``         dashed line style\n``'-.'``         dash-dot line style\n``':'``          dotted line style\n=============    ===============================\n\nExample format strings::\n\n    'b'    # blue markers with default shape\n    'or'   # red circles\n    '-g'   # green solid line\n    '--'   # dashed line with default color\n    '^k:'  # black triangle_up markers connected by a dotted line\n\n**Colors**\n\nThe supported color abbreviations are the single letter codes\n\n=============    ===============================\ncharacter        color\n=============    ===============================\n``'b'``          blue\n``'g'``          green\n``'r'``          red\n``'c'``          cyan\n``'m'``          magenta\n``'y'``          yellow\n``'k'``          black\n``'w'``          white\n=============    ===============================\n\nand the ``'CN'`` colors that index into the default property cycle.\n\nIf the color is the only part of the format string, you can\nadditionally use any  `matplotlib.colors` spec, e.g. full names\n(``'green'``) or hex strings (``'#008000'``).", "parameters": {"type": "object", "properties": {"scalex": {"type": "bool", "default": true}, "scaley": {"type": "bool", "default": true}, "data": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/532", "data": {"name": "matplotlib.pyplot.subplots[1].hist", "type": "method", "signature": "(x, bins=None, range=None, density=False, weights=None, cumulative=False, bottom=None, histtype='bar', align='mid', orientation='vertical', rwidth=None, log=False, color=None, label=None, stacked=False, *, data=None, **kwargs)", "description": "Compute and plot a histogram.\n\nThis method uses `numpy.histogram` to bin the data in *x* and count the\nnumber of values in each bin, then draws the distribution either as a\n`.BarContainer` or `.Polygon`. The *bins*, *range*, *density*, and\n*weights* parameters are forwarded to `numpy.histogram`.\n\nIf the data has already been binned and counted, use `~.bar` or\n`~.stairs` to plot the distribution::\n\n    counts, bins = np.histogram(x)\n    plt.stairs(counts, bins)\n\nAlternatively, plot pre-computed bins and counts using ``hist()`` by\ntreating each bin as a single point with a weight equal to its count::\n\n    plt.hist(bins[:-1], bins, weights=counts)\n\nThe data input *x* can be a singular array, a list of datasets of\npotentially different lengths ([*x0*, *x1*, ...]), or a 2D ndarray in\nwhich each column is a dataset. Note that the ndarray form is\ntransposed relative to the list form. If the input is an array, then\nthe return value is a tuple (*n*, *bins*, *patches*); if the input is a\nsequence of arrays, then the return value is a tuple\n([*n0*, *n1*, ...], *bins*, [*patches0*, *patches1*, ...]).\n\nMasked arrays are not supported.\n\nParameters\n----------\nx : (n,) array or sequence of (n,) arrays\n    Input values, this takes either a single array or a sequence of\n    arrays which are not required to be of the same length.\n\nbins : int or sequence or str, default: :rc:`hist.bins`\n    If *bins* is an integer, it defines the number of equal-width bins\n    in the range.\n\n    If *bins* is a sequence, it defines the bin edges, including the\n    left edge of the first bin and the right edge of the last bin;\n    in this case, bins may be unequally spaced.  All but the last\n    (righthand-most) bin is half-open.  In other words, if *bins* is::\n\n        [1, 2, 3, 4]\n\n    then the first bin is ``[1, 2)`` (including 1, but excluding 2) and\n    the second ``[2, 3)``.  The last bin, however, is ``[3, 4]``, which\n    *includes* 4.\n\n    If *bins* is a string, it is one of the binning strategies\n    supported by `numpy.histogram_bin_edges`: 'auto', 'fd', 'doane',\n    'scott', 'stone', 'rice', 'sturges', or 'sqrt'.\n\nrange : tuple or None, default: None\n    The lower and upper range of the bins. Lower and upper outliers\n    are ignored. If not provided, *range* is ``(x.min(), x.max())``.\n    Range has no effect if *bins* is a sequence.\n\n    If *bins* is a sequence or *range* is specified, autoscaling\n    is based on the specified bin range instead of the\n    range of x.\n\ndensity : bool, default: False\n    If ``True``, draw and return a probability density: each bin\n    will display the bin's raw count divided by the total number of\n    counts *and the bin width*\n    (``density = counts / (sum(counts) * np.diff(bins))``),\n    so that the area under the histogram integrates to 1\n    (``np.sum(density * np.diff(bins)) == 1``).\n\n    If *stacked* is also ``True``, the sum of the histograms is\n    normalized to 1.\n\nweights : (n,) array-like or None, default: None\n    An array of weights, of the same shape as *x*.  Each value in\n    *x* only contributes its associated weight towards the bin count\n    (instead of 1).  If *density* is ``True``, the weights are\n    normalized, so that the integral of the density over the range\n    remains 1.\n\ncumulative : bool or -1, default: False\n    If ``True``, then a histogram is computed where each bin gives the\n    counts in that bin plus all bins for smaller values. The last bin\n    gives the total number of datapoints.\n\n    If *density* is also ``True`` then the histogram is normalized such\n    that the last bin equals 1.\n\n    If *cumulative* is a number less than 0 (e.g., -1), the direction\n    of accumulation is reversed.  In this case, if *density* is also\n    ``True``, then the histogram is normalized such that the first bin\n    equals 1.\n\nbottom : array-like, scalar, or None, default: None\n    Location of the bottom of each bin, i.e. bins are drawn from\n    ``bottom`` to ``bottom + hist(x, bins)`` If a scalar, the bottom\n    of each bin is shifted by the same amount. If an array, each bin\n    is shifted independently and the length of bottom must match the\n    number of bins. If None, defaults to 0.\n\nhisttype : {'bar', 'barstacked', 'step', 'stepfilled'}, default: 'bar'\n    The type of histogram to draw.\n\n    - 'bar' is a traditional bar-type histogram.  If multiple data\n      are given the bars are arranged side by side.\n    - 'barstacked' is a bar-type histogram where multiple\n      data are stacked on top of each other.\n    - 'step' generates a lineplot that is by default unfilled.\n    - 'stepfilled' generates a lineplot that is by default filled.\n\nalign : {'left', 'mid', 'right'}, default: 'mid'\n    The horizontal alignment of the histogram bars.\n\n    - 'left': bars are centered on the left bin edges.\n    - 'mid': bars are centered between the bin edges.\n    - 'right': bars are centered on the right bin edges.\n\norientation : {'vertical', 'horizontal'}, default: 'vertical'\n    If 'horizontal', `~.Axes.barh` will be used for bar-type histograms\n    and the *bottom* kwarg will be the left edges.\n\nrwidth : float or None, default: None\n    The relative width of the bars as a fraction of the bin width.  If\n    ``None``, automatically compute the width.\n\n    Ignored if *histtype* is 'step' or 'stepfilled'.\n\nlog : bool, default: False\n    If ``True``, the histogram axis will be set to a log scale.\n\ncolor : color or array-like of colors or None, default: None\n    Color or sequence of colors, one per dataset.  Default (``None``)\n    uses the standard line color sequence.\n\nlabel : str or None, default: None\n    String, or sequence of strings to match multiple datasets.  Bar\n    charts yield multiple patches per dataset, but only the first gets\n    the label, so that `~.Axes.legend` will work as expected.\n\nstacked : bool, default: False\n    If ``True``, multiple data are stacked on top of each other If\n    ``False`` multiple data are arranged side by side if histtype is\n    'bar' or on top of each other if histtype is 'step'\n\nReturns\n-------\nn : array or list of arrays\n    The values of the histogram bins. See *density* and *weights* for a\n    description of the possible semantics.  If input *x* is an array,\n    then this is an array of length *nbins*. If input is a sequence of\n    arrays ``[data1, data2, ...]``, then this is a list of arrays with\n    the values of the histograms for each of the arrays in the same\n    order.  The dtype of the array *n* (or of its element arrays) will\n    always be float even if no weighting or normalization is used.\n\nbins : array\n    The edges of the bins. Length nbins + 1 (nbins left edges and right\n    edge of last bin).  Always a single array even when multiple data\n    sets are passed in.\n\npatches : `.BarContainer` or list of a single `.Polygon` or list of such objects\n    Container of individual artists used to create the histogram\n    or list of such containers if there are multiple input datasets.\n\nOther Parameters\n----------------\ndata : indexable object, optional\n    If given, the following parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` (unless this raises an exception):\n\n    *x*, *weights*\n\n**kwargs\n    `~matplotlib.patches.Patch` properties\n\nSee Also\n--------\nhist2d : 2D histogram with rectangular bins\nhexbin : 2D histogram with hexagonal bins\nstairs : Plot a pre-computed histogram\nbar : Plot a pre-computed histogram\n\nNotes\n-----\nFor large numbers of bins (>1000), plotting can be significantly\naccelerated by using `~.Axes.stairs` to plot a pre-computed histogram\n(``plt.stairs(*np.histogram(data))``), or by setting *histtype* to\n'step' or 'stepfilled' rather than 'bar' or 'barstacked'.", "parameters": {"type": "object", "properties": {"x": {}, "bins": {"type": "NoneType", "default": null}, "range": {"type": "NoneType", "default": null}, "density": {"type": "bool", "default": false}, "weights": {"type": "NoneType", "default": null}, "cumulative": {"type": "bool", "default": false}, "bottom": {"type": "NoneType", "default": null}, "histtype": {"type": "str", "default": "bar"}, "align": {"type": "str", "default": "mid"}, "orientation": {"type": "str", "default": "vertical"}, "rwidth": {"type": "NoneType", "default": null}, "log": {"type": "bool", "default": false}, "color": {"type": "NoneType", "default": null}, "label": {"type": "NoneType", "default": null}, "stacked": {"type": "bool", "default": false}, "data": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/532", "data": {"name": "numpy.linspace", "type": "callable", "signature": "(start, stop, num=50)", "description": "Return evenly spaced numbers over a specified interval.\n\nReturns `num` evenly spaced samples, calculated over the\ninterval [`start`, `stop`].\n\nThe endpoint of the interval can optionally be excluded.\n\n.. versionchanged:: 1.16.0\n    Non-scalar `start` and `stop` are now supported.\n\n.. versionchanged:: 1.20.0\n    Values are rounded towards ``-inf`` instead of ``0`` when an\n    integer ``dtype`` is specified. The old behavior can\n    still be obtained with ``np.linspace(start, stop, num).astype(int)``\n\nParameters\n----------\nstart : array_like\n    The starting value of the sequence.\nstop : array_like\n    The end value of the sequence, unless `endpoint` is set to False.\n    In that case, the sequence consists of all but the last of ``num + 1``\n    evenly spaced samples, so that `stop` is excluded.  Note that the step\n    size changes when `endpoint` is False.\nnum : int, optional\n    Number of samples to generate. Default is 50. Must be non-negative.\nendpoint : bool, optional\n    If True, `stop` is the last sample. Otherwise, it is not included.\n    Default is True.\nretstep : bool, optional\n    If True, return (`samples`, `step`), where `step` is the spacing\n    between samples.\ndtype : dtype, optional\n    The type of the output array.  If `dtype` is not given, the data type\n    is inferred from `start` and `stop`. The inferred dtype will never be\n    an integer; `float` is chosen even if the arguments would produce an\n    array of integers.\n\n    .. versionadded:: 1.9.0\n\naxis : int, optional\n    The axis in the result to store the samples.  Relevant only if start\n    or stop are array-like.  By default (0), the samples will be along a\n    new axis inserted at the beginning. Use -1 to get an axis at the end.\n\n    .. versionadded:: 1.16.0\n\nReturns\n-------\nsamples : ndarray\n    There are `num` equally spaced samples in the closed interval\n    ``[start, stop]`` or the half-open interval ``[start, stop)``\n    (depending on whether `endpoint` is True or False).\nstep : float, optional\n    Only returned if `retstep` is True\n\n    Size of spacing between samples.\n\n\nSee Also\n--------\narange : Similar to `linspace`, but uses a step size (instead of the\n         number of samples).\ngeomspace : Similar to `linspace`, but with numbers spaced evenly on a log\n            scale (a geometric progression).\nlogspace : Similar to `geomspace`, but with the end points specified as\n           logarithms.\n\nExamples\n--------\n>>> np.linspace(2.0, 3.0, num=5)\narray([2.  , 2.25, 2.5 , 2.75, 3.  ])\n>>> np.linspace(2.0, 3.0, num=5, endpoint=False)\narray([2. ,  2.2,  2.4,  2.6,  2.8])\n>>> np.linspace(2.0, 3.0, num=5, retstep=True)\n(array([2.  ,  2.25,  2.5 ,  2.75,  3.  ]), 0.25)\n\nGraphical illustration:\n\n>>> import matplotlib.pyplot as plt\n>>> N = 8\n>>> y = np.zeros(N)\n>>> x1 = np.linspace(0, 10, N, endpoint=True)\n>>> x2 = np.linspace(0, 10, N, endpoint=False)\n>>> plt.plot(x1, y, 'o')\n[<matplotlib.lines.Line2D object at 0x...>]\n>>> plt.plot(x2, y + 0.5, 'o')\n[<matplotlib.lines.Line2D object at 0x...>]\n>>> plt.ylim([-0.5, 1])\n(-0.5, 1)\n>>> plt.show()", "parameters": {"type": "object", "properties": {"start": {}, "stop": {}, "num": {"type": "integer", "default": 50}}}}}
{"task_id": "BigCodeBench/532", "data": {"name": "scipy.stats.norm.pdf", "type": "callable", "signature": "(x, *args, **kwds)", "description": "Probability density function at x of the given RV.\n\nParameters\n----------\nx : array_like\n    quantiles\narg1, arg2, arg3,... : array_like\n    The shape parameter(s) for the distribution (see docstring of the\n    instance object for more information)\nloc : array_like, optional\n    location parameter (default=0)\nscale : array_like, optional\n    scale parameter (default=1)\n\nReturns\n-------\npdf : ndarray\n    Probability density function evaluated at x", "parameters": {"type": "object", "properties": {"x": {}}}}}
{"task_id": "BigCodeBench/532", "data": {"name": "scipy.stats.norm.fit", "type": "callable", "signature": "(data)", "description": "Return estimates of shape (if applicable), location, and scale\nparameters from data. The default estimation method is Maximum\nLikelihood Estimation (MLE), but Method of Moments (MM)\nis also available.\n\nStarting estimates for the fit are given by input arguments;\nfor any arguments not provided with starting estimates,\n``self._fitstart(data)`` is called to generate such.\n\nOne can hold some parameters fixed to specific values by passing in\nkeyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)\nand ``floc`` and ``fscale`` (for location and scale parameters,\nrespectively).\n\nParameters\n----------\ndata : array_like or `CensoredData` instance\n    Data to use in estimating the distribution parameters.\narg1, arg2, arg3,... : floats, optional\n    Starting value(s) for any shape-characterizing arguments (those not\n    provided will be determined by a call to ``_fitstart(data)``).\n    No default value.\n**kwds : floats, optional\n    - `loc`: initial guess of the distribution's location parameter.\n    - `scale`: initial guess of the distribution's scale parameter.\n\n    Special keyword arguments are recognized as holding certain\n    parameters fixed:\n\n    - f0...fn : hold respective shape parameters fixed.\n      Alternatively, shape parameters to fix can be specified by name.\n      For example, if ``self.shapes == \"a, b\"``, ``fa`` and ``fix_a``\n      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are\n      equivalent to ``f1``.\n\n    - floc : hold location parameter fixed to specified value.\n\n    - fscale : hold scale parameter fixed to specified value.\n\n    - optimizer : The optimizer to use.  The optimizer must take\n      ``func`` and starting position as the first two arguments,\n      plus ``args`` (for extra arguments to pass to the\n      function to be optimized) and ``disp``. \n      The ``fit`` method calls the optimizer with ``disp=0`` to suppress output.\n      The optimizer must return the estimated parameters.\n\n    - method : The method to use. The default is \"MLE\" (Maximum\n      Likelihood Estimate); \"MM\" (Method of Moments)\n      is also available.\n\nRaises\n------\nTypeError, ValueError\n    If an input is invalid\n`~scipy.stats.FitError`\n    If fitting fails or the fit produced would be invalid\n\nReturns\n-------\nparameter_tuple : tuple of floats\n    Estimates for any shape parameters (if applicable), followed by\n    those for location and scale. For most random variables, shape\n    statistics will be returned, but there are exceptions (e.g.\n    ``norm``).\n\nNotes\n-----\nFor the normal distribution, method of moments and maximum likelihood\nestimation give identical fits, and explicit formulas for the estimates\nare available.\nThis function uses these explicit formulas for the maximum likelihood\nestimation of the normal distribution parameters, so the\n`optimizer` and `method` arguments are ignored.\n\nExamples\n--------\n\nGenerate some data to fit: draw random variates from the `beta`\ndistribution\n\n>>> import numpy as np\n>>> from scipy.stats import beta\n>>> a, b = 1., 2.\n>>> rng = np.random.default_rng(172786373191770012695001057628748821561)\n>>> x = beta.rvs(a, b, size=1000, random_state=rng)\n\nNow we can fit all four parameters (``a``, ``b``, ``loc`` and\n``scale``):\n\n>>> a1, b1, loc1, scale1 = beta.fit(x)\n>>> a1, b1, loc1, scale1\n(1.0198945204435628, 1.9484708982737828, 4.372241314917588e-05, 0.9979078845964814)\n\nThe fit can be done also using a custom optimizer:\n\n>>> from scipy.optimize import minimize\n>>> def custom_optimizer(func, x0, args=(), disp=0):\n...     res = minimize(func, x0, args, method=\"slsqp\", options={\"disp\": disp})\n...     if res.success:\n...         return res.x\n...     raise RuntimeError('optimization routine failed')\n>>> a1, b1, loc1, scale1 = beta.fit(x, method=\"MLE\", optimizer=custom_optimizer)\n>>> a1, b1, loc1, scale1\n(1.0198821087258905, 1.948484145914738, 4.3705304486881485e-05, 0.9979104663953395)\n\nWe can also use some prior knowledge about the dataset: let's keep\n``loc`` and ``scale`` fixed:\n\n>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)\n>>> loc1, scale1\n(0, 1)\n\nWe can also keep shape parameters fixed by using ``f``-keywords. To\nkeep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,\nequivalently, ``fa=1``:\n\n>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)\n>>> a1\n1\n\nNot all distributions return estimates for the shape parameters.\n``norm`` for example just returns estimates for location and scale:\n\n>>> from scipy.stats import norm\n>>> x = norm.rvs(a, b, size=1000, random_state=123)\n>>> loc1, scale1 = norm.fit(x)\n>>> loc1, scale1\n(0.92087172783841631, 2.0015750750324668)", "parameters": {"type": "object", "properties": {"data": {}}}}}
{"task_id": "BigCodeBench/553", "data": {"name": "pandas.DataFrame", "type": "class", "signature": "(data=None, index: 'Axes | None' = None, columns: 'Axes | None' = None, dtype: 'Dtype | None' = None, copy: 'bool | None' = None) -> 'None'", "description": "Two-dimensional, size-mutable, potentially heterogeneous tabular data.\n\nData structure also contains labeled axes (rows and columns).\nArithmetic operations align on both row and column labels. Can be\nthought of as a dict-like container for Series objects. The primary\npandas data structure.\n\nParameters\n----------\ndata : ndarray (structured or homogeneous), Iterable, dict, or DataFrame\n    Dict can contain Series, arrays, constants, dataclass or list-like objects. If\n    data is a dict, column order follows insertion-order. If a dict contains Series\n    which have an index defined, it is aligned by its index. This alignment also\n    occurs if data is a Series or a DataFrame itself. Alignment is done on\n    Series/DataFrame inputs.\n\n    If data is a list of dicts, column order follows insertion-order.\n\nindex : Index or array-like\n    Index to use for resulting frame. Will default to RangeIndex if\n    no indexing information part of input data and no index provided.\ncolumns : Index or array-like\n    Column labels to use for resulting frame when data does not have them,\n    defaulting to RangeIndex(0, 1, 2, ..., n). If data contains column labels,\n    will perform column selection instead.\ndtype : dtype, default None\n    Data type to force. Only a single dtype is allowed. If None, infer.\ncopy : bool or None, default None\n    Copy data from inputs.\n    For dict data, the default of None behaves like ``copy=True``.  For DataFrame\n    or 2d ndarray input, the default of None behaves like ``copy=False``.\n    If data is a dict containing one or more Series (possibly of different dtypes),\n    ``copy=False`` will ensure that these inputs are not copied.\n\n    .. versionchanged:: 1.3.0\n\nSee Also\n--------\nDataFrame.from_records : Constructor from tuples, also record arrays.\nDataFrame.from_dict : From dicts of Series, arrays, or dicts.\nread_csv : Read a comma-separated values (csv) file into DataFrame.\nread_table : Read general delimited file into DataFrame.\nread_clipboard : Read text from clipboard into DataFrame.\n\nNotes\n-----\nPlease reference the :ref:`User Guide <basics.dataframe>` for more information.\n\nExamples\n--------\nConstructing DataFrame from a dictionary.\n\n>>> d = {'col1': [1, 2], 'col2': [3, 4]}\n>>> df = pd.DataFrame(data=d)\n>>> df\n   col1  col2\n0     1     3\n1     2     4\n\nNotice that the inferred dtype is int64.\n\n>>> df.dtypes\ncol1    int64\ncol2    int64\ndtype: object\n\nTo enforce a single dtype:\n\n>>> df = pd.DataFrame(data=d, dtype=np.int8)\n>>> df.dtypes\ncol1    int8\ncol2    int8\ndtype: object\n\nConstructing DataFrame from a dictionary including Series:\n\n>>> d = {'col1': [0, 1, 2, 3], 'col2': pd.Series([2, 3], index=[2, 3])}\n>>> pd.DataFrame(data=d, index=[0, 1, 2, 3])\n   col1  col2\n0     0   NaN\n1     1   NaN\n2     2   2.0\n3     3   3.0\n\nConstructing DataFrame from numpy ndarray:\n\n>>> df2 = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),\n...                    columns=['a', 'b', 'c'])\n>>> df2\n   a  b  c\n0  1  2  3\n1  4  5  6\n2  7  8  9\n\nConstructing DataFrame from a numpy ndarray that has labeled columns:\n\n>>> data = np.array([(1, 2, 3), (4, 5, 6), (7, 8, 9)],\n...                 dtype=[(\"a\", \"i4\"), (\"b\", \"i4\"), (\"c\", \"i4\")])\n>>> df3 = pd.DataFrame(data, columns=['c', 'a'])\n...\n>>> df3\n   c  a\n0  3  1\n1  6  4\n2  9  7\n\nConstructing DataFrame from dataclass:\n\n>>> from dataclasses import make_dataclass\n>>> Point = make_dataclass(\"Point\", [(\"x\", int), (\"y\", int)])\n>>> pd.DataFrame([Point(0, 0), Point(0, 3), Point(2, 3)])\n   x  y\n0  0  0\n1  0  3\n2  2  3\n\nConstructing DataFrame from Series/DataFrame:\n\n>>> ser = pd.Series([1, 2, 3], index=[\"a\", \"b\", \"c\"])\n>>> df = pd.DataFrame(data=ser, index=[\"a\", \"c\"])\n>>> df\n   0\na  1\nc  3\n\n>>> df1 = pd.DataFrame([1, 2, 3], index=[\"a\", \"b\", \"c\"], columns=[\"x\"])\n>>> df2 = pd.DataFrame(data=df1, index=[\"a\", \"c\"])\n>>> df2\n   x\na  1\nc  3", "parameters": {"type": "object", "properties": {"data": {"type": "NoneType", "default": null}, "index": {"type": ["axes", "null"], "default": null}, "columns": {"type": ["axes", "null"], "default": null}, "dtype": {"type": ["dtype", "null"], "default": null}, "copy": {"type": ["bool", "null"], "default": null}}}}}
{"task_id": "BigCodeBench/553", "data": {"name": "pandas.DataFrame.plot", "type": "class", "signature": "()", "description": "Make plots of Series or DataFrame.\n\nUses the backend specified by the\noption ``plotting.backend``. By default, matplotlib is used.\n\nParameters\n----------\ndata : Series or DataFrame\n    The object for which the method is called.\nx : label or position, default None\n    Only used if data is a DataFrame.\ny : label, position or list of label, positions, default None\n    Allows plotting of one column versus another. Only used if data is a\n    DataFrame.\nkind : str\n    The kind of plot to produce:\n\n    - 'line' : line plot (default)\n    - 'bar' : vertical bar plot\n    - 'barh' : horizontal bar plot\n    - 'hist' : histogram\n    - 'box' : boxplot\n    - 'kde' : Kernel Density Estimation plot\n    - 'density' : same as 'kde'\n    - 'area' : area plot\n    - 'pie' : pie plot\n    - 'scatter' : scatter plot (DataFrame only)\n    - 'hexbin' : hexbin plot (DataFrame only)\nax : matplotlib axes object, default None\n    An axes of the current figure.\nsubplots : bool or sequence of iterables, default False\n    Whether to group columns into subplots:\n\n    - ``False`` : No subplots will be used\n    - ``True`` : Make separate subplots for each column.\n    - sequence of iterables of column labels: Create a subplot for each\n      group of columns. For example `[('a', 'c'), ('b', 'd')]` will\n      create 2 subplots: one with columns 'a' and 'c', and one\n      with columns 'b' and 'd'. Remaining columns that aren't specified\n      will be plotted in additional subplots (one per column).\n\n      .. versionadded:: 1.5.0\n\nsharex : bool, default True if ax is None else False\n    In case ``subplots=True``, share x axis and set some x axis labels\n    to invisible; defaults to True if ax is None otherwise False if\n    an ax is passed in; Be aware, that passing in both an ax and\n    ``sharex=True`` will alter all x axis labels for all axis in a figure.\nsharey : bool, default False\n    In case ``subplots=True``, share y axis and set some y axis labels to invisible.\nlayout : tuple, optional\n    (rows, columns) for the layout of subplots.\nfigsize : a tuple (width, height) in inches\n    Size of a figure object.\nuse_index : bool, default True\n    Use index as ticks for x axis.\ntitle : str or list\n    Title to use for the plot. If a string is passed, print the string\n    at the top of the figure. If a list is passed and `subplots` is\n    True, print each item in the list above the corresponding subplot.\ngrid : bool, default None (matlab style default)\n    Axis grid lines.\nlegend : bool or {'reverse'}\n    Place legend on axis subplots.\nstyle : list or dict\n    The matplotlib line style per column.\nlogx : bool or 'sym', default False\n    Use log scaling or symlog scaling on x axis.\n\nlogy : bool or 'sym' default False\n    Use log scaling or symlog scaling on y axis.\n\nloglog : bool or 'sym', default False\n    Use log scaling or symlog scaling on both x and y axes.\n\nxticks : sequence\n    Values to use for the xticks.\nyticks : sequence\n    Values to use for the yticks.\nxlim : 2-tuple/list\n    Set the x limits of the current axes.\nylim : 2-tuple/list\n    Set the y limits of the current axes.\nxlabel : label, optional\n    Name to use for the xlabel on x-axis. Default uses index name as xlabel, or the\n    x-column name for planar plots.\n\n    .. versionchanged:: 2.0.0\n\n        Now applicable to histograms.\n\nylabel : label, optional\n    Name to use for the ylabel on y-axis. Default will show no ylabel, or the\n    y-column name for planar plots.\n\n    .. versionchanged:: 2.0.0\n\n        Now applicable to histograms.\n\nrot : float, default None\n    Rotation for ticks (xticks for vertical, yticks for horizontal\n    plots).\nfontsize : float, default None\n    Font size for xticks and yticks.\ncolormap : str or matplotlib colormap object, default None\n    Colormap to select colors from. If string, load colormap with that\n    name from matplotlib.\ncolorbar : bool, optional\n    If True, plot colorbar (only relevant for 'scatter' and 'hexbin'\n    plots).\nposition : float\n    Specify relative alignments for bar plot layout.\n    From 0 (left/bottom-end) to 1 (right/top-end). Default is 0.5\n    (center).\ntable : bool, Series or DataFrame, default False\n    If True, draw a table using the data in the DataFrame and the data\n    will be transposed to meet matplotlib's default layout.\n    If a Series or DataFrame is passed, use passed data to draw a\n    table.\nyerr : DataFrame, Series, array-like, dict and str\n    See :ref:`Plotting with Error Bars <visualization.errorbars>` for\n    detail.\nxerr : DataFrame, Series, array-like, dict and str\n    Equivalent to yerr.\nstacked : bool, default False in line and bar plots, and True in area plot\n    If True, create stacked plot.\nsecondary_y : bool or sequence, default False\n    Whether to plot on the secondary y-axis if a list/tuple, which\n    columns to plot on secondary y-axis.\nmark_right : bool, default True\n    When using a secondary_y axis, automatically mark the column\n    labels with \"(right)\" in the legend.\ninclude_bool : bool, default is False\n    If True, boolean values can be plotted.\nbackend : str, default None\n    Backend to use instead of the backend specified in the option\n    ``plotting.backend``. For instance, 'matplotlib'. Alternatively, to\n    specify the ``plotting.backend`` for the whole session, set\n    ``pd.options.plotting.backend``.\n**kwargs\n    Options to pass to matplotlib plotting method.\n\nReturns\n-------\n:class:`matplotlib.axes.Axes` or numpy.ndarray of them\n    If the backend is not the default matplotlib one, the return value\n    will be the object returned by the backend.\n\nNotes\n-----\n- See matplotlib documentation online for more on this subject\n- If `kind` = 'bar' or 'barh', you can specify relative alignments\n  for bar plot layout by `position` keyword.\n  From 0 (left/bottom-end) to 1 (right/top-end). Default is 0.5\n  (center)\n\nExamples\n--------\nFor Series:\n\n.. plot::\n    :context: close-figs\n\n    >>> ser = pd.Series([1, 2, 3, 3])\n    >>> plot = ser.plot(kind='hist', title=\"My plot\")\n\nFor DataFrame:\n\n.. plot::\n    :context: close-figs\n\n    >>> df = pd.DataFrame({'length': [1.5, 0.5, 1.2, 0.9, 3],\n    ...                   'width': [0.7, 0.2, 0.15, 0.2, 1.1]},\n    ...                   index=['pig', 'rabbit', 'duck', 'chicken', 'horse'])\n    >>> plot = df.plot(title=\"DataFrame Plot\")\n\nFor SeriesGroupBy:\n\n.. plot::\n    :context: close-figs\n\n    >>> lst = [-1, -2, -3, 1, 2, 3]\n    >>> ser = pd.Series([1, 2, 2, 4, 6, 6], index=lst)\n    >>> plot = ser.groupby(lambda x: x > 0).plot(title=\"SeriesGroupBy Plot\")\n\nFor DataFrameGroupBy:\n\n.. plot::\n    :context: close-figs\n\n    >>> df = pd.DataFrame({\"col1\" : [1, 2, 3, 4],\n    ...                   \"col2\" : [\"A\", \"B\", \"A\", \"B\"]})\n    >>> plot = df.groupby(\"col2\").plot(kind=\"bar\", title=\"DataFrameGroupBy Plot\")", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/553", "data": {"name": "matplotlib.pyplot.show", "type": "callable", "signature": "()", "description": "Display all open figures.\n\nParameters\n----------\nblock : bool, optional\n    Whether to wait for all figures to be closed before returning.\n\n    If `True` block and run the GUI main loop until all figure windows\n    are closed.\n\n    If `False` ensure that all figure windows are displayed and return\n    immediately.  In this case, you are responsible for ensuring\n    that the event loop is running to have responsive figures.\n\n    Defaults to True in non-interactive mode and to False in interactive\n    mode (see `.pyplot.isinteractive`).\n\nSee Also\n--------\nion : Enable interactive mode, which shows / updates the figure after\n      every plotting command, so that calling ``show()`` is not necessary.\nioff : Disable interactive mode.\nsavefig : Save the figure to an image file instead of showing it on screen.\n\nNotes\n-----\n**Saving figures to file and showing a window at the same time**\n\nIf you want an image file as well as a user interface window, use\n`.pyplot.savefig` before `.pyplot.show`. At the end of (a blocking)\n``show()`` the figure is closed and thus unregistered from pyplot. Calling\n`.pyplot.savefig` afterwards would save a new and thus empty figure. This\nlimitation of command order does not apply if the show is non-blocking or\nif you keep a reference to the figure and use `.Figure.savefig`.\n\n**Auto-show in jupyter notebooks**\n\nThe jupyter backends (activated via ``%matplotlib inline``,\n``%matplotlib notebook``, or ``%matplotlib widget``), call ``show()`` at\nthe end of every cell by default. Thus, you usually don't have to call it\nexplicitly there.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/553", "data": {"name": "matplotlib.pyplot.close", "type": "callable", "signature": "(fig: \"None | int | str | Figure | Literal['all']\" = None) -> 'None)", "description": "Close a figure window.\n\nParameters\n----------\nfig : None or int or str or `.Figure`\n    The figure to close. There are a number of ways to specify this:\n\n    - *None*: the current figure\n    - `.Figure`: the given `.Figure` instance\n    - ``int``: a figure number\n    - ``str``: a figure name\n    - 'all': all figures", "parameters": {"type": "object", "properties": {"fig": {"type": ["\"none", "figure", "str", "literal['all']\"", "integer"], "default": null}}}}}
{"task_id": "BigCodeBench/553", "data": {"name": "matplotlib.pyplot.subplots", "type": "callable", "signature": "()", "description": "Create a figure and a set of subplots.\n\nThis utility wrapper makes it convenient to create common layouts of\nsubplots, including the enclosing figure object, in a single call.\n\nParameters\n----------\nnrows, ncols : int, default: 1\n    Number of rows/columns of the subplot grid.\n\nsharex, sharey : bool or {'none', 'all', 'row', 'col'}, default: False\n    Controls sharing of properties among x (*sharex*) or y (*sharey*)\n    axes:\n\n    - True or 'all': x- or y-axis will be shared among all subplots.\n    - False or 'none': each subplot x- or y-axis will be independent.\n    - 'row': each subplot row will share an x- or y-axis.\n    - 'col': each subplot column will share an x- or y-axis.\n\n    When subplots have a shared x-axis along a column, only the x tick\n    labels of the bottom subplot are created. Similarly, when subplots\n    have a shared y-axis along a row, only the y tick labels of the first\n    column subplot are created. To later turn other subplots' ticklabels\n    on, use `~matplotlib.axes.Axes.tick_params`.\n\n    When subplots have a shared axis that has units, calling\n    `~matplotlib.axis.Axis.set_units` will update each axis with the\n    new units.\n\nsqueeze : bool, default: True\n    - If True, extra dimensions are squeezed out from the returned\n      array of `~matplotlib.axes.Axes`:\n\n      - if only one subplot is constructed (nrows=ncols=1), the\n        resulting single Axes object is returned as a scalar.\n      - for Nx1 or 1xM subplots, the returned object is a 1D numpy\n        object array of Axes objects.\n      - for NxM, subplots with N>1 and M>1 are returned as a 2D array.\n\n    - If False, no squeezing at all is done: the returned Axes object is\n      always a 2D array containing Axes instances, even if it ends up\n      being 1x1.\n\nwidth_ratios : array-like of length *ncols*, optional\n    Defines the relative widths of the columns. Each column gets a\n    relative width of ``width_ratios[i] / sum(width_ratios)``.\n    If not given, all columns will have the same width.  Equivalent\n    to ``gridspec_kw={'width_ratios': [...]}``.\n\nheight_ratios : array-like of length *nrows*, optional\n    Defines the relative heights of the rows. Each row gets a\n    relative height of ``height_ratios[i] / sum(height_ratios)``.\n    If not given, all rows will have the same height. Convenience\n    for ``gridspec_kw={'height_ratios': [...]}``.\n\nsubplot_kw : dict, optional\n    Dict with keywords passed to the\n    `~matplotlib.figure.Figure.add_subplot` call used to create each\n    subplot.\n\ngridspec_kw : dict, optional\n    Dict with keywords passed to the `~matplotlib.gridspec.GridSpec`\n    constructor used to create the grid the subplots are placed on.\n\n**fig_kw\n    All additional keyword arguments are passed to the\n    `.pyplot.figure` call.\n\nReturns\n-------\nfig : `.Figure`\n\nax : `~matplotlib.axes.Axes` or array of Axes\n    *ax* can be either a single `~.axes.Axes` object, or an array of Axes\n    objects if more than one subplot was created.  The dimensions of the\n    resulting array can be controlled with the squeeze keyword, see above.\n\n    Typical idioms for handling the return value are::\n\n        # using the variable ax for single a Axes\n        fig, ax = plt.subplots()\n\n        # using the variable axs for multiple Axes\n        fig, axs = plt.subplots(2, 2)\n\n        # using tuple unpacking for multiple Axes\n        fig, (ax1, ax2) = plt.subplots(1, 2)\n        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n\n    The names ``ax`` and pluralized ``axs`` are preferred over ``axes``\n    because for the latter it's not clear if it refers to a single\n    `~.axes.Axes` instance or a collection of these.\n\nSee Also\n--------\n.pyplot.figure\n.pyplot.subplot\n.pyplot.axes\n.Figure.subplots\n.Figure.add_subplot\n\nExamples\n--------\n::\n\n    # First create some toy data:\n    x = np.linspace(0, 2*np.pi, 400)\n    y = np.sin(x**2)\n\n    # Create just a figure and only one subplot\n    fig, ax = plt.subplots()\n    ax.plot(x, y)\n    ax.set_title('Simple plot')\n\n    # Create two subplots and unpack the output array immediately\n    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n    ax1.plot(x, y)\n    ax1.set_title('Sharing Y axis')\n    ax2.scatter(x, y)\n\n    # Create four polar axes and access them through the returned array\n    fig, axs = plt.subplots(2, 2, subplot_kw=dict(projection=\"polar\"))\n    axs[0, 0].plot(x, y)\n    axs[1, 1].scatter(x, y)\n\n    # Share a X axis with each column of subplots\n    plt.subplots(2, 2, sharex='col')\n\n    # Share a Y axis with each row of subplots\n    plt.subplots(2, 2, sharey='row')\n\n    # Share both X and Y axes with all subplots\n    plt.subplots(2, 2, sharex='all', sharey='all')\n\n    # Note that this is the same as\n    plt.subplots(2, 2, sharex=True, sharey=True)\n\n    # Create figure number 10 with a single subplot\n    # and clears it if it already exists.\n    fig, ax = plt.subplots(num=10, clear=True)", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/553", "data": {"name": "numpy.random.seed", "type": "callable", "signature": "(*args, **kwargs)", "description": "seed(self, seed=None)\n\nReseed a legacy MT19937 BitGenerator\n\nNotes\n-----\nThis is a convenience, legacy function.\n\nThe best practice is to **not** reseed a BitGenerator, rather to\nrecreate a new one. This method is here for legacy reasons.\nThis example demonstrates best practice.\n\n>>> from numpy.random import MT19937\n>>> from numpy.random import RandomState, SeedSequence\n>>> rs = RandomState(MT19937(SeedSequence(123456789)))\n# Later, you want to restart the stream\n>>> rs = RandomState(MT19937(SeedSequence(987654321)))", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/553", "data": {"name": "numpy.random.randn", "type": "callable", "signature": "(*args, **kwargs)", "description": "randn(d0, d1, ..., dn)\n\nReturn a sample (or samples) from the \"standard normal\" distribution.\n\n.. note::\n    This is a convenience function for users porting code from Matlab,\n    and wraps `standard_normal`. That function takes a\n    tuple to specify the size of the output, which is consistent with\n    other NumPy functions like `numpy.zeros` and `numpy.ones`.\n\n.. note::\n    New code should use the ``standard_normal`` method of a ``default_rng()``\n    instance instead; please see the :ref:`random-quick-start`.\n\nIf positive int_like arguments are provided, `randn` generates an array\nof shape ``(d0, d1, ..., dn)``, filled\nwith random floats sampled from a univariate \"normal\" (Gaussian)\ndistribution of mean 0 and variance 1. A single float randomly sampled\nfrom the distribution is returned if no argument is provided.\n\nParameters\n----------\nd0, d1, ..., dn : int, optional\n    The dimensions of the returned array, must be non-negative.\n    If no argument is given a single Python float is returned.\n\nReturns\n-------\nZ : ndarray or float\n    A ``(d0, d1, ..., dn)``-shaped array of floating-point samples from\n    the standard normal distribution, or a single such float if\n    no parameters were supplied.\n\nSee Also\n--------\nstandard_normal : Similar, but takes a tuple as its argument.\nnormal : Also accepts mu and sigma arguments.\nrandom.Generator.standard_normal: which should be used for new code.\n\nNotes\n-----\nFor random samples from :math:`N(\\mu, \\sigma^2)`, use:\n\n``sigma * np.random.randn(...) + mu``\n\nExamples\n--------\n>>> np.random.randn()\n2.1923875335537315  # random\n\nTwo-by-four array of samples from N(3, 6.25):\n\n>>> 3 + 2.5 * np.random.randn(2, 4)\narray([[-4.49401501,  4.00950034, -1.81814867,  7.29718677],   # random\n       [ 0.39924804,  4.68456316,  4.99394529,  4.84057254]])  # random", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/560", "data": {"name": "datetime.datetime", "type": "class", "signature": "(self, /, *args, **kwargs)", "description": "datetime(year, month, day[, hour[, minute[, second[, microsecond[,tzinfo]]]]])\n\nThe year, month and day arguments are required. tzinfo may be None, or an\ninstance of a tzinfo subclass. The remaining arguments may be ints.", "parameters": {"type": "object", "properties": {"/": {}}}}}
{"task_id": "BigCodeBench/560", "data": {"name": "datetime.datetime.strptime", "type": "callable", "signature": "(*args, **kwargs)", "description": "string, format -> new datetime parsed from a string (like time.strptime()).", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/560", "data": {"name": "d.split('-')"}}
{"task_id": "BigCodeBench/560", "data": {"name": "d.rsplit('-', 1)"}}
{"task_id": "BigCodeBench/560", "data": {"name": "data.strip()"}}
{"task_id": "BigCodeBench/560", "data": {"name": "data.split(',')"}}
{"task_id": "BigCodeBench/560", "data": {"name": "df.set_index('Month').index"}}
{"task_id": "BigCodeBench/560", "data": {"name": "df.set_index('Month').set_index('Month')"}}
{"task_id": "BigCodeBench/560", "data": {"name": "matplotlib.pyplot.close", "type": "callable", "signature": "(fig: \"None | int | str | Figure | Literal['all']\" = None) -> 'None)", "description": "Close a figure window.\n\nParameters\n----------\nfig : None or int or str or `.Figure`\n    The figure to close. There are a number of ways to specify this:\n\n    - *None*: the current figure\n    - `.Figure`: the given `.Figure` instance\n    - ``int``: a figure number\n    - ``str``: a figure name\n    - 'all': all figures", "parameters": {"type": "object", "properties": {"fig": {"type": ["\"none", "figure", "str", "literal['all']\"", "integer"], "default": null}}}}}
{"task_id": "BigCodeBench/560", "data": {"name": "matplotlib.pyplot.subplots", "type": "class", "signature": "(**fig_kw) -> 'tuple[Figure)", "description": "Create a figure and a set of subplots.\n\nThis utility wrapper makes it convenient to create common layouts of\nsubplots, including the enclosing figure object, in a single call.\n\nParameters\n----------\nnrows, ncols : int, default: 1\n    Number of rows/columns of the subplot grid.\n\nsharex, sharey : bool or {'none', 'all', 'row', 'col'}, default: False\n    Controls sharing of properties among x (*sharex*) or y (*sharey*)\n    axes:\n\n    - True or 'all': x- or y-axis will be shared among all subplots.\n    - False or 'none': each subplot x- or y-axis will be independent.\n    - 'row': each subplot row will share an x- or y-axis.\n    - 'col': each subplot column will share an x- or y-axis.\n\n    When subplots have a shared x-axis along a column, only the x tick\n    labels of the bottom subplot are created. Similarly, when subplots\n    have a shared y-axis along a row, only the y tick labels of the first\n    column subplot are created. To later turn other subplots' ticklabels\n    on, use `~matplotlib.axes.Axes.tick_params`.\n\n    When subplots have a shared axis that has units, calling\n    `~matplotlib.axis.Axis.set_units` will update each axis with the\n    new units.\n\nsqueeze : bool, default: True\n    - If True, extra dimensions are squeezed out from the returned\n      array of `~matplotlib.axes.Axes`:\n\n      - if only one subplot is constructed (nrows=ncols=1), the\n        resulting single Axes object is returned as a scalar.\n      - for Nx1 or 1xM subplots, the returned object is a 1D numpy\n        object array of Axes objects.\n      - for NxM, subplots with N>1 and M>1 are returned as a 2D array.\n\n    - If False, no squeezing at all is done: the returned Axes object is\n      always a 2D array containing Axes instances, even if it ends up\n      being 1x1.\n\nwidth_ratios : array-like of length *ncols*, optional\n    Defines the relative widths of the columns. Each column gets a\n    relative width of ``width_ratios[i] / sum(width_ratios)``.\n    If not given, all columns will have the same width.  Equivalent\n    to ``gridspec_kw={'width_ratios': [...]}``.\n\nheight_ratios : array-like of length *nrows*, optional\n    Defines the relative heights of the rows. Each row gets a\n    relative height of ``height_ratios[i] / sum(height_ratios)``.\n    If not given, all rows will have the same height. Convenience\n    for ``gridspec_kw={'height_ratios': [...]}``.\n\nsubplot_kw : dict, optional\n    Dict with keywords passed to the\n    `~matplotlib.figure.Figure.add_subplot` call used to create each\n    subplot.\n\ngridspec_kw : dict, optional\n    Dict with keywords passed to the `~matplotlib.gridspec.GridSpec`\n    constructor used to create the grid the subplots are placed on.\n\n**fig_kw\n    All additional keyword arguments are passed to the\n    `.pyplot.figure` call.\n\nReturns\n-------\nfig : `.Figure`\n\nax : `~matplotlib.axes.Axes` or array of Axes\n    *ax* can be either a single `~.axes.Axes` object, or an array of Axes\n    objects if more than one subplot was created.  The dimensions of the\n    resulting array can be controlled with the squeeze keyword, see above.\n\n    Typical idioms for handling the return value are::\n\n        # using the variable ax for single a Axes\n        fig, ax = plt.subplots()\n\n        # using the variable axs for multiple Axes\n        fig, axs = plt.subplots(2, 2)\n\n        # using tuple unpacking for multiple Axes\n        fig, (ax1, ax2) = plt.subplots(1, 2)\n        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n\n    The names ``ax`` and pluralized ``axs`` are preferred over ``axes``\n    because for the latter it's not clear if it refers to a single\n    `~.axes.Axes` instance or a collection of these.\n\nSee Also\n--------\n.pyplot.figure\n.pyplot.subplot\n.pyplot.axes\n.Figure.subplots\n.Figure.add_subplot\n\nExamples\n--------\n::\n\n    # First create some toy data:\n    x = np.linspace(0, 2*np.pi, 400)\n    y = np.sin(x**2)\n\n    # Create just a figure and only one subplot\n    fig, ax = plt.subplots()\n    ax.plot(x, y)\n    ax.set_title('Simple plot')\n\n    # Create two subplots and unpack the output array immediately\n    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n    ax1.plot(x, y)\n    ax1.set_title('Sharing Y axis')\n    ax2.scatter(x, y)\n\n    # Create four polar axes and access them through the returned array\n    fig, axs = plt.subplots(2, 2, subplot_kw=dict(projection=\"polar\"))\n    axs[0, 0].plot(x, y)\n    axs[1, 1].scatter(x, y)\n\n    # Share a X axis with each column of subplots\n    plt.subplots(2, 2, sharex='col')\n\n    # Share a Y axis with each row of subplots\n    plt.subplots(2, 2, sharey='row')\n\n    # Share both X and Y axes with all subplots\n    plt.subplots(2, 2, sharex='all', sharey='all')\n\n    # Note that this is the same as\n    plt.subplots(2, 2, sharex=True, sharey=True)\n\n    # Create figure number 10 with a single subplot\n    # and clears it if it already exists.\n    fig, ax = plt.subplots(num=10, clear=True)", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/560", "data": {"name": "matplotlib.pyplot.subplots[1].set_ylabel", "type": "method", "signature": "(ylabel, fontdict=None, labelpad=None, *, loc=None, **kwargs)", "description": "Set the label for the y-axis.\n\nParameters\n----------\nylabel : str\n    The label text.\n\nlabelpad : float, default: :rc:`axes.labelpad`\n    Spacing in points from the Axes bounding box including ticks\n    and tick labels.  If None, the previous value is left as is.\n\nloc : {'bottom', 'center', 'top'}, default: :rc:`yaxis.labellocation`\n    The label position. This is a high-level alternative for passing\n    parameters *y* and *horizontalalignment*.\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    `.Text` properties control the appearance of the label.\n\nSee Also\n--------\ntext : Documents the properties supported by `.Text`.", "parameters": {"type": "object", "properties": {"ylabel": {}, "fontdict": {"type": "NoneType", "default": null}, "labelpad": {"type": "NoneType", "default": null}, "loc": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/560", "data": {"name": "matplotlib.pyplot.subplots[1].set_xlabel", "type": "method", "signature": "(xlabel, fontdict=None, labelpad=None, *, loc=None, **kwargs)", "description": "Set the label for the x-axis.\n\nParameters\n----------\nxlabel : str\n    The label text.\n\nlabelpad : float, default: :rc:`axes.labelpad`\n    Spacing in points from the Axes bounding box including ticks\n    and tick labels.  If None, the previous value is left as is.\n\nloc : {'left', 'center', 'right'}, default: :rc:`xaxis.labellocation`\n    The label position. This is a high-level alternative for passing\n    parameters *x* and *horizontalalignment*.\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    `.Text` properties control the appearance of the label.\n\nSee Also\n--------\ntext : Documents the properties supported by `.Text`.", "parameters": {"type": "object", "properties": {"xlabel": {}, "fontdict": {"type": "NoneType", "default": null}, "labelpad": {"type": "NoneType", "default": null}, "loc": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/560", "data": {"name": "matplotlib.pyplot.subplots[1].bar", "type": "method", "signature": "(x, height, width=0.8, bottom=None, *, align='center', data=None, **kwargs)", "description": "Make a bar plot.\n\nThe bars are positioned at *x* with the given *align*\\ment. Their\ndimensions are given by *height* and *width*. The vertical baseline\nis *bottom* (default 0).\n\nMany parameters can take either a single value applying to all bars\nor a sequence of values, one for each bar.\n\nParameters\n----------\nx : float or array-like\n    The x coordinates of the bars. See also *align* for the\n    alignment of the bars to the coordinates.\n\nheight : float or array-like\n    The height(s) of the bars.\n\n    Note that if *bottom* has units (e.g. datetime), *height* should be in\n    units that are a difference from the value of *bottom* (e.g. timedelta).\n\nwidth : float or array-like, default: 0.8\n    The width(s) of the bars.\n\n    Note that if *x* has units (e.g. datetime), then *width* should be in\n    units that are a difference (e.g. timedelta) around the *x* values.\n\nbottom : float or array-like, default: 0\n    The y coordinate(s) of the bottom side(s) of the bars.\n\n    Note that if *bottom* has units, then the y-axis will get a Locator and\n    Formatter appropriate for the units (e.g. dates, or categorical).\n\nalign : {'center', 'edge'}, default: 'center'\n    Alignment of the bars to the *x* coordinates:\n\n    - 'center': Center the base on the *x* positions.\n    - 'edge': Align the left edges of the bars with the *x* positions.\n\n    To align the bars on the right edge pass a negative *width* and\n    ``align='edge'``.\n\nReturns\n-------\n`.BarContainer`\n    Container with all the bars and optionally errorbars.\n\nOther Parameters\n----------------\ncolor : color or list of color, optional\n    The colors of the bar faces.\n\nedgecolor : color or list of color, optional\n    The colors of the bar edges.\n\nlinewidth : float or array-like, optional\n    Width of the bar edge(s). If 0, don't draw edges.\n\ntick_label : str or list of str, optional\n    The tick labels of the bars.\n    Default: None (Use default numeric labels.)\n\nlabel : str or list of str, optional\n    A single label is attached to the resulting `.BarContainer` as a\n    label for the whole dataset.\n    If a list is provided, it must be the same length as *x* and\n    labels the individual bars. Repeated labels are not de-duplicated\n    and will cause repeated label entries, so this is best used when\n    bars also differ in style (e.g., by passing a list to *color*.)\n\nxerr, yerr : float or array-like of shape(N,) or shape(2, N), optional\n    If not *None*, add horizontal / vertical errorbars to the bar tips.\n    The values are +/- sizes relative to the data:\n\n    - scalar: symmetric +/- values for all bars\n    - shape(N,): symmetric +/- values for each bar\n    - shape(2, N): Separate - and + values for each bar. First row\n      contains the lower errors, the second row contains the upper\n      errors.\n    - *None*: No errorbar. (Default)\n\n    See :doc:`/gallery/statistics/errorbar_features` for an example on\n    the usage of *xerr* and *yerr*.\n\necolor : color or list of color, default: 'black'\n    The line color of the errorbars.\n\ncapsize : float, default: :rc:`errorbar.capsize`\n   The length of the error bar caps in points.\n\nerror_kw : dict, optional\n    Dictionary of keyword arguments to be passed to the\n    `~.Axes.errorbar` method. Values of *ecolor* or *capsize* defined\n    here take precedence over the independent keyword arguments.\n\nlog : bool, default: False\n    If *True*, set the y-axis to be log scale.\n\ndata : indexable object, optional\n    If given, all parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` (unless this raises an exception).\n\n**kwargs : `.Rectangle` properties\n\nProperties:\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: scalar or None\n    angle: unknown\n    animated: bool\n    antialiased or aa: bool or None\n    bounds: (left, bottom, width, height)\n    capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    clip_box: `~matplotlib.transforms.BboxBase` or None\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    color: color\n    edgecolor or ec: color or None\n    facecolor or fc: color or None\n    figure: `~matplotlib.figure.Figure`\n    fill: bool\n    gid: str\n    hatch: {'/', '\\\\', '|', '-', '+', 'x', 'o', 'O', '.', '*'}\n    height: unknown\n    in_layout: bool\n    joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    label: object\n    linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n    linewidth or lw: float or None\n    mouseover: bool\n    path_effects: list of `.AbstractPathEffect`\n    picker: None or bool or float or callable\n    rasterized: bool\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    transform: `~matplotlib.transforms.Transform`\n    url: str\n    visible: bool\n    width: unknown\n    x: unknown\n    xy: (float, float)\n    y: unknown\n    zorder: float\n\nSee Also\n--------\nbarh : Plot a horizontal bar plot.\n\nNotes\n-----\nStacked bars can be achieved by passing individual *bottom* values per\nbar. See :doc:`/gallery/lines_bars_and_markers/bar_stacked`.", "parameters": {"type": "object", "properties": {"x": {}, "height": {}, "width": {"type": "float", "default": 0.8}, "bottom": {"type": "NoneType", "default": null}, "align": {"type": "str", "default": "center"}, "data": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/560", "data": {"name": "matplotlib.pyplot.subplots[1].set_title", "type": "method", "signature": "(label, fontdict=None, loc=None, pad=None, *, y=None, **kwargs)", "description": "Set a title for the Axes.\n\nSet one of the three available Axes titles. The available titles\nare positioned above the Axes in the center, flush with the left\nedge, and flush with the right edge.\n\nParameters\n----------\nlabel : str\n    Text to use for the title\n\nfontdict : dict\n\n    .. admonition:: Discouraged\n\n       The use of *fontdict* is discouraged. Parameters should be passed as\n       individual keyword arguments or using dictionary-unpacking\n       ``set_title(..., **fontdict)``.\n\n    A dictionary controlling the appearance of the title text,\n    the default *fontdict* is::\n\n       {'fontsize': rcParams['axes.titlesize'],\n        'fontweight': rcParams['axes.titleweight'],\n        'color': rcParams['axes.titlecolor'],\n        'verticalalignment': 'baseline',\n        'horizontalalignment': loc}\n\nloc : {'center', 'left', 'right'}, default: :rc:`axes.titlelocation`\n    Which title to set.\n\ny : float, default: :rc:`axes.titley`\n    Vertical Axes location for the title (1.0 is the top).  If\n    None (the default) and :rc:`axes.titley` is also None, y is\n    determined automatically to avoid decorators on the Axes.\n\npad : float, default: :rc:`axes.titlepad`\n    The offset of the title from the top of the Axes, in points.\n\nReturns\n-------\n`.Text`\n    The matplotlib text instance representing the title\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    Other keyword arguments are text properties, see `.Text` for a list\n    of valid text properties.", "parameters": {"type": "object", "properties": {"label": {}, "fontdict": {"type": "NoneType", "default": null}, "loc": {"type": "NoneType", "default": null}, "pad": {"type": "NoneType", "default": null}, "y": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/560", "data": {"name": "matplotlib.pyplot.xticks", "type": "callable", "signature": "(**kwargs) -> 'tuple[list[Tick] | np.ndarray)", "description": "Get or set the current tick locations and labels of the x-axis.\n\nPass no arguments to return the current values without modifying them.\n\nParameters\n----------\nticks : array-like, optional\n    The list of xtick locations.  Passing an empty list removes all xticks.\nlabels : array-like, optional\n    The labels to place at the given *ticks* locations.  This argument can\n    only be passed if *ticks* is passed as well.\nminor : bool, default: False\n    If ``False``, get/set the major ticks/labels; if ``True``, the minor\n    ticks/labels.\n**kwargs\n    `.Text` properties can be used to control the appearance of the labels.\n\nReturns\n-------\nlocs\n    The list of xtick locations.\nlabels\n    The list of xlabel `.Text` objects.\n\nNotes\n-----\nCalling this function with no arguments (e.g. ``xticks()``) is the pyplot\nequivalent of calling `~.Axes.get_xticks` and `~.Axes.get_xticklabels` on\nthe current axes.\nCalling this function with arguments is the pyplot equivalent of calling\n`~.Axes.set_xticks` and `~.Axes.set_xticklabels` on the current axes.\n\nExamples\n--------\n>>> locs, labels = xticks()  # Get the current locations and labels.\n>>> xticks(np.arange(0, 1, step=0.2))  # Set label locations.\n>>> xticks(np.arange(3), ['Tom', 'Dick', 'Sue'])  # Set text labels.\n>>> xticks([0, 1, 2], ['January', 'February', 'March'],\n...        rotation=20)  # Set text labels and properties.\n>>> xticks([])  # Disable xticks.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/567", "data": {"name": "data.split('-').split('-')"}}
{"task_id": "BigCodeBench/567", "data": {"name": "matplotlib.pyplot.gca", "type": "callable", "signature": "()", "description": "Get the current Axes.\n\nIf there is currently no Axes on this Figure, a new one is created\nusing `.Figure.add_subplot`.  (To test whether there is currently an\nAxes on a Figure, check whether ``figure.axes`` is empty.  To test\nwhether there is currently a Figure on the pyplot figure stack, check\nwhether `.pyplot.get_fignums()` is empty.)", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/567", "data": {"name": "matplotlib.pyplot.gca.set_xlabel", "type": "callable", "signature": "() -> 'Axes)", "description": "Get the current Axes.\n\nIf there is currently no Axes on this Figure, a new one is created\nusing `.Figure.add_subplot`.  (To test whether there is currently an\nAxes on a Figure, check whether ``figure.axes`` is empty.  To test\nwhether there is currently a Figure on the pyplot figure stack, check\nwhether `.pyplot.get_fignums()` is empty.)", "parameters": {"type": "object", "properties": {"-> 'Axes": {}}}}}
{"task_id": "BigCodeBench/567", "data": {"name": "matplotlib.pyplot.gca.set_ylabel", "type": "callable", "signature": "() -> 'Axes)", "description": "Get the current Axes.\n\nIf there is currently no Axes on this Figure, a new one is created\nusing `.Figure.add_subplot`.  (To test whether there is currently an\nAxes on a Figure, check whether ``figure.axes`` is empty.  To test\nwhether there is currently a Figure on the pyplot figure stack, check\nwhether `.pyplot.get_fignums()` is empty.)", "parameters": {"type": "object", "properties": {"-> 'Axes": {}}}}}
{"task_id": "BigCodeBench/567", "data": {"name": "matplotlib.pyplot.gca.set_title", "type": "callable", "signature": "() -> 'Axes)", "description": "Get the current Axes.\n\nIf there is currently no Axes on this Figure, a new one is created\nusing `.Figure.add_subplot`.  (To test whether there is currently an\nAxes on a Figure, check whether ``figure.axes`` is empty.  To test\nwhether there is currently a Figure on the pyplot figure stack, check\nwhether `.pyplot.get_fignums()` is empty.)", "parameters": {"type": "object", "properties": {"-> 'Axes": {}}}}}
{"task_id": "BigCodeBench/567", "data": {"name": "matplotlib.pyplot.gca.set_xticks", "type": "callable", "signature": "() -> 'Axes)", "description": "Get the current Axes.\n\nIf there is currently no Axes on this Figure, a new one is created\nusing `.Figure.add_subplot`.  (To test whether there is currently an\nAxes on a Figure, check whether ``figure.axes`` is empty.  To test\nwhether there is currently a Figure on the pyplot figure stack, check\nwhether `.pyplot.get_fignums()` is empty.)", "parameters": {"type": "object", "properties": {"-> 'Axes": {}}}}}
{"task_id": "BigCodeBench/567", "data": {"name": "matplotlib.pyplot.gca.hist", "type": "callable", "signature": "() -> 'Axes)", "description": "Get the current Axes.\n\nIf there is currently no Axes on this Figure, a new one is created\nusing `.Figure.add_subplot`.  (To test whether there is currently an\nAxes on a Figure, check whether ``figure.axes`` is empty.  To test\nwhether there is currently a Figure on the pyplot figure stack, check\nwhether `.pyplot.get_fignums()` is empty.)", "parameters": {"type": "object", "properties": {"-> 'Axes": {}}}}}
{"task_id": "BigCodeBench/567", "data": {"name": "matplotlib.pyplot.show", "type": "callable", "signature": "()", "description": "Display all open figures.\n\nParameters\n----------\nblock : bool, optional\n    Whether to wait for all figures to be closed before returning.\n\n    If `True` block and run the GUI main loop until all figure windows\n    are closed.\n\n    If `False` ensure that all figure windows are displayed and return\n    immediately.  In this case, you are responsible for ensuring\n    that the event loop is running to have responsive figures.\n\n    Defaults to True in non-interactive mode and to False in interactive\n    mode (see `.pyplot.isinteractive`).\n\nSee Also\n--------\nion : Enable interactive mode, which shows / updates the figure after\n      every plotting command, so that calling ``show()`` is not necessary.\nioff : Disable interactive mode.\nsavefig : Save the figure to an image file instead of showing it on screen.\n\nNotes\n-----\n**Saving figures to file and showing a window at the same time**\n\nIf you want an image file as well as a user interface window, use\n`.pyplot.savefig` before `.pyplot.show`. At the end of (a blocking)\n``show()`` the figure is closed and thus unregistered from pyplot. Calling\n`.pyplot.savefig` afterwards would save a new and thus empty figure. This\nlimitation of command order does not apply if the show is non-blocking or\nif you keep a reference to the figure and use `.Figure.savefig`.\n\n**Auto-show in jupyter notebooks**\n\nThe jupyter backends (activated via ``%matplotlib inline``,\n``%matplotlib notebook``, or ``%matplotlib widget``), call ``show()`` at\nthe end of every cell by default. Thus, you usually don't have to call it\nexplicitly there.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/567", "data": {"name": "matplotlib.pyplot.figure", "type": "callable", "signature": "(figsize: 'tuple[float, **kwargs) -> 'Figure)", "description": "Create a new figure, or activate an existing figure.\n\nParameters\n----------\nnum : int or str or `.Figure` or `.SubFigure`, optional\n    A unique identifier for the figure.\n\n    If a figure with that identifier already exists, this figure is made\n    active and returned. An integer refers to the ``Figure.number``\n    attribute, a string refers to the figure label.\n\n    If there is no figure with the identifier or *num* is not given, a new\n    figure is created, made active and returned.  If *num* is an int, it\n    will be used for the ``Figure.number`` attribute, otherwise, an\n    auto-generated integer value is used (starting at 1 and incremented\n    for each new figure). If *num* is a string, the figure label and the\n    window title is set to this value.  If num is a ``SubFigure``, its\n    parent ``Figure`` is activated.\n\nfigsize : (float, float), default: :rc:`figure.figsize`\n    Width, height in inches.\n\ndpi : float, default: :rc:`figure.dpi`\n    The resolution of the figure in dots-per-inch.\n\nfacecolor : color, default: :rc:`figure.facecolor`\n    The background color.\n\nedgecolor : color, default: :rc:`figure.edgecolor`\n    The border color.\n\nframeon : bool, default: True\n    If False, suppress drawing the figure frame.\n\nFigureClass : subclass of `~matplotlib.figure.Figure`\n    If set, an instance of this subclass will be created, rather than a\n    plain `.Figure`.\n\nclear : bool, default: False\n    If True and the figure already exists, then it is cleared.\n\nlayout : {'constrained', 'compressed', 'tight', 'none', `.LayoutEngine`, None}, default: None\n    The layout mechanism for positioning of plot elements to avoid\n    overlapping Axes decorations (labels, ticks, etc). Note that layout\n    managers can measurably slow down figure display.\n\n    - 'constrained': The constrained layout solver adjusts axes sizes\n      to avoid overlapping axes decorations.  Can handle complex plot\n      layouts and colorbars, and is thus recommended.\n\n      See :ref:`constrainedlayout_guide`\n      for examples.\n\n    - 'compressed': uses the same algorithm as 'constrained', but\n      removes extra space between fixed-aspect-ratio Axes.  Best for\n      simple grids of axes.\n\n    - 'tight': Use the tight layout mechanism. This is a relatively\n      simple algorithm that adjusts the subplot parameters so that\n      decorations do not overlap. See `.Figure.set_tight_layout` for\n      further details.\n\n    - 'none': Do not use a layout engine.\n\n    - A `.LayoutEngine` instance. Builtin layout classes are\n      `.ConstrainedLayoutEngine` and `.TightLayoutEngine`, more easily\n      accessible by 'constrained' and 'tight'.  Passing an instance\n      allows third parties to provide their own layout engine.\n\n    If not given, fall back to using the parameters *tight_layout* and\n    *constrained_layout*, including their config defaults\n    :rc:`figure.autolayout` and :rc:`figure.constrained_layout.use`.\n\n**kwargs\n    Additional keyword arguments are passed to the `.Figure` constructor.\n\nReturns\n-------\n`~matplotlib.figure.Figure`\n\nNotes\n-----\nA newly created figure is passed to the `~.FigureCanvasBase.new_manager`\nmethod or the `new_figure_manager` function provided by the current\nbackend, which install a canvas and a manager on the figure.\n\nOnce this is done, :rc:`figure.hooks` are called, one at a time, on the\nfigure; these hooks allow arbitrary customization of the figure (e.g.,\nattaching callbacks) or of associated elements (e.g., modifying the\ntoolbar).  See :doc:`/gallery/user_interfaces/mplcvd` for an example of\ntoolbar customization.\n\nIf you are creating many figures, make sure you explicitly call\n`.pyplot.close` on the figures you are not using, because this will\nenable pyplot to properly clean up the memory.\n\n`~matplotlib.rcParams` defines the default values, which can be modified\nin the matplotlibrc file.", "parameters": {"type": "object", "properties": {"figsize": {"type": "tuple[float"}}}}}
{"task_id": "BigCodeBench/567", "data": {"name": "numpy.arange", "type": "callable", "signature": "(*args, **kwargs)", "description": "arange([start,] stop[, step,], dtype=None, *, like=None)\n\nReturn evenly spaced values within a given interval.\n\n``arange`` can be called with a varying number of positional arguments:\n\n* ``arange(stop)``: Values are generated within the half-open interval\n  ``[0, stop)`` (in other words, the interval including `start` but\n  excluding `stop`).\n* ``arange(start, stop)``: Values are generated within the half-open\n  interval ``[start, stop)``.\n* ``arange(start, stop, step)`` Values are generated within the half-open\n  interval ``[start, stop)``, with spacing between values given by\n  ``step``.\n\nFor integer arguments the function is roughly equivalent to the Python\nbuilt-in :py:class:`range`, but returns an ndarray rather than a ``range``\ninstance.\n\nWhen using a non-integer step, such as 0.1, it is often better to use\n`numpy.linspace`.\n\nSee the Warning sections below for more information.\n\nParameters\n----------\nstart : integer or real, optional\n    Start of interval.  The interval includes this value.  The default\n    start value is 0.\nstop : integer or real\n    End of interval.  The interval does not include this value, except\n    in some cases where `step` is not an integer and floating point\n    round-off affects the length of `out`.\nstep : integer or real, optional\n    Spacing between values.  For any output `out`, this is the distance\n    between two adjacent values, ``out[i+1] - out[i]``.  The default\n    step size is 1.  If `step` is specified as a position argument,\n    `start` must also be given.\ndtype : dtype, optional\n    The type of the output array.  If `dtype` is not given, infer the data\n    type from the other input arguments.\nlike : array_like, optional\n    Reference object to allow the creation of arrays which are not\n    NumPy arrays. If an array-like passed in as ``like`` supports\n    the ``__array_function__`` protocol, the result will be defined\n    by it. In this case, it ensures the creation of an array object\n    compatible with that passed in via this argument.\n\n    .. versionadded:: 1.20.0\n\nReturns\n-------\narange : ndarray\n    Array of evenly spaced values.\n\n    For floating point arguments, the length of the result is\n    ``ceil((stop - start)/step)``.  Because of floating point overflow,\n    this rule may result in the last element of `out` being greater\n    than `stop`.\n\nWarnings\n--------\nThe length of the output might not be numerically stable.\n\nAnother stability issue is due to the internal implementation of\n`numpy.arange`.\nThe actual step value used to populate the array is\n``dtype(start + step) - dtype(start)`` and not `step`. Precision loss\ncan occur here, due to casting or due to using floating points when\n`start` is much larger than `step`. This can lead to unexpected\nbehaviour. For example::\n\n  >>> np.arange(0, 5, 0.5, dtype=int)\n  array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n  >>> np.arange(-3, 3, 0.5, dtype=int)\n  array([-3, -2, -1,  0,  1,  2,  3,  4,  5,  6,  7,  8])\n\nIn such cases, the use of `numpy.linspace` should be preferred.\n\nThe built-in :py:class:`range` generates :std:doc:`Python built-in integers\nthat have arbitrary size <c-api/long>`, while `numpy.arange` produces\n`numpy.int32` or `numpy.int64` numbers. This may result in incorrect\nresults for large integer values::\n\n  >>> power = 40\n  >>> modulo = 10000\n  >>> x1 = [(n ** power) % modulo for n in range(8)]\n  >>> x2 = [(n ** power) % modulo for n in np.arange(8)]\n  >>> print(x1)\n  [0, 1, 7776, 8801, 6176, 625, 6576, 4001]  # correct\n  >>> print(x2)\n  [0, 1, 7776, 7185, 0, 5969, 4816, 3361]  # incorrect\n\nSee Also\n--------\nnumpy.linspace : Evenly spaced numbers with careful handling of endpoints.\nnumpy.ogrid: Arrays of evenly spaced numbers in N-dimensions.\nnumpy.mgrid: Grid-shaped arrays of evenly spaced numbers in N-dimensions.\n\nExamples\n--------\n>>> np.arange(3)\narray([0, 1, 2])\n>>> np.arange(3.0)\narray([ 0.,  1.,  2.])\n>>> np.arange(3,7)\narray([3, 4, 5, 6])\n>>> np.arange(3,7,2)\narray([3, 5])", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/574", "data": {"name": "matplotlib.pyplot.subplots", "type": "class", "signature": "()", "description": "Create a figure and a set of subplots.\n\nThis utility wrapper makes it convenient to create common layouts of\nsubplots, including the enclosing figure object, in a single call.\n\nParameters\n----------\nnrows, ncols : int, default: 1\n    Number of rows/columns of the subplot grid.\n\nsharex, sharey : bool or {'none', 'all', 'row', 'col'}, default: False\n    Controls sharing of properties among x (*sharex*) or y (*sharey*)\n    axes:\n\n    - True or 'all': x- or y-axis will be shared among all subplots.\n    - False or 'none': each subplot x- or y-axis will be independent.\n    - 'row': each subplot row will share an x- or y-axis.\n    - 'col': each subplot column will share an x- or y-axis.\n\n    When subplots have a shared x-axis along a column, only the x tick\n    labels of the bottom subplot are created. Similarly, when subplots\n    have a shared y-axis along a row, only the y tick labels of the first\n    column subplot are created. To later turn other subplots' ticklabels\n    on, use `~matplotlib.axes.Axes.tick_params`.\n\n    When subplots have a shared axis that has units, calling\n    `~matplotlib.axis.Axis.set_units` will update each axis with the\n    new units.\n\nsqueeze : bool, default: True\n    - If True, extra dimensions are squeezed out from the returned\n      array of `~matplotlib.axes.Axes`:\n\n      - if only one subplot is constructed (nrows=ncols=1), the\n        resulting single Axes object is returned as a scalar.\n      - for Nx1 or 1xM subplots, the returned object is a 1D numpy\n        object array of Axes objects.\n      - for NxM, subplots with N>1 and M>1 are returned as a 2D array.\n\n    - If False, no squeezing at all is done: the returned Axes object is\n      always a 2D array containing Axes instances, even if it ends up\n      being 1x1.\n\nwidth_ratios : array-like of length *ncols*, optional\n    Defines the relative widths of the columns. Each column gets a\n    relative width of ``width_ratios[i] / sum(width_ratios)``.\n    If not given, all columns will have the same width.  Equivalent\n    to ``gridspec_kw={'width_ratios': [...]}``.\n\nheight_ratios : array-like of length *nrows*, optional\n    Defines the relative heights of the rows. Each row gets a\n    relative height of ``height_ratios[i] / sum(height_ratios)``.\n    If not given, all rows will have the same height. Convenience\n    for ``gridspec_kw={'height_ratios': [...]}``.\n\nsubplot_kw : dict, optional\n    Dict with keywords passed to the\n    `~matplotlib.figure.Figure.add_subplot` call used to create each\n    subplot.\n\ngridspec_kw : dict, optional\n    Dict with keywords passed to the `~matplotlib.gridspec.GridSpec`\n    constructor used to create the grid the subplots are placed on.\n\n**fig_kw\n    All additional keyword arguments are passed to the\n    `.pyplot.figure` call.\n\nReturns\n-------\nfig : `.Figure`\n\nax : `~matplotlib.axes.Axes` or array of Axes\n    *ax* can be either a single `~.axes.Axes` object, or an array of Axes\n    objects if more than one subplot was created.  The dimensions of the\n    resulting array can be controlled with the squeeze keyword, see above.\n\n    Typical idioms for handling the return value are::\n\n        # using the variable ax for single a Axes\n        fig, ax = plt.subplots()\n\n        # using the variable axs for multiple Axes\n        fig, axs = plt.subplots(2, 2)\n\n        # using tuple unpacking for multiple Axes\n        fig, (ax1, ax2) = plt.subplots(1, 2)\n        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n\n    The names ``ax`` and pluralized ``axs`` are preferred over ``axes``\n    because for the latter it's not clear if it refers to a single\n    `~.axes.Axes` instance or a collection of these.\n\nSee Also\n--------\n.pyplot.figure\n.pyplot.subplot\n.pyplot.axes\n.Figure.subplots\n.Figure.add_subplot\n\nExamples\n--------\n::\n\n    # First create some toy data:\n    x = np.linspace(0, 2*np.pi, 400)\n    y = np.sin(x**2)\n\n    # Create just a figure and only one subplot\n    fig, ax = plt.subplots()\n    ax.plot(x, y)\n    ax.set_title('Simple plot')\n\n    # Create two subplots and unpack the output array immediately\n    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n    ax1.plot(x, y)\n    ax1.set_title('Sharing Y axis')\n    ax2.scatter(x, y)\n\n    # Create four polar axes and access them through the returned array\n    fig, axs = plt.subplots(2, 2, subplot_kw=dict(projection=\"polar\"))\n    axs[0, 0].plot(x, y)\n    axs[1, 1].scatter(x, y)\n\n    # Share a X axis with each column of subplots\n    plt.subplots(2, 2, sharex='col')\n\n    # Share a Y axis with each row of subplots\n    plt.subplots(2, 2, sharey='row')\n\n    # Share both X and Y axes with all subplots\n    plt.subplots(2, 2, sharex='all', sharey='all')\n\n    # Note that this is the same as\n    plt.subplots(2, 2, sharex=True, sharey=True)\n\n    # Create figure number 10 with a single subplot\n    # and clears it if it already exists.\n    fig, ax = plt.subplots(num=10, clear=True)", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/574", "data": {"name": "matplotlib.pyplot.subplots[1].legend", "type": "method", "signature": "(*args, **kwargs)", "description": "Place a legend on the Axes.\n\nCall signatures::\n\n    legend()\n    legend(handles, labels)\n    legend(handles=handles)\n    legend(labels)\n\nThe call signatures correspond to the following different ways to use\nthis method:\n\n**1. Automatic detection of elements to be shown in the legend**\n\nThe elements to be added to the legend are automatically determined,\nwhen you do not pass in any extra arguments.\n\nIn this case, the labels are taken from the artist. You can specify\nthem either at artist creation or by calling the\n:meth:`~.Artist.set_label` method on the artist::\n\n    ax.plot([1, 2, 3], label='Inline label')\n    ax.legend()\n\nor::\n\n    line, = ax.plot([1, 2, 3])\n    line.set_label('Label via method')\n    ax.legend()\n\n.. note::\n    Specific artists can be excluded from the automatic legend element\n    selection by using a label starting with an underscore, \"_\".\n    A string starting with an underscore is the default label for all\n    artists, so calling `.Axes.legend` without any arguments and\n    without setting the labels manually will result in no legend being\n    drawn.\n\n\n**2. Explicitly listing the artists and labels in the legend**\n\nFor full control of which artists have a legend entry, it is possible\nto pass an iterable of legend artists followed by an iterable of\nlegend labels respectively::\n\n    ax.legend([line1, line2, line3], ['label1', 'label2', 'label3'])\n\n\n**3. Explicitly listing the artists in the legend**\n\nThis is similar to 2, but the labels are taken from the artists'\nlabel properties. Example::\n\n    line1, = ax.plot([1, 2, 3], label='label1')\n    line2, = ax.plot([1, 2, 3], label='label2')\n    ax.legend(handles=[line1, line2])\n\n\n**4. Labeling existing plot elements**\n\n.. admonition:: Discouraged\n\n    This call signature is discouraged, because the relation between\n    plot elements and labels is only implicit by their order and can\n    easily be mixed up.\n\nTo make a legend for all artists on an Axes, call this function with\nan iterable of strings, one for each legend item. For example::\n\n    ax.plot([1, 2, 3])\n    ax.plot([5, 6, 7])\n    ax.legend(['First line', 'Second line'])\n\n\nParameters\n----------\nhandles : list of (`.Artist` or tuple of `.Artist`), optional\n    A list of Artists (lines, patches) to be added to the legend.\n    Use this together with *labels*, if you need full control on what\n    is shown in the legend and the automatic mechanism described above\n    is not sufficient.\n\n    The length of handles and labels should be the same in this\n    case. If they are not, they are truncated to the smaller length.\n\n    If an entry contains a tuple, then the legend handler for all Artists in the\n    tuple will be placed alongside a single label.\n\nlabels : list of str, optional\n    A list of labels to show next to the artists.\n    Use this together with *handles*, if you need full control on what\n    is shown in the legend and the automatic mechanism described above\n    is not sufficient.\n\nReturns\n-------\n`~matplotlib.legend.Legend`\n\nOther Parameters\n----------------\n\nloc : str or pair of floats, default: :rc:`legend.loc`\n    The location of the legend.\n\n    The strings ``'upper left'``, ``'upper right'``, ``'lower left'``,\n    ``'lower right'`` place the legend at the corresponding corner of the\n    axes.\n\n    The strings ``'upper center'``, ``'lower center'``, ``'center left'``,\n    ``'center right'`` place the legend at the center of the corresponding edge\n    of the axes.\n\n    The string ``'center'`` places the legend at the center of the axes.\n\n    The string ``'best'`` places the legend at the location, among the nine\n    locations defined so far, with the minimum overlap with other drawn\n    artists.  This option can be quite slow for plots with large amounts of\n    data; your plotting speed may benefit from providing a specific location.\n\n    The location can also be a 2-tuple giving the coordinates of the lower-left\n    corner of the legend in axes coordinates (in which case *bbox_to_anchor*\n    will be ignored).\n\n    For back-compatibility, ``'center right'`` (but no other location) can also\n    be spelled ``'right'``, and each \"string\" location can also be given as a\n    numeric value:\n\n    ==================   =============\n    Location String      Location Code\n    ==================   =============\n    'best' (Axes only)   0\n    'upper right'        1\n    'upper left'         2\n    'lower left'         3\n    'lower right'        4\n    'right'              5\n    'center left'        6\n    'center right'       7\n    'lower center'       8\n    'upper center'       9\n    'center'             10\n    ==================   =============\n    \nbbox_to_anchor : `.BboxBase`, 2-tuple, or 4-tuple of floats\n    Box that is used to position the legend in conjunction with *loc*.\n    Defaults to `axes.bbox` (if called as a method to `.Axes.legend`) or\n    `figure.bbox` (if `.Figure.legend`).  This argument allows arbitrary\n    placement of the legend.\n\n    Bbox coordinates are interpreted in the coordinate system given by\n    *bbox_transform*, with the default transform\n    Axes or Figure coordinates, depending on which ``legend`` is called.\n\n    If a 4-tuple or `.BboxBase` is given, then it specifies the bbox\n    ``(x, y, width, height)`` that the legend is placed in.\n    To put the legend in the best location in the bottom right\n    quadrant of the axes (or figure)::\n\n        loc='best', bbox_to_anchor=(0.5, 0., 0.5, 0.5)\n\n    A 2-tuple ``(x, y)`` places the corner of the legend specified by *loc* at\n    x, y.  For example, to put the legend's upper right-hand corner in the\n    center of the axes (or figure) the following keywords can be used::\n\n        loc='upper right', bbox_to_anchor=(0.5, 0.5)\n\nncols : int, default: 1\n    The number of columns that the legend has.\n\n    For backward compatibility, the spelling *ncol* is also supported\n    but it is discouraged. If both are given, *ncols* takes precedence.\n\nprop : None or `~matplotlib.font_manager.FontProperties` or dict\n    The font properties of the legend. If None (default), the current\n    :data:`matplotlib.rcParams` will be used.\n\nfontsize : int or {'xx-small', 'x-small', 'small', 'medium', 'large', 'x-large', 'xx-large'}\n    The font size of the legend. If the value is numeric the size will be the\n    absolute font size in points. String values are relative to the current\n    default font size. This argument is only used if *prop* is not specified.\n\nlabelcolor : str or list, default: :rc:`legend.labelcolor`\n    The color of the text in the legend. Either a valid color string\n    (for example, 'red'), or a list of color strings. The labelcolor can\n    also be made to match the color of the line or marker using 'linecolor',\n    'markerfacecolor' (or 'mfc'), or 'markeredgecolor' (or 'mec').\n\n    Labelcolor can be set globally using :rc:`legend.labelcolor`. If None,\n    use :rc:`text.color`.\n\nnumpoints : int, default: :rc:`legend.numpoints`\n    The number of marker points in the legend when creating a legend\n    entry for a `.Line2D` (line).\n\nscatterpoints : int, default: :rc:`legend.scatterpoints`\n    The number of marker points in the legend when creating\n    a legend entry for a `.PathCollection` (scatter plot).\n\nscatteryoffsets : iterable of floats, default: ``[0.375, 0.5, 0.3125]``\n    The vertical offset (relative to the font size) for the markers\n    created for a scatter plot legend entry. 0.0 is at the base the\n    legend text, and 1.0 is at the top. To draw all markers at the\n    same height, set to ``[0.5]``.\n\nmarkerscale : float, default: :rc:`legend.markerscale`\n    The relative size of legend markers compared to the originally drawn ones.\n\nmarkerfirst : bool, default: True\n    If *True*, legend marker is placed to the left of the legend label.\n    If *False*, legend marker is placed to the right of the legend label.\n\nreverse : bool, default: False\n    If *True*, the legend labels are displayed in reverse order from the input.\n    If *False*, the legend labels are displayed in the same order as the input.\n\n    .. versionadded:: 3.7\n\nframeon : bool, default: :rc:`legend.frameon`\n    Whether the legend should be drawn on a patch (frame).\n\nfancybox : bool, default: :rc:`legend.fancybox`\n    Whether round edges should be enabled around the `.FancyBboxPatch` which\n    makes up the legend's background.\n\nshadow : None, bool or dict, default: :rc:`legend.shadow`\n    Whether to draw a shadow behind the legend.\n    The shadow can be configured using `.Patch` keywords.\n    Customization via :rc:`legend.shadow` is currently not supported.\n\nframealpha : float, default: :rc:`legend.framealpha`\n    The alpha transparency of the legend's background.\n    If *shadow* is activated and *framealpha* is ``None``, the default value is\n    ignored.\n\nfacecolor : \"inherit\" or color, default: :rc:`legend.facecolor`\n    The legend's background color.\n    If ``\"inherit\"``, use :rc:`axes.facecolor`.\n\nedgecolor : \"inherit\" or color, default: :rc:`legend.edgecolor`\n    The legend's background patch edge color.\n    If ``\"inherit\"``, use take :rc:`axes.edgecolor`.\n\nmode : {\"expand\", None}\n    If *mode* is set to ``\"expand\"`` the legend will be horizontally\n    expanded to fill the axes area (or *bbox_to_anchor* if defines\n    the legend's size).\n\nbbox_transform : None or `~matplotlib.transforms.Transform`\n    The transform for the bounding box (*bbox_to_anchor*). For a value\n    of ``None`` (default) the Axes'\n    :data:`~matplotlib.axes.Axes.transAxes` transform will be used.\n\ntitle : str or None\n    The legend's title. Default is no title (``None``).\n\ntitle_fontproperties : None or `~matplotlib.font_manager.FontProperties` or dict\n    The font properties of the legend's title. If None (default), the\n    *title_fontsize* argument will be used if present; if *title_fontsize* is\n    also None, the current :rc:`legend.title_fontsize` will be used.\n\ntitle_fontsize : int or {'xx-small', 'x-small', 'small', 'medium', 'large', 'x-large', 'xx-large'}, default: :rc:`legend.title_fontsize`\n    The font size of the legend's title.\n    Note: This cannot be combined with *title_fontproperties*. If you want\n    to set the fontsize alongside other font properties, use the *size*\n    parameter in *title_fontproperties*.\n\nalignment : {'center', 'left', 'right'}, default: 'center'\n    The alignment of the legend title and the box of entries. The entries\n    are aligned as a single block, so that markers always lined up.\n\nborderpad : float, default: :rc:`legend.borderpad`\n    The fractional whitespace inside the legend border, in font-size units.\n\nlabelspacing : float, default: :rc:`legend.labelspacing`\n    The vertical space between the legend entries, in font-size units.\n\nhandlelength : float, default: :rc:`legend.handlelength`\n    The length of the legend handles, in font-size units.\n\nhandleheight : float, default: :rc:`legend.handleheight`\n    The height of the legend handles, in font-size units.\n\nhandletextpad : float, default: :rc:`legend.handletextpad`\n    The pad between the legend handle and text, in font-size units.\n\nborderaxespad : float, default: :rc:`legend.borderaxespad`\n    The pad between the axes and legend border, in font-size units.\n\ncolumnspacing : float, default: :rc:`legend.columnspacing`\n    The spacing between columns, in font-size units.\n\nhandler_map : dict or None\n    The custom dictionary mapping instances or types to a legend\n    handler. This *handler_map* updates the default handler map\n    found at `matplotlib.legend.Legend.get_legend_handler_map`.\n\ndraggable : bool, default: False\n    Whether the legend can be dragged with the mouse.\n\n\nSee Also\n--------\n.Figure.legend\n\nNotes\n-----\nSome artists are not supported by this function.  See\n:ref:`legend_guide` for details.\n\nExamples\n--------\n.. plot:: gallery/text_labels_and_annotations/legend.py", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/574", "data": {"name": "matplotlib.pyplot.subplots[1].set_ylabel", "type": "method", "signature": "(ylabel, fontdict=None, labelpad=None, *, loc=None, **kwargs)", "description": "Set the label for the y-axis.\n\nParameters\n----------\nylabel : str\n    The label text.\n\nlabelpad : float, default: :rc:`axes.labelpad`\n    Spacing in points from the Axes bounding box including ticks\n    and tick labels.  If None, the previous value is left as is.\n\nloc : {'bottom', 'center', 'top'}, default: :rc:`yaxis.labellocation`\n    The label position. This is a high-level alternative for passing\n    parameters *y* and *horizontalalignment*.\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    `.Text` properties control the appearance of the label.\n\nSee Also\n--------\ntext : Documents the properties supported by `.Text`.", "parameters": {"type": "object", "properties": {"ylabel": {}, "fontdict": {"type": "NoneType", "default": null}, "labelpad": {"type": "NoneType", "default": null}, "loc": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/574", "data": {"name": "matplotlib.pyplot.subplots[1].set_xlabel", "type": "method", "signature": "(xlabel, fontdict=None, labelpad=None, *, loc=None, **kwargs)", "description": "Set the label for the x-axis.\n\nParameters\n----------\nxlabel : str\n    The label text.\n\nlabelpad : float, default: :rc:`axes.labelpad`\n    Spacing in points from the Axes bounding box including ticks\n    and tick labels.  If None, the previous value is left as is.\n\nloc : {'left', 'center', 'right'}, default: :rc:`xaxis.labellocation`\n    The label position. This is a high-level alternative for passing\n    parameters *x* and *horizontalalignment*.\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    `.Text` properties control the appearance of the label.\n\nSee Also\n--------\ntext : Documents the properties supported by `.Text`.", "parameters": {"type": "object", "properties": {"xlabel": {}, "fontdict": {"type": "NoneType", "default": null}, "labelpad": {"type": "NoneType", "default": null}, "loc": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/574", "data": {"name": "matplotlib.pyplot.subplots[1].plot", "type": "method", "signature": "(*args, scalex=True, scaley=True, data=None, **kwargs)", "description": "Plot y versus x as lines and/or markers.\n\nCall signatures::\n\n    plot([x], y, [fmt], *, data=None, **kwargs)\n    plot([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n\nThe coordinates of the points or line nodes are given by *x*, *y*.\n\nThe optional parameter *fmt* is a convenient way for defining basic\nformatting like color, marker and linestyle. It's a shortcut string\nnotation described in the *Notes* section below.\n\n>>> plot(x, y)        # plot x and y using default line style and color\n>>> plot(x, y, 'bo')  # plot x and y using blue circle markers\n>>> plot(y)           # plot y using x as index array 0..N-1\n>>> plot(y, 'r+')     # ditto, but with red plusses\n\nYou can use `.Line2D` properties as keyword arguments for more\ncontrol on the appearance. Line properties and *fmt* can be mixed.\nThe following two calls yield identical results:\n\n>>> plot(x, y, 'go--', linewidth=2, markersize=12)\n>>> plot(x, y, color='green', marker='o', linestyle='dashed',\n...      linewidth=2, markersize=12)\n\nWhen conflicting with *fmt*, keyword arguments take precedence.\n\n\n**Plotting labelled data**\n\nThere's a convenient way for plotting objects with labelled data (i.e.\ndata that can be accessed by index ``obj['y']``). Instead of giving\nthe data in *x* and *y*, you can provide the object in the *data*\nparameter and just give the labels for *x* and *y*::\n\n>>> plot('xlabel', 'ylabel', data=obj)\n\nAll indexable objects are supported. This could e.g. be a `dict`, a\n`pandas.DataFrame` or a structured numpy array.\n\n\n**Plotting multiple sets of data**\n\nThere are various ways to plot multiple sets of data.\n\n- The most straight forward way is just to call `plot` multiple times.\n  Example:\n\n  >>> plot(x1, y1, 'bo')\n  >>> plot(x2, y2, 'go')\n\n- If *x* and/or *y* are 2D arrays a separate data set will be drawn\n  for every column. If both *x* and *y* are 2D, they must have the\n  same shape. If only one of them is 2D with shape (N, m) the other\n  must have length N and will be used for every data set m.\n\n  Example:\n\n  >>> x = [1, 2, 3]\n  >>> y = np.array([[1, 2], [3, 4], [5, 6]])\n  >>> plot(x, y)\n\n  is equivalent to:\n\n  >>> for col in range(y.shape[1]):\n  ...     plot(x, y[:, col])\n\n- The third way is to specify multiple sets of *[x]*, *y*, *[fmt]*\n  groups::\n\n  >>> plot(x1, y1, 'g^', x2, y2, 'g-')\n\n  In this case, any additional keyword argument applies to all\n  datasets. Also, this syntax cannot be combined with the *data*\n  parameter.\n\nBy default, each line is assigned a different style specified by a\n'style cycle'. The *fmt* and line property parameters are only\nnecessary if you want explicit deviations from these defaults.\nAlternatively, you can also change the style cycle using\n:rc:`axes.prop_cycle`.\n\n\nParameters\n----------\nx, y : array-like or scalar\n    The horizontal / vertical coordinates of the data points.\n    *x* values are optional and default to ``range(len(y))``.\n\n    Commonly, these parameters are 1D arrays.\n\n    They can also be scalars, or two-dimensional (in that case, the\n    columns represent separate data sets).\n\n    These arguments cannot be passed as keywords.\n\nfmt : str, optional\n    A format string, e.g. 'ro' for red circles. See the *Notes*\n    section for a full description of the format strings.\n\n    Format strings are just an abbreviation for quickly setting\n    basic line properties. All of these and more can also be\n    controlled by keyword arguments.\n\n    This argument cannot be passed as keyword.\n\ndata : indexable object, optional\n    An object with labelled data. If given, provide the label names to\n    plot in *x* and *y*.\n\n    .. note::\n        Technically there's a slight ambiguity in calls where the\n        second label is a valid *fmt*. ``plot('n', 'o', data=obj)``\n        could be ``plt(x, y)`` or ``plt(y, fmt)``. In such cases,\n        the former interpretation is chosen, but a warning is issued.\n        You may suppress the warning by adding an empty format string\n        ``plot('n', 'o', '', data=obj)``.\n\nReturns\n-------\nlist of `.Line2D`\n    A list of lines representing the plotted data.\n\nOther Parameters\n----------------\nscalex, scaley : bool, default: True\n    These parameters determine if the view limits are adapted to the\n    data limits. The values are passed on to\n    `~.axes.Axes.autoscale_view`.\n\n**kwargs : `~matplotlib.lines.Line2D` properties, optional\n    *kwargs* are used to specify properties like a line label (for\n    auto legends), linewidth, antialiasing, marker face color.\n    Example::\n\n    >>> plot([1, 2, 3], [1, 2, 3], 'go-', label='line 1', linewidth=2)\n    >>> plot([1, 2, 3], [1, 4, 9], 'rs', label='line 2')\n\n    If you specify multiple lines with one plot call, the kwargs apply\n    to all those lines. In case the label object is iterable, each\n    element is used as labels for each set of data.\n\n    Here is a list of available `.Line2D` properties:\n\n    Properties:\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: scalar or None\n    animated: bool\n    antialiased or aa: bool\n    clip_box: `~matplotlib.transforms.BboxBase` or None\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    color or c: color\n    dash_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    dash_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    dashes: sequence of floats (on/off ink in points) or (None, None)\n    data: (2, N) array or two 1D arrays\n    drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n    figure: `~matplotlib.figure.Figure`\n    fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n    gapcolor: color or None\n    gid: str\n    in_layout: bool\n    label: object\n    linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n    linewidth or lw: float\n    marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n    markeredgecolor or mec: color\n    markeredgewidth or mew: float\n    markerfacecolor or mfc: color\n    markerfacecoloralt or mfcalt: color\n    markersize or ms: float\n    markevery: None or int or (int, int) or slice or list[int] or float or (float, float) or list[bool]\n    mouseover: bool\n    path_effects: list of `.AbstractPathEffect`\n    picker: float or callable[[Artist, Event], tuple[bool, dict]]\n    pickradius: float\n    rasterized: bool\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    solid_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    solid_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    transform: unknown\n    url: str\n    visible: bool\n    xdata: 1D array\n    ydata: 1D array\n    zorder: float\n\nSee Also\n--------\nscatter : XY scatter plot with markers of varying size and/or color (\n    sometimes also called bubble chart).\n\nNotes\n-----\n**Format Strings**\n\nA format string consists of a part for color, marker and line::\n\n    fmt = '[marker][line][color]'\n\nEach of them is optional. If not provided, the value from the style\ncycle is used. Exception: If ``line`` is given, but no ``marker``,\nthe data will be a line without markers.\n\nOther combinations such as ``[color][marker][line]`` are also\nsupported, but note that their parsing may be ambiguous.\n\n**Markers**\n\n=============   ===============================\ncharacter       description\n=============   ===============================\n``'.'``         point marker\n``','``         pixel marker\n``'o'``         circle marker\n``'v'``         triangle_down marker\n``'^'``         triangle_up marker\n``'<'``         triangle_left marker\n``'>'``         triangle_right marker\n``'1'``         tri_down marker\n``'2'``         tri_up marker\n``'3'``         tri_left marker\n``'4'``         tri_right marker\n``'8'``         octagon marker\n``'s'``         square marker\n``'p'``         pentagon marker\n``'P'``         plus (filled) marker\n``'*'``         star marker\n``'h'``         hexagon1 marker\n``'H'``         hexagon2 marker\n``'+'``         plus marker\n``'x'``         x marker\n``'X'``         x (filled) marker\n``'D'``         diamond marker\n``'d'``         thin_diamond marker\n``'|'``         vline marker\n``'_'``         hline marker\n=============   ===============================\n\n**Line Styles**\n\n=============    ===============================\ncharacter        description\n=============    ===============================\n``'-'``          solid line style\n``'--'``         dashed line style\n``'-.'``         dash-dot line style\n``':'``          dotted line style\n=============    ===============================\n\nExample format strings::\n\n    'b'    # blue markers with default shape\n    'or'   # red circles\n    '-g'   # green solid line\n    '--'   # dashed line with default color\n    '^k:'  # black triangle_up markers connected by a dotted line\n\n**Colors**\n\nThe supported color abbreviations are the single letter codes\n\n=============    ===============================\ncharacter        color\n=============    ===============================\n``'b'``          blue\n``'g'``          green\n``'r'``          red\n``'c'``          cyan\n``'m'``          magenta\n``'y'``          yellow\n``'k'``          black\n``'w'``          white\n=============    ===============================\n\nand the ``'CN'`` colors that index into the default property cycle.\n\nIf the color is the only part of the format string, you can\nadditionally use any  `matplotlib.colors` spec, e.g. full names\n(``'green'``) or hex strings (``'#008000'``).", "parameters": {"type": "object", "properties": {"scalex": {"type": "bool", "default": true}, "scaley": {"type": "bool", "default": true}, "data": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/574", "data": {"name": "numpy.pi", "type": "constant", "signature": null, "description": "Convert a string or number to a floating point number, if possible.", "value": "3.141592653589793", "parameters": {"type": "constant"}}}
{"task_id": "BigCodeBench/574", "data": {"name": "numpy.sin", "type": "callable", "signature": "(*args, **kwargs)", "description": "sin(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n\nTrigonometric sine, element-wise.\n\nParameters\n----------\nx : array_like\n    Angle, in radians (:math:`2 \\pi` rad equals 360 degrees).\nout : ndarray, None, or tuple of ndarray and None, optional\n    A location into which the result is stored. If provided, it must have\n    a shape that the inputs broadcast to. If not provided or None,\n    a freshly-allocated array is returned. A tuple (possible only as a\n    keyword argument) must have length equal to the number of outputs.\nwhere : array_like, optional\n    This condition is broadcast over the input. At locations where the\n    condition is True, the `out` array will be set to the ufunc result.\n    Elsewhere, the `out` array will retain its original value.\n    Note that if an uninitialized `out` array is created via the default\n    ``out=None``, locations within it where the condition is False will\n    remain uninitialized.\n**kwargs\n    For other keyword-only arguments, see the\n    :ref:`ufunc docs <ufuncs.kwargs>`.\n\nReturns\n-------\ny : array_like\n    The sine of each element of x.\n    This is a scalar if `x` is a scalar.\n\nSee Also\n--------\narcsin, sinh, cos\n\nNotes\n-----\nThe sine is one of the fundamental functions of trigonometry (the\nmathematical study of triangles).  Consider a circle of radius 1\ncentered on the origin.  A ray comes in from the :math:`+x` axis, makes\nan angle at the origin (measured counter-clockwise from that axis), and\ndeparts from the origin.  The :math:`y` coordinate of the outgoing\nray's intersection with the unit circle is the sine of that angle.  It\nranges from -1 for :math:`x=3\\pi / 2` to +1 for :math:`\\pi / 2.`  The\nfunction has zeroes where the angle is a multiple of :math:`\\pi`.\nSines of angles between :math:`\\pi` and :math:`2\\pi` are negative.\nThe numerous properties of the sine and related functions are included\nin any standard trigonometry text.\n\nExamples\n--------\nPrint sine of one angle:\n\n>>> np.sin(np.pi/2.)\n1.0\n\nPrint sines of an array of angles given in degrees:\n\n>>> np.sin(np.array((0., 30., 45., 60., 90.)) * np.pi / 180. )\narray([ 0.        ,  0.5       ,  0.70710678,  0.8660254 ,  1.        ])\n\nPlot the sine function:\n\n>>> import matplotlib.pylab as plt\n>>> x = np.linspace(-np.pi, np.pi, 201)\n>>> plt.plot(x, np.sin(x))\n>>> plt.xlabel('Angle [rad]')\n>>> plt.ylabel('sin(x)')\n>>> plt.axis('tight')\n>>> plt.show()", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/574", "data": {"name": "numpy.random.rand", "type": "callable", "signature": "(*args, **kwargs)", "description": "rand(d0, d1, ..., dn)\n\nRandom values in a given shape.\n\n.. note::\n    This is a convenience function for users porting code from Matlab,\n    and wraps `random_sample`. That function takes a\n    tuple to specify the size of the output, which is consistent with\n    other NumPy functions like `numpy.zeros` and `numpy.ones`.\n\nCreate an array of the given shape and populate it with\nrandom samples from a uniform distribution\nover ``[0, 1)``.\n\nParameters\n----------\nd0, d1, ..., dn : int, optional\n    The dimensions of the returned array, must be non-negative.\n    If no argument is given a single Python float is returned.\n\nReturns\n-------\nout : ndarray, shape ``(d0, d1, ..., dn)``\n    Random values.\n\nSee Also\n--------\nrandom\n\nExamples\n--------\n>>> np.random.rand(3,2)\narray([[ 0.14022471,  0.96360618],  #random\n       [ 0.37601032,  0.25528411],  #random\n       [ 0.49313049,  0.94909878]]) #random", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/574", "data": {"name": "numpy.linspace", "type": "callable", "signature": "(start, stop, num=50)", "description": "Return evenly spaced numbers over a specified interval.\n\nReturns `num` evenly spaced samples, calculated over the\ninterval [`start`, `stop`].\n\nThe endpoint of the interval can optionally be excluded.\n\n.. versionchanged:: 1.16.0\n    Non-scalar `start` and `stop` are now supported.\n\n.. versionchanged:: 1.20.0\n    Values are rounded towards ``-inf`` instead of ``0`` when an\n    integer ``dtype`` is specified. The old behavior can\n    still be obtained with ``np.linspace(start, stop, num).astype(int)``\n\nParameters\n----------\nstart : array_like\n    The starting value of the sequence.\nstop : array_like\n    The end value of the sequence, unless `endpoint` is set to False.\n    In that case, the sequence consists of all but the last of ``num + 1``\n    evenly spaced samples, so that `stop` is excluded.  Note that the step\n    size changes when `endpoint` is False.\nnum : int, optional\n    Number of samples to generate. Default is 50. Must be non-negative.\nendpoint : bool, optional\n    If True, `stop` is the last sample. Otherwise, it is not included.\n    Default is True.\nretstep : bool, optional\n    If True, return (`samples`, `step`), where `step` is the spacing\n    between samples.\ndtype : dtype, optional\n    The type of the output array.  If `dtype` is not given, the data type\n    is inferred from `start` and `stop`. The inferred dtype will never be\n    an integer; `float` is chosen even if the arguments would produce an\n    array of integers.\n\n    .. versionadded:: 1.9.0\n\naxis : int, optional\n    The axis in the result to store the samples.  Relevant only if start\n    or stop are array-like.  By default (0), the samples will be along a\n    new axis inserted at the beginning. Use -1 to get an axis at the end.\n\n    .. versionadded:: 1.16.0\n\nReturns\n-------\nsamples : ndarray\n    There are `num` equally spaced samples in the closed interval\n    ``[start, stop]`` or the half-open interval ``[start, stop)``\n    (depending on whether `endpoint` is True or False).\nstep : float, optional\n    Only returned if `retstep` is True\n\n    Size of spacing between samples.\n\n\nSee Also\n--------\narange : Similar to `linspace`, but uses a step size (instead of the\n         number of samples).\ngeomspace : Similar to `linspace`, but with numbers spaced evenly on a log\n            scale (a geometric progression).\nlogspace : Similar to `geomspace`, but with the end points specified as\n           logarithms.\n\nExamples\n--------\n>>> np.linspace(2.0, 3.0, num=5)\narray([2.  , 2.25, 2.5 , 2.75, 3.  ])\n>>> np.linspace(2.0, 3.0, num=5, endpoint=False)\narray([2. ,  2.2,  2.4,  2.6,  2.8])\n>>> np.linspace(2.0, 3.0, num=5, retstep=True)\n(array([2.  ,  2.25,  2.5 ,  2.75,  3.  ]), 0.25)\n\nGraphical illustration:\n\n>>> import matplotlib.pyplot as plt\n>>> N = 8\n>>> y = np.zeros(N)\n>>> x1 = np.linspace(0, 10, N, endpoint=True)\n>>> x2 = np.linspace(0, 10, N, endpoint=False)\n>>> plt.plot(x1, y, 'o')\n[<matplotlib.lines.Line2D object at 0x...>]\n>>> plt.plot(x2, y + 0.5, 'o')\n[<matplotlib.lines.Line2D object at 0x...>]\n>>> plt.ylim([-0.5, 1])\n(-0.5, 1)\n>>> plt.show()", "parameters": {"type": "object", "properties": {"start": {}, "stop": {}, "num": {"type": "integer", "default": 50}}}}}
{"task_id": "BigCodeBench/574", "data": {"name": "scipy.optimize.curve_fit", "type": "callable", "signature": "(f, xdata, ydata, p0=None, **kwargs)", "description": "Use non-linear least squares to fit a function, f, to data.\n\nAssumes ``ydata = f(xdata, *params) + eps``.\n\nParameters\n----------\nf : callable\n    The model function, f(x, ...). It must take the independent\n    variable as the first argument and the parameters to fit as\n    separate remaining arguments.\nxdata : array_like\n    The independent variable where the data is measured.\n    Should usually be an M-length sequence or an (k,M)-shaped array for\n    functions with k predictors, and each element should be float\n    convertible if it is an array like object.\nydata : array_like\n    The dependent data, a length M array - nominally ``f(xdata, ...)``.\np0 : array_like, optional\n    Initial guess for the parameters (length N). If None, then the\n    initial values will all be 1 (if the number of parameters for the\n    function can be determined using introspection, otherwise a\n    ValueError is raised).\nsigma : None or scalar or M-length sequence or MxM array, optional\n    Determines the uncertainty in `ydata`. If we define residuals as\n    ``r = ydata - f(xdata, *popt)``, then the interpretation of `sigma`\n    depends on its number of dimensions:\n\n        - A scalar or 1-D `sigma` should contain values of standard deviations of\n          errors in `ydata`. In this case, the optimized function is\n          ``chisq = sum((r / sigma) ** 2)``.\n\n        - A 2-D `sigma` should contain the covariance matrix of\n          errors in `ydata`. In this case, the optimized function is\n          ``chisq = r.T @ inv(sigma) @ r``.\n\n          .. versionadded:: 0.19\n\n    None (default) is equivalent of 1-D `sigma` filled with ones.\nabsolute_sigma : bool, optional\n    If True, `sigma` is used in an absolute sense and the estimated parameter\n    covariance `pcov` reflects these absolute values.\n\n    If False (default), only the relative magnitudes of the `sigma` values matter.\n    The returned parameter covariance matrix `pcov` is based on scaling\n    `sigma` by a constant factor. This constant is set by demanding that the\n    reduced `chisq` for the optimal parameters `popt` when using the\n    *scaled* `sigma` equals unity. In other words, `sigma` is scaled to\n    match the sample variance of the residuals after the fit. Default is False.\n    Mathematically,\n    ``pcov(absolute_sigma=False) = pcov(absolute_sigma=True) * chisq(popt)/(M-N)``\ncheck_finite : bool, optional\n    If True, check that the input arrays do not contain nans of infs,\n    and raise a ValueError if they do. Setting this parameter to\n    False may silently produce nonsensical results if the input arrays\n    do contain nans. Default is True if `nan_policy` is not specified\n    explicitly and False otherwise.\nbounds : 2-tuple of array_like or `Bounds`, optional\n    Lower and upper bounds on parameters. Defaults to no bounds.\n    There are two ways to specify the bounds:\n\n        - Instance of `Bounds` class.\n\n        - 2-tuple of array_like: Each element of the tuple must be either\n          an array with the length equal to the number of parameters, or a\n          scalar (in which case the bound is taken to be the same for all\n          parameters). Use ``np.inf`` with an appropriate sign to disable\n          bounds on all or some parameters.\n\nmethod : {'lm', 'trf', 'dogbox'}, optional\n    Method to use for optimization. See `least_squares` for more details.\n    Default is 'lm' for unconstrained problems and 'trf' if `bounds` are\n    provided. The method 'lm' won't work when the number of observations\n    is less than the number of variables, use 'trf' or 'dogbox' in this\n    case.\n\n    .. versionadded:: 0.17\njac : callable, string or None, optional\n    Function with signature ``jac(x, ...)`` which computes the Jacobian\n    matrix of the model function with respect to parameters as a dense\n    array_like structure. It will be scaled according to provided `sigma`.\n    If None (default), the Jacobian will be estimated numerically.\n    String keywords for 'trf' and 'dogbox' methods can be used to select\n    a finite difference scheme, see `least_squares`.\n\n    .. versionadded:: 0.18\nfull_output : boolean, optional\n    If True, this function returns additioal information: `infodict`,\n    `mesg`, and `ier`.\n\n    .. versionadded:: 1.9\nnan_policy : {'raise', 'omit', None}, optional\n    Defines how to handle when input contains nan.\n    The following options are available (default is None):\n\n      * 'raise': throws an error\n      * 'omit': performs the calculations ignoring nan values\n      * None: no special handling of NaNs is performed\n        (except what is done by check_finite); the behavior when NaNs\n        are present is implementation-dependent and may change.\n\n    Note that if this value is specified explicitly (not None),\n    `check_finite` will be set as False.\n\n    .. versionadded:: 1.11\n**kwargs\n    Keyword arguments passed to `leastsq` for ``method='lm'`` or\n    `least_squares` otherwise.\n\nReturns\n-------\npopt : array\n    Optimal values for the parameters so that the sum of the squared\n    residuals of ``f(xdata, *popt) - ydata`` is minimized.\npcov : 2-D array\n    The estimated approximate covariance of popt. The diagonals provide\n    the variance of the parameter estimate. To compute one standard\n    deviation errors on the parameters, use\n    ``perr = np.sqrt(np.diag(pcov))``. Note that the relationship between\n    `cov` and parameter error estimates is derived based on a linear\n    approximation to the model function around the optimum [1].\n    When this approximation becomes inaccurate, `cov` may not provide an\n    accurate measure of uncertainty.\n\n    How the `sigma` parameter affects the estimated covariance\n    depends on `absolute_sigma` argument, as described above.\n\n    If the Jacobian matrix at the solution doesn't have a full rank, then\n    'lm' method returns a matrix filled with ``np.inf``, on the other hand\n    'trf'  and 'dogbox' methods use Moore-Penrose pseudoinverse to compute\n    the covariance matrix. Covariance matrices with large condition numbers\n    (e.g. computed with `numpy.linalg.cond`) may indicate that results are\n    unreliable.\ninfodict : dict (returned only if `full_output` is True)\n    a dictionary of optional outputs with the keys:\n\n    ``nfev``\n        The number of function calls. Methods 'trf' and 'dogbox' do not\n        count function calls for numerical Jacobian approximation,\n        as opposed to 'lm' method.\n    ``fvec``\n        The residual values evaluated at the solution, for a 1-D `sigma`\n        this is ``(f(x, *popt) - ydata)/sigma``.\n    ``fjac``\n        A permutation of the R matrix of a QR\n        factorization of the final approximate\n        Jacobian matrix, stored column wise.\n        Together with ipvt, the covariance of the\n        estimate can be approximated.\n        Method 'lm' only provides this information.\n    ``ipvt``\n        An integer array of length N which defines\n        a permutation matrix, p, such that\n        fjac*p = q*r, where r is upper triangular\n        with diagonal elements of nonincreasing\n        magnitude. Column j of p is column ipvt(j)\n        of the identity matrix.\n        Method 'lm' only provides this information.\n    ``qtf``\n        The vector (transpose(q) * fvec).\n        Method 'lm' only provides this information.\n\n    .. versionadded:: 1.9\nmesg : str (returned only if `full_output` is True)\n    A string message giving information about the solution.\n\n    .. versionadded:: 1.9\nier : int (returned only if `full_output` is True)\n    An integer flag. If it is equal to 1, 2, 3 or 4, the solution was\n    found. Otherwise, the solution was not found. In either case, the\n    optional output variable `mesg` gives more information.\n\n    .. versionadded:: 1.9\n\nRaises\n------\nValueError\n    if either `ydata` or `xdata` contain NaNs, or if incompatible options\n    are used.\n\nRuntimeError\n    if the least-squares minimization fails.\n\nOptimizeWarning\n    if covariance of the parameters can not be estimated.\n\nSee Also\n--------\nleast_squares : Minimize the sum of squares of nonlinear functions.\nscipy.stats.linregress : Calculate a linear least squares regression for\n                         two sets of measurements.\n\nNotes\n-----\nUsers should ensure that inputs `xdata`, `ydata`, and the output of `f`\nare ``float64``, or else the optimization may return incorrect results.\n\nWith ``method='lm'``, the algorithm uses the Levenberg-Marquardt algorithm\nthrough `leastsq`. Note that this algorithm can only deal with\nunconstrained problems.\n\nBox constraints can be handled by methods 'trf' and 'dogbox'. Refer to\nthe docstring of `least_squares` for more information.\n\nParameters to be fitted must have similar scale. Differences of multiple\norders of magnitude can lead to incorrect results. For the 'trf' and\n'dogbox' methods, the `x_scale` keyword argument can be used to scale\nthe parameters.\n\nReferences\n----------\n[1] K. Vugrin et al. Confidence region estimation techniques for nonlinear\n    regression in groundwater flow: Three case studies. Water Resources\n    Research, Vol. 43, W03423, :doi:`10.1029/2005WR004804`\n\nExamples\n--------\n>>> import numpy as np\n>>> import matplotlib.pyplot as plt\n>>> from scipy.optimize import curve_fit\n\n>>> def func(x, a, b, c):\n...     return a * np.exp(-b * x) + c\n\nDefine the data to be fit with some noise:\n\n>>> xdata = np.linspace(0, 4, 50)\n>>> y = func(xdata, 2.5, 1.3, 0.5)\n>>> rng = np.random.default_rng()\n>>> y_noise = 0.2 * rng.normal(size=xdata.size)\n>>> ydata = y + y_noise\n>>> plt.plot(xdata, ydata, 'b-', label='data')\n\nFit for the parameters a, b, c of the function `func`:\n\n>>> popt, pcov = curve_fit(func, xdata, ydata)\n>>> popt\narray([2.56274217, 1.37268521, 0.47427475])\n>>> plt.plot(xdata, func(xdata, *popt), 'r-',\n...          label='fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt))\n\nConstrain the optimization to the region of ``0 <= a <= 3``,\n``0 <= b <= 1`` and ``0 <= c <= 0.5``:\n\n>>> popt, pcov = curve_fit(func, xdata, ydata, bounds=(0, [3., 1., 0.5]))\n>>> popt\narray([2.43736712, 1.        , 0.34463856])\n>>> plt.plot(xdata, func(xdata, *popt), 'g--',\n...          label='fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt))\n\n>>> plt.xlabel('x')\n>>> plt.ylabel('y')\n>>> plt.legend()\n>>> plt.show()\n\nFor reliable results, the model `func` should not be overparametrized;\nredundant parameters can cause unreliable covariance matrices and, in some\ncases, poorer quality fits. As a quick check of whether the model may be\noverparameterized, calculate the condition number of the covariance matrix:\n\n>>> np.linalg.cond(pcov)\n34.571092161547405  # may vary\n\nThe value is small, so it does not raise much concern. If, however, we were\nto add a fourth parameter ``d`` to `func` with the same effect as ``a``:\n\n>>> def func2(x, a, b, c, d):\n...     return a * d * np.exp(-b * x) + c  # a and d are redundant\n>>> popt, pcov = curve_fit(func2, xdata, ydata)\n>>> np.linalg.cond(pcov)\n1.13250718925596e+32  # may vary\n\nSuch a large value is cause for concern. The diagonal elements of the\ncovariance matrix, which is related to uncertainty of the fit, gives more\ninformation:\n\n>>> np.diag(pcov)\narray([1.48814742e+29, 3.78596560e-02, 5.39253738e-03, 2.76417220e+28])  # may vary\n\nNote that the first and last terms are much larger than the other elements,\nsuggesting that the optimal values of these parameters are ambiguous and\nthat only one of these parameters is needed in the model.\n\nIf the optimal parameters of `f` differ by multiple orders of magnitude, the\nresulting fit can be inaccurate. Sometimes, `curve_fit` can fail to find any\nresults:\n\n>>> ydata = func(xdata, 500000, 0.01, 15)\n>>> try:\n...     popt, pcov = curve_fit(func, xdata, ydata, method = 'trf')\n... except RuntimeError as e:\n...     print(e)\nOptimal parameters not found: The maximum number of function evaluations is exceeded.\n\nIf parameter scale is roughly known beforehand, it can be defined in\n`x_scale` argument:\n\n>>> popt, pcov = curve_fit(func, xdata, ydata, method = 'trf',\n...                        x_scale = [1000, 1, 1])\n>>> popt\narray([5.00000000e+05, 1.00000000e-02, 1.49999999e+01])", "parameters": {"type": "object", "properties": {"f": {}, "xdata": {}, "ydata": {}, "p0": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/579", "data": {"name": "collections.Counter", "type": "class", "signature": "(iterable=None, /, **kwds)", "description": "Dict subclass for counting hashable items.  Sometimes called a bag\nor multiset.  Elements are stored as dictionary keys and their counts\nare stored as dictionary values.\n\n>>> c = Counter('abcdeabcdabcaba')  # count elements from a string\n\n>>> c.most_common(3)                # three most common elements\n[('a', 5), ('b', 4), ('c', 3)]\n>>> sorted(c)                       # list all unique elements\n['a', 'b', 'c', 'd', 'e']\n>>> ''.join(sorted(c.elements()))   # list elements with repetitions\n'aaaaabbbbcccdde'\n>>> sum(c.values())                 # total of all counts\n15\n\n>>> c['a']                          # count of letter 'a'\n5\n>>> for elem in 'shazam':           # update counts from an iterable\n...     c[elem] += 1                # by adding 1 to each element's count\n>>> c['a']                          # now there are seven 'a'\n7\n>>> del c['b']                      # remove all 'b'\n>>> c['b']                          # now there are zero 'b'\n0\n\n>>> d = Counter('simsalabim')       # make another counter\n>>> c.update(d)                     # add in the second counter\n>>> c['a']                          # now there are nine 'a'\n9\n\n>>> c.clear()                       # empty the counter\n>>> c\nCounter()\n\nNote:  If a count is set to zero or reduced to zero, it will remain\nin the counter until the entry is deleted or the counter is cleared:\n\n>>> c = Counter('aaabbc')\n>>> c['b'] -= 2                     # reduce the count of 'b' by two\n>>> c.most_common()                 # 'b' is still in, but its count is zero\n[('a', 3), ('c', 1), ('b', 0)]", "parameters": {"type": "object", "properties": {"iterable": {"type": "NoneType", "default": null}, "/": {}}}}}
{"task_id": "BigCodeBench/579", "data": {"name": "collections.Counter.most_common", "type": "callable", "signature": "(self, n=None)", "description": "List the n most common elements and their counts from the most\ncommon to the least.  If n is None, then list all element counts.\n\n>>> Counter('abracadabra').most_common(3)\n[('a', 5), ('b', 2), ('r', 2)]", "parameters": {"type": "object", "properties": {"n": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/579", "data": {"name": "csv.reader", "type": "callable", "signature": "(*args, **kwargs)", "description": "csv_reader = reader(iterable [, dialect='excel']\n                        [optional keyword args])\n    for row in csv_reader:\n        process(row)\n\nThe \"iterable\" argument can be any object that returns a line\nof input for each iteration, such as a file object or a list.  The\noptional \"dialect\" parameter is discussed below.  The function\nalso accepts optional keyword arguments which override settings\nprovided by the dialect.\n\nThe returned object is an iterator.  Each iteration returns a row\nof the CSV file (which can span multiple input lines).", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/579", "data": {"name": "matplotlib.pyplot.subplots", "type": "class", "signature": "()", "description": "Create a figure and a set of subplots.\n\nThis utility wrapper makes it convenient to create common layouts of\nsubplots, including the enclosing figure object, in a single call.\n\nParameters\n----------\nnrows, ncols : int, default: 1\n    Number of rows/columns of the subplot grid.\n\nsharex, sharey : bool or {'none', 'all', 'row', 'col'}, default: False\n    Controls sharing of properties among x (*sharex*) or y (*sharey*)\n    axes:\n\n    - True or 'all': x- or y-axis will be shared among all subplots.\n    - False or 'none': each subplot x- or y-axis will be independent.\n    - 'row': each subplot row will share an x- or y-axis.\n    - 'col': each subplot column will share an x- or y-axis.\n\n    When subplots have a shared x-axis along a column, only the x tick\n    labels of the bottom subplot are created. Similarly, when subplots\n    have a shared y-axis along a row, only the y tick labels of the first\n    column subplot are created. To later turn other subplots' ticklabels\n    on, use `~matplotlib.axes.Axes.tick_params`.\n\n    When subplots have a shared axis that has units, calling\n    `~matplotlib.axis.Axis.set_units` will update each axis with the\n    new units.\n\nsqueeze : bool, default: True\n    - If True, extra dimensions are squeezed out from the returned\n      array of `~matplotlib.axes.Axes`:\n\n      - if only one subplot is constructed (nrows=ncols=1), the\n        resulting single Axes object is returned as a scalar.\n      - for Nx1 or 1xM subplots, the returned object is a 1D numpy\n        object array of Axes objects.\n      - for NxM, subplots with N>1 and M>1 are returned as a 2D array.\n\n    - If False, no squeezing at all is done: the returned Axes object is\n      always a 2D array containing Axes instances, even if it ends up\n      being 1x1.\n\nwidth_ratios : array-like of length *ncols*, optional\n    Defines the relative widths of the columns. Each column gets a\n    relative width of ``width_ratios[i] / sum(width_ratios)``.\n    If not given, all columns will have the same width.  Equivalent\n    to ``gridspec_kw={'width_ratios': [...]}``.\n\nheight_ratios : array-like of length *nrows*, optional\n    Defines the relative heights of the rows. Each row gets a\n    relative height of ``height_ratios[i] / sum(height_ratios)``.\n    If not given, all rows will have the same height. Convenience\n    for ``gridspec_kw={'height_ratios': [...]}``.\n\nsubplot_kw : dict, optional\n    Dict with keywords passed to the\n    `~matplotlib.figure.Figure.add_subplot` call used to create each\n    subplot.\n\ngridspec_kw : dict, optional\n    Dict with keywords passed to the `~matplotlib.gridspec.GridSpec`\n    constructor used to create the grid the subplots are placed on.\n\n**fig_kw\n    All additional keyword arguments are passed to the\n    `.pyplot.figure` call.\n\nReturns\n-------\nfig : `.Figure`\n\nax : `~matplotlib.axes.Axes` or array of Axes\n    *ax* can be either a single `~.axes.Axes` object, or an array of Axes\n    objects if more than one subplot was created.  The dimensions of the\n    resulting array can be controlled with the squeeze keyword, see above.\n\n    Typical idioms for handling the return value are::\n\n        # using the variable ax for single a Axes\n        fig, ax = plt.subplots()\n\n        # using the variable axs for multiple Axes\n        fig, axs = plt.subplots(2, 2)\n\n        # using tuple unpacking for multiple Axes\n        fig, (ax1, ax2) = plt.subplots(1, 2)\n        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n\n    The names ``ax`` and pluralized ``axs`` are preferred over ``axes``\n    because for the latter it's not clear if it refers to a single\n    `~.axes.Axes` instance or a collection of these.\n\nSee Also\n--------\n.pyplot.figure\n.pyplot.subplot\n.pyplot.axes\n.Figure.subplots\n.Figure.add_subplot\n\nExamples\n--------\n::\n\n    # First create some toy data:\n    x = np.linspace(0, 2*np.pi, 400)\n    y = np.sin(x**2)\n\n    # Create just a figure and only one subplot\n    fig, ax = plt.subplots()\n    ax.plot(x, y)\n    ax.set_title('Simple plot')\n\n    # Create two subplots and unpack the output array immediately\n    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n    ax1.plot(x, y)\n    ax1.set_title('Sharing Y axis')\n    ax2.scatter(x, y)\n\n    # Create four polar axes and access them through the returned array\n    fig, axs = plt.subplots(2, 2, subplot_kw=dict(projection=\"polar\"))\n    axs[0, 0].plot(x, y)\n    axs[1, 1].scatter(x, y)\n\n    # Share a X axis with each column of subplots\n    plt.subplots(2, 2, sharex='col')\n\n    # Share a Y axis with each row of subplots\n    plt.subplots(2, 2, sharey='row')\n\n    # Share both X and Y axes with all subplots\n    plt.subplots(2, 2, sharex='all', sharey='all')\n\n    # Note that this is the same as\n    plt.subplots(2, 2, sharex=True, sharey=True)\n\n    # Create figure number 10 with a single subplot\n    # and clears it if it already exists.\n    fig, ax = plt.subplots(num=10, clear=True)", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/579", "data": {"name": "matplotlib.pyplot.subplots[1].bar", "type": "method", "signature": "(x, height, width=0.8, bottom=None, *, align='center', data=None, **kwargs)", "description": "Make a bar plot.\n\nThe bars are positioned at *x* with the given *align*\\ment. Their\ndimensions are given by *height* and *width*. The vertical baseline\nis *bottom* (default 0).\n\nMany parameters can take either a single value applying to all bars\nor a sequence of values, one for each bar.\n\nParameters\n----------\nx : float or array-like\n    The x coordinates of the bars. See also *align* for the\n    alignment of the bars to the coordinates.\n\nheight : float or array-like\n    The height(s) of the bars.\n\n    Note that if *bottom* has units (e.g. datetime), *height* should be in\n    units that are a difference from the value of *bottom* (e.g. timedelta).\n\nwidth : float or array-like, default: 0.8\n    The width(s) of the bars.\n\n    Note that if *x* has units (e.g. datetime), then *width* should be in\n    units that are a difference (e.g. timedelta) around the *x* values.\n\nbottom : float or array-like, default: 0\n    The y coordinate(s) of the bottom side(s) of the bars.\n\n    Note that if *bottom* has units, then the y-axis will get a Locator and\n    Formatter appropriate for the units (e.g. dates, or categorical).\n\nalign : {'center', 'edge'}, default: 'center'\n    Alignment of the bars to the *x* coordinates:\n\n    - 'center': Center the base on the *x* positions.\n    - 'edge': Align the left edges of the bars with the *x* positions.\n\n    To align the bars on the right edge pass a negative *width* and\n    ``align='edge'``.\n\nReturns\n-------\n`.BarContainer`\n    Container with all the bars and optionally errorbars.\n\nOther Parameters\n----------------\ncolor : color or list of color, optional\n    The colors of the bar faces.\n\nedgecolor : color or list of color, optional\n    The colors of the bar edges.\n\nlinewidth : float or array-like, optional\n    Width of the bar edge(s). If 0, don't draw edges.\n\ntick_label : str or list of str, optional\n    The tick labels of the bars.\n    Default: None (Use default numeric labels.)\n\nlabel : str or list of str, optional\n    A single label is attached to the resulting `.BarContainer` as a\n    label for the whole dataset.\n    If a list is provided, it must be the same length as *x* and\n    labels the individual bars. Repeated labels are not de-duplicated\n    and will cause repeated label entries, so this is best used when\n    bars also differ in style (e.g., by passing a list to *color*.)\n\nxerr, yerr : float or array-like of shape(N,) or shape(2, N), optional\n    If not *None*, add horizontal / vertical errorbars to the bar tips.\n    The values are +/- sizes relative to the data:\n\n    - scalar: symmetric +/- values for all bars\n    - shape(N,): symmetric +/- values for each bar\n    - shape(2, N): Separate - and + values for each bar. First row\n      contains the lower errors, the second row contains the upper\n      errors.\n    - *None*: No errorbar. (Default)\n\n    See :doc:`/gallery/statistics/errorbar_features` for an example on\n    the usage of *xerr* and *yerr*.\n\necolor : color or list of color, default: 'black'\n    The line color of the errorbars.\n\ncapsize : float, default: :rc:`errorbar.capsize`\n   The length of the error bar caps in points.\n\nerror_kw : dict, optional\n    Dictionary of keyword arguments to be passed to the\n    `~.Axes.errorbar` method. Values of *ecolor* or *capsize* defined\n    here take precedence over the independent keyword arguments.\n\nlog : bool, default: False\n    If *True*, set the y-axis to be log scale.\n\ndata : indexable object, optional\n    If given, all parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` (unless this raises an exception).\n\n**kwargs : `.Rectangle` properties\n\nProperties:\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: scalar or None\n    angle: unknown\n    animated: bool\n    antialiased or aa: bool or None\n    bounds: (left, bottom, width, height)\n    capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    clip_box: `~matplotlib.transforms.BboxBase` or None\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    color: color\n    edgecolor or ec: color or None\n    facecolor or fc: color or None\n    figure: `~matplotlib.figure.Figure`\n    fill: bool\n    gid: str\n    hatch: {'/', '\\\\', '|', '-', '+', 'x', 'o', 'O', '.', '*'}\n    height: unknown\n    in_layout: bool\n    joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    label: object\n    linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n    linewidth or lw: float or None\n    mouseover: bool\n    path_effects: list of `.AbstractPathEffect`\n    picker: None or bool or float or callable\n    rasterized: bool\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    transform: `~matplotlib.transforms.Transform`\n    url: str\n    visible: bool\n    width: unknown\n    x: unknown\n    xy: (float, float)\n    y: unknown\n    zorder: float\n\nSee Also\n--------\nbarh : Plot a horizontal bar plot.\n\nNotes\n-----\nStacked bars can be achieved by passing individual *bottom* values per\nbar. See :doc:`/gallery/lines_bars_and_markers/bar_stacked`.", "parameters": {"type": "object", "properties": {"x": {}, "height": {}, "width": {"type": "float", "default": 0.8}, "bottom": {"type": "NoneType", "default": null}, "align": {"type": "str", "default": "center"}, "data": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/579", "data": {"name": "unicodedata.normalize", "type": "callable", "signature": "(form, unistr)", "description": "Return the normal form 'form' for the Unicode string unistr.\n\nValid values for form are 'NFC', 'NFKC', 'NFD', and 'NFKD'.", "parameters": {"type": "object", "properties": {"form": {}, "unistr": {}}}}}
{"task_id": "BigCodeBench/579", "data": {"name": "words.append(normalized_word)"}}
{"task_id": "BigCodeBench/582", "data": {"name": "matplotlib.pyplot.subplots", "type": "class", "signature": "()", "description": "Create a figure and a set of subplots.\n\nThis utility wrapper makes it convenient to create common layouts of\nsubplots, including the enclosing figure object, in a single call.\n\nParameters\n----------\nnrows, ncols : int, default: 1\n    Number of rows/columns of the subplot grid.\n\nsharex, sharey : bool or {'none', 'all', 'row', 'col'}, default: False\n    Controls sharing of properties among x (*sharex*) or y (*sharey*)\n    axes:\n\n    - True or 'all': x- or y-axis will be shared among all subplots.\n    - False or 'none': each subplot x- or y-axis will be independent.\n    - 'row': each subplot row will share an x- or y-axis.\n    - 'col': each subplot column will share an x- or y-axis.\n\n    When subplots have a shared x-axis along a column, only the x tick\n    labels of the bottom subplot are created. Similarly, when subplots\n    have a shared y-axis along a row, only the y tick labels of the first\n    column subplot are created. To later turn other subplots' ticklabels\n    on, use `~matplotlib.axes.Axes.tick_params`.\n\n    When subplots have a shared axis that has units, calling\n    `~matplotlib.axis.Axis.set_units` will update each axis with the\n    new units.\n\nsqueeze : bool, default: True\n    - If True, extra dimensions are squeezed out from the returned\n      array of `~matplotlib.axes.Axes`:\n\n      - if only one subplot is constructed (nrows=ncols=1), the\n        resulting single Axes object is returned as a scalar.\n      - for Nx1 or 1xM subplots, the returned object is a 1D numpy\n        object array of Axes objects.\n      - for NxM, subplots with N>1 and M>1 are returned as a 2D array.\n\n    - If False, no squeezing at all is done: the returned Axes object is\n      always a 2D array containing Axes instances, even if it ends up\n      being 1x1.\n\nwidth_ratios : array-like of length *ncols*, optional\n    Defines the relative widths of the columns. Each column gets a\n    relative width of ``width_ratios[i] / sum(width_ratios)``.\n    If not given, all columns will have the same width.  Equivalent\n    to ``gridspec_kw={'width_ratios': [...]}``.\n\nheight_ratios : array-like of length *nrows*, optional\n    Defines the relative heights of the rows. Each row gets a\n    relative height of ``height_ratios[i] / sum(height_ratios)``.\n    If not given, all rows will have the same height. Convenience\n    for ``gridspec_kw={'height_ratios': [...]}``.\n\nsubplot_kw : dict, optional\n    Dict with keywords passed to the\n    `~matplotlib.figure.Figure.add_subplot` call used to create each\n    subplot.\n\ngridspec_kw : dict, optional\n    Dict with keywords passed to the `~matplotlib.gridspec.GridSpec`\n    constructor used to create the grid the subplots are placed on.\n\n**fig_kw\n    All additional keyword arguments are passed to the\n    `.pyplot.figure` call.\n\nReturns\n-------\nfig : `.Figure`\n\nax : `~matplotlib.axes.Axes` or array of Axes\n    *ax* can be either a single `~.axes.Axes` object, or an array of Axes\n    objects if more than one subplot was created.  The dimensions of the\n    resulting array can be controlled with the squeeze keyword, see above.\n\n    Typical idioms for handling the return value are::\n\n        # using the variable ax for single a Axes\n        fig, ax = plt.subplots()\n\n        # using the variable axs for multiple Axes\n        fig, axs = plt.subplots(2, 2)\n\n        # using tuple unpacking for multiple Axes\n        fig, (ax1, ax2) = plt.subplots(1, 2)\n        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n\n    The names ``ax`` and pluralized ``axs`` are preferred over ``axes``\n    because for the latter it's not clear if it refers to a single\n    `~.axes.Axes` instance or a collection of these.\n\nSee Also\n--------\n.pyplot.figure\n.pyplot.subplot\n.pyplot.axes\n.Figure.subplots\n.Figure.add_subplot\n\nExamples\n--------\n::\n\n    # First create some toy data:\n    x = np.linspace(0, 2*np.pi, 400)\n    y = np.sin(x**2)\n\n    # Create just a figure and only one subplot\n    fig, ax = plt.subplots()\n    ax.plot(x, y)\n    ax.set_title('Simple plot')\n\n    # Create two subplots and unpack the output array immediately\n    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n    ax1.plot(x, y)\n    ax1.set_title('Sharing Y axis')\n    ax2.scatter(x, y)\n\n    # Create four polar axes and access them through the returned array\n    fig, axs = plt.subplots(2, 2, subplot_kw=dict(projection=\"polar\"))\n    axs[0, 0].plot(x, y)\n    axs[1, 1].scatter(x, y)\n\n    # Share a X axis with each column of subplots\n    plt.subplots(2, 2, sharex='col')\n\n    # Share a Y axis with each row of subplots\n    plt.subplots(2, 2, sharey='row')\n\n    # Share both X and Y axes with all subplots\n    plt.subplots(2, 2, sharex='all', sharey='all')\n\n    # Note that this is the same as\n    plt.subplots(2, 2, sharex=True, sharey=True)\n\n    # Create figure number 10 with a single subplot\n    # and clears it if it already exists.\n    fig, ax = plt.subplots(num=10, clear=True)", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/582", "data": {"name": "matplotlib.pyplot.subplots[1].get_xlim", "type": "method", "signature": "()", "description": "Return the x-axis view limits.\n\nReturns\n-------\nleft, right : (float, float)\n    The current x-axis limits in data coordinates.\n\nSee Also\n--------\n.Axes.set_xlim\n.Axes.set_xbound, .Axes.get_xbound\n.Axes.invert_xaxis, .Axes.xaxis_inverted\n\nNotes\n-----\nThe x-axis may be inverted, in which case the *left* value will\nbe greater than the *right* value.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/582", "data": {"name": "matplotlib.pyplot.subplots[1].plot", "type": "method", "signature": "(*args, scalex=True, scaley=True, data=None, **kwargs)", "description": "Plot y versus x as lines and/or markers.\n\nCall signatures::\n\n    plot([x], y, [fmt], *, data=None, **kwargs)\n    plot([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n\nThe coordinates of the points or line nodes are given by *x*, *y*.\n\nThe optional parameter *fmt* is a convenient way for defining basic\nformatting like color, marker and linestyle. It's a shortcut string\nnotation described in the *Notes* section below.\n\n>>> plot(x, y)        # plot x and y using default line style and color\n>>> plot(x, y, 'bo')  # plot x and y using blue circle markers\n>>> plot(y)           # plot y using x as index array 0..N-1\n>>> plot(y, 'r+')     # ditto, but with red plusses\n\nYou can use `.Line2D` properties as keyword arguments for more\ncontrol on the appearance. Line properties and *fmt* can be mixed.\nThe following two calls yield identical results:\n\n>>> plot(x, y, 'go--', linewidth=2, markersize=12)\n>>> plot(x, y, color='green', marker='o', linestyle='dashed',\n...      linewidth=2, markersize=12)\n\nWhen conflicting with *fmt*, keyword arguments take precedence.\n\n\n**Plotting labelled data**\n\nThere's a convenient way for plotting objects with labelled data (i.e.\ndata that can be accessed by index ``obj['y']``). Instead of giving\nthe data in *x* and *y*, you can provide the object in the *data*\nparameter and just give the labels for *x* and *y*::\n\n>>> plot('xlabel', 'ylabel', data=obj)\n\nAll indexable objects are supported. This could e.g. be a `dict`, a\n`pandas.DataFrame` or a structured numpy array.\n\n\n**Plotting multiple sets of data**\n\nThere are various ways to plot multiple sets of data.\n\n- The most straight forward way is just to call `plot` multiple times.\n  Example:\n\n  >>> plot(x1, y1, 'bo')\n  >>> plot(x2, y2, 'go')\n\n- If *x* and/or *y* are 2D arrays a separate data set will be drawn\n  for every column. If both *x* and *y* are 2D, they must have the\n  same shape. If only one of them is 2D with shape (N, m) the other\n  must have length N and will be used for every data set m.\n\n  Example:\n\n  >>> x = [1, 2, 3]\n  >>> y = np.array([[1, 2], [3, 4], [5, 6]])\n  >>> plot(x, y)\n\n  is equivalent to:\n\n  >>> for col in range(y.shape[1]):\n  ...     plot(x, y[:, col])\n\n- The third way is to specify multiple sets of *[x]*, *y*, *[fmt]*\n  groups::\n\n  >>> plot(x1, y1, 'g^', x2, y2, 'g-')\n\n  In this case, any additional keyword argument applies to all\n  datasets. Also, this syntax cannot be combined with the *data*\n  parameter.\n\nBy default, each line is assigned a different style specified by a\n'style cycle'. The *fmt* and line property parameters are only\nnecessary if you want explicit deviations from these defaults.\nAlternatively, you can also change the style cycle using\n:rc:`axes.prop_cycle`.\n\n\nParameters\n----------\nx, y : array-like or scalar\n    The horizontal / vertical coordinates of the data points.\n    *x* values are optional and default to ``range(len(y))``.\n\n    Commonly, these parameters are 1D arrays.\n\n    They can also be scalars, or two-dimensional (in that case, the\n    columns represent separate data sets).\n\n    These arguments cannot be passed as keywords.\n\nfmt : str, optional\n    A format string, e.g. 'ro' for red circles. See the *Notes*\n    section for a full description of the format strings.\n\n    Format strings are just an abbreviation for quickly setting\n    basic line properties. All of these and more can also be\n    controlled by keyword arguments.\n\n    This argument cannot be passed as keyword.\n\ndata : indexable object, optional\n    An object with labelled data. If given, provide the label names to\n    plot in *x* and *y*.\n\n    .. note::\n        Technically there's a slight ambiguity in calls where the\n        second label is a valid *fmt*. ``plot('n', 'o', data=obj)``\n        could be ``plt(x, y)`` or ``plt(y, fmt)``. In such cases,\n        the former interpretation is chosen, but a warning is issued.\n        You may suppress the warning by adding an empty format string\n        ``plot('n', 'o', '', data=obj)``.\n\nReturns\n-------\nlist of `.Line2D`\n    A list of lines representing the plotted data.\n\nOther Parameters\n----------------\nscalex, scaley : bool, default: True\n    These parameters determine if the view limits are adapted to the\n    data limits. The values are passed on to\n    `~.axes.Axes.autoscale_view`.\n\n**kwargs : `~matplotlib.lines.Line2D` properties, optional\n    *kwargs* are used to specify properties like a line label (for\n    auto legends), linewidth, antialiasing, marker face color.\n    Example::\n\n    >>> plot([1, 2, 3], [1, 2, 3], 'go-', label='line 1', linewidth=2)\n    >>> plot([1, 2, 3], [1, 4, 9], 'rs', label='line 2')\n\n    If you specify multiple lines with one plot call, the kwargs apply\n    to all those lines. In case the label object is iterable, each\n    element is used as labels for each set of data.\n\n    Here is a list of available `.Line2D` properties:\n\n    Properties:\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: scalar or None\n    animated: bool\n    antialiased or aa: bool\n    clip_box: `~matplotlib.transforms.BboxBase` or None\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    color or c: color\n    dash_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    dash_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    dashes: sequence of floats (on/off ink in points) or (None, None)\n    data: (2, N) array or two 1D arrays\n    drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n    figure: `~matplotlib.figure.Figure`\n    fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n    gapcolor: color or None\n    gid: str\n    in_layout: bool\n    label: object\n    linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n    linewidth or lw: float\n    marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n    markeredgecolor or mec: color\n    markeredgewidth or mew: float\n    markerfacecolor or mfc: color\n    markerfacecoloralt or mfcalt: color\n    markersize or ms: float\n    markevery: None or int or (int, int) or slice or list[int] or float or (float, float) or list[bool]\n    mouseover: bool\n    path_effects: list of `.AbstractPathEffect`\n    picker: float or callable[[Artist, Event], tuple[bool, dict]]\n    pickradius: float\n    rasterized: bool\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    solid_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    solid_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    transform: unknown\n    url: str\n    visible: bool\n    xdata: 1D array\n    ydata: 1D array\n    zorder: float\n\nSee Also\n--------\nscatter : XY scatter plot with markers of varying size and/or color (\n    sometimes also called bubble chart).\n\nNotes\n-----\n**Format Strings**\n\nA format string consists of a part for color, marker and line::\n\n    fmt = '[marker][line][color]'\n\nEach of them is optional. If not provided, the value from the style\ncycle is used. Exception: If ``line`` is given, but no ``marker``,\nthe data will be a line without markers.\n\nOther combinations such as ``[color][marker][line]`` are also\nsupported, but note that their parsing may be ambiguous.\n\n**Markers**\n\n=============   ===============================\ncharacter       description\n=============   ===============================\n``'.'``         point marker\n``','``         pixel marker\n``'o'``         circle marker\n``'v'``         triangle_down marker\n``'^'``         triangle_up marker\n``'<'``         triangle_left marker\n``'>'``         triangle_right marker\n``'1'``         tri_down marker\n``'2'``         tri_up marker\n``'3'``         tri_left marker\n``'4'``         tri_right marker\n``'8'``         octagon marker\n``'s'``         square marker\n``'p'``         pentagon marker\n``'P'``         plus (filled) marker\n``'*'``         star marker\n``'h'``         hexagon1 marker\n``'H'``         hexagon2 marker\n``'+'``         plus marker\n``'x'``         x marker\n``'X'``         x (filled) marker\n``'D'``         diamond marker\n``'d'``         thin_diamond marker\n``'|'``         vline marker\n``'_'``         hline marker\n=============   ===============================\n\n**Line Styles**\n\n=============    ===============================\ncharacter        description\n=============    ===============================\n``'-'``          solid line style\n``'--'``         dashed line style\n``'-.'``         dash-dot line style\n``':'``          dotted line style\n=============    ===============================\n\nExample format strings::\n\n    'b'    # blue markers with default shape\n    'or'   # red circles\n    '-g'   # green solid line\n    '--'   # dashed line with default color\n    '^k:'  # black triangle_up markers connected by a dotted line\n\n**Colors**\n\nThe supported color abbreviations are the single letter codes\n\n=============    ===============================\ncharacter        color\n=============    ===============================\n``'b'``          blue\n``'g'``          green\n``'r'``          red\n``'c'``          cyan\n``'m'``          magenta\n``'y'``          yellow\n``'k'``          black\n``'w'``          white\n=============    ===============================\n\nand the ``'CN'`` colors that index into the default property cycle.\n\nIf the color is the only part of the format string, you can\nadditionally use any  `matplotlib.colors` spec, e.g. full names\n(``'green'``) or hex strings (``'#008000'``).", "parameters": {"type": "object", "properties": {"scalex": {"type": "bool", "default": true}, "scaley": {"type": "bool", "default": true}, "data": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/582", "data": {"name": "matplotlib.pyplot.subplots[1].hist", "type": "method", "signature": "(x, bins=None, range=None, density=False, weights=None, cumulative=False, bottom=None, histtype='bar', align='mid', orientation='vertical', rwidth=None, log=False, color=None, label=None, stacked=False, *, data=None, **kwargs)", "description": "Compute and plot a histogram.\n\nThis method uses `numpy.histogram` to bin the data in *x* and count the\nnumber of values in each bin, then draws the distribution either as a\n`.BarContainer` or `.Polygon`. The *bins*, *range*, *density*, and\n*weights* parameters are forwarded to `numpy.histogram`.\n\nIf the data has already been binned and counted, use `~.bar` or\n`~.stairs` to plot the distribution::\n\n    counts, bins = np.histogram(x)\n    plt.stairs(counts, bins)\n\nAlternatively, plot pre-computed bins and counts using ``hist()`` by\ntreating each bin as a single point with a weight equal to its count::\n\n    plt.hist(bins[:-1], bins, weights=counts)\n\nThe data input *x* can be a singular array, a list of datasets of\npotentially different lengths ([*x0*, *x1*, ...]), or a 2D ndarray in\nwhich each column is a dataset. Note that the ndarray form is\ntransposed relative to the list form. If the input is an array, then\nthe return value is a tuple (*n*, *bins*, *patches*); if the input is a\nsequence of arrays, then the return value is a tuple\n([*n0*, *n1*, ...], *bins*, [*patches0*, *patches1*, ...]).\n\nMasked arrays are not supported.\n\nParameters\n----------\nx : (n,) array or sequence of (n,) arrays\n    Input values, this takes either a single array or a sequence of\n    arrays which are not required to be of the same length.\n\nbins : int or sequence or str, default: :rc:`hist.bins`\n    If *bins* is an integer, it defines the number of equal-width bins\n    in the range.\n\n    If *bins* is a sequence, it defines the bin edges, including the\n    left edge of the first bin and the right edge of the last bin;\n    in this case, bins may be unequally spaced.  All but the last\n    (righthand-most) bin is half-open.  In other words, if *bins* is::\n\n        [1, 2, 3, 4]\n\n    then the first bin is ``[1, 2)`` (including 1, but excluding 2) and\n    the second ``[2, 3)``.  The last bin, however, is ``[3, 4]``, which\n    *includes* 4.\n\n    If *bins* is a string, it is one of the binning strategies\n    supported by `numpy.histogram_bin_edges`: 'auto', 'fd', 'doane',\n    'scott', 'stone', 'rice', 'sturges', or 'sqrt'.\n\nrange : tuple or None, default: None\n    The lower and upper range of the bins. Lower and upper outliers\n    are ignored. If not provided, *range* is ``(x.min(), x.max())``.\n    Range has no effect if *bins* is a sequence.\n\n    If *bins* is a sequence or *range* is specified, autoscaling\n    is based on the specified bin range instead of the\n    range of x.\n\ndensity : bool, default: False\n    If ``True``, draw and return a probability density: each bin\n    will display the bin's raw count divided by the total number of\n    counts *and the bin width*\n    (``density = counts / (sum(counts) * np.diff(bins))``),\n    so that the area under the histogram integrates to 1\n    (``np.sum(density * np.diff(bins)) == 1``).\n\n    If *stacked* is also ``True``, the sum of the histograms is\n    normalized to 1.\n\nweights : (n,) array-like or None, default: None\n    An array of weights, of the same shape as *x*.  Each value in\n    *x* only contributes its associated weight towards the bin count\n    (instead of 1).  If *density* is ``True``, the weights are\n    normalized, so that the integral of the density over the range\n    remains 1.\n\ncumulative : bool or -1, default: False\n    If ``True``, then a histogram is computed where each bin gives the\n    counts in that bin plus all bins for smaller values. The last bin\n    gives the total number of datapoints.\n\n    If *density* is also ``True`` then the histogram is normalized such\n    that the last bin equals 1.\n\n    If *cumulative* is a number less than 0 (e.g., -1), the direction\n    of accumulation is reversed.  In this case, if *density* is also\n    ``True``, then the histogram is normalized such that the first bin\n    equals 1.\n\nbottom : array-like, scalar, or None, default: None\n    Location of the bottom of each bin, i.e. bins are drawn from\n    ``bottom`` to ``bottom + hist(x, bins)`` If a scalar, the bottom\n    of each bin is shifted by the same amount. If an array, each bin\n    is shifted independently and the length of bottom must match the\n    number of bins. If None, defaults to 0.\n\nhisttype : {'bar', 'barstacked', 'step', 'stepfilled'}, default: 'bar'\n    The type of histogram to draw.\n\n    - 'bar' is a traditional bar-type histogram.  If multiple data\n      are given the bars are arranged side by side.\n    - 'barstacked' is a bar-type histogram where multiple\n      data are stacked on top of each other.\n    - 'step' generates a lineplot that is by default unfilled.\n    - 'stepfilled' generates a lineplot that is by default filled.\n\nalign : {'left', 'mid', 'right'}, default: 'mid'\n    The horizontal alignment of the histogram bars.\n\n    - 'left': bars are centered on the left bin edges.\n    - 'mid': bars are centered between the bin edges.\n    - 'right': bars are centered on the right bin edges.\n\norientation : {'vertical', 'horizontal'}, default: 'vertical'\n    If 'horizontal', `~.Axes.barh` will be used for bar-type histograms\n    and the *bottom* kwarg will be the left edges.\n\nrwidth : float or None, default: None\n    The relative width of the bars as a fraction of the bin width.  If\n    ``None``, automatically compute the width.\n\n    Ignored if *histtype* is 'step' or 'stepfilled'.\n\nlog : bool, default: False\n    If ``True``, the histogram axis will be set to a log scale.\n\ncolor : color or array-like of colors or None, default: None\n    Color or sequence of colors, one per dataset.  Default (``None``)\n    uses the standard line color sequence.\n\nlabel : str or None, default: None\n    String, or sequence of strings to match multiple datasets.  Bar\n    charts yield multiple patches per dataset, but only the first gets\n    the label, so that `~.Axes.legend` will work as expected.\n\nstacked : bool, default: False\n    If ``True``, multiple data are stacked on top of each other If\n    ``False`` multiple data are arranged side by side if histtype is\n    'bar' or on top of each other if histtype is 'step'\n\nReturns\n-------\nn : array or list of arrays\n    The values of the histogram bins. See *density* and *weights* for a\n    description of the possible semantics.  If input *x* is an array,\n    then this is an array of length *nbins*. If input is a sequence of\n    arrays ``[data1, data2, ...]``, then this is a list of arrays with\n    the values of the histograms for each of the arrays in the same\n    order.  The dtype of the array *n* (or of its element arrays) will\n    always be float even if no weighting or normalization is used.\n\nbins : array\n    The edges of the bins. Length nbins + 1 (nbins left edges and right\n    edge of last bin).  Always a single array even when multiple data\n    sets are passed in.\n\npatches : `.BarContainer` or list of a single `.Polygon` or list of such objects\n    Container of individual artists used to create the histogram\n    or list of such containers if there are multiple input datasets.\n\nOther Parameters\n----------------\ndata : indexable object, optional\n    If given, the following parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` (unless this raises an exception):\n\n    *x*, *weights*\n\n**kwargs\n    `~matplotlib.patches.Patch` properties\n\nSee Also\n--------\nhist2d : 2D histogram with rectangular bins\nhexbin : 2D histogram with hexagonal bins\nstairs : Plot a pre-computed histogram\nbar : Plot a pre-computed histogram\n\nNotes\n-----\nFor large numbers of bins (>1000), plotting can be significantly\naccelerated by using `~.Axes.stairs` to plot a pre-computed histogram\n(``plt.stairs(*np.histogram(data))``), or by setting *histtype* to\n'step' or 'stepfilled' rather than 'bar' or 'barstacked'.", "parameters": {"type": "object", "properties": {"x": {}, "bins": {"type": "NoneType", "default": null}, "range": {"type": "NoneType", "default": null}, "density": {"type": "bool", "default": false}, "weights": {"type": "NoneType", "default": null}, "cumulative": {"type": "bool", "default": false}, "bottom": {"type": "NoneType", "default": null}, "histtype": {"type": "str", "default": "bar"}, "align": {"type": "str", "default": "mid"}, "orientation": {"type": "str", "default": "vertical"}, "rwidth": {"type": "NoneType", "default": null}, "log": {"type": "bool", "default": false}, "color": {"type": "NoneType", "default": null}, "label": {"type": "NoneType", "default": null}, "stacked": {"type": "bool", "default": false}, "data": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/582", "data": {"name": "numpy.random.randn", "type": "callable", "signature": "(*args, **kwargs)", "description": "randn(d0, d1, ..., dn)\n\nReturn a sample (or samples) from the \"standard normal\" distribution.\n\n.. note::\n    This is a convenience function for users porting code from Matlab,\n    and wraps `standard_normal`. That function takes a\n    tuple to specify the size of the output, which is consistent with\n    other NumPy functions like `numpy.zeros` and `numpy.ones`.\n\n.. note::\n    New code should use the ``standard_normal`` method of a ``default_rng()``\n    instance instead; please see the :ref:`random-quick-start`.\n\nIf positive int_like arguments are provided, `randn` generates an array\nof shape ``(d0, d1, ..., dn)``, filled\nwith random floats sampled from a univariate \"normal\" (Gaussian)\ndistribution of mean 0 and variance 1. A single float randomly sampled\nfrom the distribution is returned if no argument is provided.\n\nParameters\n----------\nd0, d1, ..., dn : int, optional\n    The dimensions of the returned array, must be non-negative.\n    If no argument is given a single Python float is returned.\n\nReturns\n-------\nZ : ndarray or float\n    A ``(d0, d1, ..., dn)``-shaped array of floating-point samples from\n    the standard normal distribution, or a single such float if\n    no parameters were supplied.\n\nSee Also\n--------\nstandard_normal : Similar, but takes a tuple as its argument.\nnormal : Also accepts mu and sigma arguments.\nrandom.Generator.standard_normal: which should be used for new code.\n\nNotes\n-----\nFor random samples from :math:`N(\\mu, \\sigma^2)`, use:\n\n``sigma * np.random.randn(...) + mu``\n\nExamples\n--------\n>>> np.random.randn()\n2.1923875335537315  # random\n\nTwo-by-four array of samples from N(3, 6.25):\n\n>>> 3 + 2.5 * np.random.randn(2, 4)\narray([[-4.49401501,  4.00950034, -1.81814867,  7.29718677],   # random\n       [ 0.39924804,  4.68456316,  4.99394529,  4.84057254]])  # random", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/582", "data": {"name": "numpy.linspace", "type": "callable", "signature": "(start, stop, num=50)", "description": "Return evenly spaced numbers over a specified interval.\n\nReturns `num` evenly spaced samples, calculated over the\ninterval [`start`, `stop`].\n\nThe endpoint of the interval can optionally be excluded.\n\n.. versionchanged:: 1.16.0\n    Non-scalar `start` and `stop` are now supported.\n\n.. versionchanged:: 1.20.0\n    Values are rounded towards ``-inf`` instead of ``0`` when an\n    integer ``dtype`` is specified. The old behavior can\n    still be obtained with ``np.linspace(start, stop, num).astype(int)``\n\nParameters\n----------\nstart : array_like\n    The starting value of the sequence.\nstop : array_like\n    The end value of the sequence, unless `endpoint` is set to False.\n    In that case, the sequence consists of all but the last of ``num + 1``\n    evenly spaced samples, so that `stop` is excluded.  Note that the step\n    size changes when `endpoint` is False.\nnum : int, optional\n    Number of samples to generate. Default is 50. Must be non-negative.\nendpoint : bool, optional\n    If True, `stop` is the last sample. Otherwise, it is not included.\n    Default is True.\nretstep : bool, optional\n    If True, return (`samples`, `step`), where `step` is the spacing\n    between samples.\ndtype : dtype, optional\n    The type of the output array.  If `dtype` is not given, the data type\n    is inferred from `start` and `stop`. The inferred dtype will never be\n    an integer; `float` is chosen even if the arguments would produce an\n    array of integers.\n\n    .. versionadded:: 1.9.0\n\naxis : int, optional\n    The axis in the result to store the samples.  Relevant only if start\n    or stop are array-like.  By default (0), the samples will be along a\n    new axis inserted at the beginning. Use -1 to get an axis at the end.\n\n    .. versionadded:: 1.16.0\n\nReturns\n-------\nsamples : ndarray\n    There are `num` equally spaced samples in the closed interval\n    ``[start, stop]`` or the half-open interval ``[start, stop)``\n    (depending on whether `endpoint` is True or False).\nstep : float, optional\n    Only returned if `retstep` is True\n\n    Size of spacing between samples.\n\n\nSee Also\n--------\narange : Similar to `linspace`, but uses a step size (instead of the\n         number of samples).\ngeomspace : Similar to `linspace`, but with numbers spaced evenly on a log\n            scale (a geometric progression).\nlogspace : Similar to `geomspace`, but with the end points specified as\n           logarithms.\n\nExamples\n--------\n>>> np.linspace(2.0, 3.0, num=5)\narray([2.  , 2.25, 2.5 , 2.75, 3.  ])\n>>> np.linspace(2.0, 3.0, num=5, endpoint=False)\narray([2. ,  2.2,  2.4,  2.6,  2.8])\n>>> np.linspace(2.0, 3.0, num=5, retstep=True)\n(array([2.  ,  2.25,  2.5 ,  2.75,  3.  ]), 0.25)\n\nGraphical illustration:\n\n>>> import matplotlib.pyplot as plt\n>>> N = 8\n>>> y = np.zeros(N)\n>>> x1 = np.linspace(0, 10, N, endpoint=True)\n>>> x2 = np.linspace(0, 10, N, endpoint=False)\n>>> plt.plot(x1, y, 'o')\n[<matplotlib.lines.Line2D object at 0x...>]\n>>> plt.plot(x2, y + 0.5, 'o')\n[<matplotlib.lines.Line2D object at 0x...>]\n>>> plt.ylim([-0.5, 1])\n(-0.5, 1)\n>>> plt.show()", "parameters": {"type": "object", "properties": {"start": {}, "stop": {}, "num": {"type": "integer", "default": 50}}}}}
{"task_id": "BigCodeBench/582", "data": {"name": "numpy.histogram_bin_edges", "type": "callable", "signature": "(a, bins=10)", "description": "Function to calculate only the edges of the bins used by the `histogram`\nfunction.\n\nParameters\n----------\na : array_like\n    Input data. The histogram is computed over the flattened array.\nbins : int or sequence of scalars or str, optional\n    If `bins` is an int, it defines the number of equal-width\n    bins in the given range (10, by default). If `bins` is a\n    sequence, it defines the bin edges, including the rightmost\n    edge, allowing for non-uniform bin widths.\n\n    If `bins` is a string from the list below, `histogram_bin_edges` will use\n    the method chosen to calculate the optimal bin width and\n    consequently the number of bins (see `Notes` for more detail on\n    the estimators) from the data that falls within the requested\n    range. While the bin width will be optimal for the actual data\n    in the range, the number of bins will be computed to fill the\n    entire range, including the empty portions. For visualisation,\n    using the 'auto' option is suggested. Weighted data is not\n    supported for automated bin size selection.\n\n    'auto'\n        Maximum of the 'sturges' and 'fd' estimators. Provides good\n        all around performance.\n\n    'fd' (Freedman Diaconis Estimator)\n        Robust (resilient to outliers) estimator that takes into\n        account data variability and data size.\n\n    'doane'\n        An improved version of Sturges' estimator that works better\n        with non-normal datasets.\n\n    'scott'\n        Less robust estimator that takes into account data variability\n        and data size.\n\n    'stone'\n        Estimator based on leave-one-out cross-validation estimate of\n        the integrated squared error. Can be regarded as a generalization\n        of Scott's rule.\n\n    'rice'\n        Estimator does not take variability into account, only data\n        size. Commonly overestimates number of bins required.\n\n    'sturges'\n        R's default method, only accounts for data size. Only\n        optimal for gaussian data and underestimates number of bins\n        for large non-gaussian datasets.\n\n    'sqrt'\n        Square root (of data size) estimator, used by Excel and\n        other programs for its speed and simplicity.\n\nrange : (float, float), optional\n    The lower and upper range of the bins.  If not provided, range\n    is simply ``(a.min(), a.max())``.  Values outside the range are\n    ignored. The first element of the range must be less than or\n    equal to the second. `range` affects the automatic bin\n    computation as well. While bin width is computed to be optimal\n    based on the actual data within `range`, the bin count will fill\n    the entire range including portions containing no data.\n\nweights : array_like, optional\n    An array of weights, of the same shape as `a`.  Each value in\n    `a` only contributes its associated weight towards the bin count\n    (instead of 1). This is currently not used by any of the bin estimators,\n    but may be in the future.\n\nReturns\n-------\nbin_edges : array of dtype float\n    The edges to pass into `histogram`\n\nSee Also\n--------\nhistogram\n\nNotes\n-----\nThe methods to estimate the optimal number of bins are well founded\nin literature, and are inspired by the choices R provides for\nhistogram visualisation. Note that having the number of bins\nproportional to :math:`n^{1/3}` is asymptotically optimal, which is\nwhy it appears in most estimators. These are simply plug-in methods\nthat give good starting points for number of bins. In the equations\nbelow, :math:`h` is the binwidth and :math:`n_h` is the number of\nbins. All estimators that compute bin counts are recast to bin width\nusing the `ptp` of the data. The final bin count is obtained from\n``np.round(np.ceil(range / h))``. The final bin width is often less\nthan what is returned by the estimators below.\n\n'auto' (maximum of the 'sturges' and 'fd' estimators)\n    A compromise to get a good value. For small datasets the Sturges\n    value will usually be chosen, while larger datasets will usually\n    default to FD.  Avoids the overly conservative behaviour of FD\n    and Sturges for small and large datasets respectively.\n    Switchover point is usually :math:`a.size \\approx 1000`.\n\n'fd' (Freedman Diaconis Estimator)\n    .. math:: h = 2 \\frac{IQR}{n^{1/3}}\n\n    The binwidth is proportional to the interquartile range (IQR)\n    and inversely proportional to cube root of a.size. Can be too\n    conservative for small datasets, but is quite good for large\n    datasets. The IQR is very robust to outliers.\n\n'scott'\n    .. math:: h = \\sigma \\sqrt[3]{\\frac{24 \\sqrt{\\pi}}{n}}\n\n    The binwidth is proportional to the standard deviation of the\n    data and inversely proportional to cube root of ``x.size``. Can\n    be too conservative for small datasets, but is quite good for\n    large datasets. The standard deviation is not very robust to\n    outliers. Values are very similar to the Freedman-Diaconis\n    estimator in the absence of outliers.\n\n'rice'\n    .. math:: n_h = 2n^{1/3}\n\n    The number of bins is only proportional to cube root of\n    ``a.size``. It tends to overestimate the number of bins and it\n    does not take into account data variability.\n\n'sturges'\n    .. math:: n_h = \\log _{2}(n) + 1\n\n    The number of bins is the base 2 log of ``a.size``.  This\n    estimator assumes normality of data and is too conservative for\n    larger, non-normal datasets. This is the default method in R's\n    ``hist`` method.\n\n'doane'\n    .. math:: n_h = 1 + \\log_{2}(n) +\n                    \\log_{2}\\left(1 + \\frac{|g_1|}{\\sigma_{g_1}}\\right)\n\n        g_1 = mean\\left[\\left(\\frac{x - \\mu}{\\sigma}\\right)^3\\right]\n\n        \\sigma_{g_1} = \\sqrt{\\frac{6(n - 2)}{(n + 1)(n + 3)}}\n\n    An improved version of Sturges' formula that produces better\n    estimates for non-normal datasets. This estimator attempts to\n    account for the skew of the data.\n\n'sqrt'\n    .. math:: n_h = \\sqrt n\n\n    The simplest and fastest estimator. Only takes into account the\n    data size.\n\nExamples\n--------\n>>> arr = np.array([0, 0, 0, 1, 2, 3, 3, 4, 5])\n>>> np.histogram_bin_edges(arr, bins='auto', range=(0, 1))\narray([0.  , 0.25, 0.5 , 0.75, 1.  ])\n>>> np.histogram_bin_edges(arr, bins=2)\narray([0. , 2.5, 5. ])\n\nFor consistency with histogram, an array of pre-computed bins is\npassed through unmodified:\n\n>>> np.histogram_bin_edges(arr, [1, 2])\narray([1, 2])\n\nThis function allows one set of bins to be computed, and reused across\nmultiple histograms:\n\n>>> shared_bins = np.histogram_bin_edges(arr, bins='auto')\n>>> shared_bins\narray([0., 1., 2., 3., 4., 5.])\n\n>>> group_id = np.array([0, 1, 1, 0, 1, 1, 0, 1, 1])\n>>> hist_0, _ = np.histogram(arr[group_id == 0], bins=shared_bins)\n>>> hist_1, _ = np.histogram(arr[group_id == 1], bins=shared_bins)\n\n>>> hist_0; hist_1\narray([1, 1, 0, 1, 0])\narray([2, 0, 1, 1, 2])\n\nWhich gives more easily comparable results than using separate bins for\neach histogram:\n\n>>> hist_0, bins_0 = np.histogram(arr[group_id == 0], bins='auto')\n>>> hist_1, bins_1 = np.histogram(arr[group_id == 1], bins='auto')\n>>> hist_0; hist_1\narray([1, 1, 1])\narray([2, 1, 1, 2])\n>>> bins_0; bins_1\narray([0., 1., 2., 3.])\narray([0.  , 1.25, 2.5 , 3.75, 5.  ])", "parameters": {"type": "object", "properties": {"a": {}, "bins": {"type": "integer", "default": 10}}}}}
{"task_id": "BigCodeBench/582", "data": {"name": "scipy.stats.norm", "type": "callable", "signature": "(*args, **kwds)", "description": "A normal continuous random variable.\n\nThe location (``loc``) keyword specifies the mean.\nThe scale (``scale``) keyword specifies the standard deviation.\n\nAs an instance of the `rv_continuous` class, `norm` object inherits from it\na collection of generic methods (see below for the full list),\nand completes them with details specific for this particular distribution.\n\nMethods\n-------\nrvs(loc=0, scale=1, size=1, random_state=None)\n    Random variates.\npdf(x, loc=0, scale=1)\n    Probability density function.\nlogpdf(x, loc=0, scale=1)\n    Log of the probability density function.\ncdf(x, loc=0, scale=1)\n    Cumulative distribution function.\nlogcdf(x, loc=0, scale=1)\n    Log of the cumulative distribution function.\nsf(x, loc=0, scale=1)\n    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\nlogsf(x, loc=0, scale=1)\n    Log of the survival function.\nppf(q, loc=0, scale=1)\n    Percent point function (inverse of ``cdf`` --- percentiles).\nisf(q, loc=0, scale=1)\n    Inverse survival function (inverse of ``sf``).\nmoment(order, loc=0, scale=1)\n    Non-central moment of the specified order.\nstats(loc=0, scale=1, moments='mv')\n    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\nentropy(loc=0, scale=1)\n    (Differential) entropy of the RV.\nfit(data)\n    Parameter estimates for generic data.\n    See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n    keyword arguments.\nexpect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n    Expected value of a function (of one argument) with respect to the distribution.\nmedian(loc=0, scale=1)\n    Median of the distribution.\nmean(loc=0, scale=1)\n    Mean of the distribution.\nvar(loc=0, scale=1)\n    Variance of the distribution.\nstd(loc=0, scale=1)\n    Standard deviation of the distribution.\ninterval(confidence, loc=0, scale=1)\n    Confidence interval with equal areas around the median.\n\nNotes\n-----\nThe probability density function for `norm` is:\n\n.. math::\n\n    f(x) = \\frac{\\exp(-x^2/2)}{\\sqrt{2\\pi}}\n\nfor a real number :math:`x`.\n\nThe probability density above is defined in the \"standardized\" form. To shift\nand/or scale the distribution use the ``loc`` and ``scale`` parameters.\nSpecifically, ``norm.pdf(x, loc, scale)`` is identically\nequivalent to ``norm.pdf(y) / scale`` with\n``y = (x - loc) / scale``. Note that shifting the location of a distribution\ndoes not make it a \"noncentral\" distribution; noncentral generalizations of\nsome distributions are available in separate classes.\n\nExamples\n--------\n>>> import numpy as np\n>>> from scipy.stats import norm\n>>> import matplotlib.pyplot as plt\n>>> fig, ax = plt.subplots(1, 1)\n\nCalculate the first four moments:\n\n\n>>> mean, var, skew, kurt = norm.stats(moments='mvsk')\n\nDisplay the probability density function (``pdf``):\n\n>>> x = np.linspace(norm.ppf(0.01),\n...                 norm.ppf(0.99), 100)\n>>> ax.plot(x, norm.pdf(x),\n...        'r-', lw=5, alpha=0.6, label='norm pdf')\n\nAlternatively, the distribution object can be called (as a function)\nto fix the shape, location and scale parameters. This returns a \"frozen\"\nRV object holding the given parameters fixed.\n\nFreeze the distribution and display the frozen ``pdf``:\n\n>>> rv = norm()\n>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n\nCheck accuracy of ``cdf`` and ``ppf``:\n\n>>> vals = norm.ppf([0.001, 0.5, 0.999])\n>>> np.allclose([0.001, 0.5, 0.999], norm.cdf(vals))\nTrue\n\nGenerate random numbers:\n\n>>> r = norm.rvs(size=1000)\n\nAnd compare the histogram:\n\n>>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n>>> ax.set_xlim([x[0], x[-1]])\n>>> ax.legend(loc='best', frameon=False)\n>>> plt.show()", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/582", "data": {"name": "scipy.stats.norm.fit", "type": "callable", "signature": "(data)", "description": "Return estimates of shape (if applicable), location, and scale\nparameters from data. The default estimation method is Maximum\nLikelihood Estimation (MLE), but Method of Moments (MM)\nis also available.\n\nStarting estimates for the fit are given by input arguments;\nfor any arguments not provided with starting estimates,\n``self._fitstart(data)`` is called to generate such.\n\nOne can hold some parameters fixed to specific values by passing in\nkeyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)\nand ``floc`` and ``fscale`` (for location and scale parameters,\nrespectively).\n\nParameters\n----------\ndata : array_like or `CensoredData` instance\n    Data to use in estimating the distribution parameters.\narg1, arg2, arg3,... : floats, optional\n    Starting value(s) for any shape-characterizing arguments (those not\n    provided will be determined by a call to ``_fitstart(data)``).\n    No default value.\n**kwds : floats, optional\n    - `loc`: initial guess of the distribution's location parameter.\n    - `scale`: initial guess of the distribution's scale parameter.\n\n    Special keyword arguments are recognized as holding certain\n    parameters fixed:\n\n    - f0...fn : hold respective shape parameters fixed.\n      Alternatively, shape parameters to fix can be specified by name.\n      For example, if ``self.shapes == \"a, b\"``, ``fa`` and ``fix_a``\n      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are\n      equivalent to ``f1``.\n\n    - floc : hold location parameter fixed to specified value.\n\n    - fscale : hold scale parameter fixed to specified value.\n\n    - optimizer : The optimizer to use.  The optimizer must take\n      ``func`` and starting position as the first two arguments,\n      plus ``args`` (for extra arguments to pass to the\n      function to be optimized) and ``disp``. \n      The ``fit`` method calls the optimizer with ``disp=0`` to suppress output.\n      The optimizer must return the estimated parameters.\n\n    - method : The method to use. The default is \"MLE\" (Maximum\n      Likelihood Estimate); \"MM\" (Method of Moments)\n      is also available.\n\nRaises\n------\nTypeError, ValueError\n    If an input is invalid\n`~scipy.stats.FitError`\n    If fitting fails or the fit produced would be invalid\n\nReturns\n-------\nparameter_tuple : tuple of floats\n    Estimates for any shape parameters (if applicable), followed by\n    those for location and scale. For most random variables, shape\n    statistics will be returned, but there are exceptions (e.g.\n    ``norm``).\n\nNotes\n-----\nFor the normal distribution, method of moments and maximum likelihood\nestimation give identical fits, and explicit formulas for the estimates\nare available.\nThis function uses these explicit formulas for the maximum likelihood\nestimation of the normal distribution parameters, so the\n`optimizer` and `method` arguments are ignored.\n\nExamples\n--------\n\nGenerate some data to fit: draw random variates from the `beta`\ndistribution\n\n>>> import numpy as np\n>>> from scipy.stats import beta\n>>> a, b = 1., 2.\n>>> rng = np.random.default_rng(172786373191770012695001057628748821561)\n>>> x = beta.rvs(a, b, size=1000, random_state=rng)\n\nNow we can fit all four parameters (``a``, ``b``, ``loc`` and\n``scale``):\n\n>>> a1, b1, loc1, scale1 = beta.fit(x)\n>>> a1, b1, loc1, scale1\n(1.0198945204435628, 1.9484708982737828, 4.372241314917588e-05, 0.9979078845964814)\n\nThe fit can be done also using a custom optimizer:\n\n>>> from scipy.optimize import minimize\n>>> def custom_optimizer(func, x0, args=(), disp=0):\n...     res = minimize(func, x0, args, method=\"slsqp\", options={\"disp\": disp})\n...     if res.success:\n...         return res.x\n...     raise RuntimeError('optimization routine failed')\n>>> a1, b1, loc1, scale1 = beta.fit(x, method=\"MLE\", optimizer=custom_optimizer)\n>>> a1, b1, loc1, scale1\n(1.0198821087258905, 1.948484145914738, 4.3705304486881485e-05, 0.9979104663953395)\n\nWe can also use some prior knowledge about the dataset: let's keep\n``loc`` and ``scale`` fixed:\n\n>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)\n>>> loc1, scale1\n(0, 1)\n\nWe can also keep shape parameters fixed by using ``f``-keywords. To\nkeep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,\nequivalently, ``fa=1``:\n\n>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)\n>>> a1\n1\n\nNot all distributions return estimates for the shape parameters.\n``norm`` for example just returns estimates for location and scale:\n\n>>> from scipy.stats import norm\n>>> x = norm.rvs(a, b, size=1000, random_state=123)\n>>> loc1, scale1 = norm.fit(x)\n>>> loc1, scale1\n(0.92087172783841631, 2.0015750750324668)", "parameters": {"type": "object", "properties": {"data": {}}}}}
{"task_id": "BigCodeBench/582", "data": {"name": "scipy.stats.norm.pdf", "type": "callable", "signature": "(x, *args, **kwds)", "description": "Probability density function at x of the given RV.\n\nParameters\n----------\nx : array_like\n    quantiles\narg1, arg2, arg3,... : array_like\n    The shape parameter(s) for the distribution (see docstring of the\n    instance object for more information)\nloc : array_like, optional\n    location parameter (default=0)\nscale : array_like, optional\n    scale parameter (default=1)\n\nReturns\n-------\npdf : ndarray\n    Probability density function evaluated at x", "parameters": {"type": "object", "properties": {"x": {}}}}}
{"task_id": "BigCodeBench/583", "data": {"name": "Crypto.Cipher.AES.MODE_EAX", "type": "constant", "signature": null, "description": "int([x]) -> integer\nint(x, base=10) -> integer\n\nConvert a number or string to an integer, or return 0 if no arguments\nare given.  If x is a number, return x.__int__().  For floating point\nnumbers, this truncates towards zero.\n\nIf x is not a number or if base is given, then x must be a string,\nbytes, or bytearray instance representing an integer literal in the\ngiven base.  The literal can be preceded by '+' or '-' and be surrounded\nby whitespace.  The base defaults to 10.  Valid bases are 0 and 2-36.\nBase 0 means to interpret the base from the string as an integer literal.\n>>> int('0b100', base=0)\n4", "value": "9", "parameters": {"type": "constant"}}}
{"task_id": "BigCodeBench/583", "data": {"name": "Crypto.Random.get_random_bytes", "type": "callable", "signature": "(size)", "description": "Return a bytes object containing random bytes suitable for cryptographic use.", "parameters": {"type": "object", "properties": {"size": {}}}}}
{"task_id": "BigCodeBench/583", "data": {"name": "Crypto.Cipher.AES.new", "type": "callable", "signature": "(key, mode)", "description": "Create a new AES cipher.\n\n:param key:\n    The secret key to use in the symmetric cipher.\n\n    It must be 16, 24 or 32 bytes long (respectively for *AES-128*,\n    *AES-192* or *AES-256*).\n\n    For ``MODE_SIV`` only, it doubles to 32, 48, or 64 bytes.\n:type key: bytes/bytearray/memoryview\n\n:param mode:\n    The chaining mode to use for encryption or decryption.\n    If in doubt, use ``MODE_EAX``.\n:type mode: One of the supported ``MODE_*`` constants\n\n:Keyword Arguments:\n    *   **iv** (*bytes*, *bytearray*, *memoryview*) --\n        (Only applicable for ``MODE_CBC``, ``MODE_CFB``, ``MODE_OFB``,\n        and ``MODE_OPENPGP`` modes).\n\n        The initialization vector to use for encryption or decryption.\n\n        For ``MODE_CBC``, ``MODE_CFB``, and ``MODE_OFB`` it must be 16 bytes long.\n\n        For ``MODE_OPENPGP`` mode only,\n        it must be 16 bytes long for encryption\n        and 18 bytes for decryption (in the latter case, it is\n        actually the *encrypted* IV which was prefixed to the ciphertext).\n\n        If not provided, a random byte string is generated (you must then\n        read its value with the :attr:`iv` attribute).\n\n    *   **nonce** (*bytes*, *bytearray*, *memoryview*) --\n        (Only applicable for ``MODE_CCM``, ``MODE_EAX``, ``MODE_GCM``,\n        ``MODE_SIV``, ``MODE_OCB``, and ``MODE_CTR``).\n\n        A value that must never be reused for any other encryption done\n        with this key (except possibly for ``MODE_SIV``, see below).\n\n        For ``MODE_EAX``, ``MODE_GCM`` and ``MODE_SIV`` there are no\n        restrictions on its length (recommended: **16** bytes).\n\n        For ``MODE_CCM``, its length must be in the range **[7..13]**.\n        Bear in mind that with CCM there is a trade-off between nonce\n        length and maximum message size. Recommendation: **11** bytes.\n\n        For ``MODE_OCB``, its length must be in the range **[1..15]**\n        (recommended: **15**).\n\n        For ``MODE_CTR``, its length must be in the range **[0..15]**\n        (recommended: **8**).\n\n        For ``MODE_SIV``, the nonce is optional, if it is not specified,\n        then no nonce is being used, which renders the encryption\n        deterministic.\n\n        If not provided, for modes other than ``MODE_SIV```, a random\n        byte string of the recommended length is used (you must then\n        read its value with the :attr:`nonce` attribute).\n\n    *   **segment_size** (*integer*) --\n        (Only ``MODE_CFB``).The number of **bits** the plaintext and ciphertext\n        are segmented in. It must be a multiple of 8.\n        If not specified, it will be assumed to be 8.\n\n    *   **mac_len** : (*integer*) --\n        (Only ``MODE_EAX``, ``MODE_GCM``, ``MODE_OCB``, ``MODE_CCM``)\n        Length of the authentication tag, in bytes.\n\n        It must be even and in the range **[4..16]**.\n        The recommended value (and the default, if not specified) is **16**.\n\n    *   **msg_len** : (*integer*) --\n        (Only ``MODE_CCM``). Length of the message to (de)cipher.\n        If not specified, ``encrypt`` must be called with the entire message.\n        Similarly, ``decrypt`` can only be called once.\n\n    *   **assoc_len** : (*integer*) --\n        (Only ``MODE_CCM``). Length of the associated data.\n        If not specified, all associated data is buffered internally,\n        which may represent a problem for very large messages.\n\n    *   **initial_value** : (*integer* or *bytes/bytearray/memoryview*) --\n        (Only ``MODE_CTR``).\n        The initial value for the counter. If not present, the cipher will\n        start counting from 0. The value is incremented by one for each block.\n        The counter number is encoded in big endian mode.\n\n    *   **counter** : (*object*) --\n        Instance of ``Crypto.Util.Counter``, which allows full customization\n        of the counter block. This parameter is incompatible to both ``nonce``\n        and ``initial_value``.\n\n    *   **use_aesni** : (*boolean*) --\n        Use Intel AES-NI hardware extensions (default: use if available).\n\n:Return: an AES object, of the applicable mode.", "parameters": {"type": "object", "properties": {"key": {}, "mode": {}}}}}
{"task_id": "BigCodeBench/583", "data": {"name": "Crypto.Cipher.AES.new.nonce", "type": "callable", "signature": "(key, mode, *args, **kwargs)", "description": "Create a new AES cipher.\n\n:param key:\n    The secret key to use in the symmetric cipher.\n\n    It must be 16, 24 or 32 bytes long (respectively for *AES-128*,\n    *AES-192* or *AES-256*).\n\n    For ``MODE_SIV`` only, it doubles to 32, 48, or 64 bytes.\n:type key: bytes/bytearray/memoryview\n\n:param mode:\n    The chaining mode to use for encryption or decryption.\n    If in doubt, use ``MODE_EAX``.\n:type mode: One of the supported ``MODE_*`` constants\n\n:Keyword Arguments:\n    *   **iv** (*bytes*, *bytearray*, *memoryview*) --\n        (Only applicable for ``MODE_CBC``, ``MODE_CFB``, ``MODE_OFB``,\n        and ``MODE_OPENPGP`` modes).\n\n        The initialization vector to use for encryption or decryption.\n\n        For ``MODE_CBC``, ``MODE_CFB``, and ``MODE_OFB`` it must be 16 bytes long.\n\n        For ``MODE_OPENPGP`` mode only,\n        it must be 16 bytes long for encryption\n        and 18 bytes for decryption (in the latter case, it is\n        actually the *encrypted* IV which was prefixed to the ciphertext).\n\n        If not provided, a random byte string is generated (you must then\n        read its value with the :attr:`iv` attribute).\n\n    *   **nonce** (*bytes*, *bytearray*, *memoryview*) --\n        (Only applicable for ``MODE_CCM``, ``MODE_EAX``, ``MODE_GCM``,\n        ``MODE_SIV``, ``MODE_OCB``, and ``MODE_CTR``).\n\n        A value that must never be reused for any other encryption done\n        with this key (except possibly for ``MODE_SIV``, see below).\n\n        For ``MODE_EAX``, ``MODE_GCM`` and ``MODE_SIV`` there are no\n        restrictions on its length (recommended: **16** bytes).\n\n        For ``MODE_CCM``, its length must be in the range **[7..13]**.\n        Bear in mind that with CCM there is a trade-off between nonce\n        length and maximum message size. Recommendation: **11** bytes.\n\n        For ``MODE_OCB``, its length must be in the range **[1..15]**\n        (recommended: **15**).\n\n        For ``MODE_CTR``, its length must be in the range **[0..15]**\n        (recommended: **8**).\n\n        For ``MODE_SIV``, the nonce is optional, if it is not specified,\n        then no nonce is being used, which renders the encryption\n        deterministic.\n\n        If not provided, for modes other than ``MODE_SIV```, a random\n        byte string of the recommended length is used (you must then\n        read its value with the :attr:`nonce` attribute).\n\n    *   **segment_size** (*integer*) --\n        (Only ``MODE_CFB``).The number of **bits** the plaintext and ciphertext\n        are segmented in. It must be a multiple of 8.\n        If not specified, it will be assumed to be 8.\n\n    *   **mac_len** : (*integer*) --\n        (Only ``MODE_EAX``, ``MODE_GCM``, ``MODE_OCB``, ``MODE_CCM``)\n        Length of the authentication tag, in bytes.\n\n        It must be even and in the range **[4..16]**.\n        The recommended value (and the default, if not specified) is **16**.\n\n    *   **msg_len** : (*integer*) --\n        (Only ``MODE_CCM``). Length of the message to (de)cipher.\n        If not specified, ``encrypt`` must be called with the entire message.\n        Similarly, ``decrypt`` can only be called once.\n\n    *   **assoc_len** : (*integer*) --\n        (Only ``MODE_CCM``). Length of the associated data.\n        If not specified, all associated data is buffered internally,\n        which may represent a problem for very large messages.\n\n    *   **initial_value** : (*integer* or *bytes/bytearray/memoryview*) --\n        (Only ``MODE_CTR``).\n        The initial value for the counter. If not present, the cipher will\n        start counting from 0. The value is incremented by one for each block.\n        The counter number is encoded in big endian mode.\n\n    *   **counter** : (*object*) --\n        Instance of ``Crypto.Util.Counter``, which allows full customization\n        of the counter block. This parameter is incompatible to both ``nonce``\n        and ``initial_value``.\n\n    *   **use_aesni** : (*boolean*) --\n        Use Intel AES-NI hardware extensions (default: use if available).\n\n:Return: an AES object, of the applicable mode.", "parameters": {"type": "object", "properties": {"key": {}, "mode": {}}}}}
{"task_id": "BigCodeBench/583", "data": {"name": "Crypto.Cipher.AES.new.encrypt_and_digest", "type": "callable", "signature": "(key)", "description": "Create a new AES cipher.\n\n:param key:\n    The secret key to use in the symmetric cipher.\n\n    It must be 16, 24 or 32 bytes long (respectively for *AES-128*,\n    *AES-192* or *AES-256*).\n\n    For ``MODE_SIV`` only, it doubles to 32, 48, or 64 bytes.\n:type key: bytes/bytearray/memoryview\n\n:param mode:\n    The chaining mode to use for encryption or decryption.\n    If in doubt, use ``MODE_EAX``.\n:type mode: One of the supported ``MODE_*`` constants\n\n:Keyword Arguments:\n    *   **iv** (*bytes*, *bytearray*, *memoryview*) --\n        (Only applicable for ``MODE_CBC``, ``MODE_CFB``, ``MODE_OFB``,\n        and ``MODE_OPENPGP`` modes).\n\n        The initialization vector to use for encryption or decryption.\n\n        For ``MODE_CBC``, ``MODE_CFB``, and ``MODE_OFB`` it must be 16 bytes long.\n\n        For ``MODE_OPENPGP`` mode only,\n        it must be 16 bytes long for encryption\n        and 18 bytes for decryption (in the latter case, it is\n        actually the *encrypted* IV which was prefixed to the ciphertext).\n\n        If not provided, a random byte string is generated (you must then\n        read its value with the :attr:`iv` attribute).\n\n    *   **nonce** (*bytes*, *bytearray*, *memoryview*) --\n        (Only applicable for ``MODE_CCM``, ``MODE_EAX``, ``MODE_GCM``,\n        ``MODE_SIV``, ``MODE_OCB``, and ``MODE_CTR``).\n\n        A value that must never be reused for any other encryption done\n        with this key (except possibly for ``MODE_SIV``, see below).\n\n        For ``MODE_EAX``, ``MODE_GCM`` and ``MODE_SIV`` there are no\n        restrictions on its length (recommended: **16** bytes).\n\n        For ``MODE_CCM``, its length must be in the range **[7..13]**.\n        Bear in mind that with CCM there is a trade-off between nonce\n        length and maximum message size. Recommendation: **11** bytes.\n\n        For ``MODE_OCB``, its length must be in the range **[1..15]**\n        (recommended: **15**).\n\n        For ``MODE_CTR``, its length must be in the range **[0..15]**\n        (recommended: **8**).\n\n        For ``MODE_SIV``, the nonce is optional, if it is not specified,\n        then no nonce is being used, which renders the encryption\n        deterministic.\n\n        If not provided, for modes other than ``MODE_SIV```, a random\n        byte string of the recommended length is used (you must then\n        read its value with the :attr:`nonce` attribute).\n\n    *   **segment_size** (*integer*) --\n        (Only ``MODE_CFB``).The number of **bits** the plaintext and ciphertext\n        are segmented in. It must be a multiple of 8.\n        If not specified, it will be assumed to be 8.\n\n    *   **mac_len** : (*integer*) --\n        (Only ``MODE_EAX``, ``MODE_GCM``, ``MODE_OCB``, ``MODE_CCM``)\n        Length of the authentication tag, in bytes.\n\n        It must be even and in the range **[4..16]**.\n        The recommended value (and the default, if not specified) is **16**.\n\n    *   **msg_len** : (*integer*) --\n        (Only ``MODE_CCM``). Length of the message to (de)cipher.\n        If not specified, ``encrypt`` must be called with the entire message.\n        Similarly, ``decrypt`` can only be called once.\n\n    *   **assoc_len** : (*integer*) --\n        (Only ``MODE_CCM``). Length of the associated data.\n        If not specified, all associated data is buffered internally,\n        which may represent a problem for very large messages.\n\n    *   **initial_value** : (*integer* or *bytes/bytearray/memoryview*) --\n        (Only ``MODE_CTR``).\n        The initial value for the counter. If not present, the cipher will\n        start counting from 0. The value is incremented by one for each block.\n        The counter number is encoded in big endian mode.\n\n    *   **counter** : (*object*) --\n        Instance of ``Crypto.Util.Counter``, which allows full customization\n        of the counter block. This parameter is incompatible to both ``nonce``\n        and ``initial_value``.\n\n    *   **use_aesni** : (*boolean*) --\n        Use Intel AES-NI hardware extensions (default: use if available).\n\n:Return: an AES object, of the applicable mode.", "parameters": {"type": "object", "properties": {"key": {}}}}}
{"task_id": "BigCodeBench/583", "data": {"name": "base64.b64encode", "type": "callable", "signature": "(s)", "description": "Encode the bytes-like object s using Base64 and return a bytes object.\n\nOptional altchars should be a byte string of length 2 which specifies an\nalternative alphabet for the '+' and '/' characters.  This allows an\napplication to e.g. generate url or filesystem safe Base64 strings.", "parameters": {"type": "object", "properties": {"s": {}}}}}
{"task_id": "BigCodeBench/583", "data": {"name": "open.write(priv_key_encrypted)"}}
{"task_id": "BigCodeBench/583", "data": {"name": "os.urandom", "type": "callable", "signature": "(size)", "description": "Return a bytes object containing random bytes suitable for cryptographic use.", "parameters": {"type": "object", "properties": {"size": {}}}}}
{"task_id": "BigCodeBench/583", "data": {"name": "rsa.newkeys", "type": "class", "signature": "(nbits: int)", "description": "Generates public and private keys, and returns them as (pub, priv).\n\nThe public key is also known as the 'encryption key', and is a\n:py:class:`rsa.PublicKey` object. The private key is also known as the\n'decryption key' and is a :py:class:`rsa.PrivateKey` object.\n\n:param nbits: the number of bits required to store ``n = p*q``.\n:param accurate: when True, ``n`` will have exactly the number of bits you\n    asked for. However, this makes key generation much slower. When False,\n    `n`` may have slightly less bits.\n:param poolsize: the number of processes to use to generate the prime\n    numbers. If set to a number > 1, a parallel algorithm will be used.\n    This requires Python 2.6 or newer.\n:param exponent: the exponent for the key; only change this if you know\n    what you're doing, as the exponent influences how difficult your\n    private key can be cracked. A very common choice for e is 65537.\n:type exponent: int\n\n:returns: a tuple (:py:class:`rsa.PublicKey`, :py:class:`rsa.PrivateKey`)\n\nThe ``poolsize`` parameter was added in *Python-RSA 3.1* and requires\nPython 2.6 or newer.", "parameters": {"type": "object", "properties": {"nbits": {"type": "integer"}}}}}
{"task_id": "BigCodeBench/583", "data": {"name": "rsa.newkeys[1].save_pkcs1"}}
{"task_id": "BigCodeBench/587", "data": {"name": "base64.b64encode", "type": "callable", "signature": "(s)", "description": "Encode the bytes-like object s using Base64 and return a bytes object.\n\nOptional altchars should be a byte string of length 2 which specifies an\nalternative alphabet for the '+' and '/' characters.  This allows an\napplication to e.g. generate url or filesystem safe Base64 strings.", "parameters": {"type": "object", "properties": {"s": {}}}}}
{"task_id": "BigCodeBench/587", "data": {"name": "cipher.encryptor().finalize()"}}
{"task_id": "BigCodeBench/587", "data": {"name": "cipher.encryptor().update(padded_data)"}}
{"task_id": "BigCodeBench/587", "data": {"name": "cryptography.hazmat.backends.default_backend", "type": "callable", "signature": "()", "description": "", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/587", "data": {"name": "cryptography.hazmat.primitives.padding.PKCS7", "type": "class", "signature": "(block_size: int)", "description": "", "parameters": {"type": "object", "properties": {"block_size": {"type": "integer"}}}}}
{"task_id": "BigCodeBench/587", "data": {"name": "cryptography.hazmat.primitives.padding.PKCS7.padder.finalize", "type": "callable", "signature": "(self)", "description": "", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/587", "data": {"name": "cryptography.hazmat.primitives.padding.PKCS7.padder.update", "type": "callable", "signature": "(self)", "description": "", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/587", "data": {"name": "cryptography.hazmat.primitives.ciphers.modes.CBC", "type": "class", "signature": "(initialization_vector: bytes)", "description": "", "parameters": {"type": "object", "properties": {"initialization_vector": {"type": "bytes"}}}}}
{"task_id": "BigCodeBench/587", "data": {"name": "cryptography.hazmat.primitives.ciphers.algorithms.AES", "type": "class", "signature": "(key: bytes)", "description": "", "parameters": {"type": "object", "properties": {"key": {"type": "bytes"}}}}}
{"task_id": "BigCodeBench/587", "data": {"name": "cryptography.hazmat.primitives.ciphers.Cipher", "type": "class", "signature": "(algorithm: cryptography.hazmat.primitives._cipheralgorithm.CipherAlgorithm, mode: Optional[cryptography.hazmat.primitives.ciphers.modes.Mode], backend=None)", "description": "", "parameters": {"type": "object", "properties": {"algorithm": {"type": "cryptography.hazmat.primitives._cipheralgorithm.cipheralgorithm"}, "mode": {"type": "optional[cryptography.hazmat.primitives.ciphers.modes.mode]"}, "backend": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/587", "data": {"name": "cryptography.hazmat.primitives.ciphers.Cipher.encryptor", "type": "callable", "signature": "(self)", "description": "", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/587", "data": {"name": "open.read()"}}
{"task_id": "BigCodeBench/587", "data": {"name": "open.write(encrypted_data)"}}
{"task_id": "BigCodeBench/587", "data": {"name": "open.write(b64encode(encrypted_aes_key))"}}
{"task_id": "BigCodeBench/587", "data": {"name": "os.urandom", "type": "callable", "signature": "(size)", "description": "Return a bytes object containing random bytes suitable for cryptographic use.", "parameters": {"type": "object", "properties": {"size": {}}}}}
{"task_id": "BigCodeBench/587", "data": {"name": "rsa.newkeys", "type": "callable", "signature": "(nbits: int)", "description": "Generates public and private keys, and returns them as (pub, priv).\n\nThe public key is also known as the 'encryption key', and is a\n:py:class:`rsa.PublicKey` object. The private key is also known as the\n'decryption key' and is a :py:class:`rsa.PrivateKey` object.\n\n:param nbits: the number of bits required to store ``n = p*q``.\n:param accurate: when True, ``n`` will have exactly the number of bits you\n    asked for. However, this makes key generation much slower. When False,\n    `n`` may have slightly less bits.\n:param poolsize: the number of processes to use to generate the prime\n    numbers. If set to a number > 1, a parallel algorithm will be used.\n    This requires Python 2.6 or newer.\n:param exponent: the exponent for the key; only change this if you know\n    what you're doing, as the exponent influences how difficult your\n    private key can be cracked. A very common choice for e is 65537.\n:type exponent: int\n\n:returns: a tuple (:py:class:`rsa.PublicKey`, :py:class:`rsa.PrivateKey`)\n\nThe ``poolsize`` parameter was added in *Python-RSA 3.1* and requires\nPython 2.6 or newer.", "parameters": {"type": "object", "properties": {"nbits": {"type": "integer"}}}}}
{"task_id": "BigCodeBench/587", "data": {"name": "rsa.encrypt", "type": "callable", "signature": "(message: bytes, pub_key: rsa.key.PublicKey) -> byte)", "description": "Encrypts the given message using PKCS#1 v1.5\n\n:param message: the message to encrypt. Must be a byte string no longer than\n    ``k-11`` bytes, where ``k`` is the number of bytes needed to encode\n    the ``n`` component of the public key.\n:param pub_key: the :py:class:`rsa.PublicKey` to encrypt with.\n:raise OverflowError: when the message is too large to fit in the padded\n    block.\n\n>>> from rsa import key, common\n>>> (pub_key, priv_key) = key.newkeys(256)\n>>> message = b'hello'\n>>> crypto = encrypt(message, pub_key)\n\nThe crypto text should be just as long as the public key 'n' component:\n\n>>> len(crypto) == common.byte_size(pub_key.n)\nTrue", "parameters": {"type": "object", "properties": {"message": {"type": "bytes"}, "pub_key": {"type": "rsa.key.publickey"}}}}}
{"task_id": "BigCodeBench/590", "data": {"name": "datetime.datetime", "type": "class", "signature": "(self, /, *args, **kwargs)", "description": "datetime(year, month, day[, hour[, minute[, second[, microsecond[,tzinfo]]]]])\n\nThe year, month and day arguments are required. tzinfo may be None, or an\ninstance of a tzinfo subclass. The remaining arguments may be ints.", "parameters": {"type": "object", "properties": {"/": {}}}}}
{"task_id": "BigCodeBench/590", "data": {"name": "datetime.datetime.now", "type": "callable", "signature": "()", "description": "Returns new datetime object representing current time local to tz.\n\n  tz\n    Timezone object.\n\nIf no tz is specified, uses local timezone.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/590", "data": {"name": "a.text"}}
{"task_id": "BigCodeBench/590", "data": {"name": "a.get('href')"}}
{"task_id": "BigCodeBench/590", "data": {"name": "urllib.error.URLError", "type": "class", "signature": "(reason)", "description": "Base class for I/O related errors.", "parameters": {"type": "object", "properties": {"reason": {}}}}}
{"task_id": "BigCodeBench/590", "data": {"name": "urllib.request.urlopen", "type": "callable", "signature": "(url)", "description": "Open the URL url, which can be either a string or a Request object.\n\n*data* must be an object specifying additional data to be sent to\nthe server, or None if no such data is needed.  See Request for\ndetails.\n\nurllib.request module uses HTTP/1.1 and includes a \"Connection:close\"\nheader in its HTTP requests.\n\nThe optional *timeout* parameter specifies a timeout in seconds for\nblocking operations like the connection attempt (if not specified, the\nglobal default timeout setting will be used). This only works for HTTP,\nHTTPS and FTP connections.\n\nIf *context* is specified, it must be a ssl.SSLContext instance describing\nthe various SSL options. See HTTPSConnection for more details.\n\nThe optional *cafile* and *capath* parameters specify a set of trusted CA\ncertificates for HTTPS requests. cafile should point to a single file\ncontaining a bundle of CA certificates, whereas capath should point to a\ndirectory of hashed certificate files. More information can be found in\nssl.SSLContext.load_verify_locations().\n\nThe *cadefault* parameter is ignored.\n\n\nThis function always returns an object which can work as a\ncontext manager and has the properties url, headers, and status.\nSee urllib.response.addinfourl for more detail on these properties.\n\nFor HTTP and HTTPS URLs, this function returns a http.client.HTTPResponse\nobject slightly modified. In addition to the three new methods above, the\nmsg attribute contains the same information as the reason attribute ---\nthe reason phrase returned by the server --- instead of the response\nheaders as it is specified in the documentation for HTTPResponse.\n\nFor FTP, file, and data URLs and requests explicitly handled by legacy\nURLopener and FancyURLopener classes, this function returns a\nurllib.response.addinfourl object.\n\nNote that None may be returned if no handler handles the request (though\nthe default installed global OpenerDirector uses UnknownHandler to ensure\nthis never happens).\n\nIn addition, if proxy settings are detected (for example, when a *_proxy\nenvironment variable like http_proxy is set), ProxyHandler is default\ninstalled and makes sure the requests are handled through the proxy.", "parameters": {"type": "object", "properties": {"url": {}}}}}
{"task_id": "BigCodeBench/590", "data": {"name": "urllib.request.urlopen.read", "type": "callable", "signature": "()", "description": "Open the URL url, which can be either a string or a Request object.\n\n*data* must be an object specifying additional data to be sent to\nthe server, or None if no such data is needed.  See Request for\ndetails.\n\nurllib.request module uses HTTP/1.1 and includes a \"Connection:close\"\nheader in its HTTP requests.\n\nThe optional *timeout* parameter specifies a timeout in seconds for\nblocking operations like the connection attempt (if not specified, the\nglobal default timeout setting will be used). This only works for HTTP,\nHTTPS and FTP connections.\n\nIf *context* is specified, it must be a ssl.SSLContext instance describing\nthe various SSL options. See HTTPSConnection for more details.\n\nThe optional *cafile* and *capath* parameters specify a set of trusted CA\ncertificates for HTTPS requests. cafile should point to a single file\ncontaining a bundle of CA certificates, whereas capath should point to a\ndirectory of hashed certificate files. More information can be found in\nssl.SSLContext.load_verify_locations().\n\nThe *cadefault* parameter is ignored.\n\n\nThis function always returns an object which can work as a\ncontext manager and has the properties url, headers, and status.\nSee urllib.response.addinfourl for more detail on these properties.\n\nFor HTTP and HTTPS URLs, this function returns a http.client.HTTPResponse\nobject slightly modified. In addition to the three new methods above, the\nmsg attribute contains the same information as the reason attribute ---\nthe reason phrase returned by the server --- instead of the response\nheaders as it is specified in the documentation for HTTPResponse.\n\nFor FTP, file, and data URLs and requests explicitly handled by legacy\nURLopener and FancyURLopener classes, this function returns a\nurllib.response.addinfourl object.\n\nNote that None may be returned if no handler handles the request (though\nthe default installed global OpenerDirector uses UnknownHandler to ensure\nthis never happens).\n\nIn addition, if proxy settings are detected (for example, when a *_proxy\nenvironment variable like http_proxy is set), ProxyHandler is default\ninstalled and makes sure the requests are handled through the proxy.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/592", "data": {"name": "datetime.datetime", "type": "class", "signature": "(self, /, *args, **kwargs)", "description": "datetime(year, month, day[, hour[, minute[, second[, microsecond[,tzinfo]]]]])\n\nThe year, month and day arguments are required. tzinfo may be None, or an\ninstance of a tzinfo subclass. The remaining arguments may be ints.", "parameters": {"type": "object", "properties": {"/": {}}}}}
{"task_id": "BigCodeBench/592", "data": {"name": "datetime.datetime.now", "type": "callable", "signature": "()", "description": "Returns new datetime object representing current time local to tz.\n\n  tz\n    Timezone object.\n\nIf no tz is specified, uses local timezone.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/592", "data": {"name": "csv.writer", "type": "callable", "signature": "(*args, **kwargs)", "description": "csv_writer = csv.writer(fileobj [, dialect='excel']\n                            [optional keyword args])\n    for row in sequence:\n        csv_writer.writerow(row)\n\n    [or]\n\n    csv_writer = csv.writer(fileobj [, dialect='excel']\n                            [optional keyword args])\n    csv_writer.writerows(rows)\n\nThe \"fileobj\" argument can be any object that supports the file API.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/592", "data": {"name": "csv.writer.writerows", "type": "callable", "signature": "(*args, **kwargs)", "description": "csv_writer = csv.writer(fileobj [, dialect='excel']\n                            [optional keyword args])\n    for row in sequence:\n        csv_writer.writerow(row)\n\n    [or]\n\n    csv_writer = csv.writer(fileobj [, dialect='excel']\n                            [optional keyword args])\n    csv_writer.writerows(rows)\n\nThe \"fileobj\" argument can be any object that supports the file API.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/592", "data": {"name": "data.append(row)"}}
{"task_id": "BigCodeBench/592", "data": {"name": "os.makedirs", "type": "callable", "signature": "(name)", "description": "makedirs(name [, mode=0o777][, exist_ok=False])\n\nSuper-mkdir; create a leaf directory and all intermediate ones.  Works like\nmkdir, except that any intermediate path segment (not just the rightmost)\nwill be created if it does not exist. If the target directory already\nexists, raise an OSError if exist_ok is False. Otherwise no exception is\nraised.  This is recursive.", "parameters": {"type": "object", "properties": {"name": {}}}}}
{"task_id": "BigCodeBench/592", "data": {"name": "os.path.exists", "type": "callable", "signature": "(path)", "description": "Test whether a path exists.  Returns False for broken symbolic links", "parameters": {"type": "object", "properties": {"path": {}}}}}
{"task_id": "BigCodeBench/592", "data": {"name": "os.path.join", "type": "callable", "signature": "(a, *p)", "description": "Join two or more pathname components, inserting '/' as needed.\nIf any component is an absolute path, all previous path components\nwill be discarded.  An empty last part will result in a path that\nends with a separator.", "parameters": {"type": "object", "properties": {"a": {}}}}}
{"task_id": "BigCodeBench/592", "data": {"name": "random.randint", "type": "callable", "signature": "(a, b)", "description": "Return random integer in range [a, b], including both end points.\n        ", "parameters": {"type": "object", "properties": {"a": {}, "b": {}}}}}
{"task_id": "BigCodeBench/593", "data": {"name": "datetime.datetime", "type": "class", "signature": "(self, /, *args, **kwargs)", "description": "datetime(year, month, day[, hour[, minute[, second[, microsecond[,tzinfo]]]]])\n\nThe year, month and day arguments are required. tzinfo may be None, or an\ninstance of a tzinfo subclass. The remaining arguments may be ints.", "parameters": {"type": "object", "properties": {"/": {}}}}}
{"task_id": "BigCodeBench/593", "data": {"name": "datetime.datetime.now", "type": "callable", "signature": "()", "description": "Returns new datetime object representing current time local to tz.\n\n  tz\n    Timezone object.\n\nIf no tz is specified, uses local timezone.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/593", "data": {"name": "csv.writer", "type": "callable", "signature": "(*args, **kwargs)", "description": "csv_writer = csv.writer(fileobj [, dialect='excel']\n                            [optional keyword args])\n    for row in sequence:\n        csv_writer.writerow(row)\n\n    [or]\n\n    csv_writer = csv.writer(fileobj [, dialect='excel']\n                            [optional keyword args])\n    csv_writer.writerows(rows)\n\nThe \"fileobj\" argument can be any object that supports the file API.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/593", "data": {"name": "csv.writer.writerows", "type": "callable", "signature": "(*args, **kwargs)", "description": "csv_writer = csv.writer(fileobj [, dialect='excel']\n                            [optional keyword args])\n    for row in sequence:\n        csv_writer.writerow(row)\n\n    [or]\n\n    csv_writer = csv.writer(fileobj [, dialect='excel']\n                            [optional keyword args])\n    csv_writer.writerows(rows)\n\nThe \"fileobj\" argument can be any object that supports the file API.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/593", "data": {"name": "data.append(row)"}}
{"task_id": "BigCodeBench/593", "data": {"name": "matplotlib.pyplot.show", "type": "callable", "signature": "()", "description": "Display all open figures.\n\nParameters\n----------\nblock : bool, optional\n    Whether to wait for all figures to be closed before returning.\n\n    If `True` block and run the GUI main loop until all figure windows\n    are closed.\n\n    If `False` ensure that all figure windows are displayed and return\n    immediately.  In this case, you are responsible for ensuring\n    that the event loop is running to have responsive figures.\n\n    Defaults to True in non-interactive mode and to False in interactive\n    mode (see `.pyplot.isinteractive`).\n\nSee Also\n--------\nion : Enable interactive mode, which shows / updates the figure after\n      every plotting command, so that calling ``show()`` is not necessary.\nioff : Disable interactive mode.\nsavefig : Save the figure to an image file instead of showing it on screen.\n\nNotes\n-----\n**Saving figures to file and showing a window at the same time**\n\nIf you want an image file as well as a user interface window, use\n`.pyplot.savefig` before `.pyplot.show`. At the end of (a blocking)\n``show()`` the figure is closed and thus unregistered from pyplot. Calling\n`.pyplot.savefig` afterwards would save a new and thus empty figure. This\nlimitation of command order does not apply if the show is non-blocking or\nif you keep a reference to the figure and use `.Figure.savefig`.\n\n**Auto-show in jupyter notebooks**\n\nThe jupyter backends (activated via ``%matplotlib inline``,\n``%matplotlib notebook``, or ``%matplotlib widget``), call ``show()`` at\nthe end of every cell by default. Thus, you usually don't have to call it\nexplicitly there.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/593", "data": {"name": "matplotlib.pyplot.xlabel", "type": "callable", "signature": "(xlabel: 'str')", "description": "Set the label for the x-axis.\n\nParameters\n----------\nxlabel : str\n    The label text.\n\nlabelpad : float, default: :rc:`axes.labelpad`\n    Spacing in points from the Axes bounding box including ticks\n    and tick labels.  If None, the previous value is left as is.\n\nloc : {'left', 'center', 'right'}, default: :rc:`xaxis.labellocation`\n    The label position. This is a high-level alternative for passing\n    parameters *x* and *horizontalalignment*.\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    `.Text` properties control the appearance of the label.\n\nSee Also\n--------\ntext : Documents the properties supported by `.Text`.", "parameters": {"type": "object", "properties": {"xlabel": {"type": "str"}}}}}
{"task_id": "BigCodeBench/593", "data": {"name": "matplotlib.pyplot.tight_layout", "type": "callable", "signature": "()", "description": "Adjust the padding between and around subplots.\n\nTo exclude an artist on the Axes from the bounding box calculation\nthat determines the subplot parameters (i.e. legend, or annotation),\nset ``a.set_in_layout(False)`` for that artist.\n\nParameters\n----------\npad : float, default: 1.08\n    Padding between the figure edge and the edges of subplots,\n    as a fraction of the font size.\nh_pad, w_pad : float, default: *pad*\n    Padding (height/width) between edges of adjacent subplots,\n    as a fraction of the font size.\nrect : tuple (left, bottom, right, top), default: (0, 0, 1, 1)\n    A rectangle in normalized figure coordinates into which the whole\n    subplots area (including labels) will fit.\n\nSee Also\n--------\n.Figure.set_layout_engine\n.pyplot.tight_layout", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/593", "data": {"name": "matplotlib.pyplot.ylabel", "type": "callable", "signature": "(ylabel: 'str')", "description": "Set the label for the y-axis.\n\nParameters\n----------\nylabel : str\n    The label text.\n\nlabelpad : float, default: :rc:`axes.labelpad`\n    Spacing in points from the Axes bounding box including ticks\n    and tick labels.  If None, the previous value is left as is.\n\nloc : {'bottom', 'center', 'top'}, default: :rc:`yaxis.labellocation`\n    The label position. This is a high-level alternative for passing\n    parameters *y* and *horizontalalignment*.\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    `.Text` properties control the appearance of the label.\n\nSee Also\n--------\ntext : Documents the properties supported by `.Text`.", "parameters": {"type": "object", "properties": {"ylabel": {"type": "str"}}}}}
{"task_id": "BigCodeBench/593", "data": {"name": "os.makedirs", "type": "callable", "signature": "(name)", "description": "makedirs(name [, mode=0o777][, exist_ok=False])\n\nSuper-mkdir; create a leaf directory and all intermediate ones.  Works like\nmkdir, except that any intermediate path segment (not just the rightmost)\nwill be created if it does not exist. If the target directory already\nexists, raise an OSError if exist_ok is False. Otherwise no exception is\nraised.  This is recursive.", "parameters": {"type": "object", "properties": {"name": {}}}}}
{"task_id": "BigCodeBench/593", "data": {"name": "os.path.exists", "type": "callable", "signature": "(path)", "description": "Test whether a path exists.  Returns False for broken symbolic links", "parameters": {"type": "object", "properties": {"path": {}}}}}
{"task_id": "BigCodeBench/593", "data": {"name": "os.path.join", "type": "callable", "signature": "(a, *p)", "description": "Join two or more pathname components, inserting '/' as needed.\nIf any component is an absolute path, all previous path components\nwill be discarded.  An empty last part will result in a path that\nends with a separator.", "parameters": {"type": "object", "properties": {"a": {}}}}}
{"task_id": "BigCodeBench/593", "data": {"name": "pandas.read_csv", "type": "callable", "signature": "(filepath_or_buffer: 'FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str]')", "description": "Read a comma-separated values (csv) file into DataFrame.\n\nAlso supports optionally iterating or breaking of the file\ninto chunks.\n\nAdditional help can be found in the online docs for\n`IO Tools <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html>`_.\n\nParameters\n----------\nfilepath_or_buffer : str, path object or file-like object\n    Any valid string path is acceptable. The string could be a URL. Valid\n    URL schemes include http, ftp, s3, gs, and file. For file URLs, a host is\n    expected. A local file could be: file://localhost/path/to/table.csv.\n\n    If you want to pass in a path object, pandas accepts any ``os.PathLike``.\n\n    By file-like object, we refer to objects with a ``read()`` method, such as\n    a file handle (e.g. via builtin ``open`` function) or ``StringIO``.\nsep : str, default ','\n    Character or regex pattern to treat as the delimiter. If ``sep=None``, the\n    C engine cannot automatically detect\n    the separator, but the Python parsing engine can, meaning the latter will\n    be used and automatically detect the separator from only the first valid\n    row of the file by Python's builtin sniffer tool, ``csv.Sniffer``.\n    In addition, separators longer than 1 character and different from\n    ``'\\s+'`` will be interpreted as regular expressions and will also force\n    the use of the Python parsing engine. Note that regex delimiters are prone\n    to ignoring quoted data. Regex example: ``'\\r\\t'``.\ndelimiter : str, optional\n    Alias for ``sep``.\nheader : int, Sequence of int, 'infer' or None, default 'infer'\n    Row number(s) containing column labels and marking the start of the\n    data (zero-indexed). Default behavior is to infer the column names: if no ``names``\n    are passed the behavior is identical to ``header=0`` and column\n    names are inferred from the first line of the file, if column\n    names are passed explicitly to ``names`` then the behavior is identical to\n    ``header=None``. Explicitly pass ``header=0`` to be able to\n    replace existing names. The header can be a list of integers that\n    specify row locations for a :class:`~pandas.MultiIndex` on the columns\n    e.g. ``[0, 1, 3]``. Intervening rows that are not specified will be\n    skipped (e.g. 2 in this example is skipped). Note that this\n    parameter ignores commented lines and empty lines if\n    ``skip_blank_lines=True``, so ``header=0`` denotes the first line of\n    data rather than the first line of the file.\nnames : Sequence of Hashable, optional\n    Sequence of column labels to apply. If the file contains a header row,\n    then you should explicitly pass ``header=0`` to override the column names.\n    Duplicates in this list are not allowed.\nindex_col : Hashable, Sequence of Hashable or False, optional\n  Column(s) to use as row label(s), denoted either by column labels or column\n  indices.  If a sequence of labels or indices is given, :class:`~pandas.MultiIndex`\n  will be formed for the row labels.\n\n  Note: ``index_col=False`` can be used to force pandas to *not* use the first\n  column as the index, e.g., when you have a malformed file with delimiters at\n  the end of each line.\nusecols : Sequence of Hashable or Callable, optional\n    Subset of columns to select, denoted either by column labels or column indices.\n    If list-like, all elements must either\n    be positional (i.e. integer indices into the document columns) or strings\n    that correspond to column names provided either by the user in ``names`` or\n    inferred from the document header row(s). If ``names`` are given, the document\n    header row(s) are not taken into account. For example, a valid list-like\n    ``usecols`` parameter would be ``[0, 1, 2]`` or ``['foo', 'bar', 'baz']``.\n    Element order is ignored, so ``usecols=[0, 1]`` is the same as ``[1, 0]``.\n    To instantiate a :class:`~pandas.DataFrame` from ``data`` with element order\n    preserved use ``pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']]``\n    for columns in ``['foo', 'bar']`` order or\n    ``pd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']]``\n    for ``['bar', 'foo']`` order.\n\n    If callable, the callable function will be evaluated against the column\n    names, returning names where the callable function evaluates to ``True``. An\n    example of a valid callable argument would be ``lambda x: x.upper() in\n    ['AAA', 'BBB', 'DDD']``. Using this parameter results in much faster\n    parsing time and lower memory usage.\ndtype : dtype or dict of {Hashable : dtype}, optional\n    Data type(s) to apply to either the whole dataset or individual columns.\n    E.g., ``{'a': np.float64, 'b': np.int32, 'c': 'Int64'}``\n    Use ``str`` or ``object`` together with suitable ``na_values`` settings\n    to preserve and not interpret ``dtype``.\n    If ``converters`` are specified, they will be applied INSTEAD\n    of ``dtype`` conversion.\n\n    .. versionadded:: 1.5.0\n\n        Support for ``defaultdict`` was added. Specify a ``defaultdict`` as input where\n        the default determines the ``dtype`` of the columns which are not explicitly\n        listed.\nengine : {'c', 'python', 'pyarrow'}, optional\n    Parser engine to use. The C and pyarrow engines are faster, while the python engine\n    is currently more feature-complete. Multithreading is currently only supported by\n    the pyarrow engine.\n\n    .. versionadded:: 1.4.0\n\n        The 'pyarrow' engine was added as an *experimental* engine, and some features\n        are unsupported, or may not work correctly, with this engine.\nconverters : dict of {Hashable : Callable}, optional\n    Functions for converting values in specified columns. Keys can either\n    be column labels or column indices.\ntrue_values : list, optional\n    Values to consider as ``True`` in addition to case-insensitive variants of 'True'.\nfalse_values : list, optional\n    Values to consider as ``False`` in addition to case-insensitive variants of 'False'.\nskipinitialspace : bool, default False\n    Skip spaces after delimiter.\nskiprows : int, list of int or Callable, optional\n    Line numbers to skip (0-indexed) or number of lines to skip (``int``)\n    at the start of the file.\n\n    If callable, the callable function will be evaluated against the row\n    indices, returning ``True`` if the row should be skipped and ``False`` otherwise.\n    An example of a valid callable argument would be ``lambda x: x in [0, 2]``.\nskipfooter : int, default 0\n    Number of lines at bottom of file to skip (Unsupported with ``engine='c'``).\nnrows : int, optional\n    Number of rows of file to read. Useful for reading pieces of large files.\nna_values : Hashable, Iterable of Hashable or dict of {Hashable : Iterable}, optional\n    Additional strings to recognize as ``NA``/``NaN``. If ``dict`` passed, specific\n    per-column ``NA`` values.  By default the following values are interpreted as\n    ``NaN``: \" \", \"#N/A\", \"#N/A N/A\", \"#NA\", \"-1.#IND\", \"-1.#QNAN\", \"-NaN\", \"-nan\",\n    \"1.#IND\", \"1.#QNAN\", \"<NA>\", \"N/A\", \"NA\", \"NULL\", \"NaN\", \"None\",\n    \"n/a\", \"nan\", \"null \".\n\nkeep_default_na : bool, default True\n    Whether or not to include the default ``NaN`` values when parsing the data.\n    Depending on whether ``na_values`` is passed in, the behavior is as follows:\n\n    * If ``keep_default_na`` is ``True``, and ``na_values`` are specified, ``na_values``\n      is appended to the default ``NaN`` values used for parsing.\n    * If ``keep_default_na`` is ``True``, and ``na_values`` are not specified, only\n      the default ``NaN`` values are used for parsing.\n    * If ``keep_default_na`` is ``False``, and ``na_values`` are specified, only\n      the ``NaN`` values specified ``na_values`` are used for parsing.\n    * If ``keep_default_na`` is ``False``, and ``na_values`` are not specified, no\n      strings will be parsed as ``NaN``.\n\n    Note that if ``na_filter`` is passed in as ``False``, the ``keep_default_na`` and\n    ``na_values`` parameters will be ignored.\nna_filter : bool, default True\n    Detect missing value markers (empty strings and the value of ``na_values``). In\n    data without any ``NA`` values, passing ``na_filter=False`` can improve the\n    performance of reading a large file.\nverbose : bool, default False\n    Indicate number of ``NA`` values placed in non-numeric columns.\n\n    .. deprecated:: 2.2.0\nskip_blank_lines : bool, default True\n    If ``True``, skip over blank lines rather than interpreting as ``NaN`` values.\nparse_dates : bool, list of Hashable, list of lists or dict of {Hashable : list}, default False\n    The behavior is as follows:\n\n    * ``bool``. If ``True`` -> try parsing the index. Note: Automatically set to\n      ``True`` if ``date_format`` or ``date_parser`` arguments have been passed.\n    * ``list`` of ``int`` or names. e.g. If ``[1, 2, 3]`` -> try parsing columns 1, 2, 3\n      each as a separate date column.\n    * ``list`` of ``list``. e.g.  If ``[[1, 3]]`` -> combine columns 1 and 3 and parse\n      as a single date column. Values are joined with a space before parsing.\n    * ``dict``, e.g. ``{'foo' : [1, 3]}`` -> parse columns 1, 3 as date and call\n      result 'foo'. Values are joined with a space before parsing.\n\n    If a column or index cannot be represented as an array of ``datetime``,\n    say because of an unparsable value or a mixture of timezones, the column\n    or index will be returned unaltered as an ``object`` data type. For\n    non-standard ``datetime`` parsing, use :func:`~pandas.to_datetime` after\n    :func:`~pandas.read_csv`.\n\n    Note: A fast-path exists for iso8601-formatted dates.\ninfer_datetime_format : bool, default False\n    If ``True`` and ``parse_dates`` is enabled, pandas will attempt to infer the\n    format of the ``datetime`` strings in the columns, and if it can be inferred,\n    switch to a faster method of parsing them. In some cases this can increase\n    the parsing speed by 5-10x.\n\n    .. deprecated:: 2.0.0\n        A strict version of this argument is now the default, passing it has no effect.\n\nkeep_date_col : bool, default False\n    If ``True`` and ``parse_dates`` specifies combining multiple columns then\n    keep the original columns.\ndate_parser : Callable, optional\n    Function to use for converting a sequence of string columns to an array of\n    ``datetime`` instances. The default uses ``dateutil.parser.parser`` to do the\n    conversion. pandas will try to call ``date_parser`` in three different ways,\n    advancing to the next if an exception occurs: 1) Pass one or more arrays\n    (as defined by ``parse_dates``) as arguments; 2) concatenate (row-wise) the\n    string values from the columns defined by ``parse_dates`` into a single array\n    and pass that; and 3) call ``date_parser`` once for each row using one or\n    more strings (corresponding to the columns defined by ``parse_dates``) as\n    arguments.\n\n    .. deprecated:: 2.0.0\n       Use ``date_format`` instead, or read in as ``object`` and then apply\n       :func:`~pandas.to_datetime` as-needed.\ndate_format : str or dict of column -> format, optional\n    Format to use for parsing dates when used in conjunction with ``parse_dates``.\n    The strftime to parse time, e.g. :const:`\"%d/%m/%Y\"`. See\n    `strftime documentation\n    <https://docs.python.org/3/library/datetime.html\n    #strftime-and-strptime-behavior>`_ for more information on choices, though\n    note that :const:`\"%f\"` will parse all the way up to nanoseconds.\n    You can also pass:\n\n    - \"ISO8601\", to parse any `ISO8601 <https://en.wikipedia.org/wiki/ISO_8601>`_\n        time string (not necessarily in exactly the same format);\n    - \"mixed\", to infer the format for each element individually. This is risky,\n        and you should probably use it along with `dayfirst`.\n\n    .. versionadded:: 2.0.0\ndayfirst : bool, default False\n    DD/MM format dates, international and European format.\ncache_dates : bool, default True\n    If ``True``, use a cache of unique, converted dates to apply the ``datetime``\n    conversion. May produce significant speed-up when parsing duplicate\n    date strings, especially ones with timezone offsets.\n\niterator : bool, default False\n    Return ``TextFileReader`` object for iteration or getting chunks with\n    ``get_chunk()``.\nchunksize : int, optional\n    Number of lines to read from the file per chunk. Passing a value will cause the\n    function to return a ``TextFileReader`` object for iteration.\n    See the `IO Tools docs\n    <https://pandas.pydata.org/pandas-docs/stable/io.html#io-chunking>`_\n    for more information on ``iterator`` and ``chunksize``.\n\ncompression : str or dict, default 'infer'\n    For on-the-fly decompression of on-disk data. If 'infer' and 'filepath_or_buffer' is\n    path-like, then detect compression from the following extensions: '.gz',\n    '.bz2', '.zip', '.xz', '.zst', '.tar', '.tar.gz', '.tar.xz' or '.tar.bz2'\n    (otherwise no compression).\n    If using 'zip' or 'tar', the ZIP file must contain only one data file to be read in.\n    Set to ``None`` for no decompression.\n    Can also be a dict with key ``'method'`` set\n    to one of {``'zip'``, ``'gzip'``, ``'bz2'``, ``'zstd'``, ``'xz'``, ``'tar'``} and\n    other key-value pairs are forwarded to\n    ``zipfile.ZipFile``, ``gzip.GzipFile``,\n    ``bz2.BZ2File``, ``zstandard.ZstdDecompressor``, ``lzma.LZMAFile`` or\n    ``tarfile.TarFile``, respectively.\n    As an example, the following could be passed for Zstandard decompression using a\n    custom compression dictionary:\n    ``compression={'method': 'zstd', 'dict_data': my_compression_dict}``.\n\n    .. versionadded:: 1.5.0\n        Added support for `.tar` files.\n\n    .. versionchanged:: 1.4.0 Zstandard support.\n\nthousands : str (length 1), optional\n    Character acting as the thousands separator in numerical values.\ndecimal : str (length 1), default '.'\n    Character to recognize as decimal point (e.g., use ',' for European data).\nlineterminator : str (length 1), optional\n    Character used to denote a line break. Only valid with C parser.\nquotechar : str (length 1), optional\n    Character used to denote the start and end of a quoted item. Quoted\n    items can include the ``delimiter`` and it will be ignored.\nquoting : {0 or csv.QUOTE_MINIMAL, 1 or csv.QUOTE_ALL, 2 or csv.QUOTE_NONNUMERIC, 3 or csv.QUOTE_NONE}, default csv.QUOTE_MINIMAL\n    Control field quoting behavior per ``csv.QUOTE_*`` constants. Default is\n    ``csv.QUOTE_MINIMAL`` (i.e., 0) which implies that only fields containing special\n    characters are quoted (e.g., characters defined in ``quotechar``, ``delimiter``,\n    or ``lineterminator``.\ndoublequote : bool, default True\n   When ``quotechar`` is specified and ``quoting`` is not ``QUOTE_NONE``, indicate\n   whether or not to interpret two consecutive ``quotechar`` elements INSIDE a\n   field as a single ``quotechar`` element.\nescapechar : str (length 1), optional\n    Character used to escape other characters.\ncomment : str (length 1), optional\n    Character indicating that the remainder of line should not be parsed.\n    If found at the beginning\n    of a line, the line will be ignored altogether. This parameter must be a\n    single character. Like empty lines (as long as ``skip_blank_lines=True``),\n    fully commented lines are ignored by the parameter ``header`` but not by\n    ``skiprows``. For example, if ``comment='#'``, parsing\n    ``#empty\\na,b,c\\n1,2,3`` with ``header=0`` will result in ``'a,b,c'`` being\n    treated as the header.\nencoding : str, optional, default 'utf-8'\n    Encoding to use for UTF when reading/writing (ex. ``'utf-8'``). `List of Python\n    standard encodings\n    <https://docs.python.org/3/library/codecs.html#standard-encodings>`_ .\n\nencoding_errors : str, optional, default 'strict'\n    How encoding errors are treated. `List of possible values\n    <https://docs.python.org/3/library/codecs.html#error-handlers>`_ .\n\n    .. versionadded:: 1.3.0\n\ndialect : str or csv.Dialect, optional\n    If provided, this parameter will override values (default or not) for the\n    following parameters: ``delimiter``, ``doublequote``, ``escapechar``,\n    ``skipinitialspace``, ``quotechar``, and ``quoting``. If it is necessary to\n    override values, a ``ParserWarning`` will be issued. See ``csv.Dialect``\n    documentation for more details.\non_bad_lines : {'error', 'warn', 'skip'} or Callable, default 'error'\n    Specifies what to do upon encountering a bad line (a line with too many fields).\n    Allowed values are :\n\n    - ``'error'``, raise an Exception when a bad line is encountered.\n    - ``'warn'``, raise a warning when a bad line is encountered and skip that line.\n    - ``'skip'``, skip bad lines without raising or warning when they are encountered.\n\n    .. versionadded:: 1.3.0\n\n    .. versionadded:: 1.4.0\n\n        - Callable, function with signature\n          ``(bad_line: list[str]) -> list[str] | None`` that will process a single\n          bad line. ``bad_line`` is a list of strings split by the ``sep``.\n          If the function returns ``None``, the bad line will be ignored.\n          If the function returns a new ``list`` of strings with more elements than\n          expected, a ``ParserWarning`` will be emitted while dropping extra elements.\n          Only supported when ``engine='python'``\n\n    .. versionchanged:: 2.2.0\n\n        - Callable, function with signature\n          as described in `pyarrow documentation\n          <https://arrow.apache.org/docs/python/generated/pyarrow.csv.ParseOptions.html\n          #pyarrow.csv.ParseOptions.invalid_row_handler>`_ when ``engine='pyarrow'``\n\ndelim_whitespace : bool, default False\n    Specifies whether or not whitespace (e.g. ``' '`` or ``'\\t'``) will be\n    used as the ``sep`` delimiter. Equivalent to setting ``sep='\\s+'``. If this option\n    is set to ``True``, nothing should be passed in for the ``delimiter``\n    parameter.\n\n    .. deprecated:: 2.2.0\n        Use ``sep=\"\\s+\"`` instead.\nlow_memory : bool, default True\n    Internally process the file in chunks, resulting in lower memory use\n    while parsing, but possibly mixed type inference.  To ensure no mixed\n    types either set ``False``, or specify the type with the ``dtype`` parameter.\n    Note that the entire file is read into a single :class:`~pandas.DataFrame`\n    regardless, use the ``chunksize`` or ``iterator`` parameter to return the data in\n    chunks. (Only valid with C parser).\nmemory_map : bool, default False\n    If a filepath is provided for ``filepath_or_buffer``, map the file object\n    directly onto memory and access the data directly from there. Using this\n    option can improve performance because there is no longer any I/O overhead.\nfloat_precision : {'high', 'legacy', 'round_trip'}, optional\n    Specifies which converter the C engine should use for floating-point\n    values. The options are ``None`` or ``'high'`` for the ordinary converter,\n    ``'legacy'`` for the original lower precision pandas converter, and\n    ``'round_trip'`` for the round-trip converter.\n\nstorage_options : dict, optional\n    Extra options that make sense for a particular storage connection, e.g.\n    host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n    are forwarded to ``urllib.request.Request`` as header options. For other\n    URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n    forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n    details, and for more examples on storage options refer `here\n    <https://pandas.pydata.org/docs/user_guide/io.html?\n    highlight=storage_options#reading-writing-remote-files>`_.\n\ndtype_backend : {'numpy_nullable', 'pyarrow'}, default 'numpy_nullable'\n    Back-end data type applied to the resultant :class:`DataFrame`\n    (still experimental). Behaviour is as follows:\n\n    * ``\"numpy_nullable\"``: returns nullable-dtype-backed :class:`DataFrame`\n      (default).\n    * ``\"pyarrow\"``: returns pyarrow-backed nullable :class:`ArrowDtype`\n      DataFrame.\n\n    .. versionadded:: 2.0\n\nReturns\n-------\nDataFrame or TextFileReader\n    A comma-separated values (csv) file is returned as two-dimensional\n    data structure with labeled axes.\n\nSee Also\n--------\nDataFrame.to_csv : Write DataFrame to a comma-separated values (csv) file.\nread_table : Read general delimited file into DataFrame.\nread_fwf : Read a table of fixed-width formatted lines into DataFrame.\n\nExamples\n--------\n>>> pd.read_csv('data.csv')  # doctest: +SKIP", "parameters": {"type": "object", "properties": {"filepath_or_buffer": {"type": ["readcsvbuffer[bytes]", "readcsvbuffer[str]", "filepath"]}}}}}
{"task_id": "BigCodeBench/593", "data": {"name": "pandas.read_csv.empty", "type": "callable", "signature": "(filepath_or_buffer: 'FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str]', *, sep: 'str | None | lib.NoDefault' = <no_default>, delimiter: 'str | None | lib.NoDefault' = None, header: \"int | Sequence[int] | None | Literal['infer']\" = 'infer', names: 'Sequence[Hashable] | None | lib.NoDefault' = <no_default>, index_col: 'IndexLabel | Literal[False] | None' = None, usecols: 'UsecolsArgType' = None, dtype: 'DtypeArg | None' = None, engine: 'CSVEngine | None' = None, converters: 'Mapping[Hashable, Callable] | None' = None, true_values: 'list | None' = None, false_values: 'list | None' = None, skipinitialspace: 'bool' = False, skiprows: 'list[int] | int | Callable[[Hashable], bool] | None' = None, skipfooter: 'int' = 0, nrows: 'int | None' = None, na_values: 'Hashable | Iterable[Hashable] | Mapping[Hashable, Iterable[Hashable]] | None' = None, keep_default_na: 'bool' = True, na_filter: 'bool' = True, verbose: 'bool | lib.NoDefault' = <no_default>, skip_blank_lines: 'bool' = True, parse_dates: 'bool | Sequence[Hashable] | None' = None, infer_datetime_format: 'bool | lib.NoDefault' = <no_default>, keep_date_col: 'bool | lib.NoDefault' = <no_default>, date_parser: 'Callable | lib.NoDefault' = <no_default>, date_format: 'str | dict[Hashable, str] | None' = None, dayfirst: 'bool' = False, cache_dates: 'bool' = True, iterator: 'bool' = False, chunksize: 'int | None' = None, compression: 'CompressionOptions' = 'infer', thousands: 'str | None' = None, decimal: 'str' = '.', lineterminator: 'str | None' = None, quotechar: 'str' = '\"', quoting: 'int' = 0, doublequote: 'bool' = True, escapechar: 'str | None' = None, comment: 'str | None' = None, encoding: 'str | None' = None, encoding_errors: 'str | None' = 'strict', dialect: 'str | csv.Dialect | None' = None, on_bad_lines: 'str' = 'error', delim_whitespace: 'bool | lib.NoDefault' = <no_default>, low_memory: 'bool' = True, memory_map: 'bool' = False, float_precision: \"Literal['high', 'legacy'] | None\" = None, storage_options: 'StorageOptions | None' = None, dtype_backend: 'DtypeBackend | lib.NoDefault' = <no_default>) -> 'DataFrame | TextFileReader'", "description": "Read a comma-separated values (csv) file into DataFrame.\n\nAlso supports optionally iterating or breaking of the file\ninto chunks.\n\nAdditional help can be found in the online docs for\n`IO Tools <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html>`_.\n\nParameters\n----------\nfilepath_or_buffer : str, path object or file-like object\n    Any valid string path is acceptable. The string could be a URL. Valid\n    URL schemes include http, ftp, s3, gs, and file. For file URLs, a host is\n    expected. A local file could be: file://localhost/path/to/table.csv.\n\n    If you want to pass in a path object, pandas accepts any ``os.PathLike``.\n\n    By file-like object, we refer to objects with a ``read()`` method, such as\n    a file handle (e.g. via builtin ``open`` function) or ``StringIO``.\nsep : str, default ','\n    Character or regex pattern to treat as the delimiter. If ``sep=None``, the\n    C engine cannot automatically detect\n    the separator, but the Python parsing engine can, meaning the latter will\n    be used and automatically detect the separator from only the first valid\n    row of the file by Python's builtin sniffer tool, ``csv.Sniffer``.\n    In addition, separators longer than 1 character and different from\n    ``'\\s+'`` will be interpreted as regular expressions and will also force\n    the use of the Python parsing engine. Note that regex delimiters are prone\n    to ignoring quoted data. Regex example: ``'\\r\\t'``.\ndelimiter : str, optional\n    Alias for ``sep``.\nheader : int, Sequence of int, 'infer' or None, default 'infer'\n    Row number(s) containing column labels and marking the start of the\n    data (zero-indexed). Default behavior is to infer the column names: if no ``names``\n    are passed the behavior is identical to ``header=0`` and column\n    names are inferred from the first line of the file, if column\n    names are passed explicitly to ``names`` then the behavior is identical to\n    ``header=None``. Explicitly pass ``header=0`` to be able to\n    replace existing names. The header can be a list of integers that\n    specify row locations for a :class:`~pandas.MultiIndex` on the columns\n    e.g. ``[0, 1, 3]``. Intervening rows that are not specified will be\n    skipped (e.g. 2 in this example is skipped). Note that this\n    parameter ignores commented lines and empty lines if\n    ``skip_blank_lines=True``, so ``header=0`` denotes the first line of\n    data rather than the first line of the file.\nnames : Sequence of Hashable, optional\n    Sequence of column labels to apply. If the file contains a header row,\n    then you should explicitly pass ``header=0`` to override the column names.\n    Duplicates in this list are not allowed.\nindex_col : Hashable, Sequence of Hashable or False, optional\n  Column(s) to use as row label(s), denoted either by column labels or column\n  indices.  If a sequence of labels or indices is given, :class:`~pandas.MultiIndex`\n  will be formed for the row labels.\n\n  Note: ``index_col=False`` can be used to force pandas to *not* use the first\n  column as the index, e.g., when you have a malformed file with delimiters at\n  the end of each line.\nusecols : Sequence of Hashable or Callable, optional\n    Subset of columns to select, denoted either by column labels or column indices.\n    If list-like, all elements must either\n    be positional (i.e. integer indices into the document columns) or strings\n    that correspond to column names provided either by the user in ``names`` or\n    inferred from the document header row(s). If ``names`` are given, the document\n    header row(s) are not taken into account. For example, a valid list-like\n    ``usecols`` parameter would be ``[0, 1, 2]`` or ``['foo', 'bar', 'baz']``.\n    Element order is ignored, so ``usecols=[0, 1]`` is the same as ``[1, 0]``.\n    To instantiate a :class:`~pandas.DataFrame` from ``data`` with element order\n    preserved use ``pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']]``\n    for columns in ``['foo', 'bar']`` order or\n    ``pd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']]``\n    for ``['bar', 'foo']`` order.\n\n    If callable, the callable function will be evaluated against the column\n    names, returning names where the callable function evaluates to ``True``. An\n    example of a valid callable argument would be ``lambda x: x.upper() in\n    ['AAA', 'BBB', 'DDD']``. Using this parameter results in much faster\n    parsing time and lower memory usage.\ndtype : dtype or dict of {Hashable : dtype}, optional\n    Data type(s) to apply to either the whole dataset or individual columns.\n    E.g., ``{'a': np.float64, 'b': np.int32, 'c': 'Int64'}``\n    Use ``str`` or ``object`` together with suitable ``na_values`` settings\n    to preserve and not interpret ``dtype``.\n    If ``converters`` are specified, they will be applied INSTEAD\n    of ``dtype`` conversion.\n\n    .. versionadded:: 1.5.0\n\n        Support for ``defaultdict`` was added. Specify a ``defaultdict`` as input where\n        the default determines the ``dtype`` of the columns which are not explicitly\n        listed.\nengine : {'c', 'python', 'pyarrow'}, optional\n    Parser engine to use. The C and pyarrow engines are faster, while the python engine\n    is currently more feature-complete. Multithreading is currently only supported by\n    the pyarrow engine.\n\n    .. versionadded:: 1.4.0\n\n        The 'pyarrow' engine was added as an *experimental* engine, and some features\n        are unsupported, or may not work correctly, with this engine.\nconverters : dict of {Hashable : Callable}, optional\n    Functions for converting values in specified columns. Keys can either\n    be column labels or column indices.\ntrue_values : list, optional\n    Values to consider as ``True`` in addition to case-insensitive variants of 'True'.\nfalse_values : list, optional\n    Values to consider as ``False`` in addition to case-insensitive variants of 'False'.\nskipinitialspace : bool, default False\n    Skip spaces after delimiter.\nskiprows : int, list of int or Callable, optional\n    Line numbers to skip (0-indexed) or number of lines to skip (``int``)\n    at the start of the file.\n\n    If callable, the callable function will be evaluated against the row\n    indices, returning ``True`` if the row should be skipped and ``False`` otherwise.\n    An example of a valid callable argument would be ``lambda x: x in [0, 2]``.\nskipfooter : int, default 0\n    Number of lines at bottom of file to skip (Unsupported with ``engine='c'``).\nnrows : int, optional\n    Number of rows of file to read. Useful for reading pieces of large files.\nna_values : Hashable, Iterable of Hashable or dict of {Hashable : Iterable}, optional\n    Additional strings to recognize as ``NA``/``NaN``. If ``dict`` passed, specific\n    per-column ``NA`` values.  By default the following values are interpreted as\n    ``NaN``: \" \", \"#N/A\", \"#N/A N/A\", \"#NA\", \"-1.#IND\", \"-1.#QNAN\", \"-NaN\", \"-nan\",\n    \"1.#IND\", \"1.#QNAN\", \"<NA>\", \"N/A\", \"NA\", \"NULL\", \"NaN\", \"None\",\n    \"n/a\", \"nan\", \"null \".\n\nkeep_default_na : bool, default True\n    Whether or not to include the default ``NaN`` values when parsing the data.\n    Depending on whether ``na_values`` is passed in, the behavior is as follows:\n\n    * If ``keep_default_na`` is ``True``, and ``na_values`` are specified, ``na_values``\n      is appended to the default ``NaN`` values used for parsing.\n    * If ``keep_default_na`` is ``True``, and ``na_values`` are not specified, only\n      the default ``NaN`` values are used for parsing.\n    * If ``keep_default_na`` is ``False``, and ``na_values`` are specified, only\n      the ``NaN`` values specified ``na_values`` are used for parsing.\n    * If ``keep_default_na`` is ``False``, and ``na_values`` are not specified, no\n      strings will be parsed as ``NaN``.\n\n    Note that if ``na_filter`` is passed in as ``False``, the ``keep_default_na`` and\n    ``na_values`` parameters will be ignored.\nna_filter : bool, default True\n    Detect missing value markers (empty strings and the value of ``na_values``). In\n    data without any ``NA`` values, passing ``na_filter=False`` can improve the\n    performance of reading a large file.\nverbose : bool, default False\n    Indicate number of ``NA`` values placed in non-numeric columns.\n\n    .. deprecated:: 2.2.0\nskip_blank_lines : bool, default True\n    If ``True``, skip over blank lines rather than interpreting as ``NaN`` values.\nparse_dates : bool, list of Hashable, list of lists or dict of {Hashable : list}, default False\n    The behavior is as follows:\n\n    * ``bool``. If ``True`` -> try parsing the index. Note: Automatically set to\n      ``True`` if ``date_format`` or ``date_parser`` arguments have been passed.\n    * ``list`` of ``int`` or names. e.g. If ``[1, 2, 3]`` -> try parsing columns 1, 2, 3\n      each as a separate date column.\n    * ``list`` of ``list``. e.g.  If ``[[1, 3]]`` -> combine columns 1 and 3 and parse\n      as a single date column. Values are joined with a space before parsing.\n    * ``dict``, e.g. ``{'foo' : [1, 3]}`` -> parse columns 1, 3 as date and call\n      result 'foo'. Values are joined with a space before parsing.\n\n    If a column or index cannot be represented as an array of ``datetime``,\n    say because of an unparsable value or a mixture of timezones, the column\n    or index will be returned unaltered as an ``object`` data type. For\n    non-standard ``datetime`` parsing, use :func:`~pandas.to_datetime` after\n    :func:`~pandas.read_csv`.\n\n    Note: A fast-path exists for iso8601-formatted dates.\ninfer_datetime_format : bool, default False\n    If ``True`` and ``parse_dates`` is enabled, pandas will attempt to infer the\n    format of the ``datetime`` strings in the columns, and if it can be inferred,\n    switch to a faster method of parsing them. In some cases this can increase\n    the parsing speed by 5-10x.\n\n    .. deprecated:: 2.0.0\n        A strict version of this argument is now the default, passing it has no effect.\n\nkeep_date_col : bool, default False\n    If ``True`` and ``parse_dates`` specifies combining multiple columns then\n    keep the original columns.\ndate_parser : Callable, optional\n    Function to use for converting a sequence of string columns to an array of\n    ``datetime`` instances. The default uses ``dateutil.parser.parser`` to do the\n    conversion. pandas will try to call ``date_parser`` in three different ways,\n    advancing to the next if an exception occurs: 1) Pass one or more arrays\n    (as defined by ``parse_dates``) as arguments; 2) concatenate (row-wise) the\n    string values from the columns defined by ``parse_dates`` into a single array\n    and pass that; and 3) call ``date_parser`` once for each row using one or\n    more strings (corresponding to the columns defined by ``parse_dates``) as\n    arguments.\n\n    .. deprecated:: 2.0.0\n       Use ``date_format`` instead, or read in as ``object`` and then apply\n       :func:`~pandas.to_datetime` as-needed.\ndate_format : str or dict of column -> format, optional\n    Format to use for parsing dates when used in conjunction with ``parse_dates``.\n    The strftime to parse time, e.g. :const:`\"%d/%m/%Y\"`. See\n    `strftime documentation\n    <https://docs.python.org/3/library/datetime.html\n    #strftime-and-strptime-behavior>`_ for more information on choices, though\n    note that :const:`\"%f\"` will parse all the way up to nanoseconds.\n    You can also pass:\n\n    - \"ISO8601\", to parse any `ISO8601 <https://en.wikipedia.org/wiki/ISO_8601>`_\n        time string (not necessarily in exactly the same format);\n    - \"mixed\", to infer the format for each element individually. This is risky,\n        and you should probably use it along with `dayfirst`.\n\n    .. versionadded:: 2.0.0\ndayfirst : bool, default False\n    DD/MM format dates, international and European format.\ncache_dates : bool, default True\n    If ``True``, use a cache of unique, converted dates to apply the ``datetime``\n    conversion. May produce significant speed-up when parsing duplicate\n    date strings, especially ones with timezone offsets.\n\niterator : bool, default False\n    Return ``TextFileReader`` object for iteration or getting chunks with\n    ``get_chunk()``.\nchunksize : int, optional\n    Number of lines to read from the file per chunk. Passing a value will cause the\n    function to return a ``TextFileReader`` object for iteration.\n    See the `IO Tools docs\n    <https://pandas.pydata.org/pandas-docs/stable/io.html#io-chunking>`_\n    for more information on ``iterator`` and ``chunksize``.\n\ncompression : str or dict, default 'infer'\n    For on-the-fly decompression of on-disk data. If 'infer' and 'filepath_or_buffer' is\n    path-like, then detect compression from the following extensions: '.gz',\n    '.bz2', '.zip', '.xz', '.zst', '.tar', '.tar.gz', '.tar.xz' or '.tar.bz2'\n    (otherwise no compression).\n    If using 'zip' or 'tar', the ZIP file must contain only one data file to be read in.\n    Set to ``None`` for no decompression.\n    Can also be a dict with key ``'method'`` set\n    to one of {``'zip'``, ``'gzip'``, ``'bz2'``, ``'zstd'``, ``'xz'``, ``'tar'``} and\n    other key-value pairs are forwarded to\n    ``zipfile.ZipFile``, ``gzip.GzipFile``,\n    ``bz2.BZ2File``, ``zstandard.ZstdDecompressor``, ``lzma.LZMAFile`` or\n    ``tarfile.TarFile``, respectively.\n    As an example, the following could be passed for Zstandard decompression using a\n    custom compression dictionary:\n    ``compression={'method': 'zstd', 'dict_data': my_compression_dict}``.\n\n    .. versionadded:: 1.5.0\n        Added support for `.tar` files.\n\n    .. versionchanged:: 1.4.0 Zstandard support.\n\nthousands : str (length 1), optional\n    Character acting as the thousands separator in numerical values.\ndecimal : str (length 1), default '.'\n    Character to recognize as decimal point (e.g., use ',' for European data).\nlineterminator : str (length 1), optional\n    Character used to denote a line break. Only valid with C parser.\nquotechar : str (length 1), optional\n    Character used to denote the start and end of a quoted item. Quoted\n    items can include the ``delimiter`` and it will be ignored.\nquoting : {0 or csv.QUOTE_MINIMAL, 1 or csv.QUOTE_ALL, 2 or csv.QUOTE_NONNUMERIC, 3 or csv.QUOTE_NONE}, default csv.QUOTE_MINIMAL\n    Control field quoting behavior per ``csv.QUOTE_*`` constants. Default is\n    ``csv.QUOTE_MINIMAL`` (i.e., 0) which implies that only fields containing special\n    characters are quoted (e.g., characters defined in ``quotechar``, ``delimiter``,\n    or ``lineterminator``.\ndoublequote : bool, default True\n   When ``quotechar`` is specified and ``quoting`` is not ``QUOTE_NONE``, indicate\n   whether or not to interpret two consecutive ``quotechar`` elements INSIDE a\n   field as a single ``quotechar`` element.\nescapechar : str (length 1), optional\n    Character used to escape other characters.\ncomment : str (length 1), optional\n    Character indicating that the remainder of line should not be parsed.\n    If found at the beginning\n    of a line, the line will be ignored altogether. This parameter must be a\n    single character. Like empty lines (as long as ``skip_blank_lines=True``),\n    fully commented lines are ignored by the parameter ``header`` but not by\n    ``skiprows``. For example, if ``comment='#'``, parsing\n    ``#empty\\na,b,c\\n1,2,3`` with ``header=0`` will result in ``'a,b,c'`` being\n    treated as the header.\nencoding : str, optional, default 'utf-8'\n    Encoding to use for UTF when reading/writing (ex. ``'utf-8'``). `List of Python\n    standard encodings\n    <https://docs.python.org/3/library/codecs.html#standard-encodings>`_ .\n\nencoding_errors : str, optional, default 'strict'\n    How encoding errors are treated. `List of possible values\n    <https://docs.python.org/3/library/codecs.html#error-handlers>`_ .\n\n    .. versionadded:: 1.3.0\n\ndialect : str or csv.Dialect, optional\n    If provided, this parameter will override values (default or not) for the\n    following parameters: ``delimiter``, ``doublequote``, ``escapechar``,\n    ``skipinitialspace``, ``quotechar``, and ``quoting``. If it is necessary to\n    override values, a ``ParserWarning`` will be issued. See ``csv.Dialect``\n    documentation for more details.\non_bad_lines : {'error', 'warn', 'skip'} or Callable, default 'error'\n    Specifies what to do upon encountering a bad line (a line with too many fields).\n    Allowed values are :\n\n    - ``'error'``, raise an Exception when a bad line is encountered.\n    - ``'warn'``, raise a warning when a bad line is encountered and skip that line.\n    - ``'skip'``, skip bad lines without raising or warning when they are encountered.\n\n    .. versionadded:: 1.3.0\n\n    .. versionadded:: 1.4.0\n\n        - Callable, function with signature\n          ``(bad_line: list[str]) -> list[str] | None`` that will process a single\n          bad line. ``bad_line`` is a list of strings split by the ``sep``.\n          If the function returns ``None``, the bad line will be ignored.\n          If the function returns a new ``list`` of strings with more elements than\n          expected, a ``ParserWarning`` will be emitted while dropping extra elements.\n          Only supported when ``engine='python'``\n\n    .. versionchanged:: 2.2.0\n\n        - Callable, function with signature\n          as described in `pyarrow documentation\n          <https://arrow.apache.org/docs/python/generated/pyarrow.csv.ParseOptions.html\n          #pyarrow.csv.ParseOptions.invalid_row_handler>`_ when ``engine='pyarrow'``\n\ndelim_whitespace : bool, default False\n    Specifies whether or not whitespace (e.g. ``' '`` or ``'\\t'``) will be\n    used as the ``sep`` delimiter. Equivalent to setting ``sep='\\s+'``. If this option\n    is set to ``True``, nothing should be passed in for the ``delimiter``\n    parameter.\n\n    .. deprecated:: 2.2.0\n        Use ``sep=\"\\s+\"`` instead.\nlow_memory : bool, default True\n    Internally process the file in chunks, resulting in lower memory use\n    while parsing, but possibly mixed type inference.  To ensure no mixed\n    types either set ``False``, or specify the type with the ``dtype`` parameter.\n    Note that the entire file is read into a single :class:`~pandas.DataFrame`\n    regardless, use the ``chunksize`` or ``iterator`` parameter to return the data in\n    chunks. (Only valid with C parser).\nmemory_map : bool, default False\n    If a filepath is provided for ``filepath_or_buffer``, map the file object\n    directly onto memory and access the data directly from there. Using this\n    option can improve performance because there is no longer any I/O overhead.\nfloat_precision : {'high', 'legacy', 'round_trip'}, optional\n    Specifies which converter the C engine should use for floating-point\n    values. The options are ``None`` or ``'high'`` for the ordinary converter,\n    ``'legacy'`` for the original lower precision pandas converter, and\n    ``'round_trip'`` for the round-trip converter.\n\nstorage_options : dict, optional\n    Extra options that make sense for a particular storage connection, e.g.\n    host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n    are forwarded to ``urllib.request.Request`` as header options. For other\n    URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n    forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n    details, and for more examples on storage options refer `here\n    <https://pandas.pydata.org/docs/user_guide/io.html?\n    highlight=storage_options#reading-writing-remote-files>`_.\n\ndtype_backend : {'numpy_nullable', 'pyarrow'}, default 'numpy_nullable'\n    Back-end data type applied to the resultant :class:`DataFrame`\n    (still experimental). Behaviour is as follows:\n\n    * ``\"numpy_nullable\"``: returns nullable-dtype-backed :class:`DataFrame`\n      (default).\n    * ``\"pyarrow\"``: returns pyarrow-backed nullable :class:`ArrowDtype`\n      DataFrame.\n\n    .. versionadded:: 2.0\n\nReturns\n-------\nDataFrame or TextFileReader\n    A comma-separated values (csv) file is returned as two-dimensional\n    data structure with labeled axes.\n\nSee Also\n--------\nDataFrame.to_csv : Write DataFrame to a comma-separated values (csv) file.\nread_table : Read general delimited file into DataFrame.\nread_fwf : Read a table of fixed-width formatted lines into DataFrame.\n\nExamples\n--------\n>>> pd.read_csv('data.csv')  # doctest: +SKIP", "parameters": {"type": "object", "properties": {"filepath_or_buffer": {"type": ["readcsvbuffer[bytes]", "readcsvbuffer[str]", "filepath"]}, "sep": {"type": ["lib.nodefault", "str", "null"], "default": "<no_default>"}, "delimiter": {"type": ["lib.nodefault", "str", "null"], "default": null}, "header": {"type": ["\"int", "literal['infer']\"", "sequence[int]", "null"], "default": "infer"}, "names": {"type": ["lib.nodefault", "sequence[hashable]", "null"], "default": "<no_default>"}, "index_col": {"type": ["indexlabel", "literal[false]", "null"], "default": null}, "usecols": {"type": "usecolsargtype", "default": null}, "dtype": {"type": ["dtypearg", "null"], "default": null}, "engine": {"type": ["csvengine", "null"], "default": null}, "converters": {"type": ["mapping[hashable, callable]", "null"], "default": null}, "true_values": {"type": ["list", "null"], "default": null}, "false_values": {"type": ["list", "null"], "default": null}, "skipinitialspace": {"type": "bool", "default": false}, "skiprows": {"type": ["null", "list[int]", "callable[[hashable], bool]", "integer"], "default": null}, "skipfooter": {"type": "integer", "default": 0}, "nrows": {"type": ["integer", "null"], "default": null}, "na_values": {"type": ["hashable", "iterable[hashable]", "mapping[hashable"]}, "Iterable[Hashable]] | None'": {"type": "NoneType", "default": null}, "keep_default_na": {"type": "bool", "default": true}, "na_filter": {"type": "bool", "default": true}, "verbose": {"type": ["bool", "lib.nodefault"], "default": "<no_default>"}, "skip_blank_lines": {"type": "bool", "default": true}, "parse_dates": {"type": ["bool", "sequence[hashable]", "null"], "default": null}, "infer_datetime_format": {"type": ["bool", "lib.nodefault"], "default": "<no_default>"}, "keep_date_col": {"type": ["bool", "lib.nodefault"], "default": "<no_default>"}, "date_parser": {"type": ["callable", "lib.nodefault"], "default": "<no_default>"}, "date_format": {"type": ["dict[hashable, str]", "str", "null"], "default": null}, "dayfirst": {"type": "bool", "default": false}, "cache_dates": {"type": "bool", "default": true}, "iterator": {"type": "bool", "default": false}, "chunksize": {"type": ["integer", "null"], "default": null}, "compression": {"type": "compressionoptions", "default": "infer"}, "thousands": {"type": ["str", "null"], "default": null}, "decimal": {"type": "str", "default": "."}, "lineterminator": {"type": ["str", "null"], "default": null}, "quotechar": {"type": "str", "default": ""}, "quoting": {"type": "integer", "default": 0}, "doublequote": {"type": "bool", "default": true}, "escapechar": {"type": ["str", "null"], "default": null}, "comment": {"type": ["str", "null"], "default": null}, "encoding": {"type": ["str", "null"], "default": null}, "encoding_errors": {"type": ["str", "null"], "default": "strict"}, "dialect": {"type": ["csv.dialect", "str", "null"], "default": null}, "on_bad_lines": {"type": "str", "default": "error"}, "delim_whitespace": {"type": ["bool", "lib.nodefault"], "default": "<no_default>"}, "low_memory": {"type": "bool", "default": true}, "memory_map": {"type": "bool", "default": false}, "float_precision": {"type": ["none\"", "\"literal['high', 'legacy']"], "default": null}, "storage_options": {"type": ["storageoptions", "null"], "default": null}, "dtype_backend": {"type": ["dtypebackend", "lib.nodefault"], "default": "<no_default>"}}}}}
{"task_id": "BigCodeBench/593", "data": {"name": "pandas.read_csv.plot", "type": "callable", "signature": "()", "description": "Read a comma-separated values (csv) file into DataFrame.\n\nAlso supports optionally iterating or breaking of the file\ninto chunks.\n\nAdditional help can be found in the online docs for\n`IO Tools <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html>`_.\n\nParameters\n----------\nfilepath_or_buffer : str, path object or file-like object\n    Any valid string path is acceptable. The string could be a URL. Valid\n    URL schemes include http, ftp, s3, gs, and file. For file URLs, a host is\n    expected. A local file could be: file://localhost/path/to/table.csv.\n\n    If you want to pass in a path object, pandas accepts any ``os.PathLike``.\n\n    By file-like object, we refer to objects with a ``read()`` method, such as\n    a file handle (e.g. via builtin ``open`` function) or ``StringIO``.\nsep : str, default ','\n    Character or regex pattern to treat as the delimiter. If ``sep=None``, the\n    C engine cannot automatically detect\n    the separator, but the Python parsing engine can, meaning the latter will\n    be used and automatically detect the separator from only the first valid\n    row of the file by Python's builtin sniffer tool, ``csv.Sniffer``.\n    In addition, separators longer than 1 character and different from\n    ``'\\s+'`` will be interpreted as regular expressions and will also force\n    the use of the Python parsing engine. Note that regex delimiters are prone\n    to ignoring quoted data. Regex example: ``'\\r\\t'``.\ndelimiter : str, optional\n    Alias for ``sep``.\nheader : int, Sequence of int, 'infer' or None, default 'infer'\n    Row number(s) containing column labels and marking the start of the\n    data (zero-indexed). Default behavior is to infer the column names: if no ``names``\n    are passed the behavior is identical to ``header=0`` and column\n    names are inferred from the first line of the file, if column\n    names are passed explicitly to ``names`` then the behavior is identical to\n    ``header=None``. Explicitly pass ``header=0`` to be able to\n    replace existing names. The header can be a list of integers that\n    specify row locations for a :class:`~pandas.MultiIndex` on the columns\n    e.g. ``[0, 1, 3]``. Intervening rows that are not specified will be\n    skipped (e.g. 2 in this example is skipped). Note that this\n    parameter ignores commented lines and empty lines if\n    ``skip_blank_lines=True``, so ``header=0`` denotes the first line of\n    data rather than the first line of the file.\nnames : Sequence of Hashable, optional\n    Sequence of column labels to apply. If the file contains a header row,\n    then you should explicitly pass ``header=0`` to override the column names.\n    Duplicates in this list are not allowed.\nindex_col : Hashable, Sequence of Hashable or False, optional\n  Column(s) to use as row label(s), denoted either by column labels or column\n  indices.  If a sequence of labels or indices is given, :class:`~pandas.MultiIndex`\n  will be formed for the row labels.\n\n  Note: ``index_col=False`` can be used to force pandas to *not* use the first\n  column as the index, e.g., when you have a malformed file with delimiters at\n  the end of each line.\nusecols : Sequence of Hashable or Callable, optional\n    Subset of columns to select, denoted either by column labels or column indices.\n    If list-like, all elements must either\n    be positional (i.e. integer indices into the document columns) or strings\n    that correspond to column names provided either by the user in ``names`` or\n    inferred from the document header row(s). If ``names`` are given, the document\n    header row(s) are not taken into account. For example, a valid list-like\n    ``usecols`` parameter would be ``[0, 1, 2]`` or ``['foo', 'bar', 'baz']``.\n    Element order is ignored, so ``usecols=[0, 1]`` is the same as ``[1, 0]``.\n    To instantiate a :class:`~pandas.DataFrame` from ``data`` with element order\n    preserved use ``pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']]``\n    for columns in ``['foo', 'bar']`` order or\n    ``pd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']]``\n    for ``['bar', 'foo']`` order.\n\n    If callable, the callable function will be evaluated against the column\n    names, returning names where the callable function evaluates to ``True``. An\n    example of a valid callable argument would be ``lambda x: x.upper() in\n    ['AAA', 'BBB', 'DDD']``. Using this parameter results in much faster\n    parsing time and lower memory usage.\ndtype : dtype or dict of {Hashable : dtype}, optional\n    Data type(s) to apply to either the whole dataset or individual columns.\n    E.g., ``{'a': np.float64, 'b': np.int32, 'c': 'Int64'}``\n    Use ``str`` or ``object`` together with suitable ``na_values`` settings\n    to preserve and not interpret ``dtype``.\n    If ``converters`` are specified, they will be applied INSTEAD\n    of ``dtype`` conversion.\n\n    .. versionadded:: 1.5.0\n\n        Support for ``defaultdict`` was added. Specify a ``defaultdict`` as input where\n        the default determines the ``dtype`` of the columns which are not explicitly\n        listed.\nengine : {'c', 'python', 'pyarrow'}, optional\n    Parser engine to use. The C and pyarrow engines are faster, while the python engine\n    is currently more feature-complete. Multithreading is currently only supported by\n    the pyarrow engine.\n\n    .. versionadded:: 1.4.0\n\n        The 'pyarrow' engine was added as an *experimental* engine, and some features\n        are unsupported, or may not work correctly, with this engine.\nconverters : dict of {Hashable : Callable}, optional\n    Functions for converting values in specified columns. Keys can either\n    be column labels or column indices.\ntrue_values : list, optional\n    Values to consider as ``True`` in addition to case-insensitive variants of 'True'.\nfalse_values : list, optional\n    Values to consider as ``False`` in addition to case-insensitive variants of 'False'.\nskipinitialspace : bool, default False\n    Skip spaces after delimiter.\nskiprows : int, list of int or Callable, optional\n    Line numbers to skip (0-indexed) or number of lines to skip (``int``)\n    at the start of the file.\n\n    If callable, the callable function will be evaluated against the row\n    indices, returning ``True`` if the row should be skipped and ``False`` otherwise.\n    An example of a valid callable argument would be ``lambda x: x in [0, 2]``.\nskipfooter : int, default 0\n    Number of lines at bottom of file to skip (Unsupported with ``engine='c'``).\nnrows : int, optional\n    Number of rows of file to read. Useful for reading pieces of large files.\nna_values : Hashable, Iterable of Hashable or dict of {Hashable : Iterable}, optional\n    Additional strings to recognize as ``NA``/``NaN``. If ``dict`` passed, specific\n    per-column ``NA`` values.  By default the following values are interpreted as\n    ``NaN``: \" \", \"#N/A\", \"#N/A N/A\", \"#NA\", \"-1.#IND\", \"-1.#QNAN\", \"-NaN\", \"-nan\",\n    \"1.#IND\", \"1.#QNAN\", \"<NA>\", \"N/A\", \"NA\", \"NULL\", \"NaN\", \"None\",\n    \"n/a\", \"nan\", \"null \".\n\nkeep_default_na : bool, default True\n    Whether or not to include the default ``NaN`` values when parsing the data.\n    Depending on whether ``na_values`` is passed in, the behavior is as follows:\n\n    * If ``keep_default_na`` is ``True``, and ``na_values`` are specified, ``na_values``\n      is appended to the default ``NaN`` values used for parsing.\n    * If ``keep_default_na`` is ``True``, and ``na_values`` are not specified, only\n      the default ``NaN`` values are used for parsing.\n    * If ``keep_default_na`` is ``False``, and ``na_values`` are specified, only\n      the ``NaN`` values specified ``na_values`` are used for parsing.\n    * If ``keep_default_na`` is ``False``, and ``na_values`` are not specified, no\n      strings will be parsed as ``NaN``.\n\n    Note that if ``na_filter`` is passed in as ``False``, the ``keep_default_na`` and\n    ``na_values`` parameters will be ignored.\nna_filter : bool, default True\n    Detect missing value markers (empty strings and the value of ``na_values``). In\n    data without any ``NA`` values, passing ``na_filter=False`` can improve the\n    performance of reading a large file.\nverbose : bool, default False\n    Indicate number of ``NA`` values placed in non-numeric columns.\n\n    .. deprecated:: 2.2.0\nskip_blank_lines : bool, default True\n    If ``True``, skip over blank lines rather than interpreting as ``NaN`` values.\nparse_dates : bool, list of Hashable, list of lists or dict of {Hashable : list}, default False\n    The behavior is as follows:\n\n    * ``bool``. If ``True`` -> try parsing the index. Note: Automatically set to\n      ``True`` if ``date_format`` or ``date_parser`` arguments have been passed.\n    * ``list`` of ``int`` or names. e.g. If ``[1, 2, 3]`` -> try parsing columns 1, 2, 3\n      each as a separate date column.\n    * ``list`` of ``list``. e.g.  If ``[[1, 3]]`` -> combine columns 1 and 3 and parse\n      as a single date column. Values are joined with a space before parsing.\n    * ``dict``, e.g. ``{'foo' : [1, 3]}`` -> parse columns 1, 3 as date and call\n      result 'foo'. Values are joined with a space before parsing.\n\n    If a column or index cannot be represented as an array of ``datetime``,\n    say because of an unparsable value or a mixture of timezones, the column\n    or index will be returned unaltered as an ``object`` data type. For\n    non-standard ``datetime`` parsing, use :func:`~pandas.to_datetime` after\n    :func:`~pandas.read_csv`.\n\n    Note: A fast-path exists for iso8601-formatted dates.\ninfer_datetime_format : bool, default False\n    If ``True`` and ``parse_dates`` is enabled, pandas will attempt to infer the\n    format of the ``datetime`` strings in the columns, and if it can be inferred,\n    switch to a faster method of parsing them. In some cases this can increase\n    the parsing speed by 5-10x.\n\n    .. deprecated:: 2.0.0\n        A strict version of this argument is now the default, passing it has no effect.\n\nkeep_date_col : bool, default False\n    If ``True`` and ``parse_dates`` specifies combining multiple columns then\n    keep the original columns.\ndate_parser : Callable, optional\n    Function to use for converting a sequence of string columns to an array of\n    ``datetime`` instances. The default uses ``dateutil.parser.parser`` to do the\n    conversion. pandas will try to call ``date_parser`` in three different ways,\n    advancing to the next if an exception occurs: 1) Pass one or more arrays\n    (as defined by ``parse_dates``) as arguments; 2) concatenate (row-wise) the\n    string values from the columns defined by ``parse_dates`` into a single array\n    and pass that; and 3) call ``date_parser`` once for each row using one or\n    more strings (corresponding to the columns defined by ``parse_dates``) as\n    arguments.\n\n    .. deprecated:: 2.0.0\n       Use ``date_format`` instead, or read in as ``object`` and then apply\n       :func:`~pandas.to_datetime` as-needed.\ndate_format : str or dict of column -> format, optional\n    Format to use for parsing dates when used in conjunction with ``parse_dates``.\n    The strftime to parse time, e.g. :const:`\"%d/%m/%Y\"`. See\n    `strftime documentation\n    <https://docs.python.org/3/library/datetime.html\n    #strftime-and-strptime-behavior>`_ for more information on choices, though\n    note that :const:`\"%f\"` will parse all the way up to nanoseconds.\n    You can also pass:\n\n    - \"ISO8601\", to parse any `ISO8601 <https://en.wikipedia.org/wiki/ISO_8601>`_\n        time string (not necessarily in exactly the same format);\n    - \"mixed\", to infer the format for each element individually. This is risky,\n        and you should probably use it along with `dayfirst`.\n\n    .. versionadded:: 2.0.0\ndayfirst : bool, default False\n    DD/MM format dates, international and European format.\ncache_dates : bool, default True\n    If ``True``, use a cache of unique, converted dates to apply the ``datetime``\n    conversion. May produce significant speed-up when parsing duplicate\n    date strings, especially ones with timezone offsets.\n\niterator : bool, default False\n    Return ``TextFileReader`` object for iteration or getting chunks with\n    ``get_chunk()``.\nchunksize : int, optional\n    Number of lines to read from the file per chunk. Passing a value will cause the\n    function to return a ``TextFileReader`` object for iteration.\n    See the `IO Tools docs\n    <https://pandas.pydata.org/pandas-docs/stable/io.html#io-chunking>`_\n    for more information on ``iterator`` and ``chunksize``.\n\ncompression : str or dict, default 'infer'\n    For on-the-fly decompression of on-disk data. If 'infer' and 'filepath_or_buffer' is\n    path-like, then detect compression from the following extensions: '.gz',\n    '.bz2', '.zip', '.xz', '.zst', '.tar', '.tar.gz', '.tar.xz' or '.tar.bz2'\n    (otherwise no compression).\n    If using 'zip' or 'tar', the ZIP file must contain only one data file to be read in.\n    Set to ``None`` for no decompression.\n    Can also be a dict with key ``'method'`` set\n    to one of {``'zip'``, ``'gzip'``, ``'bz2'``, ``'zstd'``, ``'xz'``, ``'tar'``} and\n    other key-value pairs are forwarded to\n    ``zipfile.ZipFile``, ``gzip.GzipFile``,\n    ``bz2.BZ2File``, ``zstandard.ZstdDecompressor``, ``lzma.LZMAFile`` or\n    ``tarfile.TarFile``, respectively.\n    As an example, the following could be passed for Zstandard decompression using a\n    custom compression dictionary:\n    ``compression={'method': 'zstd', 'dict_data': my_compression_dict}``.\n\n    .. versionadded:: 1.5.0\n        Added support for `.tar` files.\n\n    .. versionchanged:: 1.4.0 Zstandard support.\n\nthousands : str (length 1), optional\n    Character acting as the thousands separator in numerical values.\ndecimal : str (length 1), default '.'\n    Character to recognize as decimal point (e.g., use ',' for European data).\nlineterminator : str (length 1), optional\n    Character used to denote a line break. Only valid with C parser.\nquotechar : str (length 1), optional\n    Character used to denote the start and end of a quoted item. Quoted\n    items can include the ``delimiter`` and it will be ignored.\nquoting : {0 or csv.QUOTE_MINIMAL, 1 or csv.QUOTE_ALL, 2 or csv.QUOTE_NONNUMERIC, 3 or csv.QUOTE_NONE}, default csv.QUOTE_MINIMAL\n    Control field quoting behavior per ``csv.QUOTE_*`` constants. Default is\n    ``csv.QUOTE_MINIMAL`` (i.e., 0) which implies that only fields containing special\n    characters are quoted (e.g., characters defined in ``quotechar``, ``delimiter``,\n    or ``lineterminator``.\ndoublequote : bool, default True\n   When ``quotechar`` is specified and ``quoting`` is not ``QUOTE_NONE``, indicate\n   whether or not to interpret two consecutive ``quotechar`` elements INSIDE a\n   field as a single ``quotechar`` element.\nescapechar : str (length 1), optional\n    Character used to escape other characters.\ncomment : str (length 1), optional\n    Character indicating that the remainder of line should not be parsed.\n    If found at the beginning\n    of a line, the line will be ignored altogether. This parameter must be a\n    single character. Like empty lines (as long as ``skip_blank_lines=True``),\n    fully commented lines are ignored by the parameter ``header`` but not by\n    ``skiprows``. For example, if ``comment='#'``, parsing\n    ``#empty\\na,b,c\\n1,2,3`` with ``header=0`` will result in ``'a,b,c'`` being\n    treated as the header.\nencoding : str, optional, default 'utf-8'\n    Encoding to use for UTF when reading/writing (ex. ``'utf-8'``). `List of Python\n    standard encodings\n    <https://docs.python.org/3/library/codecs.html#standard-encodings>`_ .\n\nencoding_errors : str, optional, default 'strict'\n    How encoding errors are treated. `List of possible values\n    <https://docs.python.org/3/library/codecs.html#error-handlers>`_ .\n\n    .. versionadded:: 1.3.0\n\ndialect : str or csv.Dialect, optional\n    If provided, this parameter will override values (default or not) for the\n    following parameters: ``delimiter``, ``doublequote``, ``escapechar``,\n    ``skipinitialspace``, ``quotechar``, and ``quoting``. If it is necessary to\n    override values, a ``ParserWarning`` will be issued. See ``csv.Dialect``\n    documentation for more details.\non_bad_lines : {'error', 'warn', 'skip'} or Callable, default 'error'\n    Specifies what to do upon encountering a bad line (a line with too many fields).\n    Allowed values are :\n\n    - ``'error'``, raise an Exception when a bad line is encountered.\n    - ``'warn'``, raise a warning when a bad line is encountered and skip that line.\n    - ``'skip'``, skip bad lines without raising or warning when they are encountered.\n\n    .. versionadded:: 1.3.0\n\n    .. versionadded:: 1.4.0\n\n        - Callable, function with signature\n          ``(bad_line: list[str]) -> list[str] | None`` that will process a single\n          bad line. ``bad_line`` is a list of strings split by the ``sep``.\n          If the function returns ``None``, the bad line will be ignored.\n          If the function returns a new ``list`` of strings with more elements than\n          expected, a ``ParserWarning`` will be emitted while dropping extra elements.\n          Only supported when ``engine='python'``\n\n    .. versionchanged:: 2.2.0\n\n        - Callable, function with signature\n          as described in `pyarrow documentation\n          <https://arrow.apache.org/docs/python/generated/pyarrow.csv.ParseOptions.html\n          #pyarrow.csv.ParseOptions.invalid_row_handler>`_ when ``engine='pyarrow'``\n\ndelim_whitespace : bool, default False\n    Specifies whether or not whitespace (e.g. ``' '`` or ``'\\t'``) will be\n    used as the ``sep`` delimiter. Equivalent to setting ``sep='\\s+'``. If this option\n    is set to ``True``, nothing should be passed in for the ``delimiter``\n    parameter.\n\n    .. deprecated:: 2.2.0\n        Use ``sep=\"\\s+\"`` instead.\nlow_memory : bool, default True\n    Internally process the file in chunks, resulting in lower memory use\n    while parsing, but possibly mixed type inference.  To ensure no mixed\n    types either set ``False``, or specify the type with the ``dtype`` parameter.\n    Note that the entire file is read into a single :class:`~pandas.DataFrame`\n    regardless, use the ``chunksize`` or ``iterator`` parameter to return the data in\n    chunks. (Only valid with C parser).\nmemory_map : bool, default False\n    If a filepath is provided for ``filepath_or_buffer``, map the file object\n    directly onto memory and access the data directly from there. Using this\n    option can improve performance because there is no longer any I/O overhead.\nfloat_precision : {'high', 'legacy', 'round_trip'}, optional\n    Specifies which converter the C engine should use for floating-point\n    values. The options are ``None`` or ``'high'`` for the ordinary converter,\n    ``'legacy'`` for the original lower precision pandas converter, and\n    ``'round_trip'`` for the round-trip converter.\n\nstorage_options : dict, optional\n    Extra options that make sense for a particular storage connection, e.g.\n    host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n    are forwarded to ``urllib.request.Request`` as header options. For other\n    URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n    forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n    details, and for more examples on storage options refer `here\n    <https://pandas.pydata.org/docs/user_guide/io.html?\n    highlight=storage_options#reading-writing-remote-files>`_.\n\ndtype_backend : {'numpy_nullable', 'pyarrow'}, default 'numpy_nullable'\n    Back-end data type applied to the resultant :class:`DataFrame`\n    (still experimental). Behaviour is as follows:\n\n    * ``\"numpy_nullable\"``: returns nullable-dtype-backed :class:`DataFrame`\n      (default).\n    * ``\"pyarrow\"``: returns pyarrow-backed nullable :class:`ArrowDtype`\n      DataFrame.\n\n    .. versionadded:: 2.0\n\nReturns\n-------\nDataFrame or TextFileReader\n    A comma-separated values (csv) file is returned as two-dimensional\n    data structure with labeled axes.\n\nSee Also\n--------\nDataFrame.to_csv : Write DataFrame to a comma-separated values (csv) file.\nread_table : Read general delimited file into DataFrame.\nread_fwf : Read a table of fixed-width formatted lines into DataFrame.\n\nExamples\n--------\n>>> pd.read_csv('data.csv')  # doctest: +SKIP", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/593", "data": {"name": "random.randint", "type": "callable", "signature": "(a, b)", "description": "Return random integer in range [a, b], including both end points.\n        ", "parameters": {"type": "object", "properties": {"a": {}, "b": {}}}}}
{"task_id": "BigCodeBench/594", "data": {"name": "datetime.datetime", "type": "class", "signature": "(self, /, *args, **kwargs)", "description": "datetime(year, month, day[, hour[, minute[, second[, microsecond[,tzinfo]]]]])\n\nThe year, month and day arguments are required. tzinfo may be None, or an\ninstance of a tzinfo subclass. The remaining arguments may be ints.", "parameters": {"type": "object", "properties": {"/": {}}}}}
{"task_id": "BigCodeBench/594", "data": {"name": "datetime.datetime.now", "type": "callable", "signature": "()", "description": "Returns new datetime object representing current time local to tz.\n\n  tz\n    Timezone object.\n\nIf no tz is specified, uses local timezone.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/594", "data": {"name": "csv.writer", "type": "callable", "signature": "(*args, **kwargs)", "description": "csv_writer = csv.writer(fileobj [, dialect='excel']\n                            [optional keyword args])\n    for row in sequence:\n        csv_writer.writerow(row)\n\n    [or]\n\n    csv_writer = csv.writer(fileobj [, dialect='excel']\n                            [optional keyword args])\n    csv_writer.writerows(rows)\n\nThe \"fileobj\" argument can be any object that supports the file API.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/594", "data": {"name": "csv.writer.writerows", "type": "callable", "signature": "(*args, **kwargs)", "description": "csv_writer = csv.writer(fileobj [, dialect='excel']\n                            [optional keyword args])\n    for row in sequence:\n        csv_writer.writerow(row)\n\n    [or]\n\n    csv_writer = csv.writer(fileobj [, dialect='excel']\n                            [optional keyword args])\n    csv_writer.writerows(rows)\n\nThe \"fileobj\" argument can be any object that supports the file API.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/594", "data": {"name": "data.append(row)"}}
{"task_id": "BigCodeBench/594", "data": {"name": "os.makedirs", "type": "callable", "signature": "(name)", "description": "makedirs(name [, mode=0o777][, exist_ok=False])\n\nSuper-mkdir; create a leaf directory and all intermediate ones.  Works like\nmkdir, except that any intermediate path segment (not just the rightmost)\nwill be created if it does not exist. If the target directory already\nexists, raise an OSError if exist_ok is False. Otherwise no exception is\nraised.  This is recursive.", "parameters": {"type": "object", "properties": {"name": {}}}}}
{"task_id": "BigCodeBench/594", "data": {"name": "os.path.exists", "type": "callable", "signature": "(path)", "description": "Test whether a path exists.  Returns False for broken symbolic links", "parameters": {"type": "object", "properties": {"path": {}}}}}
{"task_id": "BigCodeBench/594", "data": {"name": "os.path.join", "type": "callable", "signature": "(a, *p)", "description": "Join two or more pathname components, inserting '/' as needed.\nIf any component is an absolute path, all previous path components\nwill be discarded.  An empty last part will result in a path that\nends with a separator.", "parameters": {"type": "object", "properties": {"a": {}}}}}
{"task_id": "BigCodeBench/594", "data": {"name": "random.randint", "type": "callable", "signature": "(a, b)", "description": "Return random integer in range [a, b], including both end points.\n        ", "parameters": {"type": "object", "properties": {"a": {}, "b": {}}}}}
{"task_id": "BigCodeBench/594", "data": {"name": "shutil.copy", "type": "callable", "signature": "(src, dst)", "description": "Copy data and mode bits (\"cp src dst\"). Return the file's destination.\n\nThe destination may be a directory.\n\nIf follow_symlinks is false, symlinks won't be followed. This\nresembles GNU's \"cp -P src dst\".\n\nIf source and destination are the same file, a SameFileError will be\nraised.", "parameters": {"type": "object", "properties": {"src": {}, "dst": {}}}}}
{"task_id": "BigCodeBench/618", "data": {"name": "match_results.append([team, team_goals, penalty_cost])"}}
{"task_id": "BigCodeBench/618", "data": {"name": "matplotlib.pyplot.close", "type": "callable", "signature": "()", "description": "Close a figure window.\n\nParameters\n----------\nfig : None or int or str or `.Figure`\n    The figure to close. There are a number of ways to specify this:\n\n    - *None*: the current figure\n    - `.Figure`: the given `.Figure` instance\n    - ``int``: a figure number\n    - ``str``: a figure name\n    - 'all': all figures", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/618", "data": {"name": "random.randint", "type": "callable", "signature": "(a, b)", "description": "Return random integer in range [a, b], including both end points.\n        ", "parameters": {"type": "object", "properties": {"a": {}, "b": {}}}}}
{"task_id": "BigCodeBench/618", "data": {"name": "seaborn.barplot", "type": "callable", "signature": "(data=None, x=None, y=None, palette=None, **kwargs)", "description": "Show point estimates and errors as rectangular bars.\n\nA bar plot represents an aggregate or statistical estimate for a numeric\nvariable with the height of each rectangle and indicates the uncertainty\naround that estimate using an error bar. Bar plots include 0 in the\naxis range, and they are a good choice when 0 is a meaningful value\nfor the variable to take.\n\nSee the :ref:`tutorial <categorical_tutorial>` for more information.\n\n.. note::\n    By default, this function treats one of the variables as categorical\n    and draws data at ordinal positions (0, 1, ... n) on the relevant axis.\n    As of version 0.13.0, this can be disabled by setting `native_scale=True`.\n\n\nParameters\n----------\ndata : DataFrame, Series, dict, array, or list of arrays\n    Dataset for plotting. If `x` and `y` are absent, this is\n    interpreted as wide-form. Otherwise it is expected to be long-form.    \nx, y, hue : names of variables in `data` or vector data\n    Inputs for plotting long-form data. See examples for interpretation.    \norder, hue_order : lists of strings\n    Order to plot the categorical levels in; otherwise the levels are\n    inferred from the data objects.    \nestimator : string or callable that maps vector -> scalar\n    Statistical function to estimate within each categorical bin.\nerrorbar : string, (string, number) tuple, callable or None\n    Name of errorbar method (either \"ci\", \"pi\", \"se\", or \"sd\"), or a tuple\n    with a method name and a level parameter, or a function that maps from a\n    vector to a (min, max) interval, or None to hide errorbar. See the\n    :doc:`errorbar tutorial </tutorial/error_bars>` for more information.\n\n    .. versionadded:: v0.12.0\nn_boot : int\n    Number of bootstrap samples used to compute confidence intervals.\nseed : int, `numpy.random.Generator`, or `numpy.random.RandomState`\n    Seed or random number generator for reproducible bootstrapping.\nunits : name of variable in `data` or vector data\n    Identifier of sampling units; used by the errorbar function to\n    perform a multilevel bootstrap and account for repeated measures\nweights : name of variable in `data` or vector data\n    Data values or column used to compute weighted statistics.\n    Note that the use of weights may limit other statistical options.\n\n    .. versionadded:: v0.13.1    \norient : \"v\" | \"h\" | \"x\" | \"y\"\n    Orientation of the plot (vertical or horizontal). This is usually\n    inferred based on the type of the input variables, but it can be used\n    to resolve ambiguity when both `x` and `y` are numeric or when\n    plotting wide-form data.\n\n    .. versionchanged:: v0.13.0\n        Added 'x'/'y' as options, equivalent to 'v'/'h'.    \ncolor : matplotlib color\n    Single color for the elements in the plot.    \npalette : palette name, list, or dict\n    Colors to use for the different levels of the ``hue`` variable. Should\n    be something that can be interpreted by :func:`color_palette`, or a\n    dictionary mapping hue levels to matplotlib colors.    \nsaturation : float\n    Proportion of the original saturation to draw fill colors in. Large\n    patches often look better with desaturated colors, but set this to\n    `1` if you want the colors to perfectly match the input values.    \nfill : bool\n    If True, use a solid patch. Otherwise, draw as line art.\n\n    .. versionadded:: v0.13.0    \nhue_norm : tuple or :class:`matplotlib.colors.Normalize` object\n    Normalization in data units for colormap applied to the `hue`\n    variable when it is numeric. Not relevant if `hue` is categorical.\n\n    .. versionadded:: v0.12.0    \nwidth : float\n    Width allotted to each element on the orient axis. When `native_scale=True`,\n    it is relative to the minimum distance between two values in the native scale.    \ndodge : \"auto\" or bool\n    When hue mapping is used, whether elements should be narrowed and shifted along\n    the orient axis to eliminate overlap. If `\"auto\"`, set to `True` when the\n    orient variable is crossed with the categorical variable or `False` otherwise.\n\n    .. versionchanged:: 0.13.0\n\n        Added `\"auto\"` mode as a new default.    \ngap : float\n    Shrink on the orient axis by this factor to add a gap between dodged elements.\n\n    .. versionadded:: 0.13.0    \nlog_scale : bool or number, or pair of bools or numbers\n    Set axis scale(s) to log. A single value sets the data axis for any numeric\n    axes in the plot. A pair of values sets each axis independently.\n    Numeric values are interpreted as the desired base (default 10).\n    When `None` or `False`, seaborn defers to the existing Axes scale.\n\n    .. versionadded:: v0.13.0    \nnative_scale : bool\n    When True, numeric or datetime values on the categorical axis will maintain\n    their original scaling rather than being converted to fixed indices.\n\n    .. versionadded:: v0.13.0    \nformatter : callable\n    Function for converting categorical data into strings. Affects both grouping\n    and tick labels.\n\n    .. versionadded:: v0.13.0    \nlegend : \"auto\", \"brief\", \"full\", or False\n    How to draw the legend. If \"brief\", numeric `hue` and `size`\n    variables will be represented with a sample of evenly spaced values.\n    If \"full\", every group will get an entry in the legend. If \"auto\",\n    choose between brief or full representation based on number of levels.\n    If `False`, no legend data is added and no legend is drawn.\n\n    .. versionadded:: v0.13.0    \ncapsize : float\n    Width of the \"caps\" on error bars, relative to bar spacing.    \nerr_kws : dict\n    Parameters of :class:`matplotlib.lines.Line2D`, for the error bar artists.\n\n    .. versionadded:: v0.13.0    \nci : float\n    Level of the confidence interval to show, in [0, 100].\n\n    .. deprecated:: v0.12.0\n        Use `errorbar=(\"ci\", ...)`.    \nerrcolor : matplotlib color\n    Color used for the error bar lines.\n\n    .. deprecated:: 0.13.0\n        Use `err_kws={'color': ...}`.    \nerrwidth : float\n    Thickness of error bar lines (and caps), in points.\n\n    .. deprecated:: 0.13.0\n        Use `err_kws={'linewidth': ...}`.    \nax : matplotlib Axes\n    Axes object to draw the plot onto, otherwise uses the current Axes.    \nkwargs : key, value mappings\n    Other parameters are passed through to :class:`matplotlib.patches.Rectangle`.\n\nReturns\n-------\nax : matplotlib Axes\n    Returns the Axes object with the plot drawn onto it.    \n\nSee Also\n--------\ncountplot : Show the counts of observations in each categorical bin.    \npointplot : Show point estimates and confidence intervals using dots.    \ncatplot : Combine a categorical plot with a :class:`FacetGrid`.    \n\nNotes\n-----\n\nFor datasets where 0 is not a meaningful value, a :func:`pointplot` will\nallow you to focus on differences between levels of one or more categorical\nvariables.\n\nIt is also important to keep in mind that a bar plot shows only the mean (or\nother aggregate) value, but it is often more informative to show the\ndistribution of values at each level of the categorical variables. In those\ncases, approaches such as a :func:`boxplot` or :func:`violinplot` may be\nmore appropriate.\n\nExamples\n--------\n.. include:: ../docstrings/barplot.rst", "parameters": {"type": "object", "properties": {"data": {"type": "NoneType", "default": null}, "x": {"type": "NoneType", "default": null}, "y": {"type": "NoneType", "default": null}, "palette": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/636", "data": {"name": "pandas.DataFrame", "type": "class", "signature": "(data=None, index: 'Axes | None' = None, columns: 'Axes | None' = None, dtype: 'Dtype | None' = None, copy: 'bool | None' = None) -> 'None'", "description": "Two-dimensional, size-mutable, potentially heterogeneous tabular data.\n\nData structure also contains labeled axes (rows and columns).\nArithmetic operations align on both row and column labels. Can be\nthought of as a dict-like container for Series objects. The primary\npandas data structure.\n\nParameters\n----------\ndata : ndarray (structured or homogeneous), Iterable, dict, or DataFrame\n    Dict can contain Series, arrays, constants, dataclass or list-like objects. If\n    data is a dict, column order follows insertion-order. If a dict contains Series\n    which have an index defined, it is aligned by its index. This alignment also\n    occurs if data is a Series or a DataFrame itself. Alignment is done on\n    Series/DataFrame inputs.\n\n    If data is a list of dicts, column order follows insertion-order.\n\nindex : Index or array-like\n    Index to use for resulting frame. Will default to RangeIndex if\n    no indexing information part of input data and no index provided.\ncolumns : Index or array-like\n    Column labels to use for resulting frame when data does not have them,\n    defaulting to RangeIndex(0, 1, 2, ..., n). If data contains column labels,\n    will perform column selection instead.\ndtype : dtype, default None\n    Data type to force. Only a single dtype is allowed. If None, infer.\ncopy : bool or None, default None\n    Copy data from inputs.\n    For dict data, the default of None behaves like ``copy=True``.  For DataFrame\n    or 2d ndarray input, the default of None behaves like ``copy=False``.\n    If data is a dict containing one or more Series (possibly of different dtypes),\n    ``copy=False`` will ensure that these inputs are not copied.\n\n    .. versionchanged:: 1.3.0\n\nSee Also\n--------\nDataFrame.from_records : Constructor from tuples, also record arrays.\nDataFrame.from_dict : From dicts of Series, arrays, or dicts.\nread_csv : Read a comma-separated values (csv) file into DataFrame.\nread_table : Read general delimited file into DataFrame.\nread_clipboard : Read text from clipboard into DataFrame.\n\nNotes\n-----\nPlease reference the :ref:`User Guide <basics.dataframe>` for more information.\n\nExamples\n--------\nConstructing DataFrame from a dictionary.\n\n>>> d = {'col1': [1, 2], 'col2': [3, 4]}\n>>> df = pd.DataFrame(data=d)\n>>> df\n   col1  col2\n0     1     3\n1     2     4\n\nNotice that the inferred dtype is int64.\n\n>>> df.dtypes\ncol1    int64\ncol2    int64\ndtype: object\n\nTo enforce a single dtype:\n\n>>> df = pd.DataFrame(data=d, dtype=np.int8)\n>>> df.dtypes\ncol1    int8\ncol2    int8\ndtype: object\n\nConstructing DataFrame from a dictionary including Series:\n\n>>> d = {'col1': [0, 1, 2, 3], 'col2': pd.Series([2, 3], index=[2, 3])}\n>>> pd.DataFrame(data=d, index=[0, 1, 2, 3])\n   col1  col2\n0     0   NaN\n1     1   NaN\n2     2   2.0\n3     3   3.0\n\nConstructing DataFrame from numpy ndarray:\n\n>>> df2 = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),\n...                    columns=['a', 'b', 'c'])\n>>> df2\n   a  b  c\n0  1  2  3\n1  4  5  6\n2  7  8  9\n\nConstructing DataFrame from a numpy ndarray that has labeled columns:\n\n>>> data = np.array([(1, 2, 3), (4, 5, 6), (7, 8, 9)],\n...                 dtype=[(\"a\", \"i4\"), (\"b\", \"i4\"), (\"c\", \"i4\")])\n>>> df3 = pd.DataFrame(data, columns=['c', 'a'])\n...\n>>> df3\n   c  a\n0  3  1\n1  6  4\n2  9  7\n\nConstructing DataFrame from dataclass:\n\n>>> from dataclasses import make_dataclass\n>>> Point = make_dataclass(\"Point\", [(\"x\", int), (\"y\", int)])\n>>> pd.DataFrame([Point(0, 0), Point(0, 3), Point(2, 3)])\n   x  y\n0  0  0\n1  0  3\n2  2  3\n\nConstructing DataFrame from Series/DataFrame:\n\n>>> ser = pd.Series([1, 2, 3], index=[\"a\", \"b\", \"c\"])\n>>> df = pd.DataFrame(data=ser, index=[\"a\", \"c\"])\n>>> df\n   0\na  1\nc  3\n\n>>> df1 = pd.DataFrame([1, 2, 3], index=[\"a\", \"b\", \"c\"], columns=[\"x\"])\n>>> df2 = pd.DataFrame(data=df1, index=[\"a\", \"c\"])\n>>> df2\n   x\na  1\nc  3", "parameters": {"type": "object", "properties": {"data": {"type": "NoneType", "default": null}, "index": {"type": ["axes", "null"], "default": null}, "columns": {"type": ["axes", "null"], "default": null}, "dtype": {"type": ["dtype", "null"], "default": null}, "copy": {"type": ["bool", "null"], "default": null}}}}}
{"task_id": "BigCodeBench/636", "data": {"name": "pandas.DataFrame.astype", "type": "callable", "signature": "(self, dtype)", "description": "Cast a pandas object to a specified dtype ``dtype``.\n\nParameters\n----------\ndtype : str, data type, Series or Mapping of column name -> data type\n    Use a str, numpy.dtype, pandas.ExtensionDtype or Python type to\n    cast entire pandas object to the same type. Alternatively, use a\n    mapping, e.g. {col: dtype, ...}, where col is a column label and dtype is\n    a numpy.dtype or Python type to cast one or more of the DataFrame's\n    columns to column-specific types.\ncopy : bool, default True\n    Return a copy when ``copy=True`` (be very careful setting\n    ``copy=False`` as changes to values then may propagate to other\n    pandas objects).\n\n    .. note::\n        The `copy` keyword will change behavior in pandas 3.0.\n        `Copy-on-Write\n        <https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html>`__\n        will be enabled by default, which means that all methods with a\n        `copy` keyword will use a lazy copy mechanism to defer the copy and\n        ignore the `copy` keyword. The `copy` keyword will be removed in a\n        future version of pandas.\n\n        You can already get the future behavior and improvements through\n        enabling copy on write ``pd.options.mode.copy_on_write = True``\nerrors : {'raise', 'ignore'}, default 'raise'\n    Control raising of exceptions on invalid data for provided dtype.\n\n    - ``raise`` : allow exceptions to be raised\n    - ``ignore`` : suppress exceptions. On error return original object.\n\nReturns\n-------\nsame type as caller\n\nSee Also\n--------\nto_datetime : Convert argument to datetime.\nto_timedelta : Convert argument to timedelta.\nto_numeric : Convert argument to a numeric type.\nnumpy.ndarray.astype : Cast a numpy array to a specified type.\n\nNotes\n-----\n.. versionchanged:: 2.0.0\n\n    Using ``astype`` to convert from timezone-naive dtype to\n    timezone-aware dtype will raise an exception.\n    Use :meth:`Series.dt.tz_localize` instead.\n\nExamples\n--------\nCreate a DataFrame:\n\n>>> d = {'col1': [1, 2], 'col2': [3, 4]}\n>>> df = pd.DataFrame(data=d)\n>>> df.dtypes\ncol1    int64\ncol2    int64\ndtype: object\n\nCast all columns to int32:\n\n>>> df.astype('int32').dtypes\ncol1    int32\ncol2    int32\ndtype: object\n\nCast col1 to int32 using a dictionary:\n\n>>> df.astype({'col1': 'int32'}).dtypes\ncol1    int32\ncol2    int64\ndtype: object\n\nCreate a series:\n\n>>> ser = pd.Series([1, 2], dtype='int32')\n>>> ser\n0    1\n1    2\ndtype: int32\n>>> ser.astype('int64')\n0    1\n1    2\ndtype: int64\n\nConvert to categorical type:\n\n>>> ser.astype('category')\n0    1\n1    2\ndtype: category\nCategories (2, int32): [1, 2]\n\nConvert to ordered categorical type with custom ordering:\n\n>>> from pandas.api.types import CategoricalDtype\n>>> cat_dtype = CategoricalDtype(\n...     categories=[2, 1], ordered=True)\n>>> ser.astype(cat_dtype)\n0    1\n1    2\ndtype: category\nCategories (2, int64): [2 < 1]\n\nCreate a series of dates:\n\n>>> ser_date = pd.Series(pd.date_range('20200101', periods=3))\n>>> ser_date\n0   2020-01-01\n1   2020-01-02\n2   2020-01-03\ndtype: datetime64[ns]", "parameters": {"type": "object", "properties": {"dtype": {}}}}}
{"task_id": "BigCodeBench/636", "data": {"name": "counts.plot(kind='bar').set_title('Non-Zero Value Counts')"}}
{"task_id": "BigCodeBench/636", "data": {"name": "df.astype(bool).sum(axis=0).plot(kind='bar')"}}
{"task_id": "BigCodeBench/636", "data": {"name": "matplotlib.pyplot.gca", "type": "callable", "signature": "()", "description": "Get the current Axes.\n\nIf there is currently no Axes on this Figure, a new one is created\nusing `.Figure.add_subplot`.  (To test whether there is currently an\nAxes on a Figure, check whether ``figure.axes`` is empty.  To test\nwhether there is currently a Figure on the pyplot figure stack, check\nwhether `.pyplot.get_fignums()` is empty.)", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/636", "data": {"name": "matplotlib.pyplot.gca.set_title", "type": "callable", "signature": "() -> 'Axes)", "description": "Get the current Axes.\n\nIf there is currently no Axes on this Figure, a new one is created\nusing `.Figure.add_subplot`.  (To test whether there is currently an\nAxes on a Figure, check whether ``figure.axes`` is empty.  To test\nwhether there is currently a Figure on the pyplot figure stack, check\nwhether `.pyplot.get_fignums()` is empty.)", "parameters": {"type": "object", "properties": {"-> 'Axes": {}}}}}
{"task_id": "BigCodeBench/636", "data": {"name": "matplotlib.pyplot.close", "type": "callable", "signature": "(fig: \"None | int | str | Figure | Literal['all']\" = None) -> 'None)", "description": "Close a figure window.\n\nParameters\n----------\nfig : None or int or str or `.Figure`\n    The figure to close. There are a number of ways to specify this:\n\n    - *None*: the current figure\n    - `.Figure`: the given `.Figure` instance\n    - ``int``: a figure number\n    - ``str``: a figure name\n    - 'all': all figures", "parameters": {"type": "object", "properties": {"fig": {"type": ["\"none", "figure", "str", "literal['all']\"", "integer"], "default": null}}}}}
{"task_id": "BigCodeBench/636", "data": {"name": "numpy.random.randint", "type": "callable", "signature": "(*args, **kwargs)", "description": "randint(low, high=None, size=None, dtype=int)\n\nReturn random integers from `low` (inclusive) to `high` (exclusive).\n\nReturn random integers from the \"discrete uniform\" distribution of\nthe specified dtype in the \"half-open\" interval [`low`, `high`). If\n`high` is None (the default), then results are from [0, `low`).\n\n.. note::\n    New code should use the ``integers`` method of a ``default_rng()``\n    instance instead; please see the :ref:`random-quick-start`.\n\nParameters\n----------\nlow : int or array-like of ints\n    Lowest (signed) integers to be drawn from the distribution (unless\n    ``high=None``, in which case this parameter is one above the\n    *highest* such integer).\nhigh : int or array-like of ints, optional\n    If provided, one above the largest (signed) integer to be drawn\n    from the distribution (see above for behavior if ``high=None``).\n    If array-like, must contain integer values\nsize : int or tuple of ints, optional\n    Output shape.  If the given shape is, e.g., ``(m, n, k)``, then\n    ``m * n * k`` samples are drawn.  Default is None, in which case a\n    single value is returned.\ndtype : dtype, optional\n    Desired dtype of the result. Byteorder must be native.\n    The default value is int.\n\n    .. versionadded:: 1.11.0\n\nReturns\n-------\nout : int or ndarray of ints\n    `size`-shaped array of random integers from the appropriate\n    distribution, or a single such random int if `size` not provided.\n\nSee Also\n--------\nrandom_integers : similar to `randint`, only for the closed\n    interval [`low`, `high`], and 1 is the lowest value if `high` is\n    omitted.\nrandom.Generator.integers: which should be used for new code.\n\nExamples\n--------\n>>> np.random.randint(2, size=10)\narray([1, 0, 0, 0, 1, 1, 0, 0, 1, 0]) # random\n>>> np.random.randint(1, size=10)\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n\nGenerate a 2 x 4 array of ints between 0 and 4, inclusive:\n\n>>> np.random.randint(5, size=(2, 4))\narray([[4, 0, 2, 1], # random\n       [3, 2, 2, 0]])\n\nGenerate a 1 x 3 array with 3 different upper bounds\n\n>>> np.random.randint(1, [3, 5, 10])\narray([2, 2, 9]) # random\n\nGenerate a 1 by 3 array with 3 different lower bounds\n\n>>> np.random.randint([1, 5, 7], 10)\narray([9, 8, 7]) # random\n\nGenerate a 2 by 4 array using broadcasting with dtype of uint8\n\n>>> np.random.randint([1, 3, 5, 7], [[10], [20]], dtype=np.uint8)\narray([[ 8,  6,  9,  7], # random\n       [ 1, 16,  9, 12]], dtype=uint8)", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/637", "data": {"name": "pandas.DataFrame", "type": "class", "signature": "(data=None, index: 'Axes | None' = None, columns: 'Axes | None' = None, dtype: 'Dtype | None' = None, copy: 'bool | None' = None) -> 'None'", "description": "Two-dimensional, size-mutable, potentially heterogeneous tabular data.\n\nData structure also contains labeled axes (rows and columns).\nArithmetic operations align on both row and column labels. Can be\nthought of as a dict-like container for Series objects. The primary\npandas data structure.\n\nParameters\n----------\ndata : ndarray (structured or homogeneous), Iterable, dict, or DataFrame\n    Dict can contain Series, arrays, constants, dataclass or list-like objects. If\n    data is a dict, column order follows insertion-order. If a dict contains Series\n    which have an index defined, it is aligned by its index. This alignment also\n    occurs if data is a Series or a DataFrame itself. Alignment is done on\n    Series/DataFrame inputs.\n\n    If data is a list of dicts, column order follows insertion-order.\n\nindex : Index or array-like\n    Index to use for resulting frame. Will default to RangeIndex if\n    no indexing information part of input data and no index provided.\ncolumns : Index or array-like\n    Column labels to use for resulting frame when data does not have them,\n    defaulting to RangeIndex(0, 1, 2, ..., n). If data contains column labels,\n    will perform column selection instead.\ndtype : dtype, default None\n    Data type to force. Only a single dtype is allowed. If None, infer.\ncopy : bool or None, default None\n    Copy data from inputs.\n    For dict data, the default of None behaves like ``copy=True``.  For DataFrame\n    or 2d ndarray input, the default of None behaves like ``copy=False``.\n    If data is a dict containing one or more Series (possibly of different dtypes),\n    ``copy=False`` will ensure that these inputs are not copied.\n\n    .. versionchanged:: 1.3.0\n\nSee Also\n--------\nDataFrame.from_records : Constructor from tuples, also record arrays.\nDataFrame.from_dict : From dicts of Series, arrays, or dicts.\nread_csv : Read a comma-separated values (csv) file into DataFrame.\nread_table : Read general delimited file into DataFrame.\nread_clipboard : Read text from clipboard into DataFrame.\n\nNotes\n-----\nPlease reference the :ref:`User Guide <basics.dataframe>` for more information.\n\nExamples\n--------\nConstructing DataFrame from a dictionary.\n\n>>> d = {'col1': [1, 2], 'col2': [3, 4]}\n>>> df = pd.DataFrame(data=d)\n>>> df\n   col1  col2\n0     1     3\n1     2     4\n\nNotice that the inferred dtype is int64.\n\n>>> df.dtypes\ncol1    int64\ncol2    int64\ndtype: object\n\nTo enforce a single dtype:\n\n>>> df = pd.DataFrame(data=d, dtype=np.int8)\n>>> df.dtypes\ncol1    int8\ncol2    int8\ndtype: object\n\nConstructing DataFrame from a dictionary including Series:\n\n>>> d = {'col1': [0, 1, 2, 3], 'col2': pd.Series([2, 3], index=[2, 3])}\n>>> pd.DataFrame(data=d, index=[0, 1, 2, 3])\n   col1  col2\n0     0   NaN\n1     1   NaN\n2     2   2.0\n3     3   3.0\n\nConstructing DataFrame from numpy ndarray:\n\n>>> df2 = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),\n...                    columns=['a', 'b', 'c'])\n>>> df2\n   a  b  c\n0  1  2  3\n1  4  5  6\n2  7  8  9\n\nConstructing DataFrame from a numpy ndarray that has labeled columns:\n\n>>> data = np.array([(1, 2, 3), (4, 5, 6), (7, 8, 9)],\n...                 dtype=[(\"a\", \"i4\"), (\"b\", \"i4\"), (\"c\", \"i4\")])\n>>> df3 = pd.DataFrame(data, columns=['c', 'a'])\n...\n>>> df3\n   c  a\n0  3  1\n1  6  4\n2  9  7\n\nConstructing DataFrame from dataclass:\n\n>>> from dataclasses import make_dataclass\n>>> Point = make_dataclass(\"Point\", [(\"x\", int), (\"y\", int)])\n>>> pd.DataFrame([Point(0, 0), Point(0, 3), Point(2, 3)])\n   x  y\n0  0  0\n1  0  3\n2  2  3\n\nConstructing DataFrame from Series/DataFrame:\n\n>>> ser = pd.Series([1, 2, 3], index=[\"a\", \"b\", \"c\"])\n>>> df = pd.DataFrame(data=ser, index=[\"a\", \"c\"])\n>>> df\n   0\na  1\nc  3\n\n>>> df1 = pd.DataFrame([1, 2, 3], index=[\"a\", \"b\", \"c\"], columns=[\"x\"])\n>>> df2 = pd.DataFrame(data=df1, index=[\"a\", \"c\"])\n>>> df2\n   x\na  1\nc  3", "parameters": {"type": "object", "properties": {"data": {"type": "NoneType", "default": null}, "index": {"type": ["axes", "null"], "default": null}, "columns": {"type": ["axes", "null"], "default": null}, "dtype": {"type": ["dtype", "null"], "default": null}, "copy": {"type": ["bool", "null"], "default": null}}}}}
{"task_id": "BigCodeBench/637", "data": {"name": "pandas.DataFrame.mean", "type": "callable", "signature": "(self)", "description": "Return the mean of the values over the requested axis.\n\nParameters\n----------\naxis : {index (0), columns (1)}\n    Axis for the function to be applied on.\n    For `Series` this parameter is unused and defaults to 0.\n\n    For DataFrames, specifying ``axis=None`` will apply the aggregation\n    across both axes.\n\n    .. versionadded:: 2.0.0\n\nskipna : bool, default True\n    Exclude NA/null values when computing the result.\nnumeric_only : bool, default False\n    Include only float, int, boolean columns. Not implemented for Series.\n\n**kwargs\n    Additional keyword arguments to be passed to the function.\n\nReturns\n-------\nSeries or scalar\n\n            Examples\n            --------\n            >>> s = pd.Series([1, 2, 3])\n            >>> s.mean()\n            2.0\n\n            With a DataFrame\n\n            >>> df = pd.DataFrame({'a': [1, 2], 'b': [2, 3]}, index=['tiger', 'zebra'])\n            >>> df\n                   a   b\n            tiger  1   2\n            zebra  2   3\n            >>> df.mean()\n            a   1.5\n            b   2.5\n            dtype: float64\n\n            Using axis=1\n\n            >>> df.mean(axis=1)\n            tiger   1.5\n            zebra   2.5\n            dtype: float64\n\n            In this case, `numeric_only` should be set to `True` to avoid\n            getting an error.\n\n            >>> df = pd.DataFrame({'a': [1, 2], 'b': ['T', 'Z']},\n            ...                   index=['tiger', 'zebra'])\n            >>> df.mean(numeric_only=True)\n            a   1.5\n            dtype: float64", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/637", "data": {"name": "matplotlib.pyplot.subplots", "type": "class", "signature": "()", "description": "Create a figure and a set of subplots.\n\nThis utility wrapper makes it convenient to create common layouts of\nsubplots, including the enclosing figure object, in a single call.\n\nParameters\n----------\nnrows, ncols : int, default: 1\n    Number of rows/columns of the subplot grid.\n\nsharex, sharey : bool or {'none', 'all', 'row', 'col'}, default: False\n    Controls sharing of properties among x (*sharex*) or y (*sharey*)\n    axes:\n\n    - True or 'all': x- or y-axis will be shared among all subplots.\n    - False or 'none': each subplot x- or y-axis will be independent.\n    - 'row': each subplot row will share an x- or y-axis.\n    - 'col': each subplot column will share an x- or y-axis.\n\n    When subplots have a shared x-axis along a column, only the x tick\n    labels of the bottom subplot are created. Similarly, when subplots\n    have a shared y-axis along a row, only the y tick labels of the first\n    column subplot are created. To later turn other subplots' ticklabels\n    on, use `~matplotlib.axes.Axes.tick_params`.\n\n    When subplots have a shared axis that has units, calling\n    `~matplotlib.axis.Axis.set_units` will update each axis with the\n    new units.\n\nsqueeze : bool, default: True\n    - If True, extra dimensions are squeezed out from the returned\n      array of `~matplotlib.axes.Axes`:\n\n      - if only one subplot is constructed (nrows=ncols=1), the\n        resulting single Axes object is returned as a scalar.\n      - for Nx1 or 1xM subplots, the returned object is a 1D numpy\n        object array of Axes objects.\n      - for NxM, subplots with N>1 and M>1 are returned as a 2D array.\n\n    - If False, no squeezing at all is done: the returned Axes object is\n      always a 2D array containing Axes instances, even if it ends up\n      being 1x1.\n\nwidth_ratios : array-like of length *ncols*, optional\n    Defines the relative widths of the columns. Each column gets a\n    relative width of ``width_ratios[i] / sum(width_ratios)``.\n    If not given, all columns will have the same width.  Equivalent\n    to ``gridspec_kw={'width_ratios': [...]}``.\n\nheight_ratios : array-like of length *nrows*, optional\n    Defines the relative heights of the rows. Each row gets a\n    relative height of ``height_ratios[i] / sum(height_ratios)``.\n    If not given, all rows will have the same height. Convenience\n    for ``gridspec_kw={'height_ratios': [...]}``.\n\nsubplot_kw : dict, optional\n    Dict with keywords passed to the\n    `~matplotlib.figure.Figure.add_subplot` call used to create each\n    subplot.\n\ngridspec_kw : dict, optional\n    Dict with keywords passed to the `~matplotlib.gridspec.GridSpec`\n    constructor used to create the grid the subplots are placed on.\n\n**fig_kw\n    All additional keyword arguments are passed to the\n    `.pyplot.figure` call.\n\nReturns\n-------\nfig : `.Figure`\n\nax : `~matplotlib.axes.Axes` or array of Axes\n    *ax* can be either a single `~.axes.Axes` object, or an array of Axes\n    objects if more than one subplot was created.  The dimensions of the\n    resulting array can be controlled with the squeeze keyword, see above.\n\n    Typical idioms for handling the return value are::\n\n        # using the variable ax for single a Axes\n        fig, ax = plt.subplots()\n\n        # using the variable axs for multiple Axes\n        fig, axs = plt.subplots(2, 2)\n\n        # using tuple unpacking for multiple Axes\n        fig, (ax1, ax2) = plt.subplots(1, 2)\n        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n\n    The names ``ax`` and pluralized ``axs`` are preferred over ``axes``\n    because for the latter it's not clear if it refers to a single\n    `~.axes.Axes` instance or a collection of these.\n\nSee Also\n--------\n.pyplot.figure\n.pyplot.subplot\n.pyplot.axes\n.Figure.subplots\n.Figure.add_subplot\n\nExamples\n--------\n::\n\n    # First create some toy data:\n    x = np.linspace(0, 2*np.pi, 400)\n    y = np.sin(x**2)\n\n    # Create just a figure and only one subplot\n    fig, ax = plt.subplots()\n    ax.plot(x, y)\n    ax.set_title('Simple plot')\n\n    # Create two subplots and unpack the output array immediately\n    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n    ax1.plot(x, y)\n    ax1.set_title('Sharing Y axis')\n    ax2.scatter(x, y)\n\n    # Create four polar axes and access them through the returned array\n    fig, axs = plt.subplots(2, 2, subplot_kw=dict(projection=\"polar\"))\n    axs[0, 0].plot(x, y)\n    axs[1, 1].scatter(x, y)\n\n    # Share a X axis with each column of subplots\n    plt.subplots(2, 2, sharex='col')\n\n    # Share a Y axis with each row of subplots\n    plt.subplots(2, 2, sharey='row')\n\n    # Share both X and Y axes with all subplots\n    plt.subplots(2, 2, sharex='all', sharey='all')\n\n    # Note that this is the same as\n    plt.subplots(2, 2, sharex=True, sharey=True)\n\n    # Create figure number 10 with a single subplot\n    # and clears it if it already exists.\n    fig, ax = plt.subplots(num=10, clear=True)", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/637", "data": {"name": "matplotlib.pyplot.subplots[1].legend", "type": "method", "signature": "(*args, **kwargs)", "description": "Place a legend on the Axes.\n\nCall signatures::\n\n    legend()\n    legend(handles, labels)\n    legend(handles=handles)\n    legend(labels)\n\nThe call signatures correspond to the following different ways to use\nthis method:\n\n**1. Automatic detection of elements to be shown in the legend**\n\nThe elements to be added to the legend are automatically determined,\nwhen you do not pass in any extra arguments.\n\nIn this case, the labels are taken from the artist. You can specify\nthem either at artist creation or by calling the\n:meth:`~.Artist.set_label` method on the artist::\n\n    ax.plot([1, 2, 3], label='Inline label')\n    ax.legend()\n\nor::\n\n    line, = ax.plot([1, 2, 3])\n    line.set_label('Label via method')\n    ax.legend()\n\n.. note::\n    Specific artists can be excluded from the automatic legend element\n    selection by using a label starting with an underscore, \"_\".\n    A string starting with an underscore is the default label for all\n    artists, so calling `.Axes.legend` without any arguments and\n    without setting the labels manually will result in no legend being\n    drawn.\n\n\n**2. Explicitly listing the artists and labels in the legend**\n\nFor full control of which artists have a legend entry, it is possible\nto pass an iterable of legend artists followed by an iterable of\nlegend labels respectively::\n\n    ax.legend([line1, line2, line3], ['label1', 'label2', 'label3'])\n\n\n**3. Explicitly listing the artists in the legend**\n\nThis is similar to 2, but the labels are taken from the artists'\nlabel properties. Example::\n\n    line1, = ax.plot([1, 2, 3], label='label1')\n    line2, = ax.plot([1, 2, 3], label='label2')\n    ax.legend(handles=[line1, line2])\n\n\n**4. Labeling existing plot elements**\n\n.. admonition:: Discouraged\n\n    This call signature is discouraged, because the relation between\n    plot elements and labels is only implicit by their order and can\n    easily be mixed up.\n\nTo make a legend for all artists on an Axes, call this function with\nan iterable of strings, one for each legend item. For example::\n\n    ax.plot([1, 2, 3])\n    ax.plot([5, 6, 7])\n    ax.legend(['First line', 'Second line'])\n\n\nParameters\n----------\nhandles : list of (`.Artist` or tuple of `.Artist`), optional\n    A list of Artists (lines, patches) to be added to the legend.\n    Use this together with *labels*, if you need full control on what\n    is shown in the legend and the automatic mechanism described above\n    is not sufficient.\n\n    The length of handles and labels should be the same in this\n    case. If they are not, they are truncated to the smaller length.\n\n    If an entry contains a tuple, then the legend handler for all Artists in the\n    tuple will be placed alongside a single label.\n\nlabels : list of str, optional\n    A list of labels to show next to the artists.\n    Use this together with *handles*, if you need full control on what\n    is shown in the legend and the automatic mechanism described above\n    is not sufficient.\n\nReturns\n-------\n`~matplotlib.legend.Legend`\n\nOther Parameters\n----------------\n\nloc : str or pair of floats, default: :rc:`legend.loc`\n    The location of the legend.\n\n    The strings ``'upper left'``, ``'upper right'``, ``'lower left'``,\n    ``'lower right'`` place the legend at the corresponding corner of the\n    axes.\n\n    The strings ``'upper center'``, ``'lower center'``, ``'center left'``,\n    ``'center right'`` place the legend at the center of the corresponding edge\n    of the axes.\n\n    The string ``'center'`` places the legend at the center of the axes.\n\n    The string ``'best'`` places the legend at the location, among the nine\n    locations defined so far, with the minimum overlap with other drawn\n    artists.  This option can be quite slow for plots with large amounts of\n    data; your plotting speed may benefit from providing a specific location.\n\n    The location can also be a 2-tuple giving the coordinates of the lower-left\n    corner of the legend in axes coordinates (in which case *bbox_to_anchor*\n    will be ignored).\n\n    For back-compatibility, ``'center right'`` (but no other location) can also\n    be spelled ``'right'``, and each \"string\" location can also be given as a\n    numeric value:\n\n    ==================   =============\n    Location String      Location Code\n    ==================   =============\n    'best' (Axes only)   0\n    'upper right'        1\n    'upper left'         2\n    'lower left'         3\n    'lower right'        4\n    'right'              5\n    'center left'        6\n    'center right'       7\n    'lower center'       8\n    'upper center'       9\n    'center'             10\n    ==================   =============\n    \nbbox_to_anchor : `.BboxBase`, 2-tuple, or 4-tuple of floats\n    Box that is used to position the legend in conjunction with *loc*.\n    Defaults to `axes.bbox` (if called as a method to `.Axes.legend`) or\n    `figure.bbox` (if `.Figure.legend`).  This argument allows arbitrary\n    placement of the legend.\n\n    Bbox coordinates are interpreted in the coordinate system given by\n    *bbox_transform*, with the default transform\n    Axes or Figure coordinates, depending on which ``legend`` is called.\n\n    If a 4-tuple or `.BboxBase` is given, then it specifies the bbox\n    ``(x, y, width, height)`` that the legend is placed in.\n    To put the legend in the best location in the bottom right\n    quadrant of the axes (or figure)::\n\n        loc='best', bbox_to_anchor=(0.5, 0., 0.5, 0.5)\n\n    A 2-tuple ``(x, y)`` places the corner of the legend specified by *loc* at\n    x, y.  For example, to put the legend's upper right-hand corner in the\n    center of the axes (or figure) the following keywords can be used::\n\n        loc='upper right', bbox_to_anchor=(0.5, 0.5)\n\nncols : int, default: 1\n    The number of columns that the legend has.\n\n    For backward compatibility, the spelling *ncol* is also supported\n    but it is discouraged. If both are given, *ncols* takes precedence.\n\nprop : None or `~matplotlib.font_manager.FontProperties` or dict\n    The font properties of the legend. If None (default), the current\n    :data:`matplotlib.rcParams` will be used.\n\nfontsize : int or {'xx-small', 'x-small', 'small', 'medium', 'large', 'x-large', 'xx-large'}\n    The font size of the legend. If the value is numeric the size will be the\n    absolute font size in points. String values are relative to the current\n    default font size. This argument is only used if *prop* is not specified.\n\nlabelcolor : str or list, default: :rc:`legend.labelcolor`\n    The color of the text in the legend. Either a valid color string\n    (for example, 'red'), or a list of color strings. The labelcolor can\n    also be made to match the color of the line or marker using 'linecolor',\n    'markerfacecolor' (or 'mfc'), or 'markeredgecolor' (or 'mec').\n\n    Labelcolor can be set globally using :rc:`legend.labelcolor`. If None,\n    use :rc:`text.color`.\n\nnumpoints : int, default: :rc:`legend.numpoints`\n    The number of marker points in the legend when creating a legend\n    entry for a `.Line2D` (line).\n\nscatterpoints : int, default: :rc:`legend.scatterpoints`\n    The number of marker points in the legend when creating\n    a legend entry for a `.PathCollection` (scatter plot).\n\nscatteryoffsets : iterable of floats, default: ``[0.375, 0.5, 0.3125]``\n    The vertical offset (relative to the font size) for the markers\n    created for a scatter plot legend entry. 0.0 is at the base the\n    legend text, and 1.0 is at the top. To draw all markers at the\n    same height, set to ``[0.5]``.\n\nmarkerscale : float, default: :rc:`legend.markerscale`\n    The relative size of legend markers compared to the originally drawn ones.\n\nmarkerfirst : bool, default: True\n    If *True*, legend marker is placed to the left of the legend label.\n    If *False*, legend marker is placed to the right of the legend label.\n\nreverse : bool, default: False\n    If *True*, the legend labels are displayed in reverse order from the input.\n    If *False*, the legend labels are displayed in the same order as the input.\n\n    .. versionadded:: 3.7\n\nframeon : bool, default: :rc:`legend.frameon`\n    Whether the legend should be drawn on a patch (frame).\n\nfancybox : bool, default: :rc:`legend.fancybox`\n    Whether round edges should be enabled around the `.FancyBboxPatch` which\n    makes up the legend's background.\n\nshadow : None, bool or dict, default: :rc:`legend.shadow`\n    Whether to draw a shadow behind the legend.\n    The shadow can be configured using `.Patch` keywords.\n    Customization via :rc:`legend.shadow` is currently not supported.\n\nframealpha : float, default: :rc:`legend.framealpha`\n    The alpha transparency of the legend's background.\n    If *shadow* is activated and *framealpha* is ``None``, the default value is\n    ignored.\n\nfacecolor : \"inherit\" or color, default: :rc:`legend.facecolor`\n    The legend's background color.\n    If ``\"inherit\"``, use :rc:`axes.facecolor`.\n\nedgecolor : \"inherit\" or color, default: :rc:`legend.edgecolor`\n    The legend's background patch edge color.\n    If ``\"inherit\"``, use take :rc:`axes.edgecolor`.\n\nmode : {\"expand\", None}\n    If *mode* is set to ``\"expand\"`` the legend will be horizontally\n    expanded to fill the axes area (or *bbox_to_anchor* if defines\n    the legend's size).\n\nbbox_transform : None or `~matplotlib.transforms.Transform`\n    The transform for the bounding box (*bbox_to_anchor*). For a value\n    of ``None`` (default) the Axes'\n    :data:`~matplotlib.axes.Axes.transAxes` transform will be used.\n\ntitle : str or None\n    The legend's title. Default is no title (``None``).\n\ntitle_fontproperties : None or `~matplotlib.font_manager.FontProperties` or dict\n    The font properties of the legend's title. If None (default), the\n    *title_fontsize* argument will be used if present; if *title_fontsize* is\n    also None, the current :rc:`legend.title_fontsize` will be used.\n\ntitle_fontsize : int or {'xx-small', 'x-small', 'small', 'medium', 'large', 'x-large', 'xx-large'}, default: :rc:`legend.title_fontsize`\n    The font size of the legend's title.\n    Note: This cannot be combined with *title_fontproperties*. If you want\n    to set the fontsize alongside other font properties, use the *size*\n    parameter in *title_fontproperties*.\n\nalignment : {'center', 'left', 'right'}, default: 'center'\n    The alignment of the legend title and the box of entries. The entries\n    are aligned as a single block, so that markers always lined up.\n\nborderpad : float, default: :rc:`legend.borderpad`\n    The fractional whitespace inside the legend border, in font-size units.\n\nlabelspacing : float, default: :rc:`legend.labelspacing`\n    The vertical space between the legend entries, in font-size units.\n\nhandlelength : float, default: :rc:`legend.handlelength`\n    The length of the legend handles, in font-size units.\n\nhandleheight : float, default: :rc:`legend.handleheight`\n    The height of the legend handles, in font-size units.\n\nhandletextpad : float, default: :rc:`legend.handletextpad`\n    The pad between the legend handle and text, in font-size units.\n\nborderaxespad : float, default: :rc:`legend.borderaxespad`\n    The pad between the axes and legend border, in font-size units.\n\ncolumnspacing : float, default: :rc:`legend.columnspacing`\n    The spacing between columns, in font-size units.\n\nhandler_map : dict or None\n    The custom dictionary mapping instances or types to a legend\n    handler. This *handler_map* updates the default handler map\n    found at `matplotlib.legend.Legend.get_legend_handler_map`.\n\ndraggable : bool, default: False\n    Whether the legend can be dragged with the mouse.\n\n\nSee Also\n--------\n.Figure.legend\n\nNotes\n-----\nSome artists are not supported by this function.  See\n:ref:`legend_guide` for details.\n\nExamples\n--------\n.. plot:: gallery/text_labels_and_annotations/legend.py", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/637", "data": {"name": "matplotlib.pyplot.subplots[1].set_title", "type": "method", "signature": "(label, fontdict=None, loc=None, pad=None, *, y=None, **kwargs)", "description": "Set a title for the Axes.\n\nSet one of the three available Axes titles. The available titles\nare positioned above the Axes in the center, flush with the left\nedge, and flush with the right edge.\n\nParameters\n----------\nlabel : str\n    Text to use for the title\n\nfontdict : dict\n\n    .. admonition:: Discouraged\n\n       The use of *fontdict* is discouraged. Parameters should be passed as\n       individual keyword arguments or using dictionary-unpacking\n       ``set_title(..., **fontdict)``.\n\n    A dictionary controlling the appearance of the title text,\n    the default *fontdict* is::\n\n       {'fontsize': rcParams['axes.titlesize'],\n        'fontweight': rcParams['axes.titleweight'],\n        'color': rcParams['axes.titlecolor'],\n        'verticalalignment': 'baseline',\n        'horizontalalignment': loc}\n\nloc : {'center', 'left', 'right'}, default: :rc:`axes.titlelocation`\n    Which title to set.\n\ny : float, default: :rc:`axes.titley`\n    Vertical Axes location for the title (1.0 is the top).  If\n    None (the default) and :rc:`axes.titley` is also None, y is\n    determined automatically to avoid decorators on the Axes.\n\npad : float, default: :rc:`axes.titlepad`\n    The offset of the title from the top of the Axes, in points.\n\nReturns\n-------\n`.Text`\n    The matplotlib text instance representing the title\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    Other keyword arguments are text properties, see `.Text` for a list\n    of valid text properties.", "parameters": {"type": "object", "properties": {"label": {}, "fontdict": {"type": "NoneType", "default": null}, "loc": {"type": "NoneType", "default": null}, "pad": {"type": "NoneType", "default": null}, "y": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/637", "data": {"name": "numpy.random.randint", "type": "callable", "signature": "(*args, **kwargs)", "description": "randint(low, high=None, size=None, dtype=int)\n\nReturn random integers from `low` (inclusive) to `high` (exclusive).\n\nReturn random integers from the \"discrete uniform\" distribution of\nthe specified dtype in the \"half-open\" interval [`low`, `high`). If\n`high` is None (the default), then results are from [0, `low`).\n\n.. note::\n    New code should use the ``integers`` method of a ``default_rng()``\n    instance instead; please see the :ref:`random-quick-start`.\n\nParameters\n----------\nlow : int or array-like of ints\n    Lowest (signed) integers to be drawn from the distribution (unless\n    ``high=None``, in which case this parameter is one above the\n    *highest* such integer).\nhigh : int or array-like of ints, optional\n    If provided, one above the largest (signed) integer to be drawn\n    from the distribution (see above for behavior if ``high=None``).\n    If array-like, must contain integer values\nsize : int or tuple of ints, optional\n    Output shape.  If the given shape is, e.g., ``(m, n, k)``, then\n    ``m * n * k`` samples are drawn.  Default is None, in which case a\n    single value is returned.\ndtype : dtype, optional\n    Desired dtype of the result. Byteorder must be native.\n    The default value is int.\n\n    .. versionadded:: 1.11.0\n\nReturns\n-------\nout : int or ndarray of ints\n    `size`-shaped array of random integers from the appropriate\n    distribution, or a single such random int if `size` not provided.\n\nSee Also\n--------\nrandom_integers : similar to `randint`, only for the closed\n    interval [`low`, `high`], and 1 is the lowest value if `high` is\n    omitted.\nrandom.Generator.integers: which should be used for new code.\n\nExamples\n--------\n>>> np.random.randint(2, size=10)\narray([1, 0, 0, 0, 1, 1, 0, 0, 1, 0]) # random\n>>> np.random.randint(1, size=10)\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n\nGenerate a 2 x 4 array of ints between 0 and 4, inclusive:\n\n>>> np.random.randint(5, size=(2, 4))\narray([[4, 0, 2, 1], # random\n       [3, 2, 2, 0]])\n\nGenerate a 1 x 3 array with 3 different upper bounds\n\n>>> np.random.randint(1, [3, 5, 10])\narray([2, 2, 9]) # random\n\nGenerate a 1 by 3 array with 3 different lower bounds\n\n>>> np.random.randint([1, 5, 7], 10)\narray([9, 8, 7]) # random\n\nGenerate a 2 by 4 array using broadcasting with dtype of uint8\n\n>>> np.random.randint([1, 3, 5, 7], [[10], [20]], dtype=np.uint8)\narray([[ 8,  6,  9,  7], # random\n       [ 1, 16,  9, 12]], dtype=uint8)", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/637", "data": {"name": "random.sample", "type": "callable", "signature": "(population, k)", "description": "Chooses k unique random elements from a population sequence or set.\n\nReturns a new list containing elements from the population while\nleaving the original population unchanged.  The resulting list is\nin selection order so that all sub-slices will also be valid random\nsamples.  This allows raffle winners (the sample) to be partitioned\ninto grand prize and second place winners (the subslices).\n\nMembers of the population need not be hashable or unique.  If the\npopulation contains repeats, then each occurrence is a possible\nselection in the sample.\n\nRepeated elements can be specified one at a time or with the optional\ncounts parameter.  For example:\n\n    sample(['red', 'blue'], counts=[4, 2], k=5)\n\nis equivalent to:\n\n    sample(['red', 'red', 'red', 'red', 'blue', 'blue'], k=5)\n\nTo choose a sample from a range of integers, use range() for the\npopulation argument.  This is especially fast and space efficient\nfor sampling from a large population:\n\n    sample(range(10000000), 60)", "parameters": {"type": "object", "properties": {"population": {}, "k": {}}}}}
{"task_id": "BigCodeBench/654", "data": {"name": "indices.size"}}
{"task_id": "BigCodeBench/654", "data": {"name": "matplotlib.pyplot.gca", "type": "callable", "signature": "()", "description": "Get the current Axes.\n\nIf there is currently no Axes on this Figure, a new one is created\nusing `.Figure.add_subplot`.  (To test whether there is currently an\nAxes on a Figure, check whether ``figure.axes`` is empty.  To test\nwhether there is currently a Figure on the pyplot figure stack, check\nwhether `.pyplot.get_fignums()` is empty.)", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/654", "data": {"name": "matplotlib.pyplot.show", "type": "callable", "signature": "()", "description": "Display all open figures.\n\nParameters\n----------\nblock : bool, optional\n    Whether to wait for all figures to be closed before returning.\n\n    If `True` block and run the GUI main loop until all figure windows\n    are closed.\n\n    If `False` ensure that all figure windows are displayed and return\n    immediately.  In this case, you are responsible for ensuring\n    that the event loop is running to have responsive figures.\n\n    Defaults to True in non-interactive mode and to False in interactive\n    mode (see `.pyplot.isinteractive`).\n\nSee Also\n--------\nion : Enable interactive mode, which shows / updates the figure after\n      every plotting command, so that calling ``show()`` is not necessary.\nioff : Disable interactive mode.\nsavefig : Save the figure to an image file instead of showing it on screen.\n\nNotes\n-----\n**Saving figures to file and showing a window at the same time**\n\nIf you want an image file as well as a user interface window, use\n`.pyplot.savefig` before `.pyplot.show`. At the end of (a blocking)\n``show()`` the figure is closed and thus unregistered from pyplot. Calling\n`.pyplot.savefig` afterwards would save a new and thus empty figure. This\nlimitation of command order does not apply if the show is non-blocking or\nif you keep a reference to the figure and use `.Figure.savefig`.\n\n**Auto-show in jupyter notebooks**\n\nThe jupyter backends (activated via ``%matplotlib inline``,\n``%matplotlib notebook``, or ``%matplotlib widget``), call ``show()`` at\nthe end of every cell by default. Thus, you usually don't have to call it\nexplicitly there.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/654", "data": {"name": "matplotlib.pyplot.legend", "type": "callable", "signature": "()", "description": "Place a legend on the Axes.\n\nCall signatures::\n\n    legend()\n    legend(handles, labels)\n    legend(handles=handles)\n    legend(labels)\n\nThe call signatures correspond to the following different ways to use\nthis method:\n\n**1. Automatic detection of elements to be shown in the legend**\n\nThe elements to be added to the legend are automatically determined,\nwhen you do not pass in any extra arguments.\n\nIn this case, the labels are taken from the artist. You can specify\nthem either at artist creation or by calling the\n:meth:`~.Artist.set_label` method on the artist::\n\n    ax.plot([1, 2, 3], label='Inline label')\n    ax.legend()\n\nor::\n\n    line, = ax.plot([1, 2, 3])\n    line.set_label('Label via method')\n    ax.legend()\n\n.. note::\n    Specific artists can be excluded from the automatic legend element\n    selection by using a label starting with an underscore, \"_\".\n    A string starting with an underscore is the default label for all\n    artists, so calling `.Axes.legend` without any arguments and\n    without setting the labels manually will result in no legend being\n    drawn.\n\n\n**2. Explicitly listing the artists and labels in the legend**\n\nFor full control of which artists have a legend entry, it is possible\nto pass an iterable of legend artists followed by an iterable of\nlegend labels respectively::\n\n    ax.legend([line1, line2, line3], ['label1', 'label2', 'label3'])\n\n\n**3. Explicitly listing the artists in the legend**\n\nThis is similar to 2, but the labels are taken from the artists'\nlabel properties. Example::\n\n    line1, = ax.plot([1, 2, 3], label='label1')\n    line2, = ax.plot([1, 2, 3], label='label2')\n    ax.legend(handles=[line1, line2])\n\n\n**4. Labeling existing plot elements**\n\n.. admonition:: Discouraged\n\n    This call signature is discouraged, because the relation between\n    plot elements and labels is only implicit by their order and can\n    easily be mixed up.\n\nTo make a legend for all artists on an Axes, call this function with\nan iterable of strings, one for each legend item. For example::\n\n    ax.plot([1, 2, 3])\n    ax.plot([5, 6, 7])\n    ax.legend(['First line', 'Second line'])\n\n\nParameters\n----------\nhandles : list of (`.Artist` or tuple of `.Artist`), optional\n    A list of Artists (lines, patches) to be added to the legend.\n    Use this together with *labels*, if you need full control on what\n    is shown in the legend and the automatic mechanism described above\n    is not sufficient.\n\n    The length of handles and labels should be the same in this\n    case. If they are not, they are truncated to the smaller length.\n\n    If an entry contains a tuple, then the legend handler for all Artists in the\n    tuple will be placed alongside a single label.\n\nlabels : list of str, optional\n    A list of labels to show next to the artists.\n    Use this together with *handles*, if you need full control on what\n    is shown in the legend and the automatic mechanism described above\n    is not sufficient.\n\nReturns\n-------\n`~matplotlib.legend.Legend`\n\nOther Parameters\n----------------\n\nloc : str or pair of floats, default: :rc:`legend.loc`\n    The location of the legend.\n\n    The strings ``'upper left'``, ``'upper right'``, ``'lower left'``,\n    ``'lower right'`` place the legend at the corresponding corner of the\n    axes.\n\n    The strings ``'upper center'``, ``'lower center'``, ``'center left'``,\n    ``'center right'`` place the legend at the center of the corresponding edge\n    of the axes.\n\n    The string ``'center'`` places the legend at the center of the axes.\n\n    The string ``'best'`` places the legend at the location, among the nine\n    locations defined so far, with the minimum overlap with other drawn\n    artists.  This option can be quite slow for plots with large amounts of\n    data; your plotting speed may benefit from providing a specific location.\n\n    The location can also be a 2-tuple giving the coordinates of the lower-left\n    corner of the legend in axes coordinates (in which case *bbox_to_anchor*\n    will be ignored).\n\n    For back-compatibility, ``'center right'`` (but no other location) can also\n    be spelled ``'right'``, and each \"string\" location can also be given as a\n    numeric value:\n\n    ==================   =============\n    Location String      Location Code\n    ==================   =============\n    'best' (Axes only)   0\n    'upper right'        1\n    'upper left'         2\n    'lower left'         3\n    'lower right'        4\n    'right'              5\n    'center left'        6\n    'center right'       7\n    'lower center'       8\n    'upper center'       9\n    'center'             10\n    ==================   =============\n    \nbbox_to_anchor : `.BboxBase`, 2-tuple, or 4-tuple of floats\n    Box that is used to position the legend in conjunction with *loc*.\n    Defaults to `axes.bbox` (if called as a method to `.Axes.legend`) or\n    `figure.bbox` (if `.Figure.legend`).  This argument allows arbitrary\n    placement of the legend.\n\n    Bbox coordinates are interpreted in the coordinate system given by\n    *bbox_transform*, with the default transform\n    Axes or Figure coordinates, depending on which ``legend`` is called.\n\n    If a 4-tuple or `.BboxBase` is given, then it specifies the bbox\n    ``(x, y, width, height)`` that the legend is placed in.\n    To put the legend in the best location in the bottom right\n    quadrant of the axes (or figure)::\n\n        loc='best', bbox_to_anchor=(0.5, 0., 0.5, 0.5)\n\n    A 2-tuple ``(x, y)`` places the corner of the legend specified by *loc* at\n    x, y.  For example, to put the legend's upper right-hand corner in the\n    center of the axes (or figure) the following keywords can be used::\n\n        loc='upper right', bbox_to_anchor=(0.5, 0.5)\n\nncols : int, default: 1\n    The number of columns that the legend has.\n\n    For backward compatibility, the spelling *ncol* is also supported\n    but it is discouraged. If both are given, *ncols* takes precedence.\n\nprop : None or `~matplotlib.font_manager.FontProperties` or dict\n    The font properties of the legend. If None (default), the current\n    :data:`matplotlib.rcParams` will be used.\n\nfontsize : int or {'xx-small', 'x-small', 'small', 'medium', 'large', 'x-large', 'xx-large'}\n    The font size of the legend. If the value is numeric the size will be the\n    absolute font size in points. String values are relative to the current\n    default font size. This argument is only used if *prop* is not specified.\n\nlabelcolor : str or list, default: :rc:`legend.labelcolor`\n    The color of the text in the legend. Either a valid color string\n    (for example, 'red'), or a list of color strings. The labelcolor can\n    also be made to match the color of the line or marker using 'linecolor',\n    'markerfacecolor' (or 'mfc'), or 'markeredgecolor' (or 'mec').\n\n    Labelcolor can be set globally using :rc:`legend.labelcolor`. If None,\n    use :rc:`text.color`.\n\nnumpoints : int, default: :rc:`legend.numpoints`\n    The number of marker points in the legend when creating a legend\n    entry for a `.Line2D` (line).\n\nscatterpoints : int, default: :rc:`legend.scatterpoints`\n    The number of marker points in the legend when creating\n    a legend entry for a `.PathCollection` (scatter plot).\n\nscatteryoffsets : iterable of floats, default: ``[0.375, 0.5, 0.3125]``\n    The vertical offset (relative to the font size) for the markers\n    created for a scatter plot legend entry. 0.0 is at the base the\n    legend text, and 1.0 is at the top. To draw all markers at the\n    same height, set to ``[0.5]``.\n\nmarkerscale : float, default: :rc:`legend.markerscale`\n    The relative size of legend markers compared to the originally drawn ones.\n\nmarkerfirst : bool, default: True\n    If *True*, legend marker is placed to the left of the legend label.\n    If *False*, legend marker is placed to the right of the legend label.\n\nreverse : bool, default: False\n    If *True*, the legend labels are displayed in reverse order from the input.\n    If *False*, the legend labels are displayed in the same order as the input.\n\n    .. versionadded:: 3.7\n\nframeon : bool, default: :rc:`legend.frameon`\n    Whether the legend should be drawn on a patch (frame).\n\nfancybox : bool, default: :rc:`legend.fancybox`\n    Whether round edges should be enabled around the `.FancyBboxPatch` which\n    makes up the legend's background.\n\nshadow : None, bool or dict, default: :rc:`legend.shadow`\n    Whether to draw a shadow behind the legend.\n    The shadow can be configured using `.Patch` keywords.\n    Customization via :rc:`legend.shadow` is currently not supported.\n\nframealpha : float, default: :rc:`legend.framealpha`\n    The alpha transparency of the legend's background.\n    If *shadow* is activated and *framealpha* is ``None``, the default value is\n    ignored.\n\nfacecolor : \"inherit\" or color, default: :rc:`legend.facecolor`\n    The legend's background color.\n    If ``\"inherit\"``, use :rc:`axes.facecolor`.\n\nedgecolor : \"inherit\" or color, default: :rc:`legend.edgecolor`\n    The legend's background patch edge color.\n    If ``\"inherit\"``, use take :rc:`axes.edgecolor`.\n\nmode : {\"expand\", None}\n    If *mode* is set to ``\"expand\"`` the legend will be horizontally\n    expanded to fill the axes area (or *bbox_to_anchor* if defines\n    the legend's size).\n\nbbox_transform : None or `~matplotlib.transforms.Transform`\n    The transform for the bounding box (*bbox_to_anchor*). For a value\n    of ``None`` (default) the Axes'\n    :data:`~matplotlib.axes.Axes.transAxes` transform will be used.\n\ntitle : str or None\n    The legend's title. Default is no title (``None``).\n\ntitle_fontproperties : None or `~matplotlib.font_manager.FontProperties` or dict\n    The font properties of the legend's title. If None (default), the\n    *title_fontsize* argument will be used if present; if *title_fontsize* is\n    also None, the current :rc:`legend.title_fontsize` will be used.\n\ntitle_fontsize : int or {'xx-small', 'x-small', 'small', 'medium', 'large', 'x-large', 'xx-large'}, default: :rc:`legend.title_fontsize`\n    The font size of the legend's title.\n    Note: This cannot be combined with *title_fontproperties*. If you want\n    to set the fontsize alongside other font properties, use the *size*\n    parameter in *title_fontproperties*.\n\nalignment : {'center', 'left', 'right'}, default: 'center'\n    The alignment of the legend title and the box of entries. The entries\n    are aligned as a single block, so that markers always lined up.\n\nborderpad : float, default: :rc:`legend.borderpad`\n    The fractional whitespace inside the legend border, in font-size units.\n\nlabelspacing : float, default: :rc:`legend.labelspacing`\n    The vertical space between the legend entries, in font-size units.\n\nhandlelength : float, default: :rc:`legend.handlelength`\n    The length of the legend handles, in font-size units.\n\nhandleheight : float, default: :rc:`legend.handleheight`\n    The height of the legend handles, in font-size units.\n\nhandletextpad : float, default: :rc:`legend.handletextpad`\n    The pad between the legend handle and text, in font-size units.\n\nborderaxespad : float, default: :rc:`legend.borderaxespad`\n    The pad between the axes and legend border, in font-size units.\n\ncolumnspacing : float, default: :rc:`legend.columnspacing`\n    The spacing between columns, in font-size units.\n\nhandler_map : dict or None\n    The custom dictionary mapping instances or types to a legend\n    handler. This *handler_map* updates the default handler map\n    found at `matplotlib.legend.Legend.get_legend_handler_map`.\n\ndraggable : bool, default: False\n    Whether the legend can be dragged with the mouse.\n\n\nSee Also\n--------\n.Figure.legend\n\nNotes\n-----\nSome artists are not supported by this function.  See\n:ref:`legend_guide` for details.\n\nExamples\n--------\n.. plot:: gallery/text_labels_and_annotations/legend.py", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/654", "data": {"name": "matplotlib.pyplot.figure", "type": "callable", "signature": "()", "description": "Create a new figure, or activate an existing figure.\n\nParameters\n----------\nnum : int or str or `.Figure` or `.SubFigure`, optional\n    A unique identifier for the figure.\n\n    If a figure with that identifier already exists, this figure is made\n    active and returned. An integer refers to the ``Figure.number``\n    attribute, a string refers to the figure label.\n\n    If there is no figure with the identifier or *num* is not given, a new\n    figure is created, made active and returned.  If *num* is an int, it\n    will be used for the ``Figure.number`` attribute, otherwise, an\n    auto-generated integer value is used (starting at 1 and incremented\n    for each new figure). If *num* is a string, the figure label and the\n    window title is set to this value.  If num is a ``SubFigure``, its\n    parent ``Figure`` is activated.\n\nfigsize : (float, float), default: :rc:`figure.figsize`\n    Width, height in inches.\n\ndpi : float, default: :rc:`figure.dpi`\n    The resolution of the figure in dots-per-inch.\n\nfacecolor : color, default: :rc:`figure.facecolor`\n    The background color.\n\nedgecolor : color, default: :rc:`figure.edgecolor`\n    The border color.\n\nframeon : bool, default: True\n    If False, suppress drawing the figure frame.\n\nFigureClass : subclass of `~matplotlib.figure.Figure`\n    If set, an instance of this subclass will be created, rather than a\n    plain `.Figure`.\n\nclear : bool, default: False\n    If True and the figure already exists, then it is cleared.\n\nlayout : {'constrained', 'compressed', 'tight', 'none', `.LayoutEngine`, None}, default: None\n    The layout mechanism for positioning of plot elements to avoid\n    overlapping Axes decorations (labels, ticks, etc). Note that layout\n    managers can measurably slow down figure display.\n\n    - 'constrained': The constrained layout solver adjusts axes sizes\n      to avoid overlapping axes decorations.  Can handle complex plot\n      layouts and colorbars, and is thus recommended.\n\n      See :ref:`constrainedlayout_guide`\n      for examples.\n\n    - 'compressed': uses the same algorithm as 'constrained', but\n      removes extra space between fixed-aspect-ratio Axes.  Best for\n      simple grids of axes.\n\n    - 'tight': Use the tight layout mechanism. This is a relatively\n      simple algorithm that adjusts the subplot parameters so that\n      decorations do not overlap. See `.Figure.set_tight_layout` for\n      further details.\n\n    - 'none': Do not use a layout engine.\n\n    - A `.LayoutEngine` instance. Builtin layout classes are\n      `.ConstrainedLayoutEngine` and `.TightLayoutEngine`, more easily\n      accessible by 'constrained' and 'tight'.  Passing an instance\n      allows third parties to provide their own layout engine.\n\n    If not given, fall back to using the parameters *tight_layout* and\n    *constrained_layout*, including their config defaults\n    :rc:`figure.autolayout` and :rc:`figure.constrained_layout.use`.\n\n**kwargs\n    Additional keyword arguments are passed to the `.Figure` constructor.\n\nReturns\n-------\n`~matplotlib.figure.Figure`\n\nNotes\n-----\nA newly created figure is passed to the `~.FigureCanvasBase.new_manager`\nmethod or the `new_figure_manager` function provided by the current\nbackend, which install a canvas and a manager on the figure.\n\nOnce this is done, :rc:`figure.hooks` are called, one at a time, on the\nfigure; these hooks allow arbitrary customization of the figure (e.g.,\nattaching callbacks) or of associated elements (e.g., modifying the\ntoolbar).  See :doc:`/gallery/user_interfaces/mplcvd` for an example of\ntoolbar customization.\n\nIf you are creating many figures, make sure you explicitly call\n`.pyplot.close` on the figures you are not using, because this will\nenable pyplot to properly clean up the memory.\n\n`~matplotlib.rcParams` defines the default values, which can be modified\nin the matplotlibrc file.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/654", "data": {"name": "matplotlib.pyplot.plot", "type": "callable", "signature": "(*args: 'float | ArrayLike | str', scalex: 'bool' = True, scaley: 'bool' = True, **kwargs) -> 'list[Line2D])", "description": "Plot y versus x as lines and/or markers.\n\nCall signatures::\n\n    plot([x], y, [fmt], *, data=None, **kwargs)\n    plot([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n\nThe coordinates of the points or line nodes are given by *x*, *y*.\n\nThe optional parameter *fmt* is a convenient way for defining basic\nformatting like color, marker and linestyle. It's a shortcut string\nnotation described in the *Notes* section below.\n\n>>> plot(x, y)        # plot x and y using default line style and color\n>>> plot(x, y, 'bo')  # plot x and y using blue circle markers\n>>> plot(y)           # plot y using x as index array 0..N-1\n>>> plot(y, 'r+')     # ditto, but with red plusses\n\nYou can use `.Line2D` properties as keyword arguments for more\ncontrol on the appearance. Line properties and *fmt* can be mixed.\nThe following two calls yield identical results:\n\n>>> plot(x, y, 'go--', linewidth=2, markersize=12)\n>>> plot(x, y, color='green', marker='o', linestyle='dashed',\n...      linewidth=2, markersize=12)\n\nWhen conflicting with *fmt*, keyword arguments take precedence.\n\n\n**Plotting labelled data**\n\nThere's a convenient way for plotting objects with labelled data (i.e.\ndata that can be accessed by index ``obj['y']``). Instead of giving\nthe data in *x* and *y*, you can provide the object in the *data*\nparameter and just give the labels for *x* and *y*::\n\n>>> plot('xlabel', 'ylabel', data=obj)\n\nAll indexable objects are supported. This could e.g. be a `dict`, a\n`pandas.DataFrame` or a structured numpy array.\n\n\n**Plotting multiple sets of data**\n\nThere are various ways to plot multiple sets of data.\n\n- The most straight forward way is just to call `plot` multiple times.\n  Example:\n\n  >>> plot(x1, y1, 'bo')\n  >>> plot(x2, y2, 'go')\n\n- If *x* and/or *y* are 2D arrays a separate data set will be drawn\n  for every column. If both *x* and *y* are 2D, they must have the\n  same shape. If only one of them is 2D with shape (N, m) the other\n  must have length N and will be used for every data set m.\n\n  Example:\n\n  >>> x = [1, 2, 3]\n  >>> y = np.array([[1, 2], [3, 4], [5, 6]])\n  >>> plot(x, y)\n\n  is equivalent to:\n\n  >>> for col in range(y.shape[1]):\n  ...     plot(x, y[:, col])\n\n- The third way is to specify multiple sets of *[x]*, *y*, *[fmt]*\n  groups::\n\n  >>> plot(x1, y1, 'g^', x2, y2, 'g-')\n\n  In this case, any additional keyword argument applies to all\n  datasets. Also, this syntax cannot be combined with the *data*\n  parameter.\n\nBy default, each line is assigned a different style specified by a\n'style cycle'. The *fmt* and line property parameters are only\nnecessary if you want explicit deviations from these defaults.\nAlternatively, you can also change the style cycle using\n:rc:`axes.prop_cycle`.\n\n\nParameters\n----------\nx, y : array-like or scalar\n    The horizontal / vertical coordinates of the data points.\n    *x* values are optional and default to ``range(len(y))``.\n\n    Commonly, these parameters are 1D arrays.\n\n    They can also be scalars, or two-dimensional (in that case, the\n    columns represent separate data sets).\n\n    These arguments cannot be passed as keywords.\n\nfmt : str, optional\n    A format string, e.g. 'ro' for red circles. See the *Notes*\n    section for a full description of the format strings.\n\n    Format strings are just an abbreviation for quickly setting\n    basic line properties. All of these and more can also be\n    controlled by keyword arguments.\n\n    This argument cannot be passed as keyword.\n\ndata : indexable object, optional\n    An object with labelled data. If given, provide the label names to\n    plot in *x* and *y*.\n\n    .. note::\n        Technically there's a slight ambiguity in calls where the\n        second label is a valid *fmt*. ``plot('n', 'o', data=obj)``\n        could be ``plt(x, y)`` or ``plt(y, fmt)``. In such cases,\n        the former interpretation is chosen, but a warning is issued.\n        You may suppress the warning by adding an empty format string\n        ``plot('n', 'o', '', data=obj)``.\n\nReturns\n-------\nlist of `.Line2D`\n    A list of lines representing the plotted data.\n\nOther Parameters\n----------------\nscalex, scaley : bool, default: True\n    These parameters determine if the view limits are adapted to the\n    data limits. The values are passed on to\n    `~.axes.Axes.autoscale_view`.\n\n**kwargs : `~matplotlib.lines.Line2D` properties, optional\n    *kwargs* are used to specify properties like a line label (for\n    auto legends), linewidth, antialiasing, marker face color.\n    Example::\n\n    >>> plot([1, 2, 3], [1, 2, 3], 'go-', label='line 1', linewidth=2)\n    >>> plot([1, 2, 3], [1, 4, 9], 'rs', label='line 2')\n\n    If you specify multiple lines with one plot call, the kwargs apply\n    to all those lines. In case the label object is iterable, each\n    element is used as labels for each set of data.\n\n    Here is a list of available `.Line2D` properties:\n\n    Properties:\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: scalar or None\n    animated: bool\n    antialiased or aa: bool\n    clip_box: `~matplotlib.transforms.BboxBase` or None\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    color or c: color\n    dash_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    dash_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    dashes: sequence of floats (on/off ink in points) or (None, None)\n    data: (2, N) array or two 1D arrays\n    drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n    figure: `~matplotlib.figure.Figure`\n    fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n    gapcolor: color or None\n    gid: str\n    in_layout: bool\n    label: object\n    linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n    linewidth or lw: float\n    marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n    markeredgecolor or mec: color\n    markeredgewidth or mew: float\n    markerfacecolor or mfc: color\n    markerfacecoloralt or mfcalt: color\n    markersize or ms: float\n    markevery: None or int or (int, int) or slice or list[int] or float or (float, float) or list[bool]\n    mouseover: bool\n    path_effects: list of `.AbstractPathEffect`\n    picker: float or callable[[Artist, Event], tuple[bool, dict]]\n    pickradius: float\n    rasterized: bool\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    solid_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    solid_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    transform: unknown\n    url: str\n    visible: bool\n    xdata: 1D array\n    ydata: 1D array\n    zorder: float\n\nSee Also\n--------\nscatter : XY scatter plot with markers of varying size and/or color (\n    sometimes also called bubble chart).\n\nNotes\n-----\n**Format Strings**\n\nA format string consists of a part for color, marker and line::\n\n    fmt = '[marker][line][color]'\n\nEach of them is optional. If not provided, the value from the style\ncycle is used. Exception: If ``line`` is given, but no ``marker``,\nthe data will be a line without markers.\n\nOther combinations such as ``[color][marker][line]`` are also\nsupported, but note that their parsing may be ambiguous.\n\n**Markers**\n\n=============   ===============================\ncharacter       description\n=============   ===============================\n``'.'``         point marker\n``','``         pixel marker\n``'o'``         circle marker\n``'v'``         triangle_down marker\n``'^'``         triangle_up marker\n``'<'``         triangle_left marker\n``'>'``         triangle_right marker\n``'1'``         tri_down marker\n``'2'``         tri_up marker\n``'3'``         tri_left marker\n``'4'``         tri_right marker\n``'8'``         octagon marker\n``'s'``         square marker\n``'p'``         pentagon marker\n``'P'``         plus (filled) marker\n``'*'``         star marker\n``'h'``         hexagon1 marker\n``'H'``         hexagon2 marker\n``'+'``         plus marker\n``'x'``         x marker\n``'X'``         x (filled) marker\n``'D'``         diamond marker\n``'d'``         thin_diamond marker\n``'|'``         vline marker\n``'_'``         hline marker\n=============   ===============================\n\n**Line Styles**\n\n=============    ===============================\ncharacter        description\n=============    ===============================\n``'-'``          solid line style\n``'--'``         dashed line style\n``'-.'``         dash-dot line style\n``':'``          dotted line style\n=============    ===============================\n\nExample format strings::\n\n    'b'    # blue markers with default shape\n    'or'   # red circles\n    '-g'   # green solid line\n    '--'   # dashed line with default color\n    '^k:'  # black triangle_up markers connected by a dotted line\n\n**Colors**\n\nThe supported color abbreviations are the single letter codes\n\n=============    ===============================\ncharacter        color\n=============    ===============================\n``'b'``          blue\n``'g'``          green\n``'r'``          red\n``'c'``          cyan\n``'m'``          magenta\n``'y'``          yellow\n``'k'``          black\n``'w'``          white\n=============    ===============================\n\nand the ``'CN'`` colors that index into the default property cycle.\n\nIf the color is the only part of the format string, you can\nadditionally use any  `matplotlib.colors` spec, e.g. full names\n(``'green'``) or hex strings (``'#008000'``).", "parameters": {"type": "object", "properties": {"scalex": {"type": "bool", "default": true}, "scaley": {"type": "bool", "default": true}}}}}
{"task_id": "BigCodeBench/654", "data": {"name": "numpy.exp", "type": "callable", "signature": "(*args, **kwargs)", "description": "exp(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n\nCalculate the exponential of all elements in the input array.\n\nParameters\n----------\nx : array_like\n    Input values.\nout : ndarray, None, or tuple of ndarray and None, optional\n    A location into which the result is stored. If provided, it must have\n    a shape that the inputs broadcast to. If not provided or None,\n    a freshly-allocated array is returned. A tuple (possible only as a\n    keyword argument) must have length equal to the number of outputs.\nwhere : array_like, optional\n    This condition is broadcast over the input. At locations where the\n    condition is True, the `out` array will be set to the ufunc result.\n    Elsewhere, the `out` array will retain its original value.\n    Note that if an uninitialized `out` array is created via the default\n    ``out=None``, locations within it where the condition is False will\n    remain uninitialized.\n**kwargs\n    For other keyword-only arguments, see the\n    :ref:`ufunc docs <ufuncs.kwargs>`.\n\nReturns\n-------\nout : ndarray or scalar\n    Output array, element-wise exponential of `x`.\n    This is a scalar if `x` is a scalar.\n\nSee Also\n--------\nexpm1 : Calculate ``exp(x) - 1`` for all elements in the array.\nexp2  : Calculate ``2**x`` for all elements in the array.\n\nNotes\n-----\nThe irrational number ``e`` is also known as Euler's number.  It is\napproximately 2.718281, and is the base of the natural logarithm,\n``ln`` (this means that, if :math:`x = \\ln y = \\log_e y`,\nthen :math:`e^x = y`. For real input, ``exp(x)`` is always positive.\n\nFor complex arguments, ``x = a + ib``, we can write\n:math:`e^x = e^a e^{ib}`.  The first term, :math:`e^a`, is already\nknown (it is the real argument, described above).  The second term,\n:math:`e^{ib}`, is :math:`\\cos b + i \\sin b`, a function with\nmagnitude 1 and a periodic phase.\n\nReferences\n----------\n.. [1] Wikipedia, \"Exponential function\",\n       https://en.wikipedia.org/wiki/Exponential_function\n.. [2] M. Abramovitz and I. A. Stegun, \"Handbook of Mathematical Functions\n       with Formulas, Graphs, and Mathematical Tables,\" Dover, 1964, p. 69,\n       https://personal.math.ubc.ca/~cbm/aands/page_69.htm\n\nExamples\n--------\nPlot the magnitude and phase of ``exp(x)`` in the complex plane:\n\n>>> import matplotlib.pyplot as plt\n\n>>> x = np.linspace(-2*np.pi, 2*np.pi, 100)\n>>> xx = x + 1j * x[:, np.newaxis] # a + ib over complex plane\n>>> out = np.exp(xx)\n\n>>> plt.subplot(121)\n>>> plt.imshow(np.abs(out),\n...            extent=[-2*np.pi, 2*np.pi, -2*np.pi, 2*np.pi], cmap='gray')\n>>> plt.title('Magnitude of exp(x)')\n\n>>> plt.subplot(122)\n>>> plt.imshow(np.angle(out),\n...            extent=[-2*np.pi, 2*np.pi, -2*np.pi, 2*np.pi], cmap='hsv')\n>>> plt.title('Phase (angle) of exp(x)')\n>>> plt.show()", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/654", "data": {"name": "numpy.arange", "type": "callable", "signature": "(*args, **kwargs)", "description": "arange([start,] stop[, step,], dtype=None, *, like=None)\n\nReturn evenly spaced values within a given interval.\n\n``arange`` can be called with a varying number of positional arguments:\n\n* ``arange(stop)``: Values are generated within the half-open interval\n  ``[0, stop)`` (in other words, the interval including `start` but\n  excluding `stop`).\n* ``arange(start, stop)``: Values are generated within the half-open\n  interval ``[start, stop)``.\n* ``arange(start, stop, step)`` Values are generated within the half-open\n  interval ``[start, stop)``, with spacing between values given by\n  ``step``.\n\nFor integer arguments the function is roughly equivalent to the Python\nbuilt-in :py:class:`range`, but returns an ndarray rather than a ``range``\ninstance.\n\nWhen using a non-integer step, such as 0.1, it is often better to use\n`numpy.linspace`.\n\nSee the Warning sections below for more information.\n\nParameters\n----------\nstart : integer or real, optional\n    Start of interval.  The interval includes this value.  The default\n    start value is 0.\nstop : integer or real\n    End of interval.  The interval does not include this value, except\n    in some cases where `step` is not an integer and floating point\n    round-off affects the length of `out`.\nstep : integer or real, optional\n    Spacing between values.  For any output `out`, this is the distance\n    between two adjacent values, ``out[i+1] - out[i]``.  The default\n    step size is 1.  If `step` is specified as a position argument,\n    `start` must also be given.\ndtype : dtype, optional\n    The type of the output array.  If `dtype` is not given, infer the data\n    type from the other input arguments.\nlike : array_like, optional\n    Reference object to allow the creation of arrays which are not\n    NumPy arrays. If an array-like passed in as ``like`` supports\n    the ``__array_function__`` protocol, the result will be defined\n    by it. In this case, it ensures the creation of an array object\n    compatible with that passed in via this argument.\n\n    .. versionadded:: 1.20.0\n\nReturns\n-------\narange : ndarray\n    Array of evenly spaced values.\n\n    For floating point arguments, the length of the result is\n    ``ceil((stop - start)/step)``.  Because of floating point overflow,\n    this rule may result in the last element of `out` being greater\n    than `stop`.\n\nWarnings\n--------\nThe length of the output might not be numerically stable.\n\nAnother stability issue is due to the internal implementation of\n`numpy.arange`.\nThe actual step value used to populate the array is\n``dtype(start + step) - dtype(start)`` and not `step`. Precision loss\ncan occur here, due to casting or due to using floating points when\n`start` is much larger than `step`. This can lead to unexpected\nbehaviour. For example::\n\n  >>> np.arange(0, 5, 0.5, dtype=int)\n  array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n  >>> np.arange(-3, 3, 0.5, dtype=int)\n  array([-3, -2, -1,  0,  1,  2,  3,  4,  5,  6,  7,  8])\n\nIn such cases, the use of `numpy.linspace` should be preferred.\n\nThe built-in :py:class:`range` generates :std:doc:`Python built-in integers\nthat have arbitrary size <c-api/long>`, while `numpy.arange` produces\n`numpy.int32` or `numpy.int64` numbers. This may result in incorrect\nresults for large integer values::\n\n  >>> power = 40\n  >>> modulo = 10000\n  >>> x1 = [(n ** power) % modulo for n in range(8)]\n  >>> x2 = [(n ** power) % modulo for n in np.arange(8)]\n  >>> print(x1)\n  [0, 1, 7776, 8801, 6176, 625, 6576, 4001]  # correct\n  >>> print(x2)\n  [0, 1, 7776, 7185, 0, 5969, 4816, 3361]  # incorrect\n\nSee Also\n--------\nnumpy.linspace : Evenly spaced numbers with careful handling of endpoints.\nnumpy.ogrid: Arrays of evenly spaced numbers in N-dimensions.\nnumpy.mgrid: Grid-shaped arrays of evenly spaced numbers in N-dimensions.\n\nExamples\n--------\n>>> np.arange(3)\narray([0, 1, 2])\n>>> np.arange(3.0)\narray([ 0.,  1.,  2.])\n>>> np.arange(3,7)\narray([3, 4, 5, 6])\n>>> np.arange(3,7,2)\narray([3, 5])", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/654", "data": {"name": "numpy.where", "type": "callable", "signature": "(*args, **kwargs)", "description": "where(condition, [x, y], /)\n\nReturn elements chosen from `x` or `y` depending on `condition`.\n\n.. note::\n    When only `condition` is provided, this function is a shorthand for\n    ``np.asarray(condition).nonzero()``. Using `nonzero` directly should be\n    preferred, as it behaves correctly for subclasses. The rest of this\n    documentation covers only the case where all three arguments are\n    provided.\n\nParameters\n----------\ncondition : array_like, bool\n    Where True, yield `x`, otherwise yield `y`.\nx, y : array_like\n    Values from which to choose. `x`, `y` and `condition` need to be\n    broadcastable to some shape.\n\nReturns\n-------\nout : ndarray\n    An array with elements from `x` where `condition` is True, and elements\n    from `y` elsewhere.\n\nSee Also\n--------\nchoose\nnonzero : The function that is called when x and y are omitted\n\nNotes\n-----\nIf all the arrays are 1-D, `where` is equivalent to::\n\n    [xv if c else yv\n     for c, xv, yv in zip(condition, x, y)]\n\nExamples\n--------\n>>> a = np.arange(10)\n>>> a\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n>>> np.where(a < 5, a, 10*a)\narray([ 0,  1,  2,  3,  4, 50, 60, 70, 80, 90])\n\nThis can be used on multidimensional arrays too:\n\n>>> np.where([[True, False], [True, True]],\n...          [[1, 2], [3, 4]],\n...          [[9, 8], [7, 6]])\narray([[1, 8],\n       [3, 4]])\n\nThe shapes of x, y, and the condition are broadcast together:\n\n>>> x, y = np.ogrid[:3, :4]\n>>> np.where(x < y, x, 10 + y)  # both x and 10+y are broadcast\narray([[10,  0,  0,  0],\n       [10, 11,  1,  1],\n       [10, 11, 12,  2]])\n\n>>> a = np.array([[0, 1, 2],\n...               [0, 2, 4],\n...               [0, 3, 6]])\n>>> np.where(a < 4, a, -1)  # -1 is broadcast\narray([[ 0,  1,  2],\n       [ 0,  2, -1],\n       [ 0,  3, -1]])", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/654", "data": {"name": "numpy.linspace", "type": "callable", "signature": "(start, stop, num=50)", "description": "Return evenly spaced numbers over a specified interval.\n\nReturns `num` evenly spaced samples, calculated over the\ninterval [`start`, `stop`].\n\nThe endpoint of the interval can optionally be excluded.\n\n.. versionchanged:: 1.16.0\n    Non-scalar `start` and `stop` are now supported.\n\n.. versionchanged:: 1.20.0\n    Values are rounded towards ``-inf`` instead of ``0`` when an\n    integer ``dtype`` is specified. The old behavior can\n    still be obtained with ``np.linspace(start, stop, num).astype(int)``\n\nParameters\n----------\nstart : array_like\n    The starting value of the sequence.\nstop : array_like\n    The end value of the sequence, unless `endpoint` is set to False.\n    In that case, the sequence consists of all but the last of ``num + 1``\n    evenly spaced samples, so that `stop` is excluded.  Note that the step\n    size changes when `endpoint` is False.\nnum : int, optional\n    Number of samples to generate. Default is 50. Must be non-negative.\nendpoint : bool, optional\n    If True, `stop` is the last sample. Otherwise, it is not included.\n    Default is True.\nretstep : bool, optional\n    If True, return (`samples`, `step`), where `step` is the spacing\n    between samples.\ndtype : dtype, optional\n    The type of the output array.  If `dtype` is not given, the data type\n    is inferred from `start` and `stop`. The inferred dtype will never be\n    an integer; `float` is chosen even if the arguments would produce an\n    array of integers.\n\n    .. versionadded:: 1.9.0\n\naxis : int, optional\n    The axis in the result to store the samples.  Relevant only if start\n    or stop are array-like.  By default (0), the samples will be along a\n    new axis inserted at the beginning. Use -1 to get an axis at the end.\n\n    .. versionadded:: 1.16.0\n\nReturns\n-------\nsamples : ndarray\n    There are `num` equally spaced samples in the closed interval\n    ``[start, stop]`` or the half-open interval ``[start, stop)``\n    (depending on whether `endpoint` is True or False).\nstep : float, optional\n    Only returned if `retstep` is True\n\n    Size of spacing between samples.\n\n\nSee Also\n--------\narange : Similar to `linspace`, but uses a step size (instead of the\n         number of samples).\ngeomspace : Similar to `linspace`, but with numbers spaced evenly on a log\n            scale (a geometric progression).\nlogspace : Similar to `geomspace`, but with the end points specified as\n           logarithms.\n\nExamples\n--------\n>>> np.linspace(2.0, 3.0, num=5)\narray([2.  , 2.25, 2.5 , 2.75, 3.  ])\n>>> np.linspace(2.0, 3.0, num=5, endpoint=False)\narray([2. ,  2.2,  2.4,  2.6,  2.8])\n>>> np.linspace(2.0, 3.0, num=5, retstep=True)\n(array([2.  ,  2.25,  2.5 ,  2.75,  3.  ]), 0.25)\n\nGraphical illustration:\n\n>>> import matplotlib.pyplot as plt\n>>> N = 8\n>>> y = np.zeros(N)\n>>> x1 = np.linspace(0, 10, N, endpoint=True)\n>>> x2 = np.linspace(0, 10, N, endpoint=False)\n>>> plt.plot(x1, y, 'o')\n[<matplotlib.lines.Line2D object at 0x...>]\n>>> plt.plot(x2, y + 0.5, 'o')\n[<matplotlib.lines.Line2D object at 0x...>]\n>>> plt.ylim([-0.5, 1])\n(-0.5, 1)\n>>> plt.show()", "parameters": {"type": "object", "properties": {"start": {}, "stop": {}, "num": {"type": "integer", "default": 50}}}}}
{"task_id": "BigCodeBench/654", "data": {"name": "scipy.optimize.curve_fit", "type": "callable", "signature": "(f, xdata, ydata, p0=None, **kwargs)", "description": "Use non-linear least squares to fit a function, f, to data.\n\nAssumes ``ydata = f(xdata, *params) + eps``.\n\nParameters\n----------\nf : callable\n    The model function, f(x, ...). It must take the independent\n    variable as the first argument and the parameters to fit as\n    separate remaining arguments.\nxdata : array_like\n    The independent variable where the data is measured.\n    Should usually be an M-length sequence or an (k,M)-shaped array for\n    functions with k predictors, and each element should be float\n    convertible if it is an array like object.\nydata : array_like\n    The dependent data, a length M array - nominally ``f(xdata, ...)``.\np0 : array_like, optional\n    Initial guess for the parameters (length N). If None, then the\n    initial values will all be 1 (if the number of parameters for the\n    function can be determined using introspection, otherwise a\n    ValueError is raised).\nsigma : None or scalar or M-length sequence or MxM array, optional\n    Determines the uncertainty in `ydata`. If we define residuals as\n    ``r = ydata - f(xdata, *popt)``, then the interpretation of `sigma`\n    depends on its number of dimensions:\n\n        - A scalar or 1-D `sigma` should contain values of standard deviations of\n          errors in `ydata`. In this case, the optimized function is\n          ``chisq = sum((r / sigma) ** 2)``.\n\n        - A 2-D `sigma` should contain the covariance matrix of\n          errors in `ydata`. In this case, the optimized function is\n          ``chisq = r.T @ inv(sigma) @ r``.\n\n          .. versionadded:: 0.19\n\n    None (default) is equivalent of 1-D `sigma` filled with ones.\nabsolute_sigma : bool, optional\n    If True, `sigma` is used in an absolute sense and the estimated parameter\n    covariance `pcov` reflects these absolute values.\n\n    If False (default), only the relative magnitudes of the `sigma` values matter.\n    The returned parameter covariance matrix `pcov` is based on scaling\n    `sigma` by a constant factor. This constant is set by demanding that the\n    reduced `chisq` for the optimal parameters `popt` when using the\n    *scaled* `sigma` equals unity. In other words, `sigma` is scaled to\n    match the sample variance of the residuals after the fit. Default is False.\n    Mathematically,\n    ``pcov(absolute_sigma=False) = pcov(absolute_sigma=True) * chisq(popt)/(M-N)``\ncheck_finite : bool, optional\n    If True, check that the input arrays do not contain nans of infs,\n    and raise a ValueError if they do. Setting this parameter to\n    False may silently produce nonsensical results if the input arrays\n    do contain nans. Default is True if `nan_policy` is not specified\n    explicitly and False otherwise.\nbounds : 2-tuple of array_like or `Bounds`, optional\n    Lower and upper bounds on parameters. Defaults to no bounds.\n    There are two ways to specify the bounds:\n\n        - Instance of `Bounds` class.\n\n        - 2-tuple of array_like: Each element of the tuple must be either\n          an array with the length equal to the number of parameters, or a\n          scalar (in which case the bound is taken to be the same for all\n          parameters). Use ``np.inf`` with an appropriate sign to disable\n          bounds on all or some parameters.\n\nmethod : {'lm', 'trf', 'dogbox'}, optional\n    Method to use for optimization. See `least_squares` for more details.\n    Default is 'lm' for unconstrained problems and 'trf' if `bounds` are\n    provided. The method 'lm' won't work when the number of observations\n    is less than the number of variables, use 'trf' or 'dogbox' in this\n    case.\n\n    .. versionadded:: 0.17\njac : callable, string or None, optional\n    Function with signature ``jac(x, ...)`` which computes the Jacobian\n    matrix of the model function with respect to parameters as a dense\n    array_like structure. It will be scaled according to provided `sigma`.\n    If None (default), the Jacobian will be estimated numerically.\n    String keywords for 'trf' and 'dogbox' methods can be used to select\n    a finite difference scheme, see `least_squares`.\n\n    .. versionadded:: 0.18\nfull_output : boolean, optional\n    If True, this function returns additioal information: `infodict`,\n    `mesg`, and `ier`.\n\n    .. versionadded:: 1.9\nnan_policy : {'raise', 'omit', None}, optional\n    Defines how to handle when input contains nan.\n    The following options are available (default is None):\n\n      * 'raise': throws an error\n      * 'omit': performs the calculations ignoring nan values\n      * None: no special handling of NaNs is performed\n        (except what is done by check_finite); the behavior when NaNs\n        are present is implementation-dependent and may change.\n\n    Note that if this value is specified explicitly (not None),\n    `check_finite` will be set as False.\n\n    .. versionadded:: 1.11\n**kwargs\n    Keyword arguments passed to `leastsq` for ``method='lm'`` or\n    `least_squares` otherwise.\n\nReturns\n-------\npopt : array\n    Optimal values for the parameters so that the sum of the squared\n    residuals of ``f(xdata, *popt) - ydata`` is minimized.\npcov : 2-D array\n    The estimated approximate covariance of popt. The diagonals provide\n    the variance of the parameter estimate. To compute one standard\n    deviation errors on the parameters, use\n    ``perr = np.sqrt(np.diag(pcov))``. Note that the relationship between\n    `cov` and parameter error estimates is derived based on a linear\n    approximation to the model function around the optimum [1].\n    When this approximation becomes inaccurate, `cov` may not provide an\n    accurate measure of uncertainty.\n\n    How the `sigma` parameter affects the estimated covariance\n    depends on `absolute_sigma` argument, as described above.\n\n    If the Jacobian matrix at the solution doesn't have a full rank, then\n    'lm' method returns a matrix filled with ``np.inf``, on the other hand\n    'trf'  and 'dogbox' methods use Moore-Penrose pseudoinverse to compute\n    the covariance matrix. Covariance matrices with large condition numbers\n    (e.g. computed with `numpy.linalg.cond`) may indicate that results are\n    unreliable.\ninfodict : dict (returned only if `full_output` is True)\n    a dictionary of optional outputs with the keys:\n\n    ``nfev``\n        The number of function calls. Methods 'trf' and 'dogbox' do not\n        count function calls for numerical Jacobian approximation,\n        as opposed to 'lm' method.\n    ``fvec``\n        The residual values evaluated at the solution, for a 1-D `sigma`\n        this is ``(f(x, *popt) - ydata)/sigma``.\n    ``fjac``\n        A permutation of the R matrix of a QR\n        factorization of the final approximate\n        Jacobian matrix, stored column wise.\n        Together with ipvt, the covariance of the\n        estimate can be approximated.\n        Method 'lm' only provides this information.\n    ``ipvt``\n        An integer array of length N which defines\n        a permutation matrix, p, such that\n        fjac*p = q*r, where r is upper triangular\n        with diagonal elements of nonincreasing\n        magnitude. Column j of p is column ipvt(j)\n        of the identity matrix.\n        Method 'lm' only provides this information.\n    ``qtf``\n        The vector (transpose(q) * fvec).\n        Method 'lm' only provides this information.\n\n    .. versionadded:: 1.9\nmesg : str (returned only if `full_output` is True)\n    A string message giving information about the solution.\n\n    .. versionadded:: 1.9\nier : int (returned only if `full_output` is True)\n    An integer flag. If it is equal to 1, 2, 3 or 4, the solution was\n    found. Otherwise, the solution was not found. In either case, the\n    optional output variable `mesg` gives more information.\n\n    .. versionadded:: 1.9\n\nRaises\n------\nValueError\n    if either `ydata` or `xdata` contain NaNs, or if incompatible options\n    are used.\n\nRuntimeError\n    if the least-squares minimization fails.\n\nOptimizeWarning\n    if covariance of the parameters can not be estimated.\n\nSee Also\n--------\nleast_squares : Minimize the sum of squares of nonlinear functions.\nscipy.stats.linregress : Calculate a linear least squares regression for\n                         two sets of measurements.\n\nNotes\n-----\nUsers should ensure that inputs `xdata`, `ydata`, and the output of `f`\nare ``float64``, or else the optimization may return incorrect results.\n\nWith ``method='lm'``, the algorithm uses the Levenberg-Marquardt algorithm\nthrough `leastsq`. Note that this algorithm can only deal with\nunconstrained problems.\n\nBox constraints can be handled by methods 'trf' and 'dogbox'. Refer to\nthe docstring of `least_squares` for more information.\n\nParameters to be fitted must have similar scale. Differences of multiple\norders of magnitude can lead to incorrect results. For the 'trf' and\n'dogbox' methods, the `x_scale` keyword argument can be used to scale\nthe parameters.\n\nReferences\n----------\n[1] K. Vugrin et al. Confidence region estimation techniques for nonlinear\n    regression in groundwater flow: Three case studies. Water Resources\n    Research, Vol. 43, W03423, :doi:`10.1029/2005WR004804`\n\nExamples\n--------\n>>> import numpy as np\n>>> import matplotlib.pyplot as plt\n>>> from scipy.optimize import curve_fit\n\n>>> def func(x, a, b, c):\n...     return a * np.exp(-b * x) + c\n\nDefine the data to be fit with some noise:\n\n>>> xdata = np.linspace(0, 4, 50)\n>>> y = func(xdata, 2.5, 1.3, 0.5)\n>>> rng = np.random.default_rng()\n>>> y_noise = 0.2 * rng.normal(size=xdata.size)\n>>> ydata = y + y_noise\n>>> plt.plot(xdata, ydata, 'b-', label='data')\n\nFit for the parameters a, b, c of the function `func`:\n\n>>> popt, pcov = curve_fit(func, xdata, ydata)\n>>> popt\narray([2.56274217, 1.37268521, 0.47427475])\n>>> plt.plot(xdata, func(xdata, *popt), 'r-',\n...          label='fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt))\n\nConstrain the optimization to the region of ``0 <= a <= 3``,\n``0 <= b <= 1`` and ``0 <= c <= 0.5``:\n\n>>> popt, pcov = curve_fit(func, xdata, ydata, bounds=(0, [3., 1., 0.5]))\n>>> popt\narray([2.43736712, 1.        , 0.34463856])\n>>> plt.plot(xdata, func(xdata, *popt), 'g--',\n...          label='fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt))\n\n>>> plt.xlabel('x')\n>>> plt.ylabel('y')\n>>> plt.legend()\n>>> plt.show()\n\nFor reliable results, the model `func` should not be overparametrized;\nredundant parameters can cause unreliable covariance matrices and, in some\ncases, poorer quality fits. As a quick check of whether the model may be\noverparameterized, calculate the condition number of the covariance matrix:\n\n>>> np.linalg.cond(pcov)\n34.571092161547405  # may vary\n\nThe value is small, so it does not raise much concern. If, however, we were\nto add a fourth parameter ``d`` to `func` with the same effect as ``a``:\n\n>>> def func2(x, a, b, c, d):\n...     return a * d * np.exp(-b * x) + c  # a and d are redundant\n>>> popt, pcov = curve_fit(func2, xdata, ydata)\n>>> np.linalg.cond(pcov)\n1.13250718925596e+32  # may vary\n\nSuch a large value is cause for concern. The diagonal elements of the\ncovariance matrix, which is related to uncertainty of the fit, gives more\ninformation:\n\n>>> np.diag(pcov)\narray([1.48814742e+29, 3.78596560e-02, 5.39253738e-03, 2.76417220e+28])  # may vary\n\nNote that the first and last terms are much larger than the other elements,\nsuggesting that the optimal values of these parameters are ambiguous and\nthat only one of these parameters is needed in the model.\n\nIf the optimal parameters of `f` differ by multiple orders of magnitude, the\nresulting fit can be inaccurate. Sometimes, `curve_fit` can fail to find any\nresults:\n\n>>> ydata = func(xdata, 500000, 0.01, 15)\n>>> try:\n...     popt, pcov = curve_fit(func, xdata, ydata, method = 'trf')\n... except RuntimeError as e:\n...     print(e)\nOptimal parameters not found: The maximum number of function evaluations is exceeded.\n\nIf parameter scale is roughly known beforehand, it can be defined in\n`x_scale` argument:\n\n>>> popt, pcov = curve_fit(func, xdata, ydata, method = 'trf',\n...                        x_scale = [1000, 1, 1])\n>>> popt\narray([5.00000000e+05, 1.00000000e-02, 1.49999999e+01])", "parameters": {"type": "object", "properties": {"f": {}, "xdata": {}, "ydata": {}, "p0": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/655", "data": {"name": "NMF(n_components=num_topics, random_state=1).fit(tfidf).components_"}}
{"task_id": "BigCodeBench/655", "data": {"name": "nltk.corpus.stopwords", "type": "constant", "signature": null, "description": "To see the API documentation for this lazily loaded corpus, first\nrun corpus.ensure_loaded(), and then run help(this_corpus).\n\nLazyCorpusLoader is a proxy object which is used to stand in for a\ncorpus object before the corpus is loaded.  This allows NLTK to\ncreate an object for each corpus, but defer the costs associated\nwith loading those corpora until the first time that they're\nactually accessed.\n\nThe first time this object is accessed in any way, it will load\nthe corresponding corpus, and transform itself into that corpus\n(by modifying its own ``__class__`` and ``__dict__`` attributes).\n\nIf the corpus can not be found, then accessing this object will\nraise an exception, displaying installation instructions for the\nNLTK data package.  Once they've properly installed the data\npackage (or modified ``nltk.data.path`` to point to its location),\nthey can then use the corpus object without restarting python.\n\n:param name: The name of the corpus\n:type name: str\n:param reader_cls: The specific CorpusReader class, e.g. PlaintextCorpusReader, WordListCorpusReader\n:type reader: nltk.corpus.reader.api.CorpusReader\n:param nltk_data_subdir: The subdirectory where the corpus is stored.\n:type nltk_data_subdir: str\n:param `*args`: Any other non-keywords arguments that `reader_cls` might need.\n:param `**kwargs`: Any other keywords arguments that `reader_cls` might need.", "value": "<WordListCorpusReader in '.../corpora/stopwords' (not loaded yet)>", "parameters": {"type": "constant"}}}
{"task_id": "BigCodeBench/655", "data": {"name": "nltk.corpus.stopwords.words", "type": "callable", "signature": "(fileids=None)", "description": "", "parameters": {"type": "object", "properties": {"fileids": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/655", "data": {"name": "nltk.download", "type": "callable", "signature": "(info_or_id=None)", "description": "", "parameters": {"type": "object", "properties": {"info_or_id": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/655", "data": {"name": "re.compile", "type": "callable", "signature": "(pattern)", "description": "Compile a regular expression pattern, returning a Pattern object.", "parameters": {"type": "object", "properties": {"pattern": {}}}}}
{"task_id": "BigCodeBench/655", "data": {"name": "re.compile.sub", "type": "callable", "signature": "(pattern, flags=0)", "description": "Compile a regular expression pattern, returning a Pattern object.", "parameters": {"type": "object", "properties": {"pattern": {}, "flags": {"type": "integer", "default": 0}}}}}
{"task_id": "BigCodeBench/655", "data": {"name": "sklearn.decomposition.NMF", "type": "class", "signature": "(n_components='warn', random_state=None)", "description": "Non-Negative Matrix Factorization (NMF).\n\nFind two non-negative matrices, i.e. matrices with all non-negative elements, (W, H)\nwhose product approximates the non-negative matrix X. This factorization can be used\nfor example for dimensionality reduction, source separation or topic extraction.\n\nThe objective function is:\n\n    .. math::\n\n        L(W, H) &= 0.5 * ||X - WH||_{loss}^2\n\n        &+ alpha\\_W * l1\\_ratio * n\\_features * ||vec(W)||_1\n\n        &+ alpha\\_H * l1\\_ratio * n\\_samples * ||vec(H)||_1\n\n        &+ 0.5 * alpha\\_W * (1 - l1\\_ratio) * n\\_features * ||W||_{Fro}^2\n\n        &+ 0.5 * alpha\\_H * (1 - l1\\_ratio) * n\\_samples * ||H||_{Fro}^2\n\nWhere:\n\n:math:`||A||_{Fro}^2 = \\sum_{i,j} A_{ij}^2` (Frobenius norm)\n\n:math:`||vec(A)||_1 = \\sum_{i,j} abs(A_{ij})` (Elementwise L1 norm)\n\nThe generic norm :math:`||X - WH||_{loss}` may represent\nthe Frobenius norm or another supported beta-divergence loss.\nThe choice between options is controlled by the `beta_loss` parameter.\n\nThe regularization terms are scaled by `n_features` for `W` and by `n_samples` for\n`H` to keep their impact balanced with respect to one another and to the data fit\nterm as independent as possible of the size `n_samples` of the training set.\n\nThe objective function is minimized with an alternating minimization of W\nand H.\n\nNote that the transformed data is named W and the components matrix is named H. In\nthe NMF literature, the naming convention is usually the opposite since the data\nmatrix X is transposed.\n\nRead more in the :ref:`User Guide <NMF>`.\n\nParameters\n----------\nn_components : int or {'auto'} or None, default=None\n    Number of components, if n_components is not set all features\n    are kept.\n    If `n_components='auto'`, the number of components is automatically inferred\n    from W or H shapes.\n\n    .. versionchanged:: 1.4\n        Added `'auto'` value.\n\ninit : {'random', 'nndsvd', 'nndsvda', 'nndsvdar', 'custom'}, default=None\n    Method used to initialize the procedure.\n    Valid options:\n\n    - `None`: 'nndsvda' if n_components <= min(n_samples, n_features),\n      otherwise random.\n\n    - `'random'`: non-negative random matrices, scaled with:\n      `sqrt(X.mean() / n_components)`\n\n    - `'nndsvd'`: Nonnegative Double Singular Value Decomposition (NNDSVD)\n      initialization (better for sparseness)\n\n    - `'nndsvda'`: NNDSVD with zeros filled with the average of X\n      (better when sparsity is not desired)\n\n    - `'nndsvdar'` NNDSVD with zeros filled with small random values\n      (generally faster, less accurate alternative to NNDSVDa\n      for when sparsity is not desired)\n\n    - `'custom'`: Use custom matrices `W` and `H` which must both be provided.\n\n    .. versionchanged:: 1.1\n        When `init=None` and n_components is less than n_samples and n_features\n        defaults to `nndsvda` instead of `nndsvd`.\n\nsolver : {'cd', 'mu'}, default='cd'\n    Numerical solver to use:\n\n    - 'cd' is a Coordinate Descent solver.\n    - 'mu' is a Multiplicative Update solver.\n\n    .. versionadded:: 0.17\n       Coordinate Descent solver.\n\n    .. versionadded:: 0.19\n       Multiplicative Update solver.\n\nbeta_loss : float or {'frobenius', 'kullback-leibler',             'itakura-saito'}, default='frobenius'\n    Beta divergence to be minimized, measuring the distance between X\n    and the dot product WH. Note that values different from 'frobenius'\n    (or 2) and 'kullback-leibler' (or 1) lead to significantly slower\n    fits. Note that for beta_loss <= 0 (or 'itakura-saito'), the input\n    matrix X cannot contain zeros. Used only in 'mu' solver.\n\n    .. versionadded:: 0.19\n\ntol : float, default=1e-4\n    Tolerance of the stopping condition.\n\nmax_iter : int, default=200\n    Maximum number of iterations before timing out.\n\nrandom_state : int, RandomState instance or None, default=None\n    Used for initialisation (when ``init`` == 'nndsvdar' or\n    'random'), and in Coordinate Descent. Pass an int for reproducible\n    results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nalpha_W : float, default=0.0\n    Constant that multiplies the regularization terms of `W`. Set it to zero\n    (default) to have no regularization on `W`.\n\n    .. versionadded:: 1.0\n\nalpha_H : float or \"same\", default=\"same\"\n    Constant that multiplies the regularization terms of `H`. Set it to zero to\n    have no regularization on `H`. If \"same\" (default), it takes the same value as\n    `alpha_W`.\n\n    .. versionadded:: 1.0\n\nl1_ratio : float, default=0.0\n    The regularization mixing parameter, with 0 <= l1_ratio <= 1.\n    For l1_ratio = 0 the penalty is an elementwise L2 penalty\n    (aka Frobenius Norm).\n    For l1_ratio = 1 it is an elementwise L1 penalty.\n    For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2.\n\n    .. versionadded:: 0.17\n       Regularization parameter *l1_ratio* used in the Coordinate Descent\n       solver.\n\nverbose : int, default=0\n    Whether to be verbose.\n\nshuffle : bool, default=False\n    If true, randomize the order of coordinates in the CD solver.\n\n    .. versionadded:: 0.17\n       *shuffle* parameter used in the Coordinate Descent solver.\n\nAttributes\n----------\ncomponents_ : ndarray of shape (n_components, n_features)\n    Factorization matrix, sometimes called 'dictionary'.\n\nn_components_ : int\n    The number of components. It is same as the `n_components` parameter\n    if it was given. Otherwise, it will be same as the number of\n    features.\n\nreconstruction_err_ : float\n    Frobenius norm of the matrix difference, or beta-divergence, between\n    the training data ``X`` and the reconstructed data ``WH`` from\n    the fitted model.\n\nn_iter_ : int\n    Actual number of iterations.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nDictionaryLearning : Find a dictionary that sparsely encodes data.\nMiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\nPCA : Principal component analysis.\nSparseCoder : Find a sparse representation of data from a fixed,\n    precomputed dictionary.\nSparsePCA : Sparse Principal Components Analysis.\nTruncatedSVD : Dimensionality reduction using truncated SVD.\n\nReferences\n----------\n.. [1] :doi:`\"Fast local algorithms for large scale nonnegative matrix and tensor\n   factorizations\" <10.1587/transfun.E92.A.708>`\n   Cichocki, Andrzej, and P. H. A. N. Anh-Huy. IEICE transactions on fundamentals\n   of electronics, communications and computer sciences 92.3: 708-721, 2009.\n\n.. [2] :doi:`\"Algorithms for nonnegative matrix factorization with the\n   beta-divergence\" <10.1162/NECO_a_00168>`\n   Fevotte, C., & Idier, J. (2011). Neural Computation, 23(9).\n\nExamples\n--------\n>>> import numpy as np\n>>> X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n>>> from sklearn.decomposition import NMF\n>>> model = NMF(n_components=2, init='random', random_state=0)\n>>> W = model.fit_transform(X)\n>>> H = model.components_", "parameters": {"type": "object", "properties": {"n_components": {"type": "str", "default": "warn"}, "random_state": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/655", "data": {"name": "sklearn.feature_extraction.text.TfidfVectorizer", "type": "class", "signature": "(stop_words=None, max_df=1.0, min_df=1)", "description": "Convert a collection of raw documents to a matrix of TF-IDF features.\n\nEquivalent to :class:`CountVectorizer` followed by\n:class:`TfidfTransformer`.\n\nFor an example of usage, see\n:ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`.\n\nFor an efficiency comparison of the different feature extractors, see\n:ref:`sphx_glr_auto_examples_text_plot_hashing_vs_dict_vectorizer.py`.\n\nRead more in the :ref:`User Guide <text_feature_extraction>`.\n\nParameters\n----------\ninput : {'filename', 'file', 'content'}, default='content'\n    - If `'filename'`, the sequence passed as an argument to fit is\n      expected to be a list of filenames that need reading to fetch\n      the raw content to analyze.\n\n    - If `'file'`, the sequence items must have a 'read' method (file-like\n      object) that is called to fetch the bytes in memory.\n\n    - If `'content'`, the input is expected to be a sequence of items that\n      can be of type string or byte.\n\nencoding : str, default='utf-8'\n    If bytes or files are given to analyze, this encoding is used to\n    decode.\n\ndecode_error : {'strict', 'ignore', 'replace'}, default='strict'\n    Instruction on what to do if a byte sequence is given to analyze that\n    contains characters not of the given `encoding`. By default, it is\n    'strict', meaning that a UnicodeDecodeError will be raised. Other\n    values are 'ignore' and 'replace'.\n\nstrip_accents : {'ascii', 'unicode'} or callable, default=None\n    Remove accents and perform other character normalization\n    during the preprocessing step.\n    'ascii' is a fast method that only works on characters that have\n    a direct ASCII mapping.\n    'unicode' is a slightly slower method that works on any characters.\n    None (default) means no character normalization is performed.\n\n    Both 'ascii' and 'unicode' use NFKD normalization from\n    :func:`unicodedata.normalize`.\n\nlowercase : bool, default=True\n    Convert all characters to lowercase before tokenizing.\n\npreprocessor : callable, default=None\n    Override the preprocessing (string transformation) stage while\n    preserving the tokenizing and n-grams generation steps.\n    Only applies if ``analyzer`` is not callable.\n\ntokenizer : callable, default=None\n    Override the string tokenization step while preserving the\n    preprocessing and n-grams generation steps.\n    Only applies if ``analyzer == 'word'``.\n\nanalyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n    Whether the feature should be made of word or character n-grams.\n    Option 'char_wb' creates character n-grams only from text inside\n    word boundaries; n-grams at the edges of words are padded with space.\n\n    If a callable is passed it is used to extract the sequence of features\n    out of the raw, unprocessed input.\n\n    .. versionchanged:: 0.21\n        Since v0.21, if ``input`` is ``'filename'`` or ``'file'``, the data\n        is first read from the file and then passed to the given callable\n        analyzer.\n\nstop_words : {'english'}, list, default=None\n    If a string, it is passed to _check_stop_list and the appropriate stop\n    list is returned. 'english' is currently the only supported string\n    value.\n    There are several known issues with 'english' and you should\n    consider an alternative (see :ref:`stop_words`).\n\n    If a list, that list is assumed to contain stop words, all of which\n    will be removed from the resulting tokens.\n    Only applies if ``analyzer == 'word'``.\n\n    If None, no stop words will be used. In this case, setting `max_df`\n    to a higher value, such as in the range (0.7, 1.0), can automatically detect\n    and filter stop words based on intra corpus document frequency of terms.\n\ntoken_pattern : str, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n    Regular expression denoting what constitutes a \"token\", only used\n    if ``analyzer == 'word'``. The default regexp selects tokens of 2\n    or more alphanumeric characters (punctuation is completely ignored\n    and always treated as a token separator).\n\n    If there is a capturing group in token_pattern then the\n    captured group content, not the entire match, becomes the token.\n    At most one capturing group is permitted.\n\nngram_range : tuple (min_n, max_n), default=(1, 1)\n    The lower and upper boundary of the range of n-values for different\n    n-grams to be extracted. All values of n such that min_n <= n <= max_n\n    will be used. For example an ``ngram_range`` of ``(1, 1)`` means only\n    unigrams, ``(1, 2)`` means unigrams and bigrams, and ``(2, 2)`` means\n    only bigrams.\n    Only applies if ``analyzer`` is not callable.\n\nmax_df : float or int, default=1.0\n    When building the vocabulary ignore terms that have a document\n    frequency strictly higher than the given threshold (corpus-specific\n    stop words).\n    If float in range [0.0, 1.0], the parameter represents a proportion of\n    documents, integer absolute counts.\n    This parameter is ignored if vocabulary is not None.\n\nmin_df : float or int, default=1\n    When building the vocabulary ignore terms that have a document\n    frequency strictly lower than the given threshold. This value is also\n    called cut-off in the literature.\n    If float in range of [0.0, 1.0], the parameter represents a proportion\n    of documents, integer absolute counts.\n    This parameter is ignored if vocabulary is not None.\n\nmax_features : int, default=None\n    If not None, build a vocabulary that only consider the top\n    `max_features` ordered by term frequency across the corpus.\n    Otherwise, all features are used.\n\n    This parameter is ignored if vocabulary is not None.\n\nvocabulary : Mapping or iterable, default=None\n    Either a Mapping (e.g., a dict) where keys are terms and values are\n    indices in the feature matrix, or an iterable over terms. If not\n    given, a vocabulary is determined from the input documents.\n\nbinary : bool, default=False\n    If True, all non-zero term counts are set to 1. This does not mean\n    outputs will have only 0/1 values, only that the tf term in tf-idf\n    is binary. (Set `binary` to True, `use_idf` to False and\n    `norm` to None to get 0/1 outputs).\n\ndtype : dtype, default=float64\n    Type of the matrix returned by fit_transform() or transform().\n\nnorm : {'l1', 'l2'} or None, default='l2'\n    Each output row will have unit norm, either:\n\n    - 'l2': Sum of squares of vector elements is 1. The cosine\n      similarity between two vectors is their dot product when l2 norm has\n      been applied.\n    - 'l1': Sum of absolute values of vector elements is 1.\n      See :func:`~sklearn.preprocessing.normalize`.\n    - None: No normalization.\n\nuse_idf : bool, default=True\n    Enable inverse-document-frequency reweighting. If False, idf(t) = 1.\n\nsmooth_idf : bool, default=True\n    Smooth idf weights by adding one to document frequencies, as if an\n    extra document was seen containing every term in the collection\n    exactly once. Prevents zero divisions.\n\nsublinear_tf : bool, default=False\n    Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).\n\nAttributes\n----------\nvocabulary_ : dict\n    A mapping of terms to feature indices.\n\nfixed_vocabulary_ : bool\n    True if a fixed vocabulary of term to indices mapping\n    is provided by the user.\n\nidf_ : array of shape (n_features,)\n    The inverse document frequency (IDF) vector; only defined\n    if ``use_idf`` is True.\n\nstop_words_ : set\n    Terms that were ignored because they either:\n\n      - occurred in too many documents (`max_df`)\n      - occurred in too few documents (`min_df`)\n      - were cut off by feature selection (`max_features`).\n\n    This is only available if no vocabulary was given.\n\nSee Also\n--------\nCountVectorizer : Transforms text into a sparse matrix of n-gram counts.\n\nTfidfTransformer : Performs the TF-IDF transformation from a provided\n    matrix of counts.\n\nNotes\n-----\nThe ``stop_words_`` attribute can get large and increase the model size\nwhen pickling. This attribute is provided only for introspection and can\nbe safely removed using delattr or set to None before pickling.\n\nExamples\n--------\n>>> from sklearn.feature_extraction.text import TfidfVectorizer\n>>> corpus = [\n...     'This is the first document.',\n...     'This document is the second document.',\n...     'And this is the third one.',\n...     'Is this the first document?',\n... ]\n>>> vectorizer = TfidfVectorizer()\n>>> X = vectorizer.fit_transform(corpus)\n>>> vectorizer.get_feature_names_out()\narray(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n       'this'], ...)\n>>> print(X.shape)\n(4, 9)", "parameters": {"type": "object", "properties": {"stop_words": {"type": "NoneType", "default": null}, "max_df": {"type": "float", "default": 1.0}, "min_df": {"type": "integer", "default": 1}}}}}
{"task_id": "BigCodeBench/655", "data": {"name": "sklearn.feature_extraction.text.TfidfVectorizer.get_feature_names", "type": "class", "signature": "()", "description": "Convert a collection of raw documents to a matrix of TF-IDF features.\n\nEquivalent to :class:`CountVectorizer` followed by\n:class:`TfidfTransformer`.\n\nFor an example of usage, see\n:ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`.\n\nFor an efficiency comparison of the different feature extractors, see\n:ref:`sphx_glr_auto_examples_text_plot_hashing_vs_dict_vectorizer.py`.\n\nRead more in the :ref:`User Guide <text_feature_extraction>`.\n\nParameters\n----------\ninput : {'filename', 'file', 'content'}, default='content'\n    - If `'filename'`, the sequence passed as an argument to fit is\n      expected to be a list of filenames that need reading to fetch\n      the raw content to analyze.\n\n    - If `'file'`, the sequence items must have a 'read' method (file-like\n      object) that is called to fetch the bytes in memory.\n\n    - If `'content'`, the input is expected to be a sequence of items that\n      can be of type string or byte.\n\nencoding : str, default='utf-8'\n    If bytes or files are given to analyze, this encoding is used to\n    decode.\n\ndecode_error : {'strict', 'ignore', 'replace'}, default='strict'\n    Instruction on what to do if a byte sequence is given to analyze that\n    contains characters not of the given `encoding`. By default, it is\n    'strict', meaning that a UnicodeDecodeError will be raised. Other\n    values are 'ignore' and 'replace'.\n\nstrip_accents : {'ascii', 'unicode'} or callable, default=None\n    Remove accents and perform other character normalization\n    during the preprocessing step.\n    'ascii' is a fast method that only works on characters that have\n    a direct ASCII mapping.\n    'unicode' is a slightly slower method that works on any characters.\n    None (default) means no character normalization is performed.\n\n    Both 'ascii' and 'unicode' use NFKD normalization from\n    :func:`unicodedata.normalize`.\n\nlowercase : bool, default=True\n    Convert all characters to lowercase before tokenizing.\n\npreprocessor : callable, default=None\n    Override the preprocessing (string transformation) stage while\n    preserving the tokenizing and n-grams generation steps.\n    Only applies if ``analyzer`` is not callable.\n\ntokenizer : callable, default=None\n    Override the string tokenization step while preserving the\n    preprocessing and n-grams generation steps.\n    Only applies if ``analyzer == 'word'``.\n\nanalyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n    Whether the feature should be made of word or character n-grams.\n    Option 'char_wb' creates character n-grams only from text inside\n    word boundaries; n-grams at the edges of words are padded with space.\n\n    If a callable is passed it is used to extract the sequence of features\n    out of the raw, unprocessed input.\n\n    .. versionchanged:: 0.21\n        Since v0.21, if ``input`` is ``'filename'`` or ``'file'``, the data\n        is first read from the file and then passed to the given callable\n        analyzer.\n\nstop_words : {'english'}, list, default=None\n    If a string, it is passed to _check_stop_list and the appropriate stop\n    list is returned. 'english' is currently the only supported string\n    value.\n    There are several known issues with 'english' and you should\n    consider an alternative (see :ref:`stop_words`).\n\n    If a list, that list is assumed to contain stop words, all of which\n    will be removed from the resulting tokens.\n    Only applies if ``analyzer == 'word'``.\n\n    If None, no stop words will be used. In this case, setting `max_df`\n    to a higher value, such as in the range (0.7, 1.0), can automatically detect\n    and filter stop words based on intra corpus document frequency of terms.\n\ntoken_pattern : str, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n    Regular expression denoting what constitutes a \"token\", only used\n    if ``analyzer == 'word'``. The default regexp selects tokens of 2\n    or more alphanumeric characters (punctuation is completely ignored\n    and always treated as a token separator).\n\n    If there is a capturing group in token_pattern then the\n    captured group content, not the entire match, becomes the token.\n    At most one capturing group is permitted.\n\nngram_range : tuple (min_n, max_n), default=(1, 1)\n    The lower and upper boundary of the range of n-values for different\n    n-grams to be extracted. All values of n such that min_n <= n <= max_n\n    will be used. For example an ``ngram_range`` of ``(1, 1)`` means only\n    unigrams, ``(1, 2)`` means unigrams and bigrams, and ``(2, 2)`` means\n    only bigrams.\n    Only applies if ``analyzer`` is not callable.\n\nmax_df : float or int, default=1.0\n    When building the vocabulary ignore terms that have a document\n    frequency strictly higher than the given threshold (corpus-specific\n    stop words).\n    If float in range [0.0, 1.0], the parameter represents a proportion of\n    documents, integer absolute counts.\n    This parameter is ignored if vocabulary is not None.\n\nmin_df : float or int, default=1\n    When building the vocabulary ignore terms that have a document\n    frequency strictly lower than the given threshold. This value is also\n    called cut-off in the literature.\n    If float in range of [0.0, 1.0], the parameter represents a proportion\n    of documents, integer absolute counts.\n    This parameter is ignored if vocabulary is not None.\n\nmax_features : int, default=None\n    If not None, build a vocabulary that only consider the top\n    `max_features` ordered by term frequency across the corpus.\n    Otherwise, all features are used.\n\n    This parameter is ignored if vocabulary is not None.\n\nvocabulary : Mapping or iterable, default=None\n    Either a Mapping (e.g., a dict) where keys are terms and values are\n    indices in the feature matrix, or an iterable over terms. If not\n    given, a vocabulary is determined from the input documents.\n\nbinary : bool, default=False\n    If True, all non-zero term counts are set to 1. This does not mean\n    outputs will have only 0/1 values, only that the tf term in tf-idf\n    is binary. (Set `binary` to True, `use_idf` to False and\n    `norm` to None to get 0/1 outputs).\n\ndtype : dtype, default=float64\n    Type of the matrix returned by fit_transform() or transform().\n\nnorm : {'l1', 'l2'} or None, default='l2'\n    Each output row will have unit norm, either:\n\n    - 'l2': Sum of squares of vector elements is 1. The cosine\n      similarity between two vectors is their dot product when l2 norm has\n      been applied.\n    - 'l1': Sum of absolute values of vector elements is 1.\n      See :func:`~sklearn.preprocessing.normalize`.\n    - None: No normalization.\n\nuse_idf : bool, default=True\n    Enable inverse-document-frequency reweighting. If False, idf(t) = 1.\n\nsmooth_idf : bool, default=True\n    Smooth idf weights by adding one to document frequencies, as if an\n    extra document was seen containing every term in the collection\n    exactly once. Prevents zero divisions.\n\nsublinear_tf : bool, default=False\n    Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).\n\nAttributes\n----------\nvocabulary_ : dict\n    A mapping of terms to feature indices.\n\nfixed_vocabulary_ : bool\n    True if a fixed vocabulary of term to indices mapping\n    is provided by the user.\n\nidf_ : array of shape (n_features,)\n    The inverse document frequency (IDF) vector; only defined\n    if ``use_idf`` is True.\n\nstop_words_ : set\n    Terms that were ignored because they either:\n\n      - occurred in too many documents (`max_df`)\n      - occurred in too few documents (`min_df`)\n      - were cut off by feature selection (`max_features`).\n\n    This is only available if no vocabulary was given.\n\nSee Also\n--------\nCountVectorizer : Transforms text into a sparse matrix of n-gram counts.\n\nTfidfTransformer : Performs the TF-IDF transformation from a provided\n    matrix of counts.\n\nNotes\n-----\nThe ``stop_words_`` attribute can get large and increase the model size\nwhen pickling. This attribute is provided only for introspection and can\nbe safely removed using delattr or set to None before pickling.\n\nExamples\n--------\n>>> from sklearn.feature_extraction.text import TfidfVectorizer\n>>> corpus = [\n...     'This is the first document.',\n...     'This document is the second document.',\n...     'And this is the third one.',\n...     'Is this the first document?',\n... ]\n>>> vectorizer = TfidfVectorizer()\n>>> X = vectorizer.fit_transform(corpus)\n>>> vectorizer.get_feature_names_out()\narray(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n       'this'], ...)\n>>> print(X.shape)\n(4, 9)", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/655", "data": {"name": "sklearn.feature_extraction.text.TfidfVectorizer.get_feature_names_out", "type": "callable", "signature": "(self)", "description": "Get output feature names for transformation.\n\nParameters\n----------\ninput_features : array-like of str or None, default=None\n    Not used, present here for API consistency by convention.\n\nReturns\n-------\nfeature_names_out : ndarray of str objects\n    Transformed feature names.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/655", "data": {"name": "sklearn.feature_extraction.text.TfidfVectorizer.fit_transform", "type": "callable", "signature": "(self, raw_documents)", "description": "Learn vocabulary and idf, return document-term matrix.\n\nThis is equivalent to fit followed by transform, but more efficiently\nimplemented.\n\nParameters\n----------\nraw_documents : iterable\n    An iterable which generates either str, unicode or file objects.\n\ny : None\n    This parameter is ignored.\n\nReturns\n-------\nX : sparse matrix of (n_samples, n_features)\n    Tf-idf-weighted document-term matrix.", "parameters": {"type": "object", "properties": {"raw_documents": {}}}}}
{"task_id": "BigCodeBench/655", "data": {"name": "text.split()"}}
{"task_id": "BigCodeBench/655", "data": {"name": "topic.argsort()"}}
{"task_id": "BigCodeBench/655", "data": {"name": "topics.append(topic_keywords)"}}
{"task_id": "BigCodeBench/657", "data": {"name": "gensim.models.Word2Vec(vector_size=100)"}}
{"task_id": "BigCodeBench/657", "data": {"name": "gensim.models.Word2Vec(sentences=tokenized_texts, vector_size=100, window=5, min_count=1, workers=4)"}}
{"task_id": "BigCodeBench/657", "data": {"name": "nltk.corpus.stopwords", "type": "constant", "signature": null, "description": "List of words, one per line.  Blank lines are ignored.", "value": "<WordListCorpusReader in '/home/aiops/zhuoyt/nltk_data/corpora/stopwords'>", "parameters": {"type": "constant"}}}
{"task_id": "BigCodeBench/657", "data": {"name": "nltk.corpus.stopwords.words", "type": "callable", "signature": "(fileids=None)", "description": "", "parameters": {"type": "object", "properties": {"fileids": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/657", "data": {"name": "re.compile", "type": "callable", "signature": "(pattern)", "description": "Compile a regular expression pattern, returning a Pattern object.", "parameters": {"type": "object", "properties": {"pattern": {}}}}}
{"task_id": "BigCodeBench/657", "data": {"name": "re.compile.sub", "type": "callable", "signature": "(pattern, flags=0)", "description": "Compile a regular expression pattern, returning a Pattern object.", "parameters": {"type": "object", "properties": {"pattern": {}, "flags": {"type": "integer", "default": 0}}}}}
{"task_id": "BigCodeBench/657", "data": {"name": "text.split()"}}
{"task_id": "BigCodeBench/678", "data": {"name": "filename.endswith('.json')"}}
{"task_id": "BigCodeBench/678", "data": {"name": "json.load", "type": "callable", "signature": "(fp)", "description": "Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\na JSON document) to a Python object.\n\n``object_hook`` is an optional function that will be called with the\nresult of any object literal decode (a ``dict``). The return value of\n``object_hook`` will be used instead of the ``dict``. This feature\ncan be used to implement custom decoders (e.g. JSON-RPC class hinting).\n\n``object_pairs_hook`` is an optional function that will be called with the\nresult of any object literal decoded with an ordered list of pairs.  The\nreturn value of ``object_pairs_hook`` will be used instead of the ``dict``.\nThis feature can be used to implement custom decoders.  If ``object_hook``\nis also defined, the ``object_pairs_hook`` takes priority.\n\nTo use a custom ``JSONDecoder`` subclass, specify it with the ``cls``\nkwarg; otherwise ``JSONDecoder`` is used.", "parameters": {"type": "object", "properties": {"fp": {}}}}}
{"task_id": "BigCodeBench/678", "data": {"name": "os.listdir", "type": "callable", "signature": "(path=None)", "description": "Return a list containing the names of the files in the directory.\n\npath can be specified as either str, bytes, or a path-like object.  If path is bytes,\n  the filenames returned will also be bytes; in all other circumstances\n  the filenames returned will be str.\nIf path is None, uses the path='.'.\nOn some platforms, path may also be specified as an open file descriptor;\\\n  the file descriptor must refer to a directory.\n  If this functionality is unavailable, using it raises NotImplementedError.\n\nThe list is in arbitrary order.  It does not include the special\nentries '.' and '..' even if they are present in the directory.", "parameters": {"type": "object", "properties": {"path": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/678", "data": {"name": "os.makedirs", "type": "callable", "signature": "(name)", "description": "makedirs(name [, mode=0o777][, exist_ok=False])\n\nSuper-mkdir; create a leaf directory and all intermediate ones.  Works like\nmkdir, except that any intermediate path segment (not just the rightmost)\nwill be created if it does not exist. If the target directory already\nexists, raise an OSError if exist_ok is False. Otherwise no exception is\nraised.  This is recursive.", "parameters": {"type": "object", "properties": {"name": {}}}}}
{"task_id": "BigCodeBench/678", "data": {"name": "os.path.join", "type": "callable", "signature": "(a, *p)", "description": "Join two or more pathname components, inserting '/' as needed.\nIf any component is an absolute path, all previous path components\nwill be discarded.  An empty last part will result in a path that\nends with a separator.", "parameters": {"type": "object", "properties": {"a": {}}}}}
{"task_id": "BigCodeBench/678", "data": {"name": "os.path.exists", "type": "callable", "signature": "(path)", "description": "Test whether a path exists.  Returns False for broken symbolic links", "parameters": {"type": "object", "properties": {"path": {}}}}}
{"task_id": "BigCodeBench/678", "data": {"name": "pandas.concat", "type": "callable", "signature": "(objs: 'Iterable[Series | DataFrame] | Mapping[HashableT)", "description": "Concatenate pandas objects along a particular axis.\n\nAllows optional set logic along the other axes.\n\nCan also add a layer of hierarchical indexing on the concatenation axis,\nwhich may be useful if the labels are the same (or overlapping) on\nthe passed axis number.\n\nParameters\n----------\nobjs : a sequence or mapping of Series or DataFrame objects\n    If a mapping is passed, the sorted keys will be used as the `keys`\n    argument, unless it is passed, in which case the values will be\n    selected (see below). Any None objects will be dropped silently unless\n    they are all None in which case a ValueError will be raised.\naxis : {0/'index', 1/'columns'}, default 0\n    The axis to concatenate along.\njoin : {'inner', 'outer'}, default 'outer'\n    How to handle indexes on other axis (or axes).\nignore_index : bool, default False\n    If True, do not use the index values along the concatenation axis. The\n    resulting axis will be labeled 0, ..., n - 1. This is useful if you are\n    concatenating objects where the concatenation axis does not have\n    meaningful indexing information. Note the index values on the other\n    axes are still respected in the join.\nkeys : sequence, default None\n    If multiple levels passed, should contain tuples. Construct\n    hierarchical index using the passed keys as the outermost level.\nlevels : list of sequences, default None\n    Specific levels (unique values) to use for constructing a\n    MultiIndex. Otherwise they will be inferred from the keys.\nnames : list, default None\n    Names for the levels in the resulting hierarchical index.\nverify_integrity : bool, default False\n    Check whether the new concatenated axis contains duplicates. This can\n    be very expensive relative to the actual data concatenation.\nsort : bool, default False\n    Sort non-concatenation axis if it is not already aligned. One exception to\n    this is when the non-concatentation axis is a DatetimeIndex and join='outer'\n    and the axis is not already aligned. In that case, the non-concatenation\n    axis is always sorted lexicographically.\ncopy : bool, default True\n    If False, do not copy data unnecessarily.\n\nReturns\n-------\nobject, type of objs\n    When concatenating all ``Series`` along the index (axis=0), a\n    ``Series`` is returned. When ``objs`` contains at least one\n    ``DataFrame``, a ``DataFrame`` is returned. When concatenating along\n    the columns (axis=1), a ``DataFrame`` is returned.\n\nSee Also\n--------\nDataFrame.join : Join DataFrames using indexes.\nDataFrame.merge : Merge DataFrames by indexes or columns.\n\nNotes\n-----\nThe keys, levels, and names arguments are all optional.\n\nA walkthrough of how this method fits in with other tools for combining\npandas objects can be found `here\n<https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html>`__.\n\nIt is not recommended to build DataFrames by adding single rows in a\nfor loop. Build a list of rows and make a DataFrame in a single concat.\n\nExamples\n--------\nCombine two ``Series``.\n\n>>> s1 = pd.Series(['a', 'b'])\n>>> s2 = pd.Series(['c', 'd'])\n>>> pd.concat([s1, s2])\n0    a\n1    b\n0    c\n1    d\ndtype: object\n\nClear the existing index and reset it in the result\nby setting the ``ignore_index`` option to ``True``.\n\n>>> pd.concat([s1, s2], ignore_index=True)\n0    a\n1    b\n2    c\n3    d\ndtype: object\n\nAdd a hierarchical index at the outermost level of\nthe data with the ``keys`` option.\n\n>>> pd.concat([s1, s2], keys=['s1', 's2'])\ns1  0    a\n    1    b\ns2  0    c\n    1    d\ndtype: object\n\nLabel the index keys you create with the ``names`` option.\n\n>>> pd.concat([s1, s2], keys=['s1', 's2'],\n...           names=['Series name', 'Row ID'])\nSeries name  Row ID\ns1           0         a\n             1         b\ns2           0         c\n             1         d\ndtype: object\n\nCombine two ``DataFrame`` objects with identical columns.\n\n>>> df1 = pd.DataFrame([['a', 1], ['b', 2]],\n...                    columns=['letter', 'number'])\n>>> df1\n  letter  number\n0      a       1\n1      b       2\n>>> df2 = pd.DataFrame([['c', 3], ['d', 4]],\n...                    columns=['letter', 'number'])\n>>> df2\n  letter  number\n0      c       3\n1      d       4\n>>> pd.concat([df1, df2])\n  letter  number\n0      a       1\n1      b       2\n0      c       3\n1      d       4\n\nCombine ``DataFrame`` objects with overlapping columns\nand return everything. Columns outside the intersection will\nbe filled with ``NaN`` values.\n\n>>> df3 = pd.DataFrame([['c', 3, 'cat'], ['d', 4, 'dog']],\n...                    columns=['letter', 'number', 'animal'])\n>>> df3\n  letter  number animal\n0      c       3    cat\n1      d       4    dog\n>>> pd.concat([df1, df3], sort=False)\n  letter  number animal\n0      a       1    NaN\n1      b       2    NaN\n0      c       3    cat\n1      d       4    dog\n\nCombine ``DataFrame`` objects with overlapping columns\nand return only those that are shared by passing ``inner`` to\nthe ``join`` keyword argument.\n\n>>> pd.concat([df1, df3], join=\"inner\")\n  letter  number\n0      a       1\n1      b       2\n0      c       3\n1      d       4\n\nCombine ``DataFrame`` objects horizontally along the x axis by\npassing in ``axis=1``.\n\n>>> df4 = pd.DataFrame([['bird', 'polly'], ['monkey', 'george']],\n...                    columns=['animal', 'name'])\n>>> pd.concat([df1, df4], axis=1)\n  letter  number  animal    name\n0      a       1    bird   polly\n1      b       2  monkey  george\n\nPrevent the result from including duplicate index values with the\n``verify_integrity`` option.\n\n>>> df5 = pd.DataFrame([1], index=['a'])\n>>> df5\n   0\na  1\n>>> df6 = pd.DataFrame([2], index=['a'])\n>>> df6\n   0\na  2\n>>> pd.concat([df5, df6], verify_integrity=True)\nTraceback (most recent call last):\n    ...\nValueError: Indexes have overlapping values: ['a']\n\nAppend a single row to the end of a ``DataFrame`` object.\n\n>>> df7 = pd.DataFrame({'a': 1, 'b': 2}, index=[0])\n>>> df7\n    a   b\n0   1   2\n>>> new_row = pd.Series({'a': 3, 'b': 4})\n>>> new_row\na    3\nb    4\ndtype: int64\n>>> pd.concat([df7, new_row.to_frame().T], ignore_index=True)\n    a   b\n0   1   2\n1   3   4", "parameters": {"type": "object", "properties": {"objs": {"type": ["dataframe]", "iterable[series", "mapping[hashablet"]}}}}}
{"task_id": "BigCodeBench/678", "data": {"name": "shutil.move", "type": "callable", "signature": "(src, dst)", "description": "Recursively move a file or directory to another location. This is\nsimilar to the Unix \"mv\" command. Return the file or directory's\ndestination.\n\nIf the destination is a directory or a symlink to a directory, the source\nis moved inside the directory. The destination path must not already\nexist.\n\nIf the destination already exists but is not a directory, it may be\noverwritten depending on os.rename() semantics.\n\nIf the destination is on our current filesystem, then rename() is used.\nOtherwise, src is copied to the destination and then removed. Symlinks are\nrecreated under the new name if os.rename() fails because of cross\nfilesystem renames.\n\nThe optional `copy_function` argument is a callable that will be used\nto copy the source or it will be delegated to `copytree`.\nBy default, copy2() is used, but any function that supports the same\nsignature (like copy()) can be used.\n\nA lot more could be done here...  A look at a mv.c shows a lot of\nthe issues this implementation glosses over.", "parameters": {"type": "object", "properties": {"src": {}, "dst": {}}}}}
{"task_id": "BigCodeBench/720", "data": {"name": "datetime.datetime", "type": "class", "signature": "(self, /, *args, **kwargs)", "description": "datetime(year, month, day[, hour[, minute[, second[, microsecond[,tzinfo]]]]])\n\nThe year, month and day arguments are required. tzinfo may be None, or an\ninstance of a tzinfo subclass. The remaining arguments may be ints.", "parameters": {"type": "object", "properties": {"/": {}}}}}
{"task_id": "BigCodeBench/720", "data": {"name": "datetime.datetime.now", "type": "callable", "signature": "()", "description": "Returns new datetime object representing current time local to tz.\n\n  tz\n    Timezone object.\n\nIf no tz is specified, uses local timezone.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/720", "data": {"name": "csv.writer", "type": "callable", "signature": "(*args, **kwargs)", "description": "csv_writer = csv.writer(fileobj [, dialect='excel']\n                            [optional keyword args])\n    for row in sequence:\n        csv_writer.writerow(row)\n\n    [or]\n\n    csv_writer = csv.writer(fileobj [, dialect='excel']\n                            [optional keyword args])\n    csv_writer.writerows(rows)\n\nThe \"fileobj\" argument can be any object that supports the file API.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/720", "data": {"name": "csv.writer.writerow", "type": "callable", "signature": "(*args, **kwargs)", "description": "csv_writer = csv.writer(fileobj [, dialect='excel']\n                            [optional keyword args])\n    for row in sequence:\n        csv_writer.writerow(row)\n\n    [or]\n\n    csv_writer = csv.writer(fileobj [, dialect='excel']\n                            [optional keyword args])\n    csv_writer.writerows(rows)\n\nThe \"fileobj\" argument can be any object that supports the file API.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/720", "data": {"name": "os.path.isfile", "type": "callable", "signature": "(path)", "description": "Test whether a path is a regular file", "parameters": {"type": "object", "properties": {"path": {}}}}}
{"task_id": "BigCodeBench/720", "data": {"name": "os.path.dirname", "type": "callable", "signature": "(p)", "description": "Returns the directory component of a pathname", "parameters": {"type": "object", "properties": {"p": {}}}}}
{"task_id": "BigCodeBench/720", "data": {"name": "os.makedirs", "type": "callable", "signature": "(name, exist_ok=False)", "description": "makedirs(name [, mode=0o777][, exist_ok=False])\n\nSuper-mkdir; create a leaf directory and all intermediate ones.  Works like\nmkdir, except that any intermediate path segment (not just the rightmost)\nwill be created if it does not exist. If the target directory already\nexists, raise an OSError if exist_ok is False. Otherwise no exception is\nraised.  This is recursive.", "parameters": {"type": "object", "properties": {"name": {}, "exist_ok": {"type": "bool", "default": false}}}}}
{"task_id": "BigCodeBench/720", "data": {"name": "random.uniform", "type": "callable", "signature": "(a, b)", "description": "Get a random number in the range [a, b) or [a, b] depending on rounding.", "parameters": {"type": "object", "properties": {"a": {}, "b": {}}}}}
{"task_id": "BigCodeBench/723", "data": {"name": "bs4.BeautifulSoup", "type": "class", "signature": "(markup='', features=None, builder=None, parse_only=None, from_encoding=None, exclude_encodings=None, element_classes=None, **kwargs)", "description": "A data structure representing a parsed HTML or XML document.\n\nMost of the methods you'll call on a BeautifulSoup object are inherited from\nPageElement or Tag.\n\nInternally, this class defines the basic interface called by the\ntree builders when converting an HTML/XML document into a data\nstructure. The interface abstracts away the differences between\nparsers. To write a new tree builder, you'll need to understand\nthese methods as a whole.\n\nThese methods will be called by the BeautifulSoup constructor:\n  * reset()\n  * feed(markup)\n\nThe tree builder may call these methods from its feed() implementation:\n  * handle_starttag(name, attrs) # See note about return value\n  * handle_endtag(name)\n  * handle_data(data) # Appends to the current data node\n  * endData(containerClass) # Ends the current data node\n\nNo matter how complicated the underlying parser is, you should be\nable to build a tree using 'start tag' events, 'end tag' events,\n'data' events, and \"done with data\" events.\n\nIf you encounter an empty-element tag (aka a self-closing tag,\nlike HTML's <br> tag), call handle_starttag and then\nhandle_endtag.", "parameters": {"type": "object", "properties": {"markup": {"type": "str", "default": ""}, "features": {"type": "NoneType", "default": null}, "builder": {"type": "NoneType", "default": null}, "parse_only": {"type": "NoneType", "default": null}, "from_encoding": {"type": "NoneType", "default": null}, "exclude_encodings": {"type": "NoneType", "default": null}, "element_classes": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/723", "data": {"name": "bs4.BeautifulSoup.find", "type": "callable", "signature": "(self, name=None, attrs={}, **kwargs)", "description": "Look in the children of this PageElement and find the first\nPageElement that matches the given criteria.\n\nAll find_* methods take a common set of arguments. See the online\ndocumentation for detailed explanations.\n\n:param name: A filter on tag name.\n:param attrs: A dictionary of filters on attribute values.\n:param recursive: If this is True, find() will perform a\n    recursive search of this PageElement's children. Otherwise,\n    only the direct children will be considered.\n:param limit: Stop looking after finding this many results.\n:kwargs: A dictionary of filters on attribute values.\n:return: A PageElement.\n:rtype: bs4.element.PageElement", "parameters": {"type": "object", "properties": {"name": {"type": "NoneType", "default": null}, "attrs": {"type": "str", "default": "{}"}}}}}
{"task_id": "BigCodeBench/723", "data": {"name": "csv.writer", "type": "callable", "signature": "(*args, **kwargs)", "description": "csv_writer = csv.writer(fileobj [, dialect='excel']\n                            [optional keyword args])\n    for row in sequence:\n        csv_writer.writerow(row)\n\n    [or]\n\n    csv_writer = csv.writer(fileobj [, dialect='excel']\n                            [optional keyword args])\n    csv_writer.writerows(rows)\n\nThe \"fileobj\" argument can be any object that supports the file API.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/723", "data": {"name": "csv.writer.writerows", "type": "callable", "signature": "(*args, **kwargs)", "description": "csv_writer = csv.writer(fileobj [, dialect='excel']\n                            [optional keyword args])\n    for row in sequence:\n        csv_writer.writerow(row)\n\n    [or]\n\n    csv_writer = csv.writer(fileobj [, dialect='excel']\n                            [optional keyword args])\n    csv_writer.writerows(rows)\n\nThe \"fileobj\" argument can be any object that supports the file API.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/723", "data": {"name": "data.append(row)"}}
{"task_id": "BigCodeBench/723", "data": {"name": "os.remove", "type": "callable", "signature": "(path)", "description": "Remove a file (same as unlink()).\n\nIf dir_fd is not None, it should be a file descriptor open to a directory,\n  and path should be relative; path will then be relative to that directory.\ndir_fd may not be implemented on your platform.\n  If it is unavailable, using it will raise a NotImplementedError.", "parameters": {"type": "object", "properties": {"path": {}}}}}
{"task_id": "BigCodeBench/723", "data": {"name": "os.path.exists", "type": "callable", "signature": "(path)", "description": "Test whether a path exists.  Returns False for broken symbolic links", "parameters": {"type": "object", "properties": {"path": {}}}}}
{"task_id": "BigCodeBench/723", "data": {"name": "soup.find('table', attrs={'class': 'data-table'}).find_all('tr')"}}
{"task_id": "BigCodeBench/723", "data": {"name": "tr.text"}}
{"task_id": "BigCodeBench/723", "data": {"name": "tr.find_all('td')"}}
{"task_id": "BigCodeBench/723", "data": {"name": "urllib.request.urlopen", "type": "callable", "signature": "(url)", "description": "Open the URL url, which can be either a string or a Request object.\n\n*data* must be an object specifying additional data to be sent to\nthe server, or None if no such data is needed.  See Request for\ndetails.\n\nurllib.request module uses HTTP/1.1 and includes a \"Connection:close\"\nheader in its HTTP requests.\n\nThe optional *timeout* parameter specifies a timeout in seconds for\nblocking operations like the connection attempt (if not specified, the\nglobal default timeout setting will be used). This only works for HTTP,\nHTTPS and FTP connections.\n\nIf *context* is specified, it must be a ssl.SSLContext instance describing\nthe various SSL options. See HTTPSConnection for more details.\n\nThe optional *cafile* and *capath* parameters specify a set of trusted CA\ncertificates for HTTPS requests. cafile should point to a single file\ncontaining a bundle of CA certificates, whereas capath should point to a\ndirectory of hashed certificate files. More information can be found in\nssl.SSLContext.load_verify_locations().\n\nThe *cadefault* parameter is ignored.\n\n\nThis function always returns an object which can work as a\ncontext manager and has the properties url, headers, and status.\nSee urllib.response.addinfourl for more detail on these properties.\n\nFor HTTP and HTTPS URLs, this function returns a http.client.HTTPResponse\nobject slightly modified. In addition to the three new methods above, the\nmsg attribute contains the same information as the reason attribute ---\nthe reason phrase returned by the server --- instead of the response\nheaders as it is specified in the documentation for HTTPResponse.\n\nFor FTP, file, and data URLs and requests explicitly handled by legacy\nURLopener and FancyURLopener classes, this function returns a\nurllib.response.addinfourl object.\n\nNote that None may be returned if no handler handles the request (though\nthe default installed global OpenerDirector uses UnknownHandler to ensure\nthis never happens).\n\nIn addition, if proxy settings are detected (for example, when a *_proxy\nenvironment variable like http_proxy is set), ProxyHandler is default\ninstalled and makes sure the requests are handled through the proxy.", "parameters": {"type": "object", "properties": {"url": {}}}}}
{"task_id": "BigCodeBench/752", "data": {"name": "LinearRegression().fit(X_train, y_train).score(X_test, y_test)"}}
{"task_id": "BigCodeBench/752", "data": {"name": "data.empty"}}
{"task_id": "BigCodeBench/752", "data": {"name": "data.dtypes"}}
{"task_id": "BigCodeBench/752", "data": {"name": "data.columns"}}
{"task_id": "BigCodeBench/752", "data": {"name": "data.drop(columns=[target_column])"}}
{"task_id": "BigCodeBench/752", "data": {"name": "numpy.issubdtype", "type": "callable", "signature": "(arg1, arg2)", "description": "Returns True if first argument is a typecode lower/equal in type hierarchy.\n\nThis is like the builtin :func:`issubclass`, but for `dtype`\\ s.\n\nParameters\n----------\narg1, arg2 : dtype_like\n    `dtype` or object coercible to one\n\nReturns\n-------\nout : bool\n\nSee Also\n--------\n:ref:`arrays.scalars` : Overview of the numpy type hierarchy.\nissubsctype, issubclass_\n\nExamples\n--------\n`issubdtype` can be used to check the type of arrays:\n\n>>> ints = np.array([1, 2, 3], dtype=np.int32)\n>>> np.issubdtype(ints.dtype, np.integer)\nTrue\n>>> np.issubdtype(ints.dtype, np.floating)\nFalse\n\n>>> floats = np.array([1, 2, 3], dtype=np.float32)\n>>> np.issubdtype(floats.dtype, np.integer)\nFalse\n>>> np.issubdtype(floats.dtype, np.floating)\nTrue\n\nSimilar types of different sizes are not subdtypes of each other:\n\n>>> np.issubdtype(np.float64, np.float32)\nFalse\n>>> np.issubdtype(np.float32, np.float64)\nFalse\n\nbut both are subtypes of `floating`:\n\n>>> np.issubdtype(np.float64, np.floating)\nTrue\n>>> np.issubdtype(np.float32, np.floating)\nTrue\n\nFor convenience, dtype-like objects are allowed too:\n\n>>> np.issubdtype('S1', np.string_)\nTrue\n>>> np.issubdtype('i4', np.signedinteger)\nTrue", "parameters": {"type": "object", "properties": {"arg1": {}, "arg2": {}}}}}
{"task_id": "BigCodeBench/752", "data": {"name": "sklearn.linear_model.LinearRegression", "type": "class", "signature": "()", "description": "Ordinary least squares Linear Regression.\n\nLinearRegression fits a linear model with coefficients w = (w1, ..., wp)\nto minimize the residual sum of squares between the observed targets in\nthe dataset, and the targets predicted by the linear approximation.\n\nParameters\n----------\nfit_intercept : bool, default=True\n    Whether to calculate the intercept for this model. If set\n    to False, no intercept will be used in calculations\n    (i.e. data is expected to be centered).\n\ncopy_X : bool, default=True\n    If True, X will be copied; else, it may be overwritten.\n\nn_jobs : int, default=None\n    The number of jobs to use for the computation. This will only provide\n    speedup in case of sufficiently large problems, that is if firstly\n    `n_targets > 1` and secondly `X` is sparse or if `positive` is set\n    to `True`. ``None`` means 1 unless in a\n    :obj:`joblib.parallel_backend` context. ``-1`` means using all\n    processors. See :term:`Glossary <n_jobs>` for more details.\n\npositive : bool, default=False\n    When set to ``True``, forces the coefficients to be positive. This\n    option is only supported for dense arrays.\n\n    .. versionadded:: 0.24\n\nAttributes\n----------\ncoef_ : array of shape (n_features, ) or (n_targets, n_features)\n    Estimated coefficients for the linear regression problem.\n    If multiple targets are passed during the fit (y 2D), this\n    is a 2D array of shape (n_targets, n_features), while if only\n    one target is passed, this is a 1D array of length n_features.\n\nrank_ : int\n    Rank of matrix `X`. Only available when `X` is dense.\n\nsingular_ : array of shape (min(X, y),)\n    Singular values of `X`. Only available when `X` is dense.\n\nintercept_ : float or array of shape (n_targets,)\n    Independent term in the linear model. Set to 0.0 if\n    `fit_intercept = False`.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nRidge : Ridge regression addresses some of the\n    problems of Ordinary Least Squares by imposing a penalty on the\n    size of the coefficients with l2 regularization.\nLasso : The Lasso is a linear model that estimates\n    sparse coefficients with l1 regularization.\nElasticNet : Elastic-Net is a linear regression\n    model trained with both l1 and l2 -norm regularization of the\n    coefficients.\n\nNotes\n-----\nFrom the implementation point of view, this is just plain Ordinary\nLeast Squares (scipy.linalg.lstsq) or Non Negative Least Squares\n(scipy.optimize.nnls) wrapped as a predictor object.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.linear_model import LinearRegression\n>>> X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n>>> # y = 1 * x_0 + 2 * x_1 + 3\n>>> y = np.dot(X, np.array([1, 2])) + 3\n>>> reg = LinearRegression().fit(X, y)\n>>> reg.score(X, y)\n1.0\n>>> reg.coef_\narray([1., 2.])\n>>> reg.intercept_\n3.0...\n>>> reg.predict(np.array([[3, 5]]))\narray([16.])", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/752", "data": {"name": "sklearn.model_selection.train_test_split", "type": "callable", "signature": "(*arrays, test_size=None, random_state=None)", "description": "Split arrays or matrices into random train and test subsets.\n\nQuick utility that wraps input validation,\n``next(ShuffleSplit().split(X, y))``, and application to input data\ninto a single call for splitting (and optionally subsampling) data into a\none-liner.\n\nRead more in the :ref:`User Guide <cross_validation>`.\n\nParameters\n----------\n*arrays : sequence of indexables with same length / shape[0]\n    Allowed inputs are lists, numpy arrays, scipy-sparse\n    matrices or pandas dataframes.\n\ntest_size : float or int, default=None\n    If float, should be between 0.0 and 1.0 and represent the proportion\n    of the dataset to include in the test split. If int, represents the\n    absolute number of test samples. If None, the value is set to the\n    complement of the train size. If ``train_size`` is also None, it will\n    be set to 0.25.\n\ntrain_size : float or int, default=None\n    If float, should be between 0.0 and 1.0 and represent the\n    proportion of the dataset to include in the train split. If\n    int, represents the absolute number of train samples. If None,\n    the value is automatically set to the complement of the test size.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the shuffling applied to the data before applying the split.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nshuffle : bool, default=True\n    Whether or not to shuffle the data before splitting. If shuffle=False\n    then stratify must be None.\n\nstratify : array-like, default=None\n    If not None, data is split in a stratified fashion, using this as\n    the class labels.\n    Read more in the :ref:`User Guide <stratification>`.\n\nReturns\n-------\nsplitting : list, length=2 * len(arrays)\n    List containing train-test split of inputs.\n\n    .. versionadded:: 0.16\n        If the input is sparse, the output will be a\n        ``scipy.sparse.csr_matrix``. Else, output type is the same as the\n        input type.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.model_selection import train_test_split\n>>> X, y = np.arange(10).reshape((5, 2)), range(5)\n>>> X\narray([[0, 1],\n       [2, 3],\n       [4, 5],\n       [6, 7],\n       [8, 9]])\n>>> list(y)\n[0, 1, 2, 3, 4]\n\n>>> X_train, X_test, y_train, y_test = train_test_split(\n...     X, y, test_size=0.33, random_state=42)\n...\n>>> X_train\narray([[4, 5],\n       [0, 1],\n       [6, 7]])\n>>> y_train\n[2, 0, 3]\n>>> X_test\narray([[2, 3],\n       [8, 9]])\n>>> y_test\n[1, 4]\n\n>>> train_test_split(y, shuffle=False)\n[[0, 1, 2], [3, 4]]", "parameters": {"type": "object", "properties": {"test_size": {"type": "NoneType", "default": null}, "random_state": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/760", "data": {"name": "datetime.datetime", "type": "class", "signature": "(self, /, *args, **kwargs)", "description": "datetime(year, month, day[, hour[, minute[, second[, microsecond[,tzinfo]]]]])\n\nThe year, month and day arguments are required. tzinfo may be None, or an\ninstance of a tzinfo subclass. The remaining arguments may be ints.", "parameters": {"type": "object", "properties": {"/": {}}}}}
{"task_id": "BigCodeBench/760", "data": {"name": "datetime.datetime.datetime", "type": "class", "signature": "(self, /, *args, **kwargs)", "description": "datetime(year, month, day[, hour[, minute[, second[, microsecond[,tzinfo]]]]])\n\nThe year, month and day arguments are required. tzinfo may be None, or an\ninstance of a tzinfo subclass. The remaining arguments may be ints.", "parameters": {"type": "object", "properties": {"/": {}}}}}
{"task_id": "BigCodeBench/760", "data": {"name": "codecs.encode", "type": "callable", "signature": "(obj, encoding='utf-8')", "description": "Encodes obj using the codec registered for encoding.\n\nThe default encoding is 'utf-8'.  errors may be given to set a\ndifferent error handling scheme.  Default is 'strict' meaning that encoding\nerrors raise a ValueError.  Other possible values are 'ignore', 'replace'\nand 'backslashreplace' as well as any other name registered with\ncodecs.register_error that can handle ValueErrors.", "parameters": {"type": "object", "properties": {"obj": {}, "encoding": {"type": "str", "default": "utf-8"}}}}}
{"task_id": "BigCodeBench/760", "data": {"name": "data.append([i, name, dob, email])"}}
{"task_id": "BigCodeBench/760", "data": {"name": "name.lower()"}}
{"task_id": "BigCodeBench/760", "data": {"name": "numpy.random.randint", "type": "callable", "signature": "(*args, **kwargs)", "description": "randint(low, high=None, size=None, dtype=int)\n\nReturn random integers from `low` (inclusive) to `high` (exclusive).\n\nReturn random integers from the \"discrete uniform\" distribution of\nthe specified dtype in the \"half-open\" interval [`low`, `high`). If\n`high` is None (the default), then results are from [0, `low`).\n\n.. note::\n    New code should use the ``integers`` method of a ``default_rng()``\n    instance instead; please see the :ref:`random-quick-start`.\n\nParameters\n----------\nlow : int or array-like of ints\n    Lowest (signed) integers to be drawn from the distribution (unless\n    ``high=None``, in which case this parameter is one above the\n    *highest* such integer).\nhigh : int or array-like of ints, optional\n    If provided, one above the largest (signed) integer to be drawn\n    from the distribution (see above for behavior if ``high=None``).\n    If array-like, must contain integer values\nsize : int or tuple of ints, optional\n    Output shape.  If the given shape is, e.g., ``(m, n, k)``, then\n    ``m * n * k`` samples are drawn.  Default is None, in which case a\n    single value is returned.\ndtype : dtype, optional\n    Desired dtype of the result. Byteorder must be native.\n    The default value is int.\n\n    .. versionadded:: 1.11.0\n\nReturns\n-------\nout : int or ndarray of ints\n    `size`-shaped array of random integers from the appropriate\n    distribution, or a single such random int if `size` not provided.\n\nSee Also\n--------\nrandom_integers : similar to `randint`, only for the closed\n    interval [`low`, `high`], and 1 is the lowest value if `high` is\n    omitted.\nrandom.Generator.integers: which should be used for new code.\n\nExamples\n--------\n>>> np.random.randint(2, size=10)\narray([1, 0, 0, 0, 1, 1, 0, 0, 1, 0]) # random\n>>> np.random.randint(1, size=10)\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n\nGenerate a 2 x 4 array of ints between 0 and 4, inclusive:\n\n>>> np.random.randint(5, size=(2, 4))\narray([[4, 0, 2, 1], # random\n       [3, 2, 2, 0]])\n\nGenerate a 1 x 3 array with 3 different upper bounds\n\n>>> np.random.randint(1, [3, 5, 10])\narray([2, 2, 9]) # random\n\nGenerate a 1 by 3 array with 3 different lower bounds\n\n>>> np.random.randint([1, 5, 7], 10)\narray([9, 8, 7]) # random\n\nGenerate a 2 by 4 array using broadcasting with dtype of uint8\n\n>>> np.random.randint([1, 3, 5, 7], [[10], [20]], dtype=np.uint8)\narray([[ 8,  6,  9,  7], # random\n       [ 1, 16,  9, 12]], dtype=uint8)", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/760", "data": {"name": "numpy.random.seed", "type": "callable", "signature": "(*args, **kwargs)", "description": "seed(self, seed=None)\n\nReseed a legacy MT19937 BitGenerator\n\nNotes\n-----\nThis is a convenience, legacy function.\n\nThe best practice is to **not** reseed a BitGenerator, rather to\nrecreate a new one. This method is here for legacy reasons.\nThis example demonstrates best practice.\n\n>>> from numpy.random import MT19937\n>>> from numpy.random import RandomState, SeedSequence\n>>> rs = RandomState(MT19937(SeedSequence(123456789)))\n# Later, you want to restart the stream\n>>> rs = RandomState(MT19937(SeedSequence(987654321)))", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/760", "data": {"name": "numpy.random.choice", "type": "callable", "signature": "(*args, **kwargs)", "description": "choice(a, size=None, replace=True, p=None)\n\nGenerates a random sample from a given 1-D array\n\n.. versionadded:: 1.7.0\n\n.. note::\n    New code should use the ``choice`` method of a ``default_rng()``\n    instance instead; please see the :ref:`random-quick-start`.\n\nParameters\n----------\na : 1-D array-like or int\n    If an ndarray, a random sample is generated from its elements.\n    If an int, the random sample is generated as if it were ``np.arange(a)``\nsize : int or tuple of ints, optional\n    Output shape.  If the given shape is, e.g., ``(m, n, k)``, then\n    ``m * n * k`` samples are drawn.  Default is None, in which case a\n    single value is returned.\nreplace : boolean, optional\n    Whether the sample is with or without replacement. Default is True,\n    meaning that a value of ``a`` can be selected multiple times.\np : 1-D array-like, optional\n    The probabilities associated with each entry in a.\n    If not given, the sample assumes a uniform distribution over all\n    entries in ``a``.\n\nReturns\n-------\nsamples : single item or ndarray\n    The generated random samples\n\nRaises\n------\nValueError\n    If a is an int and less than zero, if a or p are not 1-dimensional,\n    if a is an array-like of size 0, if p is not a vector of\n    probabilities, if a and p have different lengths, or if\n    replace=False and the sample size is greater than the population\n    size\n\nSee Also\n--------\nrandint, shuffle, permutation\nrandom.Generator.choice: which should be used in new code\n\nNotes\n-----\nSetting user-specified probabilities through ``p`` uses a more general but less\nefficient sampler than the default. The general sampler produces a different sample\nthan the optimized sampler even if each element of ``p`` is 1 / len(a).\n\nSampling random rows from a 2-D array is not possible with this function,\nbut is possible with `Generator.choice` through its ``axis`` keyword.\n\nExamples\n--------\nGenerate a uniform random sample from np.arange(5) of size 3:\n\n>>> np.random.choice(5, 3)\narray([0, 3, 4]) # random\n>>> #This is equivalent to np.random.randint(0,5,3)\n\nGenerate a non-uniform random sample from np.arange(5) of size 3:\n\n>>> np.random.choice(5, 3, p=[0.1, 0, 0.3, 0.6, 0])\narray([3, 3, 0]) # random\n\nGenerate a uniform random sample from np.arange(5) of size 3 without\nreplacement:\n\n>>> np.random.choice(5, 3, replace=False)\narray([3,1,0]) # random\n>>> #This is equivalent to np.random.permutation(np.arange(5))[:3]\n\nGenerate a non-uniform random sample from np.arange(5) of size\n3 without replacement:\n\n>>> np.random.choice(5, 3, replace=False, p=[0.1, 0, 0.3, 0.6, 0])\narray([2, 3, 0]) # random\n\nAny of the above can be repeated with an arbitrary array-like\ninstead of just integers. For instance:\n\n>>> aa_milne_arr = ['pooh', 'rabbit', 'piglet', 'Christopher']\n>>> np.random.choice(aa_milne_arr, 5, p=[0.5, 0.1, 0.1, 0.3])\narray(['pooh', 'pooh', 'pooh', 'Christopher', 'piglet'], # random\n      dtype='<U11')", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/760", "data": {"name": "re.sub", "type": "callable", "signature": "(pattern, repl, string)", "description": "Return the string obtained by replacing the leftmost\nnon-overlapping occurrences of the pattern in string by the\nreplacement repl.  repl can be either a string or a callable;\nif a string, backslash escapes in it are processed.  If it is\na callable, it's passed the Match object and must return\na replacement string to be used.", "parameters": {"type": "object", "properties": {"pattern": {}, "repl": {}, "string": {}}}}}
{"task_id": "BigCodeBench/763", "data": {"name": "collections.defaultdict", "type": "class", "signature": "(self, /, *args, **kwargs)", "description": "defaultdict(default_factory=None, /, [...]) --> dict with default factory\n\nThe default factory is called without arguments to produce\na new value when a key is not present, in __getitem__ only.\nA defaultdict compares equal to a dict with the same items.\nAll remaining arguments are treated the same as if they were\npassed to the dict constructor, including keyword arguments.", "parameters": {"type": "object", "properties": {"/": {}}}}}
{"task_id": "BigCodeBench/763", "data": {"name": "collections.defaultdict.items", "type": "callable", "signature": "()", "description": "D.items() -> a set-like object providing a view on D's items", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/763", "data": {"name": "csv.DictWriter", "type": "class", "signature": "(f, fieldnames, restval='', extrasaction='raise', dialect='excel', *args, **kwds)", "description": "", "parameters": {"type": "object", "properties": {"f": {}, "fieldnames": {}, "restval": {"type": "str", "default": ""}, "extrasaction": {"type": "str", "default": "raise"}, "dialect": {"type": "str", "default": "excel"}}}}}
{"task_id": "BigCodeBench/763", "data": {"name": "csv.DictWriter.writeheader", "type": "callable", "signature": "(self)", "description": "", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/763", "data": {"name": "csv.DictWriter.writerow", "type": "callable", "signature": "(self, rowdict)", "description": "", "parameters": {"type": "object", "properties": {"rowdict": {}}}}}
{"task_id": "BigCodeBench/763", "data": {"name": "d.items()"}}
{"task_id": "BigCodeBench/763", "data": {"name": "json.load", "type": "callable", "signature": "(fp)", "description": "Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\na JSON document) to a Python object.\n\n``object_hook`` is an optional function that will be called with the\nresult of any object literal decode (a ``dict``). The return value of\n``object_hook`` will be used instead of the ``dict``. This feature\ncan be used to implement custom decoders (e.g. JSON-RPC class hinting).\n\n``object_pairs_hook`` is an optional function that will be called with the\nresult of any object literal decoded with an ordered list of pairs.  The\nreturn value of ``object_pairs_hook`` will be used instead of the ``dict``.\nThis feature can be used to implement custom decoders.  If ``object_hook``\nis also defined, the ``object_pairs_hook`` takes priority.\n\nTo use a custom ``JSONDecoder`` subclass, specify it with the ``cls``\nkwarg; otherwise ``JSONDecoder`` is used.", "parameters": {"type": "object", "properties": {"fp": {}}}}}
{"task_id": "BigCodeBench/763", "data": {"name": "numpy.mean", "type": "callable", "signature": "(a)", "description": "Compute the arithmetic mean along the specified axis.\n\nReturns the average of the array elements.  The average is taken over\nthe flattened array by default, otherwise over the specified axis.\n`float64` intermediate and return values are used for integer inputs.\n\nParameters\n----------\na : array_like\n    Array containing numbers whose mean is desired. If `a` is not an\n    array, a conversion is attempted.\naxis : None or int or tuple of ints, optional\n    Axis or axes along which the means are computed. The default is to\n    compute the mean of the flattened array.\n\n    .. versionadded:: 1.7.0\n\n    If this is a tuple of ints, a mean is performed over multiple axes,\n    instead of a single axis or all the axes as before.\ndtype : data-type, optional\n    Type to use in computing the mean.  For integer inputs, the default\n    is `float64`; for floating point inputs, it is the same as the\n    input dtype.\nout : ndarray, optional\n    Alternate output array in which to place the result.  The default\n    is ``None``; if provided, it must have the same shape as the\n    expected output, but the type will be cast if necessary.\n    See :ref:`ufuncs-output-type` for more details.\n\nkeepdims : bool, optional\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\n    If the default value is passed, then `keepdims` will not be\n    passed through to the `mean` method of sub-classes of\n    `ndarray`, however any non-default value will be.  If the\n    sub-class' method does not implement `keepdims` any\n    exceptions will be raised.\n\nwhere : array_like of bool, optional\n    Elements to include in the mean. See `~numpy.ufunc.reduce` for details.\n\n    .. versionadded:: 1.20.0\n\nReturns\n-------\nm : ndarray, see dtype parameter above\n    If `out=None`, returns a new array containing the mean values,\n    otherwise a reference to the output array is returned.\n\nSee Also\n--------\naverage : Weighted average\nstd, var, nanmean, nanstd, nanvar\n\nNotes\n-----\nThe arithmetic mean is the sum of the elements along the axis divided\nby the number of elements.\n\nNote that for floating-point input, the mean is computed using the\nsame precision the input has.  Depending on the input data, this can\ncause the results to be inaccurate, especially for `float32` (see\nexample below).  Specifying a higher-precision accumulator using the\n`dtype` keyword can alleviate this issue.\n\nBy default, `float16` results are computed using `float32` intermediates\nfor extra precision.\n\nExamples\n--------\n>>> a = np.array([[1, 2], [3, 4]])\n>>> np.mean(a)\n2.5\n>>> np.mean(a, axis=0)\narray([2., 3.])\n>>> np.mean(a, axis=1)\narray([1.5, 3.5])\n\nIn single precision, `mean` can be inaccurate:\n\n>>> a = np.zeros((2, 512*512), dtype=np.float32)\n>>> a[0, :] = 1.0\n>>> a[1, :] = 0.1\n>>> np.mean(a)\n0.54999924\n\nComputing the mean in float64 is more accurate:\n\n>>> np.mean(a, dtype=np.float64)\n0.55000000074505806 # may vary\n\nSpecifying a where argument:\n\n>>> a = np.array([[5, 9, 13], [14, 10, 12], [11, 15, 19]])\n>>> np.mean(a)\n12.0\n>>> np.mean(a, where=[[True], [False], [False]])\n9.0", "parameters": {"type": "object", "properties": {"a": {}}}}}
{"task_id": "BigCodeBench/763", "data": {"name": "numpy.median", "type": "callable", "signature": "(a)", "description": "Compute the median along the specified axis.\n\nReturns the median of the array elements.\n\nParameters\n----------\na : array_like\n    Input array or object that can be converted to an array.\naxis : {int, sequence of int, None}, optional\n    Axis or axes along which the medians are computed. The default\n    is to compute the median along a flattened version of the array.\n    A sequence of axes is supported since version 1.9.0.\nout : ndarray, optional\n    Alternative output array in which to place the result. It must\n    have the same shape and buffer length as the expected output,\n    but the type (of the output) will be cast if necessary.\noverwrite_input : bool, optional\n   If True, then allow use of memory of input array `a` for\n   calculations. The input array will be modified by the call to\n   `median`. This will save memory when you do not need to preserve\n   the contents of the input array. Treat the input as undefined,\n   but it will probably be fully or partially sorted. Default is\n   False. If `overwrite_input` is ``True`` and `a` is not already an\n   `ndarray`, an error will be raised.\nkeepdims : bool, optional\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the original `arr`.\n\n    .. versionadded:: 1.9.0\n\nReturns\n-------\nmedian : ndarray\n    A new array holding the result. If the input contains integers\n    or floats smaller than ``float64``, then the output data-type is\n    ``np.float64``.  Otherwise, the data-type of the output is the\n    same as that of the input. If `out` is specified, that array is\n    returned instead.\n\nSee Also\n--------\nmean, percentile\n\nNotes\n-----\nGiven a vector ``V`` of length ``N``, the median of ``V`` is the\nmiddle value of a sorted copy of ``V``, ``V_sorted`` - i\ne., ``V_sorted[(N-1)/2]``, when ``N`` is odd, and the average of the\ntwo middle values of ``V_sorted`` when ``N`` is even.\n\nExamples\n--------\n>>> a = np.array([[10, 7, 4], [3, 2, 1]])\n>>> a\narray([[10,  7,  4],\n       [ 3,  2,  1]])\n>>> np.median(a)\n3.5\n>>> np.median(a, axis=0)\narray([6.5, 4.5, 2.5])\n>>> np.median(a, axis=1)\narray([7.,  2.])\n>>> m = np.median(a, axis=0)\n>>> out = np.zeros_like(m)\n>>> np.median(a, axis=0, out=m)\narray([6.5,  4.5,  2.5])\n>>> m\narray([6.5,  4.5,  2.5])\n>>> b = a.copy()\n>>> np.median(b, axis=1, overwrite_input=True)\narray([7.,  2.])\n>>> assert not np.all(a==b)\n>>> b = a.copy()\n>>> np.median(b, axis=None, overwrite_input=True)\n3.5\n>>> assert not np.all(a==b)", "parameters": {"type": "object", "properties": {"a": {}}}}}
{"task_id": "BigCodeBench/763", "data": {"name": "result.items()"}}
{"task_id": "BigCodeBench/765", "data": {"name": "copied_files.append(str(target_file))"}}
{"task_id": "BigCodeBench/765", "data": {"name": "kwargs.items()"}}
{"task_id": "BigCodeBench/765", "data": {"name": "os.path.isfile", "type": "callable", "signature": "(path)", "description": "Test whether a path is a regular file", "parameters": {"type": "object", "properties": {"path": {}}}}}
{"task_id": "BigCodeBench/765", "data": {"name": "os.makedirs", "type": "callable", "signature": "(name)", "description": "makedirs(name [, mode=0o777][, exist_ok=False])\n\nSuper-mkdir; create a leaf directory and all intermediate ones.  Works like\nmkdir, except that any intermediate path segment (not just the rightmost)\nwill be created if it does not exist. If the target directory already\nexists, raise an OSError if exist_ok is False. Otherwise no exception is\nraised.  This is recursive.", "parameters": {"type": "object", "properties": {"name": {}}}}}
{"task_id": "BigCodeBench/765", "data": {"name": "os.path.exists", "type": "callable", "signature": "(path)", "description": "Test whether a path exists.  Returns False for broken symbolic links", "parameters": {"type": "object", "properties": {"path": {}}}}}
{"task_id": "BigCodeBench/765", "data": {"name": "shutil.copyfile", "type": "callable", "signature": "(src, dst)", "description": "Copy data from src to dst in the most efficient way possible.\n\nIf follow_symlinks is not set and src is a symbolic link, a new\nsymlink will be created instead of copying the file it points to.", "parameters": {"type": "object", "properties": {"src": {}, "dst": {}}}}}
{"task_id": "BigCodeBench/771", "data": {"name": "csv.reader", "type": "callable", "signature": "(*args, **kwargs)", "description": "csv_reader = reader(iterable [, dialect='excel']\n                        [optional keyword args])\n    for row in csv_reader:\n        process(row)\n\nThe \"iterable\" argument can be any object that returns a line\nof input for each iteration, such as a file object or a list.  The\noptional \"dialect\" parameter is discussed below.  The function\nalso accepts optional keyword arguments which override settings\nprovided by the dialect.\n\nThe returned object is an iterator.  Each iteration returns a row\nof the CSV file (which can span multiple input lines).", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/771", "data": {"name": "csv.writer", "type": "callable", "signature": "(*args, **kwargs)", "description": "csv_writer = csv.writer(fileobj [, dialect='excel']\n                            [optional keyword args])\n    for row in sequence:\n        csv_writer.writerow(row)\n\n    [or]\n\n    csv_writer = csv.writer(fileobj [, dialect='excel']\n                            [optional keyword args])\n    csv_writer.writerows(rows)\n\nThe \"fileobj\" argument can be any object that supports the file API.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/771", "data": {"name": "csv.writer.writerows", "type": "callable", "signature": "(*args, **kwargs)", "description": "csv_writer = csv.writer(fileobj [, dialect='excel']\n                            [optional keyword args])\n    for row in sequence:\n        csv_writer.writerow(row)\n\n    [or]\n\n    csv_writer = csv.writer(fileobj [, dialect='excel']\n                            [optional keyword args])\n    csv_writer.writerows(rows)\n\nThe \"fileobj\" argument can be any object that supports the file API.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/771", "data": {"name": "file_pattern.match(filename).group(1)"}}
{"task_id": "BigCodeBench/771", "data": {"name": "new_files.append(new_filename)"}}
{"task_id": "BigCodeBench/771", "data": {"name": "os.listdir", "type": "callable", "signature": "(path=None)", "description": "Return a list containing the names of the files in the directory.\n\npath can be specified as either str, bytes, or a path-like object.  If path is bytes,\n  the filenames returned will also be bytes; in all other circumstances\n  the filenames returned will be str.\nIf path is None, uses the path='.'.\nOn some platforms, path may also be specified as an open file descriptor;\\\n  the file descriptor must refer to a directory.\n  If this functionality is unavailable, using it raises NotImplementedError.\n\nThe list is in arbitrary order.  It does not include the special\nentries '.' and '..' even if they are present in the directory.", "parameters": {"type": "object", "properties": {"path": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/771", "data": {"name": "re.compile", "type": "callable", "signature": "(pattern)", "description": "Compile a regular expression pattern, returning a Pattern object.", "parameters": {"type": "object", "properties": {"pattern": {}}}}}
{"task_id": "BigCodeBench/771", "data": {"name": "re.compile.match", "type": "callable", "signature": "(pattern)", "description": "Compile a regular expression pattern, returning a Pattern object.", "parameters": {"type": "object", "properties": {"pattern": {}}}}}
{"task_id": "BigCodeBench/777", "data": {"name": "zipfile.ZipFile", "type": "class", "signature": "(file, mode='r', compression=0, allowZip64=True, compresslevel=None, *, strict_timestamps=True)", "description": "Class with methods to open, read, write, close, list zip files.\n\nz = ZipFile(file, mode=\"r\", compression=ZIP_STORED, allowZip64=True,\n            compresslevel=None)\n\nfile: Either the path to the file, or a file-like object.\n      If it is a path, the file will be opened and closed by ZipFile.\nmode: The mode can be either read 'r', write 'w', exclusive create 'x',\n      or append 'a'.\ncompression: ZIP_STORED (no compression), ZIP_DEFLATED (requires zlib),\n             ZIP_BZIP2 (requires bz2) or ZIP_LZMA (requires lzma).\nallowZip64: if True ZipFile will create files with ZIP64 extensions when\n            needed, otherwise it will raise an exception when this would\n            be necessary.\ncompresslevel: None (default for the given compression type) or an integer\n               specifying the level to pass to the compressor.\n               When using ZIP_STORED or ZIP_LZMA this keyword has no effect.\n               When using ZIP_DEFLATED integers 0 through 9 are accepted.\n               When using ZIP_BZIP2 integers 1 through 9 are accepted.", "parameters": {"type": "object", "properties": {"file": {}, "mode": {"type": "str", "default": "r"}, "compression": {"type": "integer", "default": 0}, "allowZip64": {"type": "bool", "default": true}, "compresslevel": {"type": "NoneType", "default": null}, "strict_timestamps": {"type": "bool", "default": true}}}}}
{"task_id": "BigCodeBench/777", "data": {"name": "zipfile.ZipFile.extractall", "type": "callable", "signature": "(self, path=None)", "description": "Extract all members from the archive to the current working\ndirectory. `path' specifies a different directory to extract to.\n`members' is optional and must be a subset of the list returned\nby namelist().", "parameters": {"type": "object", "properties": {"path": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/777", "data": {"name": "extracted_dirs.append(extract_path)"}}
{"task_id": "BigCodeBench/777", "data": {"name": "os.listdir", "type": "callable", "signature": "(path=None)", "description": "Return a list containing the names of the files in the directory.\n\npath can be specified as either str, bytes, or a path-like object.  If path is bytes,\n  the filenames returned will also be bytes; in all other circumstances\n  the filenames returned will be str.\nIf path is None, uses the path='.'.\nOn some platforms, path may also be specified as an open file descriptor;\\\n  the file descriptor must refer to a directory.\n  If this functionality is unavailable, using it raises NotImplementedError.\n\nThe list is in arbitrary order.  It does not include the special\nentries '.' and '..' even if they are present in the directory.", "parameters": {"type": "object", "properties": {"path": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/777", "data": {"name": "os.path.join", "type": "callable", "signature": "(a, *p)", "description": "Join two or more pathname components, inserting '/' as needed.\nIf any component is an absolute path, all previous path components\nwill be discarded.  An empty last part will result in a path that\nends with a separator.", "parameters": {"type": "object", "properties": {"a": {}}}}}
{"task_id": "BigCodeBench/777", "data": {"name": "os.makedirs", "type": "callable", "signature": "(name, exist_ok=False)", "description": "makedirs(name [, mode=0o777][, exist_ok=False])\n\nSuper-mkdir; create a leaf directory and all intermediate ones.  Works like\nmkdir, except that any intermediate path segment (not just the rightmost)\nwill be created if it does not exist. If the target directory already\nexists, raise an OSError if exist_ok is False. Otherwise no exception is\nraised.  This is recursive.", "parameters": {"type": "object", "properties": {"name": {}, "exist_ok": {"type": "bool", "default": false}}}}}
{"task_id": "BigCodeBench/777", "data": {"name": "re.match", "type": "callable", "signature": "(pattern, string)", "description": "Try to apply the pattern at the start of the string, returning\na Match object, or None if no match was found.", "parameters": {"type": "object", "properties": {"pattern": {}, "string": {}}}}}
{"task_id": "BigCodeBench/777", "data": {"name": "re.match.group", "type": "callable", "signature": "(pattern)", "description": "Try to apply the pattern at the start of the string, returning\na Match object, or None if no match was found.", "parameters": {"type": "object", "properties": {"pattern": {}}}}}
{"task_id": "BigCodeBench/785", "data": {"name": "glob.glob", "type": "callable", "signature": "(pathname)", "description": "Return a list of paths matching a pathname pattern.\n\nThe pattern may contain simple shell-style wildcards a la\nfnmatch. However, unlike fnmatch, filenames starting with a\ndot are special cases that are not matched by '*' and '?'\npatterns.\n\nIf recursive is true, the pattern '**' will match any files and\nzero or more directories and subdirectories.", "parameters": {"type": "object", "properties": {"pathname": {}}}}}
{"task_id": "BigCodeBench/785", "data": {"name": "os.remove", "type": "callable", "signature": "(path)", "description": "Remove a file (same as unlink()).\n\nIf dir_fd is not None, it should be a file descriptor open to a directory,\n  and path should be relative; path will then be relative to that directory.\ndir_fd may not be implemented on your platform.\n  If it is unavailable, using it will raise a NotImplementedError.", "parameters": {"type": "object", "properties": {"path": {}}}}}
{"task_id": "BigCodeBench/785", "data": {"name": "os.makedirs", "type": "callable", "signature": "(name)", "description": "makedirs(name [, mode=0o777][, exist_ok=False])\n\nSuper-mkdir; create a leaf directory and all intermediate ones.  Works like\nmkdir, except that any intermediate path segment (not just the rightmost)\nwill be created if it does not exist. If the target directory already\nexists, raise an OSError if exist_ok is False. Otherwise no exception is\nraised.  This is recursive.", "parameters": {"type": "object", "properties": {"name": {}}}}}
{"task_id": "BigCodeBench/785", "data": {"name": "os.path.exists", "type": "callable", "signature": "(path)", "description": "Test whether a path exists.  Returns False for broken symbolic links", "parameters": {"type": "object", "properties": {"path": {}}}}}
{"task_id": "BigCodeBench/785", "data": {"name": "os.path.join", "type": "callable", "signature": "(a, *p)", "description": "Join two or more pathname components, inserting '/' as needed.\nIf any component is an absolute path, all previous path components\nwill be discarded.  An empty last part will result in a path that\nends with a separator.", "parameters": {"type": "object", "properties": {"a": {}}}}}
{"task_id": "BigCodeBench/785", "data": {"name": "subprocess.run", "type": "callable", "signature": "(*popenargs)", "description": "Run command with arguments and return a CompletedProcess instance.\n\nThe returned instance will have attributes args, returncode, stdout and\nstderr. By default, stdout and stderr are not captured, and those attributes\nwill be None. Pass stdout=PIPE and/or stderr=PIPE in order to capture them.\n\nIf check is True and the exit code was non-zero, it raises a\nCalledProcessError. The CalledProcessError object will have the return code\nin the returncode attribute, and output & stderr attributes if those streams\nwere captured.\n\nIf timeout is given, and the process takes too long, a TimeoutExpired\nexception will be raised.\n\nThere is an optional argument \"input\", allowing you to\npass bytes or a string to the subprocess's stdin.  If you use this argument\nyou may not also use the Popen constructor's \"stdin\" argument, as\nit will be used internally.\n\nBy default, all communication is in bytes, and therefore any \"input\" should\nbe bytes, and the stdout and stderr will be bytes. If in text mode, any\n\"input\" should be a string, and stdout and stderr will be strings decoded\naccording to locale encoding, or by \"encoding\" if set. Text mode is\ntriggered by setting any of text, encoding, errors or universal_newlines.\n\nThe other arguments are the same as for the Popen constructor.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/800", "data": {"name": "csv.writer", "type": "callable", "signature": "(*args, **kwargs)", "description": "csv_writer = csv.writer(fileobj [, dialect='excel']\n                            [optional keyword args])\n    for row in sequence:\n        csv_writer.writerow(row)\n\n    [or]\n\n    csv_writer = csv.writer(fileobj [, dialect='excel']\n                            [optional keyword args])\n    csv_writer.writerows(rows)\n\nThe \"fileobj\" argument can be any object that supports the file API.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/800", "data": {"name": "csv.writer.writerows", "type": "callable", "signature": "(*args, **kwargs)", "description": "csv_writer = csv.writer(fileobj [, dialect='excel']\n                            [optional keyword args])\n    for row in sequence:\n        csv_writer.writerow(row)\n\n    [or]\n\n    csv_writer = csv.writer(fileobj [, dialect='excel']\n                            [optional keyword args])\n    csv_writer.writerows(rows)\n\nThe \"fileobj\" argument can be any object that supports the file API.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/800", "data": {"name": "goals.items()"}}
{"task_id": "BigCodeBench/800", "data": {"name": "os.path.exists", "type": "callable", "signature": "(path)", "description": "Test whether a path exists.  Returns False for broken symbolic links", "parameters": {"type": "object", "properties": {"path": {}}}}}
{"task_id": "BigCodeBench/800", "data": {"name": "penalties.items()"}}
{"task_id": "BigCodeBench/800", "data": {"name": "row.get('goals', 0)"}}
{"task_id": "BigCodeBench/800", "data": {"name": "row.get('penalties', 0)"}}
{"task_id": "BigCodeBench/826", "data": {"name": "os.listdir", "type": "callable", "signature": "(path=None)", "description": "Return a list containing the names of the files in the directory.\n\npath can be specified as either str, bytes, or a path-like object.  If path is bytes,\n  the filenames returned will also be bytes; in all other circumstances\n  the filenames returned will be str.\nIf path is None, uses the path='.'.\nOn some platforms, path may also be specified as an open file descriptor;\\\n  the file descriptor must refer to a directory.\n  If this functionality is unavailable, using it raises NotImplementedError.\n\nThe list is in arbitrary order.  It does not include the special\nentries '.' and '..' even if they are present in the directory.", "parameters": {"type": "object", "properties": {"path": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/826", "data": {"name": "os.makedirs", "type": "callable", "signature": "(name)", "description": "makedirs(name [, mode=0o777][, exist_ok=False])\n\nSuper-mkdir; create a leaf directory and all intermediate ones.  Works like\nmkdir, except that any intermediate path segment (not just the rightmost)\nwill be created if it does not exist. If the target directory already\nexists, raise an OSError if exist_ok is False. Otherwise no exception is\nraised.  This is recursive.", "parameters": {"type": "object", "properties": {"name": {}}}}}
{"task_id": "BigCodeBench/826", "data": {"name": "os.path.exists", "type": "callable", "signature": "(path)", "description": "Test whether a path exists.  Returns False for broken symbolic links", "parameters": {"type": "object", "properties": {"path": {}}}}}
{"task_id": "BigCodeBench/826", "data": {"name": "os.path.join", "type": "callable", "signature": "(a, *p)", "description": "Join two or more pathname components, inserting '/' as needed.\nIf any component is an absolute path, all previous path components\nwill be discarded.  An empty last part will result in a path that\nends with a separator.", "parameters": {"type": "object", "properties": {"a": {}}}}}
{"task_id": "BigCodeBench/826", "data": {"name": "re.match", "type": "callable", "signature": "(pattern, string)", "description": "Try to apply the pattern at the start of the string, returning\na Match object, or None if no match was found.", "parameters": {"type": "object", "properties": {"pattern": {}, "string": {}}}}}
{"task_id": "BigCodeBench/826", "data": {"name": "shutil.move", "type": "callable", "signature": "(src, dst)", "description": "Recursively move a file or directory to another location. This is\nsimilar to the Unix \"mv\" command. Return the file or directory's\ndestination.\n\nIf the destination is a directory or a symlink to a directory, the source\nis moved inside the directory. The destination path must not already\nexist.\n\nIf the destination already exists but is not a directory, it may be\noverwritten depending on os.rename() semantics.\n\nIf the destination is on our current filesystem, then rename() is used.\nOtherwise, src is copied to the destination and then removed. Symlinks are\nrecreated under the new name if os.rename() fails because of cross\nfilesystem renames.\n\nThe optional `copy_function` argument is a callable that will be used\nto copy the source or it will be delegated to `copytree`.\nBy default, copy2() is used, but any function that supports the same\nsignature (like copy()) can be used.\n\nA lot more could be done here...  A look at a mv.c shows a lot of\nthe issues this implementation glosses over.", "parameters": {"type": "object", "properties": {"src": {}, "dst": {}}}}}
{"task_id": "BigCodeBench/845", "data": {"name": "collections.Counter", "type": "class", "signature": "(iterable=None, /, **kwds)", "description": "Dict subclass for counting hashable items.  Sometimes called a bag\nor multiset.  Elements are stored as dictionary keys and their counts\nare stored as dictionary values.\n\n>>> c = Counter('abcdeabcdabcaba')  # count elements from a string\n\n>>> c.most_common(3)                # three most common elements\n[('a', 5), ('b', 4), ('c', 3)]\n>>> sorted(c)                       # list all unique elements\n['a', 'b', 'c', 'd', 'e']\n>>> ''.join(sorted(c.elements()))   # list elements with repetitions\n'aaaaabbbbcccdde'\n>>> sum(c.values())                 # total of all counts\n15\n\n>>> c['a']                          # count of letter 'a'\n5\n>>> for elem in 'shazam':           # update counts from an iterable\n...     c[elem] += 1                # by adding 1 to each element's count\n>>> c['a']                          # now there are seven 'a'\n7\n>>> del c['b']                      # remove all 'b'\n>>> c['b']                          # now there are zero 'b'\n0\n\n>>> d = Counter('simsalabim')       # make another counter\n>>> c.update(d)                     # add in the second counter\n>>> c['a']                          # now there are nine 'a'\n9\n\n>>> c.clear()                       # empty the counter\n>>> c\nCounter()\n\nNote:  If a count is set to zero or reduced to zero, it will remain\nin the counter until the entry is deleted or the counter is cleared:\n\n>>> c = Counter('aaabbc')\n>>> c['b'] -= 2                     # reduce the count of 'b' by two\n>>> c.most_common()                 # 'b' is still in, but its count is zero\n[('a', 3), ('c', 1), ('b', 0)]", "parameters": {"type": "object", "properties": {"iterable": {"type": "NoneType", "default": null}, "/": {}}}}}
{"task_id": "BigCodeBench/845", "data": {"name": "collections.Counter.keys", "type": "callable", "signature": "()", "description": "D.keys() -> a set-like object providing a view on D's keys", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/845", "data": {"name": "ALPHANUMERIC.sub(' ', text1).lower().split()"}}
{"task_id": "BigCodeBench/845", "data": {"name": "ALPHANUMERIC.sub(' ', text2).lower().split()"}}
{"task_id": "BigCodeBench/845", "data": {"name": "Levenshtein.ratio", "type": "callable", "signature": "(*args, **kwargs)", "description": "Compute similarity of two strings.\n\nratio(string1, string2)\n\nThe similarity is a number between 0 and 1, it's usually equal or\nsomewhat higher than difflib.SequenceMatcher.ratio(), because it's\nbased on real minimal edit distance.\n\nExamples:\n\n>>> ratio('Hello world!', 'Holly grail!')  # doctest: +ELLIPSIS\n0.583333...\n>>> ratio('Brian', 'Jesus')\n0.0\n\nReally?  I thought there was some similarity.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/845", "data": {"name": "numpy.sqrt", "type": "callable", "signature": "(*args, **kwargs)", "description": "sqrt(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n\nReturn the non-negative square-root of an array, element-wise.\n\nParameters\n----------\nx : array_like\n    The values whose square-roots are required.\nout : ndarray, None, or tuple of ndarray and None, optional\n    A location into which the result is stored. If provided, it must have\n    a shape that the inputs broadcast to. If not provided or None,\n    a freshly-allocated array is returned. A tuple (possible only as a\n    keyword argument) must have length equal to the number of outputs.\nwhere : array_like, optional\n    This condition is broadcast over the input. At locations where the\n    condition is True, the `out` array will be set to the ufunc result.\n    Elsewhere, the `out` array will retain its original value.\n    Note that if an uninitialized `out` array is created via the default\n    ``out=None``, locations within it where the condition is False will\n    remain uninitialized.\n**kwargs\n    For other keyword-only arguments, see the\n    :ref:`ufunc docs <ufuncs.kwargs>`.\n\nReturns\n-------\ny : ndarray\n    An array of the same shape as `x`, containing the positive\n    square-root of each element in `x`.  If any element in `x` is\n    complex, a complex array is returned (and the square-roots of\n    negative reals are calculated).  If all of the elements in `x`\n    are real, so is `y`, with negative elements returning ``nan``.\n    If `out` was provided, `y` is a reference to it.\n    This is a scalar if `x` is a scalar.\n\nSee Also\n--------\nemath.sqrt\n    A version which returns complex numbers when given negative reals.\n    Note: 0.0 and -0.0 are handled differently for complex inputs.\n\nNotes\n-----\n*sqrt* has--consistent with common convention--as its branch cut the\nreal \"interval\" [`-inf`, 0), and is continuous from above on it.\nA branch cut is a curve in the complex plane across which a given\ncomplex function fails to be continuous.\n\nExamples\n--------\n>>> np.sqrt([1,4,9])\narray([ 1.,  2.,  3.])\n\n>>> np.sqrt([4, -1, -3+4J])\narray([ 2.+0.j,  0.+1.j,  1.+2.j])\n\n>>> np.sqrt([4, -1, np.inf])\narray([ 2., nan, inf])", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/845", "data": {"name": "re.compile", "type": "callable", "signature": "(pattern)", "description": "Compile a regular expression pattern, returning a Pattern object.", "parameters": {"type": "object", "properties": {"pattern": {}}}}}
{"task_id": "BigCodeBench/845", "data": {"name": "re.compile.sub", "type": "callable", "signature": "(pattern, flags=0)", "description": "Compile a regular expression pattern, returning a Pattern object.", "parameters": {"type": "object", "properties": {"pattern": {}, "flags": {"type": "integer", "default": 0}}}}}
{"task_id": "BigCodeBench/854", "data": {"name": "functools.reduce", "type": "callable", "signature": "(*args, **kwargs)", "description": "reduce(function, iterable[, initial]) -> value\n\nApply a function of two arguments cumulatively to the items of a sequence\nor iterable, from left to right, so as to reduce the iterable to a single\nvalue.  For example, reduce(lambda x, y: x+y, [1, 2, 3, 4, 5]) calculates\n((((1+2)+3)+4)+5).  If initial is present, it is placed before the items\nof the iterable in the calculation, and serves as a default when the\niterable is empty.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/854", "data": {"name": "math.factorial", "type": "callable", "signature": "(x)", "description": "Find x!.\n\nRaise a ValueError if x is negative or non-integral.", "parameters": {"type": "object", "properties": {"x": {}}}}}
{"task_id": "BigCodeBench/857", "data": {"name": "glob.glob", "type": "callable", "signature": "(pathname)", "description": "Return a list of paths matching a pathname pattern.\n\nThe pattern may contain simple shell-style wildcards a la\nfnmatch. However, unlike fnmatch, filenames starting with a\ndot are special cases that are not matched by '*' and '?'\npatterns.\n\nIf recursive is true, the pattern '**' will match any files and\nzero or more directories and subdirectories.", "parameters": {"type": "object", "properties": {"pathname": {}}}}}
{"task_id": "BigCodeBench/857", "data": {"name": "os.path.basename", "type": "callable", "signature": "(p)", "description": "Returns the final component of a pathname", "parameters": {"type": "object", "properties": {"p": {}}}}}
{"task_id": "BigCodeBench/857", "data": {"name": "os.path.join", "type": "callable", "signature": "(a, *p)", "description": "Join two or more pathname components, inserting '/' as needed.\nIf any component is an absolute path, all previous path components\nwill be discarded.  An empty last part will result in a path that\nends with a separator.", "parameters": {"type": "object", "properties": {"a": {}}}}}
{"task_id": "BigCodeBench/857", "data": {"name": "shutil.move", "type": "callable", "signature": "(src, dst)", "description": "Recursively move a file or directory to another location. This is\nsimilar to the Unix \"mv\" command. Return the file or directory's\ndestination.\n\nIf the destination is a directory or a symlink to a directory, the source\nis moved inside the directory. The destination path must not already\nexist.\n\nIf the destination already exists but is not a directory, it may be\noverwritten depending on os.rename() semantics.\n\nIf the destination is on our current filesystem, then rename() is used.\nOtherwise, src is copied to the destination and then removed. Symlinks are\nrecreated under the new name if os.rename() fails because of cross\nfilesystem renames.\n\nThe optional `copy_function` argument is a callable that will be used\nto copy the source or it will be delegated to `copytree`.\nBy default, copy2() is used, but any function that supports the same\nsignature (like copy()) can be used.\n\nA lot more could be done here...  A look at a mv.c shows a lot of\nthe issues this implementation glosses over.", "parameters": {"type": "object", "properties": {"src": {}, "dst": {}}}}}
{"task_id": "BigCodeBench/857", "data": {"name": "time.sleep", "type": "callable", "signature": "(*args, **kwargs)", "description": "sleep(seconds)\n\nDelay execution for a given number of seconds.  The argument may be\na floating point number for subsecond precision.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/857", "data": {"name": "transferred_files.append(os.path.basename(src_file))"}}
{"task_id": "BigCodeBench/857", "data": {"name": "warnings.simplefilter", "type": "callable", "signature": "(action)", "description": "Insert a simple entry into the list of warnings filters (at the front).\n\nA simple filter matches all modules and messages.\n'action' -- one of \"error\", \"ignore\", \"always\", \"default\", \"module\",\n            or \"once\"\n'category' -- a class that the warning must be a subclass of\n'lineno' -- an integer line number, 0 matches all warnings\n'append' -- if true, append to the list of filters", "parameters": {"type": "object", "properties": {"action": {}}}}}
{"task_id": "BigCodeBench/857", "data": {"name": "warnings.warn", "type": "callable", "signature": "(message)", "description": "Issue a warning, or maybe ignore it or raise an exception.", "parameters": {"type": "object", "properties": {"message": {}}}}}
{"task_id": "BigCodeBench/865", "data": {"name": "numpy.array", "type": "callable", "signature": "(*args, **kwargs)", "description": "array(object, dtype=None, *, copy=True, order='K', subok=False, ndmin=0,\n      like=None)\n\nCreate an array.\n\nParameters\n----------\nobject : array_like\n    An array, any object exposing the array interface, an object whose\n    __array__ method returns an array, or any (nested) sequence.\n    If object is a scalar, a 0-dimensional array containing object is\n    returned.\ndtype : data-type, optional\n    The desired data-type for the array.  If not given, then the type will\n    be determined as the minimum type required to hold the objects in the\n    sequence.\ncopy : bool, optional\n    If true (default), then the object is copied.  Otherwise, a copy will\n    only be made if __array__ returns a copy, if obj is a nested sequence,\n    or if a copy is needed to satisfy any of the other requirements\n    (`dtype`, `order`, etc.).\norder : {'K', 'A', 'C', 'F'}, optional\n    Specify the memory layout of the array. If object is not an array, the\n    newly created array will be in C order (row major) unless 'F' is\n    specified, in which case it will be in Fortran order (column major).\n    If object is an array the following holds.\n\n    ===== ========= ===================================================\n    order  no copy                     copy=True\n    ===== ========= ===================================================\n    'K'   unchanged F & C order preserved, otherwise most similar order\n    'A'   unchanged F order if input is F and not C, otherwise C order\n    'C'   C order   C order\n    'F'   F order   F order\n    ===== ========= ===================================================\n\n    When ``copy=False`` and a copy is made for other reasons, the result is\n    the same as if ``copy=True``, with some exceptions for 'A', see the\n    Notes section. The default order is 'K'.\nsubok : bool, optional\n    If True, then sub-classes will be passed-through, otherwise\n    the returned array will be forced to be a base-class array (default).\nndmin : int, optional\n    Specifies the minimum number of dimensions that the resulting\n    array should have.  Ones will be prepended to the shape as\n    needed to meet this requirement.\nlike : array_like, optional\n    Reference object to allow the creation of arrays which are not\n    NumPy arrays. If an array-like passed in as ``like`` supports\n    the ``__array_function__`` protocol, the result will be defined\n    by it. In this case, it ensures the creation of an array object\n    compatible with that passed in via this argument.\n\n    .. versionadded:: 1.20.0\n\nReturns\n-------\nout : ndarray\n    An array object satisfying the specified requirements.\n\nSee Also\n--------\nempty_like : Return an empty array with shape and type of input.\nones_like : Return an array of ones with shape and type of input.\nzeros_like : Return an array of zeros with shape and type of input.\nfull_like : Return a new array with shape of input filled with value.\nempty : Return a new uninitialized array.\nones : Return a new array setting values to one.\nzeros : Return a new array setting values to zero.\nfull : Return a new array of given shape filled with value.\n\n\nNotes\n-----\nWhen order is 'A' and `object` is an array in neither 'C' nor 'F' order,\nand a copy is forced by a change in dtype, then the order of the result is\nnot necessarily 'C' as expected. This is likely a bug.\n\nExamples\n--------\n>>> np.array([1, 2, 3])\narray([1, 2, 3])\n\nUpcasting:\n\n>>> np.array([1, 2, 3.0])\narray([ 1.,  2.,  3.])\n\nMore than one dimension:\n\n>>> np.array([[1, 2], [3, 4]])\narray([[1, 2],\n       [3, 4]])\n\nMinimum dimensions 2:\n\n>>> np.array([1, 2, 3], ndmin=2)\narray([[1, 2, 3]])\n\nType provided:\n\n>>> np.array([1, 2, 3], dtype=complex)\narray([ 1.+0.j,  2.+0.j,  3.+0.j])\n\nData-type consisting of more than one element:\n\n>>> x = np.array([(1,2),(3,4)],dtype=[('a','<i4'),('b','<i4')])\n>>> x['a']\narray([1, 3])\n\nCreating an array from sub-classes:\n\n>>> np.array(np.mat('1 2; 3 4'))\narray([[1, 2],\n       [3, 4]])\n\n>>> np.array(np.mat('1 2; 3 4'), subok=True)\nmatrix([[1, 2],\n        [3, 4]])", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/865", "data": {"name": "scipy.stats.zscore", "type": "callable", "signature": "(a)", "description": "Compute the z score.\n\nCompute the z score of each value in the sample, relative to the\nsample mean and standard deviation.\n\nParameters\n----------\na : array_like\n    An array like object containing the sample data.\naxis : int or None, optional\n    Axis along which to operate. Default is 0. If None, compute over\n    the whole array `a`.\nddof : int, optional\n    Degrees of freedom correction in the calculation of the\n    standard deviation. Default is 0.\nnan_policy : {'propagate', 'raise', 'omit'}, optional\n    Defines how to handle when input contains nan. 'propagate' returns nan,\n    'raise' throws an error, 'omit' performs the calculations ignoring nan\n    values. Default is 'propagate'.  Note that when the value is 'omit',\n    nans in the input also propagate to the output, but they do not affect\n    the z-scores computed for the non-nan values.\n\nReturns\n-------\nzscore : array_like\n    The z-scores, standardized by mean and standard deviation of\n    input array `a`.\n\nSee Also\n--------\nnumpy.mean : Arithmetic average\nnumpy.std : Arithmetic standard deviation\nscipy.stats.gzscore : Geometric standard score\n\nNotes\n-----\nThis function preserves ndarray subclasses, and works also with\nmatrices and masked arrays (it uses `asanyarray` instead of\n`asarray` for parameters).\n\nReferences\n----------\n.. [1] \"Standard score\", *Wikipedia*,\n       https://en.wikipedia.org/wiki/Standard_score.\n.. [2] Huck, S. W., Cross, T. L., Clark, S. B, \"Overcoming misconceptions\n       about Z-scores\", Teaching Statistics, vol. 8, pp. 38-40, 1986\n\nExamples\n--------\n>>> import numpy as np\n>>> a = np.array([ 0.7972,  0.0767,  0.4383,  0.7866,  0.8091,\n...                0.1954,  0.6307,  0.6599,  0.1065,  0.0508])\n>>> from scipy import stats\n>>> stats.zscore(a)\narray([ 1.1273, -1.247 , -0.0552,  1.0923,  1.1664, -0.8559,  0.5786,\n        0.6748, -1.1488, -1.3324])\n\nComputing along a specified axis, using n-1 degrees of freedom\n(``ddof=1``) to calculate the standard deviation:\n\n>>> b = np.array([[ 0.3148,  0.0478,  0.6243,  0.4608],\n...               [ 0.7149,  0.0775,  0.6072,  0.9656],\n...               [ 0.6341,  0.1403,  0.9759,  0.4064],\n...               [ 0.5918,  0.6948,  0.904 ,  0.3721],\n...               [ 0.0921,  0.2481,  0.1188,  0.1366]])\n>>> stats.zscore(b, axis=1, ddof=1)\narray([[-0.19264823, -1.28415119,  1.07259584,  0.40420358],\n       [ 0.33048416, -1.37380874,  0.04251374,  1.00081084],\n       [ 0.26796377, -1.12598418,  1.23283094, -0.37481053],\n       [-0.22095197,  0.24468594,  1.19042819, -1.21416216],\n       [-0.82780366,  1.4457416 , -0.43867764, -0.1792603 ]])\n\nAn example with `nan_policy='omit'`:\n\n>>> x = np.array([[25.11, 30.10, np.nan, 32.02, 43.15],\n...               [14.95, 16.06, 121.25, 94.35, 29.81]])\n>>> stats.zscore(x, axis=1, nan_policy='omit')\narray([[-1.13490897, -0.37830299,         nan, -0.08718406,  1.60039602],\n       [-0.91611681, -0.89090508,  1.4983032 ,  0.88731639, -0.5785977 ]])", "parameters": {"type": "object", "properties": {"a": {}}}}}
{"task_id": "BigCodeBench/865", "data": {"name": "sklearn.preprocessing.MinMaxScaler", "type": "class", "signature": "()", "description": "Transform features by scaling each feature to a given range.\n\nThis estimator scales and translates each feature individually such\nthat it is in the given range on the training set, e.g. between\nzero and one.\n\nThe transformation is given by::\n\n    X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n    X_scaled = X_std * (max - min) + min\n\nwhere min, max = feature_range.\n\nThis transformation is often used as an alternative to zero mean,\nunit variance scaling.\n\n`MinMaxScaler` doesn't reduce the effect of outliers, but it linearly\nscales them down into a fixed range, where the largest occurring data point\ncorresponds to the maximum value and the smallest one corresponds to the\nminimum value. For an example visualization, refer to :ref:`Compare\nMinMaxScaler with other scalers <plot_all_scaling_minmax_scaler_section>`.\n\nRead more in the :ref:`User Guide <preprocessing_scaler>`.\n\nParameters\n----------\nfeature_range : tuple (min, max), default=(0, 1)\n    Desired range of transformed data.\n\ncopy : bool, default=True\n    Set to False to perform inplace row normalization and avoid a\n    copy (if the input is already a numpy array).\n\nclip : bool, default=False\n    Set to True to clip transformed values of held-out data to\n    provided `feature range`.\n\n    .. versionadded:: 0.24\n\nAttributes\n----------\nmin_ : ndarray of shape (n_features,)\n    Per feature adjustment for minimum. Equivalent to\n    ``min - X.min(axis=0) * self.scale_``\n\nscale_ : ndarray of shape (n_features,)\n    Per feature relative scaling of the data. Equivalent to\n    ``(max - min) / (X.max(axis=0) - X.min(axis=0))``\n\n    .. versionadded:: 0.17\n       *scale_* attribute.\n\ndata_min_ : ndarray of shape (n_features,)\n    Per feature minimum seen in the data\n\n    .. versionadded:: 0.17\n       *data_min_*\n\ndata_max_ : ndarray of shape (n_features,)\n    Per feature maximum seen in the data\n\n    .. versionadded:: 0.17\n       *data_max_*\n\ndata_range_ : ndarray of shape (n_features,)\n    Per feature range ``(data_max_ - data_min_)`` seen in the data\n\n    .. versionadded:: 0.17\n       *data_range_*\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nn_samples_seen_ : int\n    The number of samples processed by the estimator.\n    It will be reset on new calls to fit, but increments across\n    ``partial_fit`` calls.\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nminmax_scale : Equivalent function without the estimator API.\n\nNotes\n-----\nNaNs are treated as missing values: disregarded in fit, and maintained in\ntransform.\n\nExamples\n--------\n>>> from sklearn.preprocessing import MinMaxScaler\n>>> data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n>>> scaler = MinMaxScaler()\n>>> print(scaler.fit(data))\nMinMaxScaler()\n>>> print(scaler.data_max_)\n[ 1. 18.]\n>>> print(scaler.transform(data))\n[[0.   0.  ]\n [0.25 0.25]\n [0.5  0.5 ]\n [1.   1.  ]]\n>>> print(scaler.transform([[2, 2]]))\n[[1.5 0. ]]", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/865", "data": {"name": "sklearn.preprocessing.MinMaxScaler.fit_transform", "type": "callable", "signature": "(self, X)", "description": "Fit to data, then transform it.\n\nFits transformer to `X` and `y` with optional parameters `fit_params`\nand returns a transformed version of `X`.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Input samples.\n\ny :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n    Target values (None for unsupervised transformations).\n\n**fit_params : dict\n    Additional fit parameters.\n\nReturns\n-------\nX_new : ndarray array of shape (n_samples, n_features_new)\n    Transformed array.", "parameters": {"type": "object", "properties": {"X": {}}}}}
{"task_id": "BigCodeBench/870", "data": {"name": "mean_values.append(np.nan)"}}
{"task_id": "BigCodeBench/870", "data": {"name": "mean_values.append(np.nanmean(numeric_values))"}}
{"task_id": "BigCodeBench/870", "data": {"name": "numpy.nan", "type": "constant", "signature": null, "description": "Convert a string or number to a floating point number, if possible.", "value": "nan", "parameters": {"type": "constant"}}}
{"task_id": "BigCodeBench/870", "data": {"name": "numpy.nanmean", "type": "callable", "signature": "(a)", "description": "Compute the arithmetic mean along the specified axis, ignoring NaNs.\n\nReturns the average of the array elements.  The average is taken over\nthe flattened array by default, otherwise over the specified axis.\n`float64` intermediate and return values are used for integer inputs.\n\nFor all-NaN slices, NaN is returned and a `RuntimeWarning` is raised.\n\n.. versionadded:: 1.8.0\n\nParameters\n----------\na : array_like\n    Array containing numbers whose mean is desired. If `a` is not an\n    array, a conversion is attempted.\naxis : {int, tuple of int, None}, optional\n    Axis or axes along which the means are computed. The default is to compute\n    the mean of the flattened array.\ndtype : data-type, optional\n    Type to use in computing the mean.  For integer inputs, the default\n    is `float64`; for inexact inputs, it is the same as the input\n    dtype.\nout : ndarray, optional\n    Alternate output array in which to place the result.  The default\n    is ``None``; if provided, it must have the same shape as the\n    expected output, but the type will be cast if necessary. See\n    :ref:`ufuncs-output-type` for more details.\nkeepdims : bool, optional\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the original `a`.\n\n    If the value is anything but the default, then\n    `keepdims` will be passed through to the `mean` or `sum` methods\n    of sub-classes of `ndarray`.  If the sub-classes methods\n    does not implement `keepdims` any exceptions will be raised.\nwhere : array_like of bool, optional\n    Elements to include in the mean. See `~numpy.ufunc.reduce` for details.\n\n    .. versionadded:: 1.22.0\n\nReturns\n-------\nm : ndarray, see dtype parameter above\n    If `out=None`, returns a new array containing the mean values,\n    otherwise a reference to the output array is returned. Nan is\n    returned for slices that contain only NaNs.\n\nSee Also\n--------\naverage : Weighted average\nmean : Arithmetic mean taken while not ignoring NaNs\nvar, nanvar\n\nNotes\n-----\nThe arithmetic mean is the sum of the non-NaN elements along the axis\ndivided by the number of non-NaN elements.\n\nNote that for floating-point input, the mean is computed using the same\nprecision the input has.  Depending on the input data, this can cause\nthe results to be inaccurate, especially for `float32`.  Specifying a\nhigher-precision accumulator using the `dtype` keyword can alleviate\nthis issue.\n\nExamples\n--------\n>>> a = np.array([[1, np.nan], [3, 4]])\n>>> np.nanmean(a)\n2.6666666666666665\n>>> np.nanmean(a, axis=0)\narray([2.,  4.])\n>>> np.nanmean(a, axis=1)\narray([1.,  3.5]) # may vary", "parameters": {"type": "object", "properties": {"a": {}}}}}
{"task_id": "BigCodeBench/879", "data": {"name": "data.empty"}}
{"task_id": "BigCodeBench/879", "data": {"name": "numpy.issubdtype", "type": "callable", "signature": "(arg1, arg2)", "description": "Returns True if first argument is a typecode lower/equal in type hierarchy.\n\nThis is like the builtin :func:`issubclass`, but for `dtype`\\ s.\n\nParameters\n----------\narg1, arg2 : dtype_like\n    `dtype` or object coercible to one\n\nReturns\n-------\nout : bool\n\nSee Also\n--------\n:ref:`arrays.scalars` : Overview of the numpy type hierarchy.\nissubsctype, issubclass_\n\nExamples\n--------\n`issubdtype` can be used to check the type of arrays:\n\n>>> ints = np.array([1, 2, 3], dtype=np.int32)\n>>> np.issubdtype(ints.dtype, np.integer)\nTrue\n>>> np.issubdtype(ints.dtype, np.floating)\nFalse\n\n>>> floats = np.array([1, 2, 3], dtype=np.float32)\n>>> np.issubdtype(floats.dtype, np.integer)\nFalse\n>>> np.issubdtype(floats.dtype, np.floating)\nTrue\n\nSimilar types of different sizes are not subdtypes of each other:\n\n>>> np.issubdtype(np.float64, np.float32)\nFalse\n>>> np.issubdtype(np.float32, np.float64)\nFalse\n\nbut both are subtypes of `floating`:\n\n>>> np.issubdtype(np.float64, np.floating)\nTrue\n>>> np.issubdtype(np.float32, np.floating)\nTrue\n\nFor convenience, dtype-like objects are allowed too:\n\n>>> np.issubdtype('S1', np.string_)\nTrue\n>>> np.issubdtype('i4', np.signedinteger)\nTrue", "parameters": {"type": "object", "properties": {"arg1": {}, "arg2": {}}}}}
{"task_id": "BigCodeBench/879", "data": {"name": "pandas.crosstab", "type": "callable", "signature": "(index, columns)", "description": "Compute a simple cross tabulation of two (or more) factors.\n\nBy default, computes a frequency table of the factors unless an\narray of values and an aggregation function are passed.\n\nParameters\n----------\nindex : array-like, Series, or list of arrays/Series\n    Values to group by in the rows.\ncolumns : array-like, Series, or list of arrays/Series\n    Values to group by in the columns.\nvalues : array-like, optional\n    Array of values to aggregate according to the factors.\n    Requires `aggfunc` be specified.\nrownames : sequence, default None\n    If passed, must match number of row arrays passed.\ncolnames : sequence, default None\n    If passed, must match number of column arrays passed.\naggfunc : function, optional\n    If specified, requires `values` be specified as well.\nmargins : bool, default False\n    Add row/column margins (subtotals).\nmargins_name : str, default 'All'\n    Name of the row/column that will contain the totals\n    when margins is True.\ndropna : bool, default True\n    Do not include columns whose entries are all NaN.\nnormalize : bool, {'all', 'index', 'columns'}, or {0,1}, default False\n    Normalize by dividing all values by the sum of values.\n\n    - If passed 'all' or `True`, will normalize over all values.\n    - If passed 'index' will normalize over each row.\n    - If passed 'columns' will normalize over each column.\n    - If margins is `True`, will also normalize margin values.\n\nReturns\n-------\nDataFrame\n    Cross tabulation of the data.\n\nSee Also\n--------\nDataFrame.pivot : Reshape data based on column values.\npivot_table : Create a pivot table as a DataFrame.\n\nNotes\n-----\nAny Series passed will have their name attributes used unless row or column\nnames for the cross-tabulation are specified.\n\nAny input passed containing Categorical data will have **all** of its\ncategories included in the cross-tabulation, even if the actual data does\nnot contain any instances of a particular category.\n\nIn the event that there aren't overlapping indexes an empty DataFrame will\nbe returned.\n\nReference :ref:`the user guide <reshaping.crosstabulations>` for more examples.\n\nExamples\n--------\n>>> a = np.array([\"foo\", \"foo\", \"foo\", \"foo\", \"bar\", \"bar\",\n...               \"bar\", \"bar\", \"foo\", \"foo\", \"foo\"], dtype=object)\n>>> b = np.array([\"one\", \"one\", \"one\", \"two\", \"one\", \"one\",\n...               \"one\", \"two\", \"two\", \"two\", \"one\"], dtype=object)\n>>> c = np.array([\"dull\", \"dull\", \"shiny\", \"dull\", \"dull\", \"shiny\",\n...               \"shiny\", \"dull\", \"shiny\", \"shiny\", \"shiny\"],\n...              dtype=object)\n>>> pd.crosstab(a, [b, c], rownames=['a'], colnames=['b', 'c'])\nb   one        two\nc   dull shiny dull shiny\na\nbar    1     2    1     0\nfoo    2     2    1     2\n\nHere 'c' and 'f' are not represented in the data and will not be\nshown in the output because dropna is True by default. Set\ndropna=False to preserve categories with no data.\n\n>>> foo = pd.Categorical(['a', 'b'], categories=['a', 'b', 'c'])\n>>> bar = pd.Categorical(['d', 'e'], categories=['d', 'e', 'f'])\n>>> pd.crosstab(foo, bar)\ncol_0  d  e\nrow_0\na      1  0\nb      0  1\n>>> pd.crosstab(foo, bar, dropna=False)\ncol_0  d  e  f\nrow_0\na      1  0  0\nb      0  1  0\nc      0  0  0", "parameters": {"type": "object", "properties": {"index": {}, "columns": {}}}}}
{"task_id": "BigCodeBench/879", "data": {"name": "scipy.stats.chi2_contingency", "type": "callable", "signature": "(observed)", "description": "Chi-square test of independence of variables in a contingency table.\n\nThis function computes the chi-square statistic and p-value for the\nhypothesis test of independence of the observed frequencies in the\ncontingency table [1]_ `observed`.  The expected frequencies are computed\nbased on the marginal sums under the assumption of independence; see\n`scipy.stats.contingency.expected_freq`.  The number of degrees of\nfreedom is (expressed using numpy functions and attributes)::\n\n    dof = observed.size - sum(observed.shape) + observed.ndim - 1\n\n\nParameters\n----------\nobserved : array_like\n    The contingency table. The table contains the observed frequencies\n    (i.e. number of occurrences) in each category.  In the two-dimensional\n    case, the table is often described as an \"R x C table\".\ncorrection : bool, optional\n    If True, *and* the degrees of freedom is 1, apply Yates' correction\n    for continuity.  The effect of the correction is to adjust each\n    observed value by 0.5 towards the corresponding expected value.\nlambda_ : float or str, optional\n    By default, the statistic computed in this test is Pearson's\n    chi-squared statistic [2]_.  `lambda_` allows a statistic from the\n    Cressie-Read power divergence family [3]_ to be used instead.  See\n    `scipy.stats.power_divergence` for details.\n\nReturns\n-------\nres : Chi2ContingencyResult\n    An object containing attributes:\n\n    statistic : float\n        The test statistic.\n    pvalue : float\n        The p-value of the test.\n    dof : int\n        The degrees of freedom.\n    expected_freq : ndarray, same shape as `observed`\n        The expected frequencies, based on the marginal sums of the table.\n\nSee Also\n--------\nscipy.stats.contingency.expected_freq\nscipy.stats.fisher_exact\nscipy.stats.chisquare\nscipy.stats.power_divergence\nscipy.stats.barnard_exact\nscipy.stats.boschloo_exact\n\nNotes\n-----\nAn often quoted guideline for the validity of this calculation is that\nthe test should be used only if the observed and expected frequencies\nin each cell are at least 5.\n\nThis is a test for the independence of different categories of a\npopulation. The test is only meaningful when the dimension of\n`observed` is two or more.  Applying the test to a one-dimensional\ntable will always result in `expected` equal to `observed` and a\nchi-square statistic equal to 0.\n\nThis function does not handle masked arrays, because the calculation\ndoes not make sense with missing values.\n\nLike `scipy.stats.chisquare`, this function computes a chi-square\nstatistic; the convenience this function provides is to figure out the\nexpected frequencies and degrees of freedom from the given contingency\ntable. If these were already known, and if the Yates' correction was not\nrequired, one could use `scipy.stats.chisquare`.  That is, if one calls::\n\n    res = chi2_contingency(obs, correction=False)\n\nthen the following is true::\n\n    (res.statistic, res.pvalue) == stats.chisquare(obs.ravel(),\n                                                   f_exp=ex.ravel(),\n                                                   ddof=obs.size - 1 - dof)\n\nThe `lambda_` argument was added in version 0.13.0 of scipy.\n\nReferences\n----------\n.. [1] \"Contingency table\",\n       https://en.wikipedia.org/wiki/Contingency_table\n.. [2] \"Pearson's chi-squared test\",\n       https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test\n.. [3] Cressie, N. and Read, T. R. C., \"Multinomial Goodness-of-Fit\n       Tests\", J. Royal Stat. Soc. Series B, Vol. 46, No. 3 (1984),\n       pp. 440-464.\n.. [4] Berger, Jeffrey S. et al. \"Aspirin for the Primary Prevention of\n       Cardiovascular Events in Women and Men: A Sex-Specific\n       Meta-analysis of Randomized Controlled Trials.\"\n       JAMA, 295(3):306-313, :doi:`10.1001/jama.295.3.306`, 2006.\n\nExamples\n--------\nIn [4]_, the use of aspirin to prevent cardiovascular events in women\nand men was investigated. The study notably concluded:\n\n    ...aspirin therapy reduced the risk of a composite of\n    cardiovascular events due to its effect on reducing the risk of\n    ischemic stroke in women [...]\n\nThe article lists studies of various cardiovascular events. Let's\nfocus on the ischemic stoke in women.\n\nThe following table summarizes the results of the experiment in which\nparticipants took aspirin or a placebo on a regular basis for several\nyears. Cases of ischemic stroke were recorded::\n\n                      Aspirin   Control/Placebo\n    Ischemic stroke     176           230\n    No stroke         21035         21018\n\nIs there evidence that the aspirin reduces the risk of ischemic stroke?\nWe begin by formulating a null hypothesis :math:`H_0`:\n\n    The effect of aspirin is equivalent to that of placebo.\n\nLet's assess the plausibility of this hypothesis with\na chi-square test.\n\n>>> import numpy as np\n>>> from scipy.stats import chi2_contingency\n>>> table = np.array([[176, 230], [21035, 21018]])\n>>> res = chi2_contingency(table)\n>>> res.statistic\n6.892569132546561\n>>> res.pvalue\n0.008655478161175739\n\nUsing a significance level of 5%, we would reject the null hypothesis in\nfavor of the alternative hypothesis: \"the effect of aspirin\nis not equivalent to the effect of placebo\".\nBecause `scipy.stats.contingency.chi2_contingency` performs a two-sided\ntest, the alternative hypothesis does not indicate the direction of the\neffect. We can use `stats.contingency.odds_ratio` to support the\nconclusion that aspirin *reduces* the risk of ischemic stroke.\n\nBelow are further examples showing how larger contingency tables can be\ntested.\n\nA two-way example (2 x 3):\n\n>>> obs = np.array([[10, 10, 20], [20, 20, 20]])\n>>> res = chi2_contingency(obs)\n>>> res.statistic\n2.7777777777777777\n>>> res.pvalue\n0.24935220877729619\n>>> res.dof\n2\n>>> res.expected_freq\narray([[ 12.,  12.,  16.],\n       [ 18.,  18.,  24.]])\n\nPerform the test using the log-likelihood ratio (i.e. the \"G-test\")\ninstead of Pearson's chi-squared statistic.\n\n>>> res = chi2_contingency(obs, lambda_=\"log-likelihood\")\n>>> res.statistic\n2.7688587616781319\n>>> res.pvalue\n0.25046668010954165\n\nA four-way example (2 x 2 x 2 x 2):\n\n>>> obs = np.array(\n...     [[[[12, 17],\n...        [11, 16]],\n...       [[11, 12],\n...        [15, 16]]],\n...      [[[23, 15],\n...        [30, 22]],\n...       [[14, 17],\n...        [15, 16]]]])\n>>> res = chi2_contingency(obs)\n>>> res.statistic\n8.7584514426741897\n>>> res.pvalue\n0.64417725029295503", "parameters": {"type": "object", "properties": {"observed": {}}}}}
{"task_id": "BigCodeBench/897", "data": {"name": "matplotlib.pyplot.subplots", "type": "class", "signature": "()", "description": "Create a figure and a set of subplots.\n\nThis utility wrapper makes it convenient to create common layouts of\nsubplots, including the enclosing figure object, in a single call.\n\nParameters\n----------\nnrows, ncols : int, default: 1\n    Number of rows/columns of the subplot grid.\n\nsharex, sharey : bool or {'none', 'all', 'row', 'col'}, default: False\n    Controls sharing of properties among x (*sharex*) or y (*sharey*)\n    axes:\n\n    - True or 'all': x- or y-axis will be shared among all subplots.\n    - False or 'none': each subplot x- or y-axis will be independent.\n    - 'row': each subplot row will share an x- or y-axis.\n    - 'col': each subplot column will share an x- or y-axis.\n\n    When subplots have a shared x-axis along a column, only the x tick\n    labels of the bottom subplot are created. Similarly, when subplots\n    have a shared y-axis along a row, only the y tick labels of the first\n    column subplot are created. To later turn other subplots' ticklabels\n    on, use `~matplotlib.axes.Axes.tick_params`.\n\n    When subplots have a shared axis that has units, calling\n    `~matplotlib.axis.Axis.set_units` will update each axis with the\n    new units.\n\nsqueeze : bool, default: True\n    - If True, extra dimensions are squeezed out from the returned\n      array of `~matplotlib.axes.Axes`:\n\n      - if only one subplot is constructed (nrows=ncols=1), the\n        resulting single Axes object is returned as a scalar.\n      - for Nx1 or 1xM subplots, the returned object is a 1D numpy\n        object array of Axes objects.\n      - for NxM, subplots with N>1 and M>1 are returned as a 2D array.\n\n    - If False, no squeezing at all is done: the returned Axes object is\n      always a 2D array containing Axes instances, even if it ends up\n      being 1x1.\n\nwidth_ratios : array-like of length *ncols*, optional\n    Defines the relative widths of the columns. Each column gets a\n    relative width of ``width_ratios[i] / sum(width_ratios)``.\n    If not given, all columns will have the same width.  Equivalent\n    to ``gridspec_kw={'width_ratios': [...]}``.\n\nheight_ratios : array-like of length *nrows*, optional\n    Defines the relative heights of the rows. Each row gets a\n    relative height of ``height_ratios[i] / sum(height_ratios)``.\n    If not given, all rows will have the same height. Convenience\n    for ``gridspec_kw={'height_ratios': [...]}``.\n\nsubplot_kw : dict, optional\n    Dict with keywords passed to the\n    `~matplotlib.figure.Figure.add_subplot` call used to create each\n    subplot.\n\ngridspec_kw : dict, optional\n    Dict with keywords passed to the `~matplotlib.gridspec.GridSpec`\n    constructor used to create the grid the subplots are placed on.\n\n**fig_kw\n    All additional keyword arguments are passed to the\n    `.pyplot.figure` call.\n\nReturns\n-------\nfig : `.Figure`\n\nax : `~matplotlib.axes.Axes` or array of Axes\n    *ax* can be either a single `~.axes.Axes` object, or an array of Axes\n    objects if more than one subplot was created.  The dimensions of the\n    resulting array can be controlled with the squeeze keyword, see above.\n\n    Typical idioms for handling the return value are::\n\n        # using the variable ax for single a Axes\n        fig, ax = plt.subplots()\n\n        # using the variable axs for multiple Axes\n        fig, axs = plt.subplots(2, 2)\n\n        # using tuple unpacking for multiple Axes\n        fig, (ax1, ax2) = plt.subplots(1, 2)\n        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n\n    The names ``ax`` and pluralized ``axs`` are preferred over ``axes``\n    because for the latter it's not clear if it refers to a single\n    `~.axes.Axes` instance or a collection of these.\n\nSee Also\n--------\n.pyplot.figure\n.pyplot.subplot\n.pyplot.axes\n.Figure.subplots\n.Figure.add_subplot\n\nExamples\n--------\n::\n\n    # First create some toy data:\n    x = np.linspace(0, 2*np.pi, 400)\n    y = np.sin(x**2)\n\n    # Create just a figure and only one subplot\n    fig, ax = plt.subplots()\n    ax.plot(x, y)\n    ax.set_title('Simple plot')\n\n    # Create two subplots and unpack the output array immediately\n    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n    ax1.plot(x, y)\n    ax1.set_title('Sharing Y axis')\n    ax2.scatter(x, y)\n\n    # Create four polar axes and access them through the returned array\n    fig, axs = plt.subplots(2, 2, subplot_kw=dict(projection=\"polar\"))\n    axs[0, 0].plot(x, y)\n    axs[1, 1].scatter(x, y)\n\n    # Share a X axis with each column of subplots\n    plt.subplots(2, 2, sharex='col')\n\n    # Share a Y axis with each row of subplots\n    plt.subplots(2, 2, sharey='row')\n\n    # Share both X and Y axes with all subplots\n    plt.subplots(2, 2, sharex='all', sharey='all')\n\n    # Note that this is the same as\n    plt.subplots(2, 2, sharex=True, sharey=True)\n\n    # Create figure number 10 with a single subplot\n    # and clears it if it already exists.\n    fig, ax = plt.subplots(num=10, clear=True)", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/897", "data": {"name": "matplotlib.pyplot.subplots[1].set_ylabel", "type": "method", "signature": "(ylabel, fontdict=None, labelpad=None, *, loc=None, **kwargs)", "description": "Set the label for the y-axis.\n\nParameters\n----------\nylabel : str\n    The label text.\n\nlabelpad : float, default: :rc:`axes.labelpad`\n    Spacing in points from the Axes bounding box including ticks\n    and tick labels.  If None, the previous value is left as is.\n\nloc : {'bottom', 'center', 'top'}, default: :rc:`yaxis.labellocation`\n    The label position. This is a high-level alternative for passing\n    parameters *y* and *horizontalalignment*.\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    `.Text` properties control the appearance of the label.\n\nSee Also\n--------\ntext : Documents the properties supported by `.Text`.", "parameters": {"type": "object", "properties": {"ylabel": {}, "fontdict": {"type": "NoneType", "default": null}, "labelpad": {"type": "NoneType", "default": null}, "loc": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/897", "data": {"name": "matplotlib.pyplot.subplots[1].set_xlabel", "type": "method", "signature": "(xlabel, fontdict=None, labelpad=None, *, loc=None, **kwargs)", "description": "Set the label for the x-axis.\n\nParameters\n----------\nxlabel : str\n    The label text.\n\nlabelpad : float, default: :rc:`axes.labelpad`\n    Spacing in points from the Axes bounding box including ticks\n    and tick labels.  If None, the previous value is left as is.\n\nloc : {'left', 'center', 'right'}, default: :rc:`xaxis.labellocation`\n    The label position. This is a high-level alternative for passing\n    parameters *x* and *horizontalalignment*.\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    `.Text` properties control the appearance of the label.\n\nSee Also\n--------\ntext : Documents the properties supported by `.Text`.", "parameters": {"type": "object", "properties": {"xlabel": {}, "fontdict": {"type": "NoneType", "default": null}, "labelpad": {"type": "NoneType", "default": null}, "loc": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/897", "data": {"name": "matplotlib.pyplot.subplots[1].set_title", "type": "method", "signature": "(label, fontdict=None, loc=None, pad=None, *, y=None, **kwargs)", "description": "Set a title for the Axes.\n\nSet one of the three available Axes titles. The available titles\nare positioned above the Axes in the center, flush with the left\nedge, and flush with the right edge.\n\nParameters\n----------\nlabel : str\n    Text to use for the title\n\nfontdict : dict\n\n    .. admonition:: Discouraged\n\n       The use of *fontdict* is discouraged. Parameters should be passed as\n       individual keyword arguments or using dictionary-unpacking\n       ``set_title(..., **fontdict)``.\n\n    A dictionary controlling the appearance of the title text,\n    the default *fontdict* is::\n\n       {'fontsize': rcParams['axes.titlesize'],\n        'fontweight': rcParams['axes.titleweight'],\n        'color': rcParams['axes.titlecolor'],\n        'verticalalignment': 'baseline',\n        'horizontalalignment': loc}\n\nloc : {'center', 'left', 'right'}, default: :rc:`axes.titlelocation`\n    Which title to set.\n\ny : float, default: :rc:`axes.titley`\n    Vertical Axes location for the title (1.0 is the top).  If\n    None (the default) and :rc:`axes.titley` is also None, y is\n    determined automatically to avoid decorators on the Axes.\n\npad : float, default: :rc:`axes.titlepad`\n    The offset of the title from the top of the Axes, in points.\n\nReturns\n-------\n`.Text`\n    The matplotlib text instance representing the title\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    Other keyword arguments are text properties, see `.Text` for a list\n    of valid text properties.", "parameters": {"type": "object", "properties": {"label": {}, "fontdict": {"type": "NoneType", "default": null}, "loc": {"type": "NoneType", "default": null}, "pad": {"type": "NoneType", "default": null}, "y": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/897", "data": {"name": "matplotlib.pyplot.subplots[1].hist", "type": "method", "signature": "(x, bins=None, range=None, density=False, weights=None, cumulative=False, bottom=None, histtype='bar', align='mid', orientation='vertical', rwidth=None, log=False, color=None, label=None, stacked=False, *, data=None, **kwargs)", "description": "Compute and plot a histogram.\n\nThis method uses `numpy.histogram` to bin the data in *x* and count the\nnumber of values in each bin, then draws the distribution either as a\n`.BarContainer` or `.Polygon`. The *bins*, *range*, *density*, and\n*weights* parameters are forwarded to `numpy.histogram`.\n\nIf the data has already been binned and counted, use `~.bar` or\n`~.stairs` to plot the distribution::\n\n    counts, bins = np.histogram(x)\n    plt.stairs(counts, bins)\n\nAlternatively, plot pre-computed bins and counts using ``hist()`` by\ntreating each bin as a single point with a weight equal to its count::\n\n    plt.hist(bins[:-1], bins, weights=counts)\n\nThe data input *x* can be a singular array, a list of datasets of\npotentially different lengths ([*x0*, *x1*, ...]), or a 2D ndarray in\nwhich each column is a dataset. Note that the ndarray form is\ntransposed relative to the list form. If the input is an array, then\nthe return value is a tuple (*n*, *bins*, *patches*); if the input is a\nsequence of arrays, then the return value is a tuple\n([*n0*, *n1*, ...], *bins*, [*patches0*, *patches1*, ...]).\n\nMasked arrays are not supported.\n\nParameters\n----------\nx : (n,) array or sequence of (n,) arrays\n    Input values, this takes either a single array or a sequence of\n    arrays which are not required to be of the same length.\n\nbins : int or sequence or str, default: :rc:`hist.bins`\n    If *bins* is an integer, it defines the number of equal-width bins\n    in the range.\n\n    If *bins* is a sequence, it defines the bin edges, including the\n    left edge of the first bin and the right edge of the last bin;\n    in this case, bins may be unequally spaced.  All but the last\n    (righthand-most) bin is half-open.  In other words, if *bins* is::\n\n        [1, 2, 3, 4]\n\n    then the first bin is ``[1, 2)`` (including 1, but excluding 2) and\n    the second ``[2, 3)``.  The last bin, however, is ``[3, 4]``, which\n    *includes* 4.\n\n    If *bins* is a string, it is one of the binning strategies\n    supported by `numpy.histogram_bin_edges`: 'auto', 'fd', 'doane',\n    'scott', 'stone', 'rice', 'sturges', or 'sqrt'.\n\nrange : tuple or None, default: None\n    The lower and upper range of the bins. Lower and upper outliers\n    are ignored. If not provided, *range* is ``(x.min(), x.max())``.\n    Range has no effect if *bins* is a sequence.\n\n    If *bins* is a sequence or *range* is specified, autoscaling\n    is based on the specified bin range instead of the\n    range of x.\n\ndensity : bool, default: False\n    If ``True``, draw and return a probability density: each bin\n    will display the bin's raw count divided by the total number of\n    counts *and the bin width*\n    (``density = counts / (sum(counts) * np.diff(bins))``),\n    so that the area under the histogram integrates to 1\n    (``np.sum(density * np.diff(bins)) == 1``).\n\n    If *stacked* is also ``True``, the sum of the histograms is\n    normalized to 1.\n\nweights : (n,) array-like or None, default: None\n    An array of weights, of the same shape as *x*.  Each value in\n    *x* only contributes its associated weight towards the bin count\n    (instead of 1).  If *density* is ``True``, the weights are\n    normalized, so that the integral of the density over the range\n    remains 1.\n\ncumulative : bool or -1, default: False\n    If ``True``, then a histogram is computed where each bin gives the\n    counts in that bin plus all bins for smaller values. The last bin\n    gives the total number of datapoints.\n\n    If *density* is also ``True`` then the histogram is normalized such\n    that the last bin equals 1.\n\n    If *cumulative* is a number less than 0 (e.g., -1), the direction\n    of accumulation is reversed.  In this case, if *density* is also\n    ``True``, then the histogram is normalized such that the first bin\n    equals 1.\n\nbottom : array-like, scalar, or None, default: None\n    Location of the bottom of each bin, i.e. bins are drawn from\n    ``bottom`` to ``bottom + hist(x, bins)`` If a scalar, the bottom\n    of each bin is shifted by the same amount. If an array, each bin\n    is shifted independently and the length of bottom must match the\n    number of bins. If None, defaults to 0.\n\nhisttype : {'bar', 'barstacked', 'step', 'stepfilled'}, default: 'bar'\n    The type of histogram to draw.\n\n    - 'bar' is a traditional bar-type histogram.  If multiple data\n      are given the bars are arranged side by side.\n    - 'barstacked' is a bar-type histogram where multiple\n      data are stacked on top of each other.\n    - 'step' generates a lineplot that is by default unfilled.\n    - 'stepfilled' generates a lineplot that is by default filled.\n\nalign : {'left', 'mid', 'right'}, default: 'mid'\n    The horizontal alignment of the histogram bars.\n\n    - 'left': bars are centered on the left bin edges.\n    - 'mid': bars are centered between the bin edges.\n    - 'right': bars are centered on the right bin edges.\n\norientation : {'vertical', 'horizontal'}, default: 'vertical'\n    If 'horizontal', `~.Axes.barh` will be used for bar-type histograms\n    and the *bottom* kwarg will be the left edges.\n\nrwidth : float or None, default: None\n    The relative width of the bars as a fraction of the bin width.  If\n    ``None``, automatically compute the width.\n\n    Ignored if *histtype* is 'step' or 'stepfilled'.\n\nlog : bool, default: False\n    If ``True``, the histogram axis will be set to a log scale.\n\ncolor : color or array-like of colors or None, default: None\n    Color or sequence of colors, one per dataset.  Default (``None``)\n    uses the standard line color sequence.\n\nlabel : str or None, default: None\n    String, or sequence of strings to match multiple datasets.  Bar\n    charts yield multiple patches per dataset, but only the first gets\n    the label, so that `~.Axes.legend` will work as expected.\n\nstacked : bool, default: False\n    If ``True``, multiple data are stacked on top of each other If\n    ``False`` multiple data are arranged side by side if histtype is\n    'bar' or on top of each other if histtype is 'step'\n\nReturns\n-------\nn : array or list of arrays\n    The values of the histogram bins. See *density* and *weights* for a\n    description of the possible semantics.  If input *x* is an array,\n    then this is an array of length *nbins*. If input is a sequence of\n    arrays ``[data1, data2, ...]``, then this is a list of arrays with\n    the values of the histograms for each of the arrays in the same\n    order.  The dtype of the array *n* (or of its element arrays) will\n    always be float even if no weighting or normalization is used.\n\nbins : array\n    The edges of the bins. Length nbins + 1 (nbins left edges and right\n    edge of last bin).  Always a single array even when multiple data\n    sets are passed in.\n\npatches : `.BarContainer` or list of a single `.Polygon` or list of such objects\n    Container of individual artists used to create the histogram\n    or list of such containers if there are multiple input datasets.\n\nOther Parameters\n----------------\ndata : indexable object, optional\n    If given, the following parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` (unless this raises an exception):\n\n    *x*, *weights*\n\n**kwargs\n    `~matplotlib.patches.Patch` properties\n\nSee Also\n--------\nhist2d : 2D histogram with rectangular bins\nhexbin : 2D histogram with hexagonal bins\nstairs : Plot a pre-computed histogram\nbar : Plot a pre-computed histogram\n\nNotes\n-----\nFor large numbers of bins (>1000), plotting can be significantly\naccelerated by using `~.Axes.stairs` to plot a pre-computed histogram\n(``plt.stairs(*np.histogram(data))``), or by setting *histtype* to\n'step' or 'stepfilled' rather than 'bar' or 'barstacked'.", "parameters": {"type": "object", "properties": {"x": {}, "bins": {"type": "NoneType", "default": null}, "range": {"type": "NoneType", "default": null}, "density": {"type": "bool", "default": false}, "weights": {"type": "NoneType", "default": null}, "cumulative": {"type": "bool", "default": false}, "bottom": {"type": "NoneType", "default": null}, "histtype": {"type": "str", "default": "bar"}, "align": {"type": "str", "default": "mid"}, "orientation": {"type": "str", "default": "vertical"}, "rwidth": {"type": "NoneType", "default": null}, "log": {"type": "bool", "default": false}, "color": {"type": "NoneType", "default": null}, "label": {"type": "NoneType", "default": null}, "stacked": {"type": "bool", "default": false}, "data": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/897", "data": {"name": "numpy.arange", "type": "callable", "signature": "(*args, **kwargs)", "description": "arange([start,] stop[, step,], dtype=None, *, like=None)\n\nReturn evenly spaced values within a given interval.\n\n``arange`` can be called with a varying number of positional arguments:\n\n* ``arange(stop)``: Values are generated within the half-open interval\n  ``[0, stop)`` (in other words, the interval including `start` but\n  excluding `stop`).\n* ``arange(start, stop)``: Values are generated within the half-open\n  interval ``[start, stop)``.\n* ``arange(start, stop, step)`` Values are generated within the half-open\n  interval ``[start, stop)``, with spacing between values given by\n  ``step``.\n\nFor integer arguments the function is roughly equivalent to the Python\nbuilt-in :py:class:`range`, but returns an ndarray rather than a ``range``\ninstance.\n\nWhen using a non-integer step, such as 0.1, it is often better to use\n`numpy.linspace`.\n\nSee the Warning sections below for more information.\n\nParameters\n----------\nstart : integer or real, optional\n    Start of interval.  The interval includes this value.  The default\n    start value is 0.\nstop : integer or real\n    End of interval.  The interval does not include this value, except\n    in some cases where `step` is not an integer and floating point\n    round-off affects the length of `out`.\nstep : integer or real, optional\n    Spacing between values.  For any output `out`, this is the distance\n    between two adjacent values, ``out[i+1] - out[i]``.  The default\n    step size is 1.  If `step` is specified as a position argument,\n    `start` must also be given.\ndtype : dtype, optional\n    The type of the output array.  If `dtype` is not given, infer the data\n    type from the other input arguments.\nlike : array_like, optional\n    Reference object to allow the creation of arrays which are not\n    NumPy arrays. If an array-like passed in as ``like`` supports\n    the ``__array_function__`` protocol, the result will be defined\n    by it. In this case, it ensures the creation of an array object\n    compatible with that passed in via this argument.\n\n    .. versionadded:: 1.20.0\n\nReturns\n-------\narange : ndarray\n    Array of evenly spaced values.\n\n    For floating point arguments, the length of the result is\n    ``ceil((stop - start)/step)``.  Because of floating point overflow,\n    this rule may result in the last element of `out` being greater\n    than `stop`.\n\nWarnings\n--------\nThe length of the output might not be numerically stable.\n\nAnother stability issue is due to the internal implementation of\n`numpy.arange`.\nThe actual step value used to populate the array is\n``dtype(start + step) - dtype(start)`` and not `step`. Precision loss\ncan occur here, due to casting or due to using floating points when\n`start` is much larger than `step`. This can lead to unexpected\nbehaviour. For example::\n\n  >>> np.arange(0, 5, 0.5, dtype=int)\n  array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n  >>> np.arange(-3, 3, 0.5, dtype=int)\n  array([-3, -2, -1,  0,  1,  2,  3,  4,  5,  6,  7,  8])\n\nIn such cases, the use of `numpy.linspace` should be preferred.\n\nThe built-in :py:class:`range` generates :std:doc:`Python built-in integers\nthat have arbitrary size <c-api/long>`, while `numpy.arange` produces\n`numpy.int32` or `numpy.int64` numbers. This may result in incorrect\nresults for large integer values::\n\n  >>> power = 40\n  >>> modulo = 10000\n  >>> x1 = [(n ** power) % modulo for n in range(8)]\n  >>> x2 = [(n ** power) % modulo for n in np.arange(8)]\n  >>> print(x1)\n  [0, 1, 7776, 8801, 6176, 625, 6576, 4001]  # correct\n  >>> print(x2)\n  [0, 1, 7776, 7185, 0, 5969, 4816, 3361]  # incorrect\n\nSee Also\n--------\nnumpy.linspace : Evenly spaced numbers with careful handling of endpoints.\nnumpy.ogrid: Arrays of evenly spaced numbers in N-dimensions.\nnumpy.mgrid: Grid-shaped arrays of evenly spaced numbers in N-dimensions.\n\nExamples\n--------\n>>> np.arange(3)\narray([0, 1, 2])\n>>> np.arange(3.0)\narray([ 0.,  1.,  2.])\n>>> np.arange(3,7)\narray([3, 4, 5, 6])\n>>> np.arange(3,7,2)\narray([3, 5])", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/897", "data": {"name": "numpy.bincount", "type": "callable", "signature": "(*args, **kwargs)", "description": "bincount(x, /, weights=None, minlength=0)\n\nCount number of occurrences of each value in array of non-negative ints.\n\nThe number of bins (of size 1) is one larger than the largest value in\n`x`. If `minlength` is specified, there will be at least this number\nof bins in the output array (though it will be longer if necessary,\ndepending on the contents of `x`).\nEach bin gives the number of occurrences of its index value in `x`.\nIf `weights` is specified the input array is weighted by it, i.e. if a\nvalue ``n`` is found at position ``i``, ``out[n] += weight[i]`` instead\nof ``out[n] += 1``.\n\nParameters\n----------\nx : array_like, 1 dimension, nonnegative ints\n    Input array.\nweights : array_like, optional\n    Weights, array of the same shape as `x`.\nminlength : int, optional\n    A minimum number of bins for the output array.\n\n    .. versionadded:: 1.6.0\n\nReturns\n-------\nout : ndarray of ints\n    The result of binning the input array.\n    The length of `out` is equal to ``np.amax(x)+1``.\n\nRaises\n------\nValueError\n    If the input is not 1-dimensional, or contains elements with negative\n    values, or if `minlength` is negative.\nTypeError\n    If the type of the input is float or complex.\n\nSee Also\n--------\nhistogram, digitize, unique\n\nExamples\n--------\n>>> np.bincount(np.arange(5))\narray([1, 1, 1, 1, 1])\n>>> np.bincount(np.array([0, 1, 1, 3, 2, 1, 7]))\narray([1, 3, 1, 1, 0, 0, 0, 1])\n\n>>> x = np.array([0, 1, 1, 3, 2, 1, 7, 23])\n>>> np.bincount(x).size == np.amax(x)+1\nTrue\n\nThe input array needs to be of integer dtype, otherwise a\nTypeError is raised:\n\n>>> np.bincount(np.arange(5, dtype=float))\nTraceback (most recent call last):\n  ...\nTypeError: Cannot cast array data from dtype('float64') to dtype('int64')\naccording to the rule 'safe'\n\nA possible use of ``bincount`` is to perform sums over\nvariable-size chunks of an array, using the ``weights`` keyword.\n\n>>> w = np.array([0.3, 0.5, 0.2, 0.7, 1., -0.6]) # weights\n>>> x = np.array([0, 1, 1, 2, 2, 2])\n>>> np.bincount(x,  weights=w)\narray([ 0.3,  0.7,  1.1])", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/897", "data": {"name": "random.seed", "type": "callable", "signature": "(a=None)", "description": "Initialize internal state from a seed.\n\nThe only supported seed types are None, int, float,\nstr, bytes, and bytearray.\n\nNone or no argument seeds from current time or from an operating\nsystem specific randomness source if available.\n\nIf *a* is an int, all bits are used.\n\nFor version 2 (the default), all of the bits are used if *a* is a str,\nbytes, or bytearray.  For version 1 (provided for reproducing random\nsequences from older versions of Python), the algorithm for str and\nbytes generates a narrower range of seeds.", "parameters": {"type": "object", "properties": {"a": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/897", "data": {"name": "random.choice", "type": "callable", "signature": "(seq)", "description": "Choose a random element from a non-empty sequence.", "parameters": {"type": "object", "properties": {"seq": {}}}}}
{"task_id": "BigCodeBench/906", "data": {"name": "zipfile.ZipFile", "type": "class", "signature": "(file, mode='r', compression=0, allowZip64=True, compresslevel=None, *, strict_timestamps=True)", "description": "Class with methods to open, read, write, close, list zip files.\n\nz = ZipFile(file, mode=\"r\", compression=ZIP_STORED, allowZip64=True,\n            compresslevel=None)\n\nfile: Either the path to the file, or a file-like object.\n      If it is a path, the file will be opened and closed by ZipFile.\nmode: The mode can be either read 'r', write 'w', exclusive create 'x',\n      or append 'a'.\ncompression: ZIP_STORED (no compression), ZIP_DEFLATED (requires zlib),\n             ZIP_BZIP2 (requires bz2) or ZIP_LZMA (requires lzma).\nallowZip64: if True ZipFile will create files with ZIP64 extensions when\n            needed, otherwise it will raise an exception when this would\n            be necessary.\ncompresslevel: None (default for the given compression type) or an integer\n               specifying the level to pass to the compressor.\n               When using ZIP_STORED or ZIP_LZMA this keyword has no effect.\n               When using ZIP_DEFLATED integers 0 through 9 are accepted.\n               When using ZIP_BZIP2 integers 1 through 9 are accepted.", "parameters": {"type": "object", "properties": {"file": {}, "mode": {"type": "str", "default": "r"}, "compression": {"type": "integer", "default": 0}, "allowZip64": {"type": "bool", "default": true}, "compresslevel": {"type": "NoneType", "default": null}, "strict_timestamps": {"type": "bool", "default": true}}}}}
{"task_id": "BigCodeBench/906", "data": {"name": "zipfile.ZipFile.write", "type": "callable", "signature": "(self, filename, arcname=None)", "description": "Put the bytes from filename into the archive under the name\narcname.", "parameters": {"type": "object", "properties": {"filename": {}, "arcname": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/906", "data": {"name": "os.path.splitext", "type": "callable", "signature": "(p)", "description": "Split the extension from a pathname.\n\nExtension is everything from the last dot to the end, ignoring\nleading dots.  Returns \"(root, ext)\"; ext may be empty.", "parameters": {"type": "object", "properties": {"p": {}}}}}
{"task_id": "BigCodeBench/906", "data": {"name": "os.listdir", "type": "callable", "signature": "(path=None)", "description": "Return a list containing the names of the files in the directory.\n\npath can be specified as either str, bytes, or a path-like object.  If path is bytes,\n  the filenames returned will also be bytes; in all other circumstances\n  the filenames returned will be str.\nIf path is None, uses the path='.'.\nOn some platforms, path may also be specified as an open file descriptor;\\\n  the file descriptor must refer to a directory.\n  If this functionality is unavailable, using it raises NotImplementedError.\n\nThe list is in arbitrary order.  It does not include the special\nentries '.' and '..' even if they are present in the directory.", "parameters": {"type": "object", "properties": {"path": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/906", "data": {"name": "os.path.join", "type": "callable", "signature": "(a, *p)", "description": "Join two or more pathname components, inserting '/' as needed.\nIf any component is an absolute path, all previous path components\nwill be discarded.  An empty last part will result in a path that\nends with a separator.", "parameters": {"type": "object", "properties": {"a": {}}}}}
{"task_id": "BigCodeBench/906", "data": {"name": "os.makedirs", "type": "callable", "signature": "(name, exist_ok=False)", "description": "makedirs(name [, mode=0o777][, exist_ok=False])\n\nSuper-mkdir; create a leaf directory and all intermediate ones.  Works like\nmkdir, except that any intermediate path segment (not just the rightmost)\nwill be created if it does not exist. If the target directory already\nexists, raise an OSError if exist_ok is False. Otherwise no exception is\nraised.  This is recursive.", "parameters": {"type": "object", "properties": {"name": {}, "exist_ok": {"type": "bool", "default": false}}}}}
{"task_id": "BigCodeBench/906", "data": {"name": "re.search", "type": "callable", "signature": "(pattern, string)", "description": "Scan through string looking for a match to the pattern, returning\na Match object, or None if no match was found.", "parameters": {"type": "object", "properties": {"pattern": {}, "string": {}}}}}
{"task_id": "BigCodeBench/906", "data": {"name": "shutil.move", "type": "callable", "signature": "(src, dst)", "description": "Recursively move a file or directory to another location. This is\nsimilar to the Unix \"mv\" command. Return the file or directory's\ndestination.\n\nIf the destination is a directory or a symlink to a directory, the source\nis moved inside the directory. The destination path must not already\nexist.\n\nIf the destination already exists but is not a directory, it may be\noverwritten depending on os.rename() semantics.\n\nIf the destination is on our current filesystem, then rename() is used.\nOtherwise, src is copied to the destination and then removed. Symlinks are\nrecreated under the new name if os.rename() fails because of cross\nfilesystem renames.\n\nThe optional `copy_function` argument is a callable that will be used\nto copy the source or it will be delegated to `copytree`.\nBy default, copy2() is used, but any function that supports the same\nsignature (like copy()) can be used.\n\nA lot more could be done here...  A look at a mv.c shows a lot of\nthe issues this implementation glosses over.", "parameters": {"type": "object", "properties": {"src": {}, "dst": {}}}}}
{"task_id": "BigCodeBench/914", "data": {"name": "pandas.Timestamp", "type": "class", "signature": "(ts_input=<object object at 0x7fa4cd54e4f0>, year=None, month=None, day=None, hour=None, minute=None, second=None, microsecond=None, tzinfo=None, *, nanosecond=None, tz=None, unit=None, fold=None)", "description": "Pandas replacement for python datetime.datetime object.\n\nTimestamp is the pandas equivalent of python's Datetime\nand is interchangeable with it in most cases. It's the type used\nfor the entries that make up a DatetimeIndex, and other timeseries\noriented data structures in pandas.\n\nParameters\n----------\nts_input : datetime-like, str, int, float\n    Value to be converted to Timestamp.\nyear, month, day : int\nhour, minute, second, microsecond : int, optional, default 0\ntzinfo : datetime.tzinfo, optional, default None\nnanosecond : int, optional, default 0\ntz : str, pytz.timezone, dateutil.tz.tzfile or None\n    Time zone for time which Timestamp will have.\nunit : str\n    Unit used for conversion if ts_input is of type int or float. The\n    valid values are 'D', 'h', 'm', 's', 'ms', 'us', and 'ns'. For\n    example, 's' means seconds and 'ms' means milliseconds.\n\n    For float inputs, the result will be stored in nanoseconds, and\n    the unit attribute will be set as ``'ns'``.\nfold : {0, 1}, default None, keyword-only\n    Due to daylight saving time, one wall clock time can occur twice\n    when shifting from summer to winter time; fold describes whether the\n    datetime-like corresponds  to the first (0) or the second time (1)\n    the wall clock hits the ambiguous time.\n\nNotes\n-----\nThere are essentially three calling conventions for the constructor. The\nprimary form accepts four parameters. They can be passed by position or\nkeyword.\n\nThe other two forms mimic the parameters from ``datetime.datetime``. They\ncan be passed by either position or keyword, but not both mixed together.\n\nExamples\n--------\nUsing the primary calling convention:\n\nThis converts a datetime-like string\n\n>>> pd.Timestamp('2017-01-01T12')\nTimestamp('2017-01-01 12:00:00')\n\nThis converts a float representing a Unix epoch in units of seconds\n\n>>> pd.Timestamp(1513393355.5, unit='s')\nTimestamp('2017-12-16 03:02:35.500000')\n\nThis converts an int representing a Unix-epoch in units of seconds\nand for a particular timezone\n\n>>> pd.Timestamp(1513393355, unit='s', tz='US/Pacific')\nTimestamp('2017-12-15 19:02:35-0800', tz='US/Pacific')\n\nUsing the other two forms that mimic the API for ``datetime.datetime``:\n\n>>> pd.Timestamp(2017, 1, 1, 12)\nTimestamp('2017-01-01 12:00:00')\n\n>>> pd.Timestamp(year=2017, month=1, day=1, hour=12)\nTimestamp('2017-01-01 12:00:00')", "parameters": {"type": "object", "properties": {"ts_input": {"type": "str", "default": "<object object at 0x7fa4cd54e4f0>"}, "year": {"type": "NoneType", "default": null}, "month": {"type": "NoneType", "default": null}, "day": {"type": "NoneType", "default": null}, "hour": {"type": "NoneType", "default": null}, "minute": {"type": "NoneType", "default": null}, "second": {"type": "NoneType", "default": null}, "microsecond": {"type": "NoneType", "default": null}, "tzinfo": {"type": "NoneType", "default": null}, "nanosecond": {"type": "NoneType", "default": null}, "tz": {"type": "NoneType", "default": null}, "unit": {"type": "NoneType", "default": null}, "fold": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/914", "data": {"name": "pandas.Timestamp.timestamp", "type": "callable", "signature": "(self)", "description": "Return POSIX timestamp as float.\n\nExamples\n--------\n>>> ts = pd.Timestamp('2020-03-14T15:32:52.192548')\n>>> ts.timestamp()\n1584199972.192548", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/914", "data": {"name": "matplotlib.pyplot.subplots", "type": "class", "signature": "()", "description": "Create a figure and a set of subplots.\n\nThis utility wrapper makes it convenient to create common layouts of\nsubplots, including the enclosing figure object, in a single call.\n\nParameters\n----------\nnrows, ncols : int, default: 1\n    Number of rows/columns of the subplot grid.\n\nsharex, sharey : bool or {'none', 'all', 'row', 'col'}, default: False\n    Controls sharing of properties among x (*sharex*) or y (*sharey*)\n    axes:\n\n    - True or 'all': x- or y-axis will be shared among all subplots.\n    - False or 'none': each subplot x- or y-axis will be independent.\n    - 'row': each subplot row will share an x- or y-axis.\n    - 'col': each subplot column will share an x- or y-axis.\n\n    When subplots have a shared x-axis along a column, only the x tick\n    labels of the bottom subplot are created. Similarly, when subplots\n    have a shared y-axis along a row, only the y tick labels of the first\n    column subplot are created. To later turn other subplots' ticklabels\n    on, use `~matplotlib.axes.Axes.tick_params`.\n\n    When subplots have a shared axis that has units, calling\n    `~matplotlib.axis.Axis.set_units` will update each axis with the\n    new units.\n\nsqueeze : bool, default: True\n    - If True, extra dimensions are squeezed out from the returned\n      array of `~matplotlib.axes.Axes`:\n\n      - if only one subplot is constructed (nrows=ncols=1), the\n        resulting single Axes object is returned as a scalar.\n      - for Nx1 or 1xM subplots, the returned object is a 1D numpy\n        object array of Axes objects.\n      - for NxM, subplots with N>1 and M>1 are returned as a 2D array.\n\n    - If False, no squeezing at all is done: the returned Axes object is\n      always a 2D array containing Axes instances, even if it ends up\n      being 1x1.\n\nwidth_ratios : array-like of length *ncols*, optional\n    Defines the relative widths of the columns. Each column gets a\n    relative width of ``width_ratios[i] / sum(width_ratios)``.\n    If not given, all columns will have the same width.  Equivalent\n    to ``gridspec_kw={'width_ratios': [...]}``.\n\nheight_ratios : array-like of length *nrows*, optional\n    Defines the relative heights of the rows. Each row gets a\n    relative height of ``height_ratios[i] / sum(height_ratios)``.\n    If not given, all rows will have the same height. Convenience\n    for ``gridspec_kw={'height_ratios': [...]}``.\n\nsubplot_kw : dict, optional\n    Dict with keywords passed to the\n    `~matplotlib.figure.Figure.add_subplot` call used to create each\n    subplot.\n\ngridspec_kw : dict, optional\n    Dict with keywords passed to the `~matplotlib.gridspec.GridSpec`\n    constructor used to create the grid the subplots are placed on.\n\n**fig_kw\n    All additional keyword arguments are passed to the\n    `.pyplot.figure` call.\n\nReturns\n-------\nfig : `.Figure`\n\nax : `~matplotlib.axes.Axes` or array of Axes\n    *ax* can be either a single `~.axes.Axes` object, or an array of Axes\n    objects if more than one subplot was created.  The dimensions of the\n    resulting array can be controlled with the squeeze keyword, see above.\n\n    Typical idioms for handling the return value are::\n\n        # using the variable ax for single a Axes\n        fig, ax = plt.subplots()\n\n        # using the variable axs for multiple Axes\n        fig, axs = plt.subplots(2, 2)\n\n        # using tuple unpacking for multiple Axes\n        fig, (ax1, ax2) = plt.subplots(1, 2)\n        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n\n    The names ``ax`` and pluralized ``axs`` are preferred over ``axes``\n    because for the latter it's not clear if it refers to a single\n    `~.axes.Axes` instance or a collection of these.\n\nSee Also\n--------\n.pyplot.figure\n.pyplot.subplot\n.pyplot.axes\n.Figure.subplots\n.Figure.add_subplot\n\nExamples\n--------\n::\n\n    # First create some toy data:\n    x = np.linspace(0, 2*np.pi, 400)\n    y = np.sin(x**2)\n\n    # Create just a figure and only one subplot\n    fig, ax = plt.subplots()\n    ax.plot(x, y)\n    ax.set_title('Simple plot')\n\n    # Create two subplots and unpack the output array immediately\n    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n    ax1.plot(x, y)\n    ax1.set_title('Sharing Y axis')\n    ax2.scatter(x, y)\n\n    # Create four polar axes and access them through the returned array\n    fig, axs = plt.subplots(2, 2, subplot_kw=dict(projection=\"polar\"))\n    axs[0, 0].plot(x, y)\n    axs[1, 1].scatter(x, y)\n\n    # Share a X axis with each column of subplots\n    plt.subplots(2, 2, sharex='col')\n\n    # Share a Y axis with each row of subplots\n    plt.subplots(2, 2, sharey='row')\n\n    # Share both X and Y axes with all subplots\n    plt.subplots(2, 2, sharex='all', sharey='all')\n\n    # Note that this is the same as\n    plt.subplots(2, 2, sharex=True, sharey=True)\n\n    # Create figure number 10 with a single subplot\n    # and clears it if it already exists.\n    fig, ax = plt.subplots(num=10, clear=True)", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/914", "data": {"name": "matplotlib.pyplot.subplots[1].scatter", "type": "method", "signature": "(x, y, s=None, c=None, marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=None, linewidths=None, *, edgecolors=None, plotnonfinite=False, data=None, **kwargs)", "description": "A scatter plot of *y* vs. *x* with varying marker size and/or color.\n\nParameters\n----------\nx, y : float or array-like, shape (n, )\n    The data positions.\n\ns : float or array-like, shape (n, ), optional\n    The marker size in points**2 (typographic points are 1/72 in.).\n    Default is ``rcParams['lines.markersize'] ** 2``.\n\n    The linewidth and edgecolor can visually interact with the marker\n    size, and can lead to artifacts if the marker size is smaller than\n    the linewidth.\n\n    If the linewidth is greater than 0 and the edgecolor is anything\n    but *'none'*, then the effective size of the marker will be\n    increased by half the linewidth because the stroke will be centered\n    on the edge of the shape.\n\n    To eliminate the marker edge either set *linewidth=0* or\n    *edgecolor='none'*.\n\nc : array-like or list of colors or color, optional\n    The marker colors. Possible values:\n\n    - A scalar or sequence of n numbers to be mapped to colors using\n      *cmap* and *norm*.\n    - A 2D array in which the rows are RGB or RGBA.\n    - A sequence of colors of length n.\n    - A single color format string.\n\n    Note that *c* should not be a single numeric RGB or RGBA sequence\n    because that is indistinguishable from an array of values to be\n    colormapped. If you want to specify the same RGB or RGBA value for\n    all points, use a 2D array with a single row.  Otherwise,\n    value-matching will have precedence in case of a size matching with\n    *x* and *y*.\n\n    If you wish to specify a single color for all points\n    prefer the *color* keyword argument.\n\n    Defaults to `None`. In that case the marker color is determined\n    by the value of *color*, *facecolor* or *facecolors*. In case\n    those are not specified or `None`, the marker color is determined\n    by the next color of the ``Axes``' current \"shape and fill\" color\n    cycle. This cycle defaults to :rc:`axes.prop_cycle`.\n\nmarker : `~.markers.MarkerStyle`, default: :rc:`scatter.marker`\n    The marker style. *marker* can be either an instance of the class\n    or the text shorthand for a particular marker.\n    See :mod:`matplotlib.markers` for more information about marker\n    styles.\n\ncmap : str or `~matplotlib.colors.Colormap`, default: :rc:`image.cmap`\n    The Colormap instance or registered colormap name used to map scalar data\n    to colors.\n\n    This parameter is ignored if *c* is RGB(A).\n\nnorm : str or `~matplotlib.colors.Normalize`, optional\n    The normalization method used to scale scalar data to the [0, 1] range\n    before mapping to colors using *cmap*. By default, a linear scaling is\n    used, mapping the lowest value to 0 and the highest to 1.\n\n    If given, this can be one of the following:\n\n    - An instance of `.Normalize` or one of its subclasses\n      (see :ref:`colormapnorms`).\n    - A scale name, i.e. one of \"linear\", \"log\", \"symlog\", \"logit\", etc.  For a\n      list of available scales, call `matplotlib.scale.get_scale_names()`.\n      In that case, a suitable `.Normalize` subclass is dynamically generated\n      and instantiated.\n\n    This parameter is ignored if *c* is RGB(A).\n\nvmin, vmax : float, optional\n    When using scalar data and no explicit *norm*, *vmin* and *vmax* define\n    the data range that the colormap covers. By default, the colormap covers\n    the complete value range of the supplied data. It is an error to use\n    *vmin*/*vmax* when a *norm* instance is given (but using a `str` *norm*\n    name together with *vmin*/*vmax* is acceptable).\n\n    This parameter is ignored if *c* is RGB(A).\n\nalpha : float, default: None\n    The alpha blending value, between 0 (transparent) and 1 (opaque).\n\nlinewidths : float or array-like, default: :rc:`lines.linewidth`\n    The linewidth of the marker edges. Note: The default *edgecolors*\n    is 'face'. You may want to change this as well.\n\nedgecolors : {'face', 'none', *None*} or color or sequence of color, default: :rc:`scatter.edgecolors`\n    The edge color of the marker. Possible values:\n\n    - 'face': The edge color will always be the same as the face color.\n    - 'none': No patch boundary will be drawn.\n    - A color or sequence of colors.\n\n    For non-filled markers, *edgecolors* is ignored. Instead, the color\n    is determined like with 'face', i.e. from *c*, *colors*, or\n    *facecolors*.\n\nplotnonfinite : bool, default: False\n    Whether to plot points with nonfinite *c* (i.e. ``inf``, ``-inf``\n    or ``nan``). If ``True`` the points are drawn with the *bad*\n    colormap color (see `.Colormap.set_bad`).\n\nReturns\n-------\n`~matplotlib.collections.PathCollection`\n\nOther Parameters\n----------------\ndata : indexable object, optional\n    If given, the following parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` (unless this raises an exception):\n\n    *x*, *y*, *s*, *linewidths*, *edgecolors*, *c*, *facecolor*, *facecolors*, *color*\n**kwargs : `~matplotlib.collections.Collection` properties\n\nSee Also\n--------\nplot : To plot scatter plots when markers are identical in size and\n    color.\n\nNotes\n-----\n* The `.plot` function will be faster for scatterplots where markers\n  don't vary in size or color.\n\n* Any or all of *x*, *y*, *s*, and *c* may be masked arrays, in which\n  case all masks will be combined and only unmasked points will be\n  plotted.\n\n* Fundamentally, scatter works with 1D arrays; *x*, *y*, *s*, and *c*\n  may be input as N-D arrays, but within scatter they will be\n  flattened. The exception is *c*, which will be flattened only if its\n  size matches the size of *x* and *y*.", "parameters": {"type": "object", "properties": {"x": {}, "y": {}, "s": {"type": "NoneType", "default": null}, "c": {"type": "NoneType", "default": null}, "marker": {"type": "NoneType", "default": null}, "cmap": {"type": "NoneType", "default": null}, "norm": {"type": "NoneType", "default": null}, "vmin": {"type": "NoneType", "default": null}, "vmax": {"type": "NoneType", "default": null}, "alpha": {"type": "NoneType", "default": null}, "linewidths": {"type": "NoneType", "default": null}, "edgecolors": {"type": "NoneType", "default": null}, "plotnonfinite": {"type": "bool", "default": false}, "data": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/914", "data": {"name": "matplotlib.pyplot.subplots[1].plot", "type": "method", "signature": "(*args, scalex=True, scaley=True, data=None, **kwargs)", "description": "Plot y versus x as lines and/or markers.\n\nCall signatures::\n\n    plot([x], y, [fmt], *, data=None, **kwargs)\n    plot([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n\nThe coordinates of the points or line nodes are given by *x*, *y*.\n\nThe optional parameter *fmt* is a convenient way for defining basic\nformatting like color, marker and linestyle. It's a shortcut string\nnotation described in the *Notes* section below.\n\n>>> plot(x, y)        # plot x and y using default line style and color\n>>> plot(x, y, 'bo')  # plot x and y using blue circle markers\n>>> plot(y)           # plot y using x as index array 0..N-1\n>>> plot(y, 'r+')     # ditto, but with red plusses\n\nYou can use `.Line2D` properties as keyword arguments for more\ncontrol on the appearance. Line properties and *fmt* can be mixed.\nThe following two calls yield identical results:\n\n>>> plot(x, y, 'go--', linewidth=2, markersize=12)\n>>> plot(x, y, color='green', marker='o', linestyle='dashed',\n...      linewidth=2, markersize=12)\n\nWhen conflicting with *fmt*, keyword arguments take precedence.\n\n\n**Plotting labelled data**\n\nThere's a convenient way for plotting objects with labelled data (i.e.\ndata that can be accessed by index ``obj['y']``). Instead of giving\nthe data in *x* and *y*, you can provide the object in the *data*\nparameter and just give the labels for *x* and *y*::\n\n>>> plot('xlabel', 'ylabel', data=obj)\n\nAll indexable objects are supported. This could e.g. be a `dict`, a\n`pandas.DataFrame` or a structured numpy array.\n\n\n**Plotting multiple sets of data**\n\nThere are various ways to plot multiple sets of data.\n\n- The most straight forward way is just to call `plot` multiple times.\n  Example:\n\n  >>> plot(x1, y1, 'bo')\n  >>> plot(x2, y2, 'go')\n\n- If *x* and/or *y* are 2D arrays a separate data set will be drawn\n  for every column. If both *x* and *y* are 2D, they must have the\n  same shape. If only one of them is 2D with shape (N, m) the other\n  must have length N and will be used for every data set m.\n\n  Example:\n\n  >>> x = [1, 2, 3]\n  >>> y = np.array([[1, 2], [3, 4], [5, 6]])\n  >>> plot(x, y)\n\n  is equivalent to:\n\n  >>> for col in range(y.shape[1]):\n  ...     plot(x, y[:, col])\n\n- The third way is to specify multiple sets of *[x]*, *y*, *[fmt]*\n  groups::\n\n  >>> plot(x1, y1, 'g^', x2, y2, 'g-')\n\n  In this case, any additional keyword argument applies to all\n  datasets. Also, this syntax cannot be combined with the *data*\n  parameter.\n\nBy default, each line is assigned a different style specified by a\n'style cycle'. The *fmt* and line property parameters are only\nnecessary if you want explicit deviations from these defaults.\nAlternatively, you can also change the style cycle using\n:rc:`axes.prop_cycle`.\n\n\nParameters\n----------\nx, y : array-like or scalar\n    The horizontal / vertical coordinates of the data points.\n    *x* values are optional and default to ``range(len(y))``.\n\n    Commonly, these parameters are 1D arrays.\n\n    They can also be scalars, or two-dimensional (in that case, the\n    columns represent separate data sets).\n\n    These arguments cannot be passed as keywords.\n\nfmt : str, optional\n    A format string, e.g. 'ro' for red circles. See the *Notes*\n    section for a full description of the format strings.\n\n    Format strings are just an abbreviation for quickly setting\n    basic line properties. All of these and more can also be\n    controlled by keyword arguments.\n\n    This argument cannot be passed as keyword.\n\ndata : indexable object, optional\n    An object with labelled data. If given, provide the label names to\n    plot in *x* and *y*.\n\n    .. note::\n        Technically there's a slight ambiguity in calls where the\n        second label is a valid *fmt*. ``plot('n', 'o', data=obj)``\n        could be ``plt(x, y)`` or ``plt(y, fmt)``. In such cases,\n        the former interpretation is chosen, but a warning is issued.\n        You may suppress the warning by adding an empty format string\n        ``plot('n', 'o', '', data=obj)``.\n\nReturns\n-------\nlist of `.Line2D`\n    A list of lines representing the plotted data.\n\nOther Parameters\n----------------\nscalex, scaley : bool, default: True\n    These parameters determine if the view limits are adapted to the\n    data limits. The values are passed on to\n    `~.axes.Axes.autoscale_view`.\n\n**kwargs : `~matplotlib.lines.Line2D` properties, optional\n    *kwargs* are used to specify properties like a line label (for\n    auto legends), linewidth, antialiasing, marker face color.\n    Example::\n\n    >>> plot([1, 2, 3], [1, 2, 3], 'go-', label='line 1', linewidth=2)\n    >>> plot([1, 2, 3], [1, 4, 9], 'rs', label='line 2')\n\n    If you specify multiple lines with one plot call, the kwargs apply\n    to all those lines. In case the label object is iterable, each\n    element is used as labels for each set of data.\n\n    Here is a list of available `.Line2D` properties:\n\n    Properties:\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: scalar or None\n    animated: bool\n    antialiased or aa: bool\n    clip_box: `~matplotlib.transforms.BboxBase` or None\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    color or c: color\n    dash_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    dash_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    dashes: sequence of floats (on/off ink in points) or (None, None)\n    data: (2, N) array or two 1D arrays\n    drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n    figure: `~matplotlib.figure.Figure`\n    fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n    gapcolor: color or None\n    gid: str\n    in_layout: bool\n    label: object\n    linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n    linewidth or lw: float\n    marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n    markeredgecolor or mec: color\n    markeredgewidth or mew: float\n    markerfacecolor or mfc: color\n    markerfacecoloralt or mfcalt: color\n    markersize or ms: float\n    markevery: None or int or (int, int) or slice or list[int] or float or (float, float) or list[bool]\n    mouseover: bool\n    path_effects: list of `.AbstractPathEffect`\n    picker: float or callable[[Artist, Event], tuple[bool, dict]]\n    pickradius: float\n    rasterized: bool\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    solid_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    solid_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    transform: unknown\n    url: str\n    visible: bool\n    xdata: 1D array\n    ydata: 1D array\n    zorder: float\n\nSee Also\n--------\nscatter : XY scatter plot with markers of varying size and/or color (\n    sometimes also called bubble chart).\n\nNotes\n-----\n**Format Strings**\n\nA format string consists of a part for color, marker and line::\n\n    fmt = '[marker][line][color]'\n\nEach of them is optional. If not provided, the value from the style\ncycle is used. Exception: If ``line`` is given, but no ``marker``,\nthe data will be a line without markers.\n\nOther combinations such as ``[color][marker][line]`` are also\nsupported, but note that their parsing may be ambiguous.\n\n**Markers**\n\n=============   ===============================\ncharacter       description\n=============   ===============================\n``'.'``         point marker\n``','``         pixel marker\n``'o'``         circle marker\n``'v'``         triangle_down marker\n``'^'``         triangle_up marker\n``'<'``         triangle_left marker\n``'>'``         triangle_right marker\n``'1'``         tri_down marker\n``'2'``         tri_up marker\n``'3'``         tri_left marker\n``'4'``         tri_right marker\n``'8'``         octagon marker\n``'s'``         square marker\n``'p'``         pentagon marker\n``'P'``         plus (filled) marker\n``'*'``         star marker\n``'h'``         hexagon1 marker\n``'H'``         hexagon2 marker\n``'+'``         plus marker\n``'x'``         x marker\n``'X'``         x (filled) marker\n``'D'``         diamond marker\n``'d'``         thin_diamond marker\n``'|'``         vline marker\n``'_'``         hline marker\n=============   ===============================\n\n**Line Styles**\n\n=============    ===============================\ncharacter        description\n=============    ===============================\n``'-'``          solid line style\n``'--'``         dashed line style\n``'-.'``         dash-dot line style\n``':'``          dotted line style\n=============    ===============================\n\nExample format strings::\n\n    'b'    # blue markers with default shape\n    'or'   # red circles\n    '-g'   # green solid line\n    '--'   # dashed line with default color\n    '^k:'  # black triangle_up markers connected by a dotted line\n\n**Colors**\n\nThe supported color abbreviations are the single letter codes\n\n=============    ===============================\ncharacter        color\n=============    ===============================\n``'b'``          blue\n``'g'``          green\n``'r'``          red\n``'c'``          cyan\n``'m'``          magenta\n``'y'``          yellow\n``'k'``          black\n``'w'``          white\n=============    ===============================\n\nand the ``'CN'`` colors that index into the default property cycle.\n\nIf the color is the only part of the format string, you can\nadditionally use any  `matplotlib.colors` spec, e.g. full names\n(``'green'``) or hex strings (``'#008000'``).", "parameters": {"type": "object", "properties": {"scalex": {"type": "bool", "default": true}, "scaley": {"type": "bool", "default": true}, "data": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/914", "data": {"name": "numpy.array", "type": "callable", "signature": "(*args, **kwargs)", "description": "array(object, dtype=None, *, copy=True, order='K', subok=False, ndmin=0,\n      like=None)\n\nCreate an array.\n\nParameters\n----------\nobject : array_like\n    An array, any object exposing the array interface, an object whose\n    __array__ method returns an array, or any (nested) sequence.\n    If object is a scalar, a 0-dimensional array containing object is\n    returned.\ndtype : data-type, optional\n    The desired data-type for the array.  If not given, then the type will\n    be determined as the minimum type required to hold the objects in the\n    sequence.\ncopy : bool, optional\n    If true (default), then the object is copied.  Otherwise, a copy will\n    only be made if __array__ returns a copy, if obj is a nested sequence,\n    or if a copy is needed to satisfy any of the other requirements\n    (`dtype`, `order`, etc.).\norder : {'K', 'A', 'C', 'F'}, optional\n    Specify the memory layout of the array. If object is not an array, the\n    newly created array will be in C order (row major) unless 'F' is\n    specified, in which case it will be in Fortran order (column major).\n    If object is an array the following holds.\n\n    ===== ========= ===================================================\n    order  no copy                     copy=True\n    ===== ========= ===================================================\n    'K'   unchanged F & C order preserved, otherwise most similar order\n    'A'   unchanged F order if input is F and not C, otherwise C order\n    'C'   C order   C order\n    'F'   F order   F order\n    ===== ========= ===================================================\n\n    When ``copy=False`` and a copy is made for other reasons, the result is\n    the same as if ``copy=True``, with some exceptions for 'A', see the\n    Notes section. The default order is 'K'.\nsubok : bool, optional\n    If True, then sub-classes will be passed-through, otherwise\n    the returned array will be forced to be a base-class array (default).\nndmin : int, optional\n    Specifies the minimum number of dimensions that the resulting\n    array should have.  Ones will be prepended to the shape as\n    needed to meet this requirement.\nlike : array_like, optional\n    Reference object to allow the creation of arrays which are not\n    NumPy arrays. If an array-like passed in as ``like`` supports\n    the ``__array_function__`` protocol, the result will be defined\n    by it. In this case, it ensures the creation of an array object\n    compatible with that passed in via this argument.\n\n    .. versionadded:: 1.20.0\n\nReturns\n-------\nout : ndarray\n    An array object satisfying the specified requirements.\n\nSee Also\n--------\nempty_like : Return an empty array with shape and type of input.\nones_like : Return an array of ones with shape and type of input.\nzeros_like : Return an array of zeros with shape and type of input.\nfull_like : Return a new array with shape of input filled with value.\nempty : Return a new uninitialized array.\nones : Return a new array setting values to one.\nzeros : Return a new array setting values to zero.\nfull : Return a new array of given shape filled with value.\n\n\nNotes\n-----\nWhen order is 'A' and `object` is an array in neither 'C' nor 'F' order,\nand a copy is forced by a change in dtype, then the order of the result is\nnot necessarily 'C' as expected. This is likely a bug.\n\nExamples\n--------\n>>> np.array([1, 2, 3])\narray([1, 2, 3])\n\nUpcasting:\n\n>>> np.array([1, 2, 3.0])\narray([ 1.,  2.,  3.])\n\nMore than one dimension:\n\n>>> np.array([[1, 2], [3, 4]])\narray([[1, 2],\n       [3, 4]])\n\nMinimum dimensions 2:\n\n>>> np.array([1, 2, 3], ndmin=2)\narray([[1, 2, 3]])\n\nType provided:\n\n>>> np.array([1, 2, 3], dtype=complex)\narray([ 1.+0.j,  2.+0.j,  3.+0.j])\n\nData-type consisting of more than one element:\n\n>>> x = np.array([(1,2),(3,4)],dtype=[('a','<i4'),('b','<i4')])\n>>> x['a']\narray([1, 3])\n\nCreating an array from sub-classes:\n\n>>> np.array(np.mat('1 2; 3 4'))\narray([[1, 2],\n       [3, 4]])\n\n>>> np.array(np.mat('1 2; 3 4'), subok=True)\nmatrix([[1, 2],\n        [3, 4]])", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/914", "data": {"name": "pandas.to_datetime", "type": "callable", "signature": "(arg: 'DatetimeScalarOrArrayConvertible | DictConvertible')", "description": "Convert argument to datetime.\n\nThis function converts a scalar, array-like, :class:`Series` or\n:class:`DataFrame`/dict-like to a pandas datetime object.\n\nParameters\n----------\narg : int, float, str, datetime, list, tuple, 1-d array, Series, DataFrame/dict-like\n    The object to convert to a datetime. If a :class:`DataFrame` is provided, the\n    method expects minimally the following columns: :const:`\"year\"`,\n    :const:`\"month\"`, :const:`\"day\"`. The column \"year\"\n    must be specified in 4-digit format.\nerrors : {'ignore', 'raise', 'coerce'}, default 'raise'\n    - If :const:`'raise'`, then invalid parsing will raise an exception.\n    - If :const:`'coerce'`, then invalid parsing will be set as :const:`NaT`.\n    - If :const:`'ignore'`, then invalid parsing will return the input.\ndayfirst : bool, default False\n    Specify a date parse order if `arg` is str or is list-like.\n    If :const:`True`, parses dates with the day first, e.g. :const:`\"10/11/12\"`\n    is parsed as :const:`2012-11-10`.\n\n    .. warning::\n\n        ``dayfirst=True`` is not strict, but will prefer to parse\n        with day first.\n\nyearfirst : bool, default False\n    Specify a date parse order if `arg` is str or is list-like.\n\n    - If :const:`True` parses dates with the year first, e.g.\n      :const:`\"10/11/12\"` is parsed as :const:`2010-11-12`.\n    - If both `dayfirst` and `yearfirst` are :const:`True`, `yearfirst` is\n      preceded (same as :mod:`dateutil`).\n\n    .. warning::\n\n        ``yearfirst=True`` is not strict, but will prefer to parse\n        with year first.\n\nutc : bool, default False\n    Control timezone-related parsing, localization and conversion.\n\n    - If :const:`True`, the function *always* returns a timezone-aware\n      UTC-localized :class:`Timestamp`, :class:`Series` or\n      :class:`DatetimeIndex`. To do this, timezone-naive inputs are\n      *localized* as UTC, while timezone-aware inputs are *converted* to UTC.\n\n    - If :const:`False` (default), inputs will not be coerced to UTC.\n      Timezone-naive inputs will remain naive, while timezone-aware ones\n      will keep their time offsets. Limitations exist for mixed\n      offsets (typically, daylight savings), see :ref:`Examples\n      <to_datetime_tz_examples>` section for details.\n\n    .. warning::\n\n        In a future version of pandas, parsing datetimes with mixed time\n        zones will raise an error unless `utc=True`.\n        Please specify `utc=True` to opt in to the new behaviour\n        and silence this warning. To create a `Series` with mixed offsets and\n        `object` dtype, please use `apply` and `datetime.datetime.strptime`.\n\n    See also: pandas general documentation about `timezone conversion and\n    localization\n    <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html\n    #time-zone-handling>`_.\n\nformat : str, default None\n    The strftime to parse time, e.g. :const:`\"%d/%m/%Y\"`. See\n    `strftime documentation\n    <https://docs.python.org/3/library/datetime.html\n    #strftime-and-strptime-behavior>`_ for more information on choices, though\n    note that :const:`\"%f\"` will parse all the way up to nanoseconds.\n    You can also pass:\n\n    - \"ISO8601\", to parse any `ISO8601 <https://en.wikipedia.org/wiki/ISO_8601>`_\n      time string (not necessarily in exactly the same format);\n    - \"mixed\", to infer the format for each element individually. This is risky,\n      and you should probably use it along with `dayfirst`.\n\n    .. note::\n\n        If a :class:`DataFrame` is passed, then `format` has no effect.\n\nexact : bool, default True\n    Control how `format` is used:\n\n    - If :const:`True`, require an exact `format` match.\n    - If :const:`False`, allow the `format` to match anywhere in the target\n      string.\n\n    Cannot be used alongside ``format='ISO8601'`` or ``format='mixed'``.\nunit : str, default 'ns'\n    The unit of the arg (D,s,ms,us,ns) denote the unit, which is an\n    integer or float number. This will be based off the origin.\n    Example, with ``unit='ms'`` and ``origin='unix'``, this would calculate\n    the number of milliseconds to the unix epoch start.\ninfer_datetime_format : bool, default False\n    If :const:`True` and no `format` is given, attempt to infer the format\n    of the datetime strings based on the first non-NaN element,\n    and if it can be inferred, switch to a faster method of parsing them.\n    In some cases this can increase the parsing speed by ~5-10x.\n\n    .. deprecated:: 2.0.0\n        A strict version of this argument is now the default, passing it has\n        no effect.\n\norigin : scalar, default 'unix'\n    Define the reference date. The numeric values would be parsed as number\n    of units (defined by `unit`) since this reference date.\n\n    - If :const:`'unix'` (or POSIX) time; origin is set to 1970-01-01.\n    - If :const:`'julian'`, unit must be :const:`'D'`, and origin is set to\n      beginning of Julian Calendar. Julian day number :const:`0` is assigned\n      to the day starting at noon on January 1, 4713 BC.\n    - If Timestamp convertible (Timestamp, dt.datetime, np.datetimt64 or date\n      string), origin is set to Timestamp identified by origin.\n    - If a float or integer, origin is the difference\n      (in units determined by the ``unit`` argument) relative to 1970-01-01.\ncache : bool, default True\n    If :const:`True`, use a cache of unique, converted dates to apply the\n    datetime conversion. May produce significant speed-up when parsing\n    duplicate date strings, especially ones with timezone offsets. The cache\n    is only used when there are at least 50 values. The presence of\n    out-of-bounds values will render the cache unusable and may slow down\n    parsing.\n\nReturns\n-------\ndatetime\n    If parsing succeeded.\n    Return type depends on input (types in parenthesis correspond to\n    fallback in case of unsuccessful timezone or out-of-range timestamp\n    parsing):\n\n    - scalar: :class:`Timestamp` (or :class:`datetime.datetime`)\n    - array-like: :class:`DatetimeIndex` (or :class:`Series` with\n      :class:`object` dtype containing :class:`datetime.datetime`)\n    - Series: :class:`Series` of :class:`datetime64` dtype (or\n      :class:`Series` of :class:`object` dtype containing\n      :class:`datetime.datetime`)\n    - DataFrame: :class:`Series` of :class:`datetime64` dtype (or\n      :class:`Series` of :class:`object` dtype containing\n      :class:`datetime.datetime`)\n\nRaises\n------\nParserError\n    When parsing a date from string fails.\nValueError\n    When another datetime conversion error happens. For example when one\n    of 'year', 'month', day' columns is missing in a :class:`DataFrame`, or\n    when a Timezone-aware :class:`datetime.datetime` is found in an array-like\n    of mixed time offsets, and ``utc=False``.\n\nSee Also\n--------\nDataFrame.astype : Cast argument to a specified dtype.\nto_timedelta : Convert argument to timedelta.\nconvert_dtypes : Convert dtypes.\n\nNotes\n-----\n\nMany input types are supported, and lead to different output types:\n\n- **scalars** can be int, float, str, datetime object (from stdlib :mod:`datetime`\n  module or :mod:`numpy`). They are converted to :class:`Timestamp` when\n  possible, otherwise they are converted to :class:`datetime.datetime`.\n  None/NaN/null scalars are converted to :const:`NaT`.\n\n- **array-like** can contain int, float, str, datetime objects. They are\n  converted to :class:`DatetimeIndex` when possible, otherwise they are\n  converted to :class:`Index` with :class:`object` dtype, containing\n  :class:`datetime.datetime`. None/NaN/null entries are converted to\n  :const:`NaT` in both cases.\n\n- **Series** are converted to :class:`Series` with :class:`datetime64`\n  dtype when possible, otherwise they are converted to :class:`Series` with\n  :class:`object` dtype, containing :class:`datetime.datetime`. None/NaN/null\n  entries are converted to :const:`NaT` in both cases.\n\n- **DataFrame/dict-like** are converted to :class:`Series` with\n  :class:`datetime64` dtype. For each row a datetime is created from assembling\n  the various dataframe columns. Column keys can be common abbreviations\n  like ['year', 'month', 'day', 'minute', 'second', 'ms', 'us', 'ns']) or\n  plurals of the same.\n\nThe following causes are responsible for :class:`datetime.datetime` objects\nbeing returned (possibly inside an :class:`Index` or a :class:`Series` with\n:class:`object` dtype) instead of a proper pandas designated type\n(:class:`Timestamp`, :class:`DatetimeIndex` or :class:`Series`\nwith :class:`datetime64` dtype):\n\n- when any input element is before :const:`Timestamp.min` or after\n  :const:`Timestamp.max`, see `timestamp limitations\n  <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html\n  #timeseries-timestamp-limits>`_.\n\n- when ``utc=False`` (default) and the input is an array-like or\n  :class:`Series` containing mixed naive/aware datetime, or aware with mixed\n  time offsets. Note that this happens in the (quite frequent) situation when\n  the timezone has a daylight savings policy. In that case you may wish to\n  use ``utc=True``.\n\nExamples\n--------\n\n**Handling various input formats**\n\nAssembling a datetime from multiple columns of a :class:`DataFrame`. The keys\ncan be common abbreviations like ['year', 'month', 'day', 'minute', 'second',\n'ms', 'us', 'ns']) or plurals of the same\n\n>>> df = pd.DataFrame({'year': [2015, 2016],\n...                    'month': [2, 3],\n...                    'day': [4, 5]})\n>>> pd.to_datetime(df)\n0   2015-02-04\n1   2016-03-05\ndtype: datetime64[ns]\n\nUsing a unix epoch time\n\n>>> pd.to_datetime(1490195805, unit='s')\nTimestamp('2017-03-22 15:16:45')\n>>> pd.to_datetime(1490195805433502912, unit='ns')\nTimestamp('2017-03-22 15:16:45.433502912')\n\n.. warning:: For float arg, precision rounding might happen. To prevent\n    unexpected behavior use a fixed-width exact type.\n\nUsing a non-unix epoch origin\n\n>>> pd.to_datetime([1, 2, 3], unit='D',\n...                origin=pd.Timestamp('1960-01-01'))\nDatetimeIndex(['1960-01-02', '1960-01-03', '1960-01-04'],\n              dtype='datetime64[ns]', freq=None)\n\n**Differences with strptime behavior**\n\n:const:`\"%f\"` will parse all the way up to nanoseconds.\n\n>>> pd.to_datetime('2018-10-26 12:00:00.0000000011',\n...                format='%Y-%m-%d %H:%M:%S.%f')\nTimestamp('2018-10-26 12:00:00.000000001')\n\n**Non-convertible date/times**\n\nPassing ``errors='coerce'`` will force an out-of-bounds date to :const:`NaT`,\nin addition to forcing non-dates (or non-parseable dates) to :const:`NaT`.\n\n>>> pd.to_datetime('13000101', format='%Y%m%d', errors='coerce')\nNaT\n\n.. _to_datetime_tz_examples:\n\n**Timezones and time offsets**\n\nThe default behaviour (``utc=False``) is as follows:\n\n- Timezone-naive inputs are converted to timezone-naive :class:`DatetimeIndex`:\n\n>>> pd.to_datetime(['2018-10-26 12:00:00', '2018-10-26 13:00:15'])\nDatetimeIndex(['2018-10-26 12:00:00', '2018-10-26 13:00:15'],\n              dtype='datetime64[ns]', freq=None)\n\n- Timezone-aware inputs *with constant time offset* are converted to\n  timezone-aware :class:`DatetimeIndex`:\n\n>>> pd.to_datetime(['2018-10-26 12:00 -0500', '2018-10-26 13:00 -0500'])\nDatetimeIndex(['2018-10-26 12:00:00-05:00', '2018-10-26 13:00:00-05:00'],\n              dtype='datetime64[ns, UTC-05:00]', freq=None)\n\n- However, timezone-aware inputs *with mixed time offsets* (for example\n  issued from a timezone with daylight savings, such as Europe/Paris)\n  are **not successfully converted** to a :class:`DatetimeIndex`.\n  Parsing datetimes with mixed time zones will show a warning unless\n  `utc=True`. If you specify `utc=False` the warning below will be shown\n  and a simple :class:`Index` containing :class:`datetime.datetime`\n  objects will be returned:\n\n>>> pd.to_datetime(['2020-10-25 02:00 +0200',\n...                 '2020-10-25 04:00 +0100'])  # doctest: +SKIP\nFutureWarning: In a future version of pandas, parsing datetimes with mixed\ntime zones will raise an error unless `utc=True`. Please specify `utc=True`\nto opt in to the new behaviour and silence this warning. To create a `Series`\nwith mixed offsets and `object` dtype, please use `apply` and\n`datetime.datetime.strptime`.\nIndex([2020-10-25 02:00:00+02:00, 2020-10-25 04:00:00+01:00],\n      dtype='object')\n\n- A mix of timezone-aware and timezone-naive inputs is also converted to\n  a simple :class:`Index` containing :class:`datetime.datetime` objects:\n\n>>> from datetime import datetime\n>>> pd.to_datetime([\"2020-01-01 01:00:00-01:00\",\n...                 datetime(2020, 1, 1, 3, 0)])  # doctest: +SKIP\nFutureWarning: In a future version of pandas, parsing datetimes with mixed\ntime zones will raise an error unless `utc=True`. Please specify `utc=True`\nto opt in to the new behaviour and silence this warning. To create a `Series`\nwith mixed offsets and `object` dtype, please use `apply` and\n`datetime.datetime.strptime`.\nIndex([2020-01-01 01:00:00-01:00, 2020-01-01 03:00:00], dtype='object')\n\n|\n\nSetting ``utc=True`` solves most of the above issues:\n\n- Timezone-naive inputs are *localized* as UTC\n\n>>> pd.to_datetime(['2018-10-26 12:00', '2018-10-26 13:00'], utc=True)\nDatetimeIndex(['2018-10-26 12:00:00+00:00', '2018-10-26 13:00:00+00:00'],\n              dtype='datetime64[ns, UTC]', freq=None)\n\n- Timezone-aware inputs are *converted* to UTC (the output represents the\n  exact same datetime, but viewed from the UTC time offset `+00:00`).\n\n>>> pd.to_datetime(['2018-10-26 12:00 -0530', '2018-10-26 12:00 -0500'],\n...                utc=True)\nDatetimeIndex(['2018-10-26 17:30:00+00:00', '2018-10-26 17:00:00+00:00'],\n              dtype='datetime64[ns, UTC]', freq=None)\n\n- Inputs can contain both string or datetime, the above\n  rules still apply\n\n>>> pd.to_datetime(['2018-10-26 12:00', datetime(2020, 1, 1, 18)], utc=True)\nDatetimeIndex(['2018-10-26 12:00:00+00:00', '2020-01-01 18:00:00+00:00'],\n              dtype='datetime64[ns, UTC]', freq=None)", "parameters": {"type": "object", "properties": {"arg": {"type": ["dictconvertible", "datetimescalarorarrayconvertible"]}}}}}
{"task_id": "BigCodeBench/914", "data": {"name": "sklearn.linear_model.LinearRegression", "type": "class", "signature": "()", "description": "Ordinary least squares Linear Regression.\n\nLinearRegression fits a linear model with coefficients w = (w1, ..., wp)\nto minimize the residual sum of squares between the observed targets in\nthe dataset, and the targets predicted by the linear approximation.\n\nParameters\n----------\nfit_intercept : bool, default=True\n    Whether to calculate the intercept for this model. If set\n    to False, no intercept will be used in calculations\n    (i.e. data is expected to be centered).\n\ncopy_X : bool, default=True\n    If True, X will be copied; else, it may be overwritten.\n\nn_jobs : int, default=None\n    The number of jobs to use for the computation. This will only provide\n    speedup in case of sufficiently large problems, that is if firstly\n    `n_targets > 1` and secondly `X` is sparse or if `positive` is set\n    to `True`. ``None`` means 1 unless in a\n    :obj:`joblib.parallel_backend` context. ``-1`` means using all\n    processors. See :term:`Glossary <n_jobs>` for more details.\n\npositive : bool, default=False\n    When set to ``True``, forces the coefficients to be positive. This\n    option is only supported for dense arrays.\n\n    .. versionadded:: 0.24\n\nAttributes\n----------\ncoef_ : array of shape (n_features, ) or (n_targets, n_features)\n    Estimated coefficients for the linear regression problem.\n    If multiple targets are passed during the fit (y 2D), this\n    is a 2D array of shape (n_targets, n_features), while if only\n    one target is passed, this is a 1D array of length n_features.\n\nrank_ : int\n    Rank of matrix `X`. Only available when `X` is dense.\n\nsingular_ : array of shape (min(X, y),)\n    Singular values of `X`. Only available when `X` is dense.\n\nintercept_ : float or array of shape (n_targets,)\n    Independent term in the linear model. Set to 0.0 if\n    `fit_intercept = False`.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nRidge : Ridge regression addresses some of the\n    problems of Ordinary Least Squares by imposing a penalty on the\n    size of the coefficients with l2 regularization.\nLasso : The Lasso is a linear model that estimates\n    sparse coefficients with l1 regularization.\nElasticNet : Elastic-Net is a linear regression\n    model trained with both l1 and l2 -norm regularization of the\n    coefficients.\n\nNotes\n-----\nFrom the implementation point of view, this is just plain Ordinary\nLeast Squares (scipy.linalg.lstsq) or Non Negative Least Squares\n(scipy.optimize.nnls) wrapped as a predictor object.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.linear_model import LinearRegression\n>>> X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n>>> # y = 1 * x_0 + 2 * x_1 + 3\n>>> y = np.dot(X, np.array([1, 2])) + 3\n>>> reg = LinearRegression().fit(X, y)\n>>> reg.score(X, y)\n1.0\n>>> reg.coef_\narray([1., 2.])\n>>> reg.intercept_\n3.0...\n>>> reg.predict(np.array([[3, 5]]))\narray([16.])", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/914", "data": {"name": "sklearn.linear_model.LinearRegression.fit", "type": "callable", "signature": "(self, X, y)", "description": "Fit linear model.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training data.\n\ny : array-like of shape (n_samples,) or (n_samples, n_targets)\n    Target values. Will be cast to X's dtype if necessary.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Individual weights for each sample.\n\n    .. versionadded:: 0.17\n       parameter *sample_weight* support to LinearRegression.\n\nReturns\n-------\nself : object\n    Fitted Estimator.", "parameters": {"type": "object", "properties": {"X": {}, "y": {}}}}}
{"task_id": "BigCodeBench/914", "data": {"name": "sklearn.linear_model.LinearRegression.predict", "type": "callable", "signature": "(self, X)", "description": "Predict using the linear model.\n\nParameters\n----------\nX : array-like or sparse matrix, shape (n_samples, n_features)\n    Samples.\n\nReturns\n-------\nC : array, shape (n_samples,)\n    Returns predicted values.", "parameters": {"type": "object", "properties": {"X": {}}}}}
{"task_id": "BigCodeBench/915", "data": {"name": "matplotlib.pyplot.subplots", "type": "class", "signature": "(**fig_kw) -> 'tuple[Figure)", "description": "Create a figure and a set of subplots.\n\nThis utility wrapper makes it convenient to create common layouts of\nsubplots, including the enclosing figure object, in a single call.\n\nParameters\n----------\nnrows, ncols : int, default: 1\n    Number of rows/columns of the subplot grid.\n\nsharex, sharey : bool or {'none', 'all', 'row', 'col'}, default: False\n    Controls sharing of properties among x (*sharex*) or y (*sharey*)\n    axes:\n\n    - True or 'all': x- or y-axis will be shared among all subplots.\n    - False or 'none': each subplot x- or y-axis will be independent.\n    - 'row': each subplot row will share an x- or y-axis.\n    - 'col': each subplot column will share an x- or y-axis.\n\n    When subplots have a shared x-axis along a column, only the x tick\n    labels of the bottom subplot are created. Similarly, when subplots\n    have a shared y-axis along a row, only the y tick labels of the first\n    column subplot are created. To later turn other subplots' ticklabels\n    on, use `~matplotlib.axes.Axes.tick_params`.\n\n    When subplots have a shared axis that has units, calling\n    `~matplotlib.axis.Axis.set_units` will update each axis with the\n    new units.\n\nsqueeze : bool, default: True\n    - If True, extra dimensions are squeezed out from the returned\n      array of `~matplotlib.axes.Axes`:\n\n      - if only one subplot is constructed (nrows=ncols=1), the\n        resulting single Axes object is returned as a scalar.\n      - for Nx1 or 1xM subplots, the returned object is a 1D numpy\n        object array of Axes objects.\n      - for NxM, subplots with N>1 and M>1 are returned as a 2D array.\n\n    - If False, no squeezing at all is done: the returned Axes object is\n      always a 2D array containing Axes instances, even if it ends up\n      being 1x1.\n\nwidth_ratios : array-like of length *ncols*, optional\n    Defines the relative widths of the columns. Each column gets a\n    relative width of ``width_ratios[i] / sum(width_ratios)``.\n    If not given, all columns will have the same width.  Equivalent\n    to ``gridspec_kw={'width_ratios': [...]}``.\n\nheight_ratios : array-like of length *nrows*, optional\n    Defines the relative heights of the rows. Each row gets a\n    relative height of ``height_ratios[i] / sum(height_ratios)``.\n    If not given, all rows will have the same height. Convenience\n    for ``gridspec_kw={'height_ratios': [...]}``.\n\nsubplot_kw : dict, optional\n    Dict with keywords passed to the\n    `~matplotlib.figure.Figure.add_subplot` call used to create each\n    subplot.\n\ngridspec_kw : dict, optional\n    Dict with keywords passed to the `~matplotlib.gridspec.GridSpec`\n    constructor used to create the grid the subplots are placed on.\n\n**fig_kw\n    All additional keyword arguments are passed to the\n    `.pyplot.figure` call.\n\nReturns\n-------\nfig : `.Figure`\n\nax : `~matplotlib.axes.Axes` or array of Axes\n    *ax* can be either a single `~.axes.Axes` object, or an array of Axes\n    objects if more than one subplot was created.  The dimensions of the\n    resulting array can be controlled with the squeeze keyword, see above.\n\n    Typical idioms for handling the return value are::\n\n        # using the variable ax for single a Axes\n        fig, ax = plt.subplots()\n\n        # using the variable axs for multiple Axes\n        fig, axs = plt.subplots(2, 2)\n\n        # using tuple unpacking for multiple Axes\n        fig, (ax1, ax2) = plt.subplots(1, 2)\n        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n\n    The names ``ax`` and pluralized ``axs`` are preferred over ``axes``\n    because for the latter it's not clear if it refers to a single\n    `~.axes.Axes` instance or a collection of these.\n\nSee Also\n--------\n.pyplot.figure\n.pyplot.subplot\n.pyplot.axes\n.Figure.subplots\n.Figure.add_subplot\n\nExamples\n--------\n::\n\n    # First create some toy data:\n    x = np.linspace(0, 2*np.pi, 400)\n    y = np.sin(x**2)\n\n    # Create just a figure and only one subplot\n    fig, ax = plt.subplots()\n    ax.plot(x, y)\n    ax.set_title('Simple plot')\n\n    # Create two subplots and unpack the output array immediately\n    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n    ax1.plot(x, y)\n    ax1.set_title('Sharing Y axis')\n    ax2.scatter(x, y)\n\n    # Create four polar axes and access them through the returned array\n    fig, axs = plt.subplots(2, 2, subplot_kw=dict(projection=\"polar\"))\n    axs[0, 0].plot(x, y)\n    axs[1, 1].scatter(x, y)\n\n    # Share a X axis with each column of subplots\n    plt.subplots(2, 2, sharex='col')\n\n    # Share a Y axis with each row of subplots\n    plt.subplots(2, 2, sharey='row')\n\n    # Share both X and Y axes with all subplots\n    plt.subplots(2, 2, sharex='all', sharey='all')\n\n    # Note that this is the same as\n    plt.subplots(2, 2, sharex=True, sharey=True)\n\n    # Create figure number 10 with a single subplot\n    # and clears it if it already exists.\n    fig, ax = plt.subplots(num=10, clear=True)", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/915", "data": {"name": "matplotlib.pyplot.subplots[1].legend", "type": "method", "signature": "(*args, **kwargs)", "description": "Place a legend on the Axes.\n\nCall signatures::\n\n    legend()\n    legend(handles, labels)\n    legend(handles=handles)\n    legend(labels)\n\nThe call signatures correspond to the following different ways to use\nthis method:\n\n**1. Automatic detection of elements to be shown in the legend**\n\nThe elements to be added to the legend are automatically determined,\nwhen you do not pass in any extra arguments.\n\nIn this case, the labels are taken from the artist. You can specify\nthem either at artist creation or by calling the\n:meth:`~.Artist.set_label` method on the artist::\n\n    ax.plot([1, 2, 3], label='Inline label')\n    ax.legend()\n\nor::\n\n    line, = ax.plot([1, 2, 3])\n    line.set_label('Label via method')\n    ax.legend()\n\n.. note::\n    Specific artists can be excluded from the automatic legend element\n    selection by using a label starting with an underscore, \"_\".\n    A string starting with an underscore is the default label for all\n    artists, so calling `.Axes.legend` without any arguments and\n    without setting the labels manually will result in no legend being\n    drawn.\n\n\n**2. Explicitly listing the artists and labels in the legend**\n\nFor full control of which artists have a legend entry, it is possible\nto pass an iterable of legend artists followed by an iterable of\nlegend labels respectively::\n\n    ax.legend([line1, line2, line3], ['label1', 'label2', 'label3'])\n\n\n**3. Explicitly listing the artists in the legend**\n\nThis is similar to 2, but the labels are taken from the artists'\nlabel properties. Example::\n\n    line1, = ax.plot([1, 2, 3], label='label1')\n    line2, = ax.plot([1, 2, 3], label='label2')\n    ax.legend(handles=[line1, line2])\n\n\n**4. Labeling existing plot elements**\n\n.. admonition:: Discouraged\n\n    This call signature is discouraged, because the relation between\n    plot elements and labels is only implicit by their order and can\n    easily be mixed up.\n\nTo make a legend for all artists on an Axes, call this function with\nan iterable of strings, one for each legend item. For example::\n\n    ax.plot([1, 2, 3])\n    ax.plot([5, 6, 7])\n    ax.legend(['First line', 'Second line'])\n\n\nParameters\n----------\nhandles : list of (`.Artist` or tuple of `.Artist`), optional\n    A list of Artists (lines, patches) to be added to the legend.\n    Use this together with *labels*, if you need full control on what\n    is shown in the legend and the automatic mechanism described above\n    is not sufficient.\n\n    The length of handles and labels should be the same in this\n    case. If they are not, they are truncated to the smaller length.\n\n    If an entry contains a tuple, then the legend handler for all Artists in the\n    tuple will be placed alongside a single label.\n\nlabels : list of str, optional\n    A list of labels to show next to the artists.\n    Use this together with *handles*, if you need full control on what\n    is shown in the legend and the automatic mechanism described above\n    is not sufficient.\n\nReturns\n-------\n`~matplotlib.legend.Legend`\n\nOther Parameters\n----------------\n\nloc : str or pair of floats, default: :rc:`legend.loc`\n    The location of the legend.\n\n    The strings ``'upper left'``, ``'upper right'``, ``'lower left'``,\n    ``'lower right'`` place the legend at the corresponding corner of the\n    axes.\n\n    The strings ``'upper center'``, ``'lower center'``, ``'center left'``,\n    ``'center right'`` place the legend at the center of the corresponding edge\n    of the axes.\n\n    The string ``'center'`` places the legend at the center of the axes.\n\n    The string ``'best'`` places the legend at the location, among the nine\n    locations defined so far, with the minimum overlap with other drawn\n    artists.  This option can be quite slow for plots with large amounts of\n    data; your plotting speed may benefit from providing a specific location.\n\n    The location can also be a 2-tuple giving the coordinates of the lower-left\n    corner of the legend in axes coordinates (in which case *bbox_to_anchor*\n    will be ignored).\n\n    For back-compatibility, ``'center right'`` (but no other location) can also\n    be spelled ``'right'``, and each \"string\" location can also be given as a\n    numeric value:\n\n    ==================   =============\n    Location String      Location Code\n    ==================   =============\n    'best' (Axes only)   0\n    'upper right'        1\n    'upper left'         2\n    'lower left'         3\n    'lower right'        4\n    'right'              5\n    'center left'        6\n    'center right'       7\n    'lower center'       8\n    'upper center'       9\n    'center'             10\n    ==================   =============\n    \nbbox_to_anchor : `.BboxBase`, 2-tuple, or 4-tuple of floats\n    Box that is used to position the legend in conjunction with *loc*.\n    Defaults to `axes.bbox` (if called as a method to `.Axes.legend`) or\n    `figure.bbox` (if `.Figure.legend`).  This argument allows arbitrary\n    placement of the legend.\n\n    Bbox coordinates are interpreted in the coordinate system given by\n    *bbox_transform*, with the default transform\n    Axes or Figure coordinates, depending on which ``legend`` is called.\n\n    If a 4-tuple or `.BboxBase` is given, then it specifies the bbox\n    ``(x, y, width, height)`` that the legend is placed in.\n    To put the legend in the best location in the bottom right\n    quadrant of the axes (or figure)::\n\n        loc='best', bbox_to_anchor=(0.5, 0., 0.5, 0.5)\n\n    A 2-tuple ``(x, y)`` places the corner of the legend specified by *loc* at\n    x, y.  For example, to put the legend's upper right-hand corner in the\n    center of the axes (or figure) the following keywords can be used::\n\n        loc='upper right', bbox_to_anchor=(0.5, 0.5)\n\nncols : int, default: 1\n    The number of columns that the legend has.\n\n    For backward compatibility, the spelling *ncol* is also supported\n    but it is discouraged. If both are given, *ncols* takes precedence.\n\nprop : None or `~matplotlib.font_manager.FontProperties` or dict\n    The font properties of the legend. If None (default), the current\n    :data:`matplotlib.rcParams` will be used.\n\nfontsize : int or {'xx-small', 'x-small', 'small', 'medium', 'large', 'x-large', 'xx-large'}\n    The font size of the legend. If the value is numeric the size will be the\n    absolute font size in points. String values are relative to the current\n    default font size. This argument is only used if *prop* is not specified.\n\nlabelcolor : str or list, default: :rc:`legend.labelcolor`\n    The color of the text in the legend. Either a valid color string\n    (for example, 'red'), or a list of color strings. The labelcolor can\n    also be made to match the color of the line or marker using 'linecolor',\n    'markerfacecolor' (or 'mfc'), or 'markeredgecolor' (or 'mec').\n\n    Labelcolor can be set globally using :rc:`legend.labelcolor`. If None,\n    use :rc:`text.color`.\n\nnumpoints : int, default: :rc:`legend.numpoints`\n    The number of marker points in the legend when creating a legend\n    entry for a `.Line2D` (line).\n\nscatterpoints : int, default: :rc:`legend.scatterpoints`\n    The number of marker points in the legend when creating\n    a legend entry for a `.PathCollection` (scatter plot).\n\nscatteryoffsets : iterable of floats, default: ``[0.375, 0.5, 0.3125]``\n    The vertical offset (relative to the font size) for the markers\n    created for a scatter plot legend entry. 0.0 is at the base the\n    legend text, and 1.0 is at the top. To draw all markers at the\n    same height, set to ``[0.5]``.\n\nmarkerscale : float, default: :rc:`legend.markerscale`\n    The relative size of legend markers compared to the originally drawn ones.\n\nmarkerfirst : bool, default: True\n    If *True*, legend marker is placed to the left of the legend label.\n    If *False*, legend marker is placed to the right of the legend label.\n\nreverse : bool, default: False\n    If *True*, the legend labels are displayed in reverse order from the input.\n    If *False*, the legend labels are displayed in the same order as the input.\n\n    .. versionadded:: 3.7\n\nframeon : bool, default: :rc:`legend.frameon`\n    Whether the legend should be drawn on a patch (frame).\n\nfancybox : bool, default: :rc:`legend.fancybox`\n    Whether round edges should be enabled around the `.FancyBboxPatch` which\n    makes up the legend's background.\n\nshadow : None, bool or dict, default: :rc:`legend.shadow`\n    Whether to draw a shadow behind the legend.\n    The shadow can be configured using `.Patch` keywords.\n    Customization via :rc:`legend.shadow` is currently not supported.\n\nframealpha : float, default: :rc:`legend.framealpha`\n    The alpha transparency of the legend's background.\n    If *shadow* is activated and *framealpha* is ``None``, the default value is\n    ignored.\n\nfacecolor : \"inherit\" or color, default: :rc:`legend.facecolor`\n    The legend's background color.\n    If ``\"inherit\"``, use :rc:`axes.facecolor`.\n\nedgecolor : \"inherit\" or color, default: :rc:`legend.edgecolor`\n    The legend's background patch edge color.\n    If ``\"inherit\"``, use take :rc:`axes.edgecolor`.\n\nmode : {\"expand\", None}\n    If *mode* is set to ``\"expand\"`` the legend will be horizontally\n    expanded to fill the axes area (or *bbox_to_anchor* if defines\n    the legend's size).\n\nbbox_transform : None or `~matplotlib.transforms.Transform`\n    The transform for the bounding box (*bbox_to_anchor*). For a value\n    of ``None`` (default) the Axes'\n    :data:`~matplotlib.axes.Axes.transAxes` transform will be used.\n\ntitle : str or None\n    The legend's title. Default is no title (``None``).\n\ntitle_fontproperties : None or `~matplotlib.font_manager.FontProperties` or dict\n    The font properties of the legend's title. If None (default), the\n    *title_fontsize* argument will be used if present; if *title_fontsize* is\n    also None, the current :rc:`legend.title_fontsize` will be used.\n\ntitle_fontsize : int or {'xx-small', 'x-small', 'small', 'medium', 'large', 'x-large', 'xx-large'}, default: :rc:`legend.title_fontsize`\n    The font size of the legend's title.\n    Note: This cannot be combined with *title_fontproperties*. If you want\n    to set the fontsize alongside other font properties, use the *size*\n    parameter in *title_fontproperties*.\n\nalignment : {'center', 'left', 'right'}, default: 'center'\n    The alignment of the legend title and the box of entries. The entries\n    are aligned as a single block, so that markers always lined up.\n\nborderpad : float, default: :rc:`legend.borderpad`\n    The fractional whitespace inside the legend border, in font-size units.\n\nlabelspacing : float, default: :rc:`legend.labelspacing`\n    The vertical space between the legend entries, in font-size units.\n\nhandlelength : float, default: :rc:`legend.handlelength`\n    The length of the legend handles, in font-size units.\n\nhandleheight : float, default: :rc:`legend.handleheight`\n    The height of the legend handles, in font-size units.\n\nhandletextpad : float, default: :rc:`legend.handletextpad`\n    The pad between the legend handle and text, in font-size units.\n\nborderaxespad : float, default: :rc:`legend.borderaxespad`\n    The pad between the axes and legend border, in font-size units.\n\ncolumnspacing : float, default: :rc:`legend.columnspacing`\n    The spacing between columns, in font-size units.\n\nhandler_map : dict or None\n    The custom dictionary mapping instances or types to a legend\n    handler. This *handler_map* updates the default handler map\n    found at `matplotlib.legend.Legend.get_legend_handler_map`.\n\ndraggable : bool, default: False\n    Whether the legend can be dragged with the mouse.\n\n\nSee Also\n--------\n.Figure.legend\n\nNotes\n-----\nSome artists are not supported by this function.  See\n:ref:`legend_guide` for details.\n\nExamples\n--------\n.. plot:: gallery/text_labels_and_annotations/legend.py", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/915", "data": {"name": "matplotlib.pyplot.subplots[1].set_xlabel", "type": "method", "signature": "(xlabel, fontdict=None, labelpad=None, *, loc=None, **kwargs)", "description": "Set the label for the x-axis.\n\nParameters\n----------\nxlabel : str\n    The label text.\n\nlabelpad : float, default: :rc:`axes.labelpad`\n    Spacing in points from the Axes bounding box including ticks\n    and tick labels.  If None, the previous value is left as is.\n\nloc : {'left', 'center', 'right'}, default: :rc:`xaxis.labellocation`\n    The label position. This is a high-level alternative for passing\n    parameters *x* and *horizontalalignment*.\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    `.Text` properties control the appearance of the label.\n\nSee Also\n--------\ntext : Documents the properties supported by `.Text`.", "parameters": {"type": "object", "properties": {"xlabel": {}, "fontdict": {"type": "NoneType", "default": null}, "labelpad": {"type": "NoneType", "default": null}, "loc": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/915", "data": {"name": "matplotlib.pyplot.subplots[1].set_ylabel", "type": "method", "signature": "(ylabel, fontdict=None, labelpad=None, *, loc=None, **kwargs)", "description": "Set the label for the y-axis.\n\nParameters\n----------\nylabel : str\n    The label text.\n\nlabelpad : float, default: :rc:`axes.labelpad`\n    Spacing in points from the Axes bounding box including ticks\n    and tick labels.  If None, the previous value is left as is.\n\nloc : {'bottom', 'center', 'top'}, default: :rc:`yaxis.labellocation`\n    The label position. This is a high-level alternative for passing\n    parameters *y* and *horizontalalignment*.\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    `.Text` properties control the appearance of the label.\n\nSee Also\n--------\ntext : Documents the properties supported by `.Text`.", "parameters": {"type": "object", "properties": {"ylabel": {}, "fontdict": {"type": "NoneType", "default": null}, "labelpad": {"type": "NoneType", "default": null}, "loc": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/915", "data": {"name": "matplotlib.pyplot.subplots[1].set_title", "type": "method", "signature": "(label, fontdict=None, loc=None, pad=None, *, y=None, **kwargs)", "description": "Set a title for the Axes.\n\nSet one of the three available Axes titles. The available titles\nare positioned above the Axes in the center, flush with the left\nedge, and flush with the right edge.\n\nParameters\n----------\nlabel : str\n    Text to use for the title\n\nfontdict : dict\n\n    .. admonition:: Discouraged\n\n       The use of *fontdict* is discouraged. Parameters should be passed as\n       individual keyword arguments or using dictionary-unpacking\n       ``set_title(..., **fontdict)``.\n\n    A dictionary controlling the appearance of the title text,\n    the default *fontdict* is::\n\n       {'fontsize': rcParams['axes.titlesize'],\n        'fontweight': rcParams['axes.titleweight'],\n        'color': rcParams['axes.titlecolor'],\n        'verticalalignment': 'baseline',\n        'horizontalalignment': loc}\n\nloc : {'center', 'left', 'right'}, default: :rc:`axes.titlelocation`\n    Which title to set.\n\ny : float, default: :rc:`axes.titley`\n    Vertical Axes location for the title (1.0 is the top).  If\n    None (the default) and :rc:`axes.titley` is also None, y is\n    determined automatically to avoid decorators on the Axes.\n\npad : float, default: :rc:`axes.titlepad`\n    The offset of the title from the top of the Axes, in points.\n\nReturns\n-------\n`.Text`\n    The matplotlib text instance representing the title\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    Other keyword arguments are text properties, see `.Text` for a list\n    of valid text properties.", "parameters": {"type": "object", "properties": {"label": {}, "fontdict": {"type": "NoneType", "default": null}, "loc": {"type": "NoneType", "default": null}, "pad": {"type": "NoneType", "default": null}, "y": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/915", "data": {"name": "matplotlib.pyplot.subplots[1].plot", "type": "method", "signature": "(*args, scalex=True, scaley=True, data=None, **kwargs)", "description": "Plot y versus x as lines and/or markers.\n\nCall signatures::\n\n    plot([x], y, [fmt], *, data=None, **kwargs)\n    plot([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n\nThe coordinates of the points or line nodes are given by *x*, *y*.\n\nThe optional parameter *fmt* is a convenient way for defining basic\nformatting like color, marker and linestyle. It's a shortcut string\nnotation described in the *Notes* section below.\n\n>>> plot(x, y)        # plot x and y using default line style and color\n>>> plot(x, y, 'bo')  # plot x and y using blue circle markers\n>>> plot(y)           # plot y using x as index array 0..N-1\n>>> plot(y, 'r+')     # ditto, but with red plusses\n\nYou can use `.Line2D` properties as keyword arguments for more\ncontrol on the appearance. Line properties and *fmt* can be mixed.\nThe following two calls yield identical results:\n\n>>> plot(x, y, 'go--', linewidth=2, markersize=12)\n>>> plot(x, y, color='green', marker='o', linestyle='dashed',\n...      linewidth=2, markersize=12)\n\nWhen conflicting with *fmt*, keyword arguments take precedence.\n\n\n**Plotting labelled data**\n\nThere's a convenient way for plotting objects with labelled data (i.e.\ndata that can be accessed by index ``obj['y']``). Instead of giving\nthe data in *x* and *y*, you can provide the object in the *data*\nparameter and just give the labels for *x* and *y*::\n\n>>> plot('xlabel', 'ylabel', data=obj)\n\nAll indexable objects are supported. This could e.g. be a `dict`, a\n`pandas.DataFrame` or a structured numpy array.\n\n\n**Plotting multiple sets of data**\n\nThere are various ways to plot multiple sets of data.\n\n- The most straight forward way is just to call `plot` multiple times.\n  Example:\n\n  >>> plot(x1, y1, 'bo')\n  >>> plot(x2, y2, 'go')\n\n- If *x* and/or *y* are 2D arrays a separate data set will be drawn\n  for every column. If both *x* and *y* are 2D, they must have the\n  same shape. If only one of them is 2D with shape (N, m) the other\n  must have length N and will be used for every data set m.\n\n  Example:\n\n  >>> x = [1, 2, 3]\n  >>> y = np.array([[1, 2], [3, 4], [5, 6]])\n  >>> plot(x, y)\n\n  is equivalent to:\n\n  >>> for col in range(y.shape[1]):\n  ...     plot(x, y[:, col])\n\n- The third way is to specify multiple sets of *[x]*, *y*, *[fmt]*\n  groups::\n\n  >>> plot(x1, y1, 'g^', x2, y2, 'g-')\n\n  In this case, any additional keyword argument applies to all\n  datasets. Also, this syntax cannot be combined with the *data*\n  parameter.\n\nBy default, each line is assigned a different style specified by a\n'style cycle'. The *fmt* and line property parameters are only\nnecessary if you want explicit deviations from these defaults.\nAlternatively, you can also change the style cycle using\n:rc:`axes.prop_cycle`.\n\n\nParameters\n----------\nx, y : array-like or scalar\n    The horizontal / vertical coordinates of the data points.\n    *x* values are optional and default to ``range(len(y))``.\n\n    Commonly, these parameters are 1D arrays.\n\n    They can also be scalars, or two-dimensional (in that case, the\n    columns represent separate data sets).\n\n    These arguments cannot be passed as keywords.\n\nfmt : str, optional\n    A format string, e.g. 'ro' for red circles. See the *Notes*\n    section for a full description of the format strings.\n\n    Format strings are just an abbreviation for quickly setting\n    basic line properties. All of these and more can also be\n    controlled by keyword arguments.\n\n    This argument cannot be passed as keyword.\n\ndata : indexable object, optional\n    An object with labelled data. If given, provide the label names to\n    plot in *x* and *y*.\n\n    .. note::\n        Technically there's a slight ambiguity in calls where the\n        second label is a valid *fmt*. ``plot('n', 'o', data=obj)``\n        could be ``plt(x, y)`` or ``plt(y, fmt)``. In such cases,\n        the former interpretation is chosen, but a warning is issued.\n        You may suppress the warning by adding an empty format string\n        ``plot('n', 'o', '', data=obj)``.\n\nReturns\n-------\nlist of `.Line2D`\n    A list of lines representing the plotted data.\n\nOther Parameters\n----------------\nscalex, scaley : bool, default: True\n    These parameters determine if the view limits are adapted to the\n    data limits. The values are passed on to\n    `~.axes.Axes.autoscale_view`.\n\n**kwargs : `~matplotlib.lines.Line2D` properties, optional\n    *kwargs* are used to specify properties like a line label (for\n    auto legends), linewidth, antialiasing, marker face color.\n    Example::\n\n    >>> plot([1, 2, 3], [1, 2, 3], 'go-', label='line 1', linewidth=2)\n    >>> plot([1, 2, 3], [1, 4, 9], 'rs', label='line 2')\n\n    If you specify multiple lines with one plot call, the kwargs apply\n    to all those lines. In case the label object is iterable, each\n    element is used as labels for each set of data.\n\n    Here is a list of available `.Line2D` properties:\n\n    Properties:\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: scalar or None\n    animated: bool\n    antialiased or aa: bool\n    clip_box: `~matplotlib.transforms.BboxBase` or None\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    color or c: color\n    dash_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    dash_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    dashes: sequence of floats (on/off ink in points) or (None, None)\n    data: (2, N) array or two 1D arrays\n    drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n    figure: `~matplotlib.figure.Figure`\n    fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n    gapcolor: color or None\n    gid: str\n    in_layout: bool\n    label: object\n    linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n    linewidth or lw: float\n    marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n    markeredgecolor or mec: color\n    markeredgewidth or mew: float\n    markerfacecolor or mfc: color\n    markerfacecoloralt or mfcalt: color\n    markersize or ms: float\n    markevery: None or int or (int, int) or slice or list[int] or float or (float, float) or list[bool]\n    mouseover: bool\n    path_effects: list of `.AbstractPathEffect`\n    picker: float or callable[[Artist, Event], tuple[bool, dict]]\n    pickradius: float\n    rasterized: bool\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    solid_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    solid_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    transform: unknown\n    url: str\n    visible: bool\n    xdata: 1D array\n    ydata: 1D array\n    zorder: float\n\nSee Also\n--------\nscatter : XY scatter plot with markers of varying size and/or color (\n    sometimes also called bubble chart).\n\nNotes\n-----\n**Format Strings**\n\nA format string consists of a part for color, marker and line::\n\n    fmt = '[marker][line][color]'\n\nEach of them is optional. If not provided, the value from the style\ncycle is used. Exception: If ``line`` is given, but no ``marker``,\nthe data will be a line without markers.\n\nOther combinations such as ``[color][marker][line]`` are also\nsupported, but note that their parsing may be ambiguous.\n\n**Markers**\n\n=============   ===============================\ncharacter       description\n=============   ===============================\n``'.'``         point marker\n``','``         pixel marker\n``'o'``         circle marker\n``'v'``         triangle_down marker\n``'^'``         triangle_up marker\n``'<'``         triangle_left marker\n``'>'``         triangle_right marker\n``'1'``         tri_down marker\n``'2'``         tri_up marker\n``'3'``         tri_left marker\n``'4'``         tri_right marker\n``'8'``         octagon marker\n``'s'``         square marker\n``'p'``         pentagon marker\n``'P'``         plus (filled) marker\n``'*'``         star marker\n``'h'``         hexagon1 marker\n``'H'``         hexagon2 marker\n``'+'``         plus marker\n``'x'``         x marker\n``'X'``         x (filled) marker\n``'D'``         diamond marker\n``'d'``         thin_diamond marker\n``'|'``         vline marker\n``'_'``         hline marker\n=============   ===============================\n\n**Line Styles**\n\n=============    ===============================\ncharacter        description\n=============    ===============================\n``'-'``          solid line style\n``'--'``         dashed line style\n``'-.'``         dash-dot line style\n``':'``          dotted line style\n=============    ===============================\n\nExample format strings::\n\n    'b'    # blue markers with default shape\n    'or'   # red circles\n    '-g'   # green solid line\n    '--'   # dashed line with default color\n    '^k:'  # black triangle_up markers connected by a dotted line\n\n**Colors**\n\nThe supported color abbreviations are the single letter codes\n\n=============    ===============================\ncharacter        color\n=============    ===============================\n``'b'``          blue\n``'g'``          green\n``'r'``          red\n``'c'``          cyan\n``'m'``          magenta\n``'y'``          yellow\n``'k'``          black\n``'w'``          white\n=============    ===============================\n\nand the ``'CN'`` colors that index into the default property cycle.\n\nIf the color is the only part of the format string, you can\nadditionally use any  `matplotlib.colors` spec, e.g. full names\n(``'green'``) or hex strings (``'#008000'``).", "parameters": {"type": "object", "properties": {"scalex": {"type": "bool", "default": true}, "scaley": {"type": "bool", "default": true}, "data": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/915", "data": {"name": "numpy.abs", "type": "callable", "signature": "(*args, **kwargs)", "description": "absolute(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n\nCalculate the absolute value element-wise.\n\n``np.abs`` is a shorthand for this function.\n\nParameters\n----------\nx : array_like\n    Input array.\nout : ndarray, None, or tuple of ndarray and None, optional\n    A location into which the result is stored. If provided, it must have\n    a shape that the inputs broadcast to. If not provided or None,\n    a freshly-allocated array is returned. A tuple (possible only as a\n    keyword argument) must have length equal to the number of outputs.\nwhere : array_like, optional\n    This condition is broadcast over the input. At locations where the\n    condition is True, the `out` array will be set to the ufunc result.\n    Elsewhere, the `out` array will retain its original value.\n    Note that if an uninitialized `out` array is created via the default\n    ``out=None``, locations within it where the condition is False will\n    remain uninitialized.\n**kwargs\n    For other keyword-only arguments, see the\n    :ref:`ufunc docs <ufuncs.kwargs>`.\n\nReturns\n-------\nabsolute : ndarray\n    An ndarray containing the absolute value of\n    each element in `x`.  For complex input, ``a + ib``, the\n    absolute value is :math:`\\sqrt{ a^2 + b^2 }`.\n    This is a scalar if `x` is a scalar.\n\nExamples\n--------\n>>> x = np.array([-1.2, 1.2])\n>>> np.absolute(x)\narray([ 1.2,  1.2])\n>>> np.absolute(1.2 + 1j)\n1.5620499351813308\n\nPlot the function over ``[-10, 10]``:\n\n>>> import matplotlib.pyplot as plt\n\n>>> x = np.linspace(start=-10, stop=10, num=101)\n>>> plt.plot(x, np.absolute(x))\n>>> plt.show()\n\nPlot the function over the complex plane:\n\n>>> xx = x + 1j * x[:, np.newaxis]\n>>> plt.imshow(np.abs(xx), extent=[-10, 10, -10, 10], cmap='gray')\n>>> plt.show()\n\nThe `abs` function can be used as a shorthand for ``np.absolute`` on\nndarrays.\n\n>>> x = np.array([-1.2, 1.2])\n>>> abs(x)\narray([1.2, 1.2])", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/915", "data": {"name": "scipy.stats.zscore", "type": "callable", "signature": "(a)", "description": "Compute the z score.\n\nCompute the z score of each value in the sample, relative to the\nsample mean and standard deviation.\n\nParameters\n----------\na : array_like\n    An array like object containing the sample data.\naxis : int or None, optional\n    Axis along which to operate. Default is 0. If None, compute over\n    the whole array `a`.\nddof : int, optional\n    Degrees of freedom correction in the calculation of the\n    standard deviation. Default is 0.\nnan_policy : {'propagate', 'raise', 'omit'}, optional\n    Defines how to handle when input contains nan. 'propagate' returns nan,\n    'raise' throws an error, 'omit' performs the calculations ignoring nan\n    values. Default is 'propagate'.  Note that when the value is 'omit',\n    nans in the input also propagate to the output, but they do not affect\n    the z-scores computed for the non-nan values.\n\nReturns\n-------\nzscore : array_like\n    The z-scores, standardized by mean and standard deviation of\n    input array `a`.\n\nSee Also\n--------\nnumpy.mean : Arithmetic average\nnumpy.std : Arithmetic standard deviation\nscipy.stats.gzscore : Geometric standard score\n\nNotes\n-----\nThis function preserves ndarray subclasses, and works also with\nmatrices and masked arrays (it uses `asanyarray` instead of\n`asarray` for parameters).\n\nReferences\n----------\n.. [1] \"Standard score\", *Wikipedia*,\n       https://en.wikipedia.org/wiki/Standard_score.\n.. [2] Huck, S. W., Cross, T. L., Clark, S. B, \"Overcoming misconceptions\n       about Z-scores\", Teaching Statistics, vol. 8, pp. 38-40, 1986\n\nExamples\n--------\n>>> import numpy as np\n>>> a = np.array([ 0.7972,  0.0767,  0.4383,  0.7866,  0.8091,\n...                0.1954,  0.6307,  0.6599,  0.1065,  0.0508])\n>>> from scipy import stats\n>>> stats.zscore(a)\narray([ 1.1273, -1.247 , -0.0552,  1.0923,  1.1664, -0.8559,  0.5786,\n        0.6748, -1.1488, -1.3324])\n\nComputing along a specified axis, using n-1 degrees of freedom\n(``ddof=1``) to calculate the standard deviation:\n\n>>> b = np.array([[ 0.3148,  0.0478,  0.6243,  0.4608],\n...               [ 0.7149,  0.0775,  0.6072,  0.9656],\n...               [ 0.6341,  0.1403,  0.9759,  0.4064],\n...               [ 0.5918,  0.6948,  0.904 ,  0.3721],\n...               [ 0.0921,  0.2481,  0.1188,  0.1366]])\n>>> stats.zscore(b, axis=1, ddof=1)\narray([[-0.19264823, -1.28415119,  1.07259584,  0.40420358],\n       [ 0.33048416, -1.37380874,  0.04251374,  1.00081084],\n       [ 0.26796377, -1.12598418,  1.23283094, -0.37481053],\n       [-0.22095197,  0.24468594,  1.19042819, -1.21416216],\n       [-0.82780366,  1.4457416 , -0.43867764, -0.1792603 ]])\n\nAn example with `nan_policy='omit'`:\n\n>>> x = np.array([[25.11, 30.10, np.nan, 32.02, 43.15],\n...               [14.95, 16.06, 121.25, 94.35, 29.81]])\n>>> stats.zscore(x, axis=1, nan_policy='omit')\narray([[-1.13490897, -0.37830299,         nan, -0.08718406,  1.60039602],\n       [-0.91611681, -0.89090508,  1.4983032 ,  0.88731639, -0.5785977 ]])", "parameters": {"type": "object", "properties": {"a": {}}}}}
{"task_id": "BigCodeBench/916", "data": {"name": "matplotlib.pyplot.close", "type": "callable", "signature": "(fig: \"None | int | str | Figure | Literal['all']\" = None) -> 'None)", "description": "Close a figure window.\n\nParameters\n----------\nfig : None or int or str or `.Figure`\n    The figure to close. There are a number of ways to specify this:\n\n    - *None*: the current figure\n    - `.Figure`: the given `.Figure` instance\n    - ``int``: a figure number\n    - ``str``: a figure name\n    - 'all': all figures", "parameters": {"type": "object", "properties": {"fig": {"type": ["\"none", "figure", "str", "literal['all']\"", "integer"], "default": null}}}}}
{"task_id": "BigCodeBench/916", "data": {"name": "matplotlib.pyplot.tight_layout", "type": "callable", "signature": "()", "description": "Adjust the padding between and around subplots.\n\nTo exclude an artist on the Axes from the bounding box calculation\nthat determines the subplot parameters (i.e. legend, or annotation),\nset ``a.set_in_layout(False)`` for that artist.\n\nParameters\n----------\npad : float, default: 1.08\n    Padding between the figure edge and the edges of subplots,\n    as a fraction of the font size.\nh_pad, w_pad : float, default: *pad*\n    Padding (height/width) between edges of adjacent subplots,\n    as a fraction of the font size.\nrect : tuple (left, bottom, right, top), default: (0, 0, 1, 1)\n    A rectangle in normalized figure coordinates into which the whole\n    subplots area (including labels) will fit.\n\nSee Also\n--------\n.Figure.set_layout_engine\n.pyplot.tight_layout", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/916", "data": {"name": "matplotlib.pyplot.subplots", "type": "callable", "signature": "(nrows: 'int' = 1, ncols: 'int' = 1, **fig_kw) -> 'tuple[Figure)", "description": "Create a figure and a set of subplots.\n\nThis utility wrapper makes it convenient to create common layouts of\nsubplots, including the enclosing figure object, in a single call.\n\nParameters\n----------\nnrows, ncols : int, default: 1\n    Number of rows/columns of the subplot grid.\n\nsharex, sharey : bool or {'none', 'all', 'row', 'col'}, default: False\n    Controls sharing of properties among x (*sharex*) or y (*sharey*)\n    axes:\n\n    - True or 'all': x- or y-axis will be shared among all subplots.\n    - False or 'none': each subplot x- or y-axis will be independent.\n    - 'row': each subplot row will share an x- or y-axis.\n    - 'col': each subplot column will share an x- or y-axis.\n\n    When subplots have a shared x-axis along a column, only the x tick\n    labels of the bottom subplot are created. Similarly, when subplots\n    have a shared y-axis along a row, only the y tick labels of the first\n    column subplot are created. To later turn other subplots' ticklabels\n    on, use `~matplotlib.axes.Axes.tick_params`.\n\n    When subplots have a shared axis that has units, calling\n    `~matplotlib.axis.Axis.set_units` will update each axis with the\n    new units.\n\nsqueeze : bool, default: True\n    - If True, extra dimensions are squeezed out from the returned\n      array of `~matplotlib.axes.Axes`:\n\n      - if only one subplot is constructed (nrows=ncols=1), the\n        resulting single Axes object is returned as a scalar.\n      - for Nx1 or 1xM subplots, the returned object is a 1D numpy\n        object array of Axes objects.\n      - for NxM, subplots with N>1 and M>1 are returned as a 2D array.\n\n    - If False, no squeezing at all is done: the returned Axes object is\n      always a 2D array containing Axes instances, even if it ends up\n      being 1x1.\n\nwidth_ratios : array-like of length *ncols*, optional\n    Defines the relative widths of the columns. Each column gets a\n    relative width of ``width_ratios[i] / sum(width_ratios)``.\n    If not given, all columns will have the same width.  Equivalent\n    to ``gridspec_kw={'width_ratios': [...]}``.\n\nheight_ratios : array-like of length *nrows*, optional\n    Defines the relative heights of the rows. Each row gets a\n    relative height of ``height_ratios[i] / sum(height_ratios)``.\n    If not given, all rows will have the same height. Convenience\n    for ``gridspec_kw={'height_ratios': [...]}``.\n\nsubplot_kw : dict, optional\n    Dict with keywords passed to the\n    `~matplotlib.figure.Figure.add_subplot` call used to create each\n    subplot.\n\ngridspec_kw : dict, optional\n    Dict with keywords passed to the `~matplotlib.gridspec.GridSpec`\n    constructor used to create the grid the subplots are placed on.\n\n**fig_kw\n    All additional keyword arguments are passed to the\n    `.pyplot.figure` call.\n\nReturns\n-------\nfig : `.Figure`\n\nax : `~matplotlib.axes.Axes` or array of Axes\n    *ax* can be either a single `~.axes.Axes` object, or an array of Axes\n    objects if more than one subplot was created.  The dimensions of the\n    resulting array can be controlled with the squeeze keyword, see above.\n\n    Typical idioms for handling the return value are::\n\n        # using the variable ax for single a Axes\n        fig, ax = plt.subplots()\n\n        # using the variable axs for multiple Axes\n        fig, axs = plt.subplots(2, 2)\n\n        # using tuple unpacking for multiple Axes\n        fig, (ax1, ax2) = plt.subplots(1, 2)\n        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n\n    The names ``ax`` and pluralized ``axs`` are preferred over ``axes``\n    because for the latter it's not clear if it refers to a single\n    `~.axes.Axes` instance or a collection of these.\n\nSee Also\n--------\n.pyplot.figure\n.pyplot.subplot\n.pyplot.axes\n.Figure.subplots\n.Figure.add_subplot\n\nExamples\n--------\n::\n\n    # First create some toy data:\n    x = np.linspace(0, 2*np.pi, 400)\n    y = np.sin(x**2)\n\n    # Create just a figure and only one subplot\n    fig, ax = plt.subplots()\n    ax.plot(x, y)\n    ax.set_title('Simple plot')\n\n    # Create two subplots and unpack the output array immediately\n    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n    ax1.plot(x, y)\n    ax1.set_title('Sharing Y axis')\n    ax2.scatter(x, y)\n\n    # Create four polar axes and access them through the returned array\n    fig, axs = plt.subplots(2, 2, subplot_kw=dict(projection=\"polar\"))\n    axs[0, 0].plot(x, y)\n    axs[1, 1].scatter(x, y)\n\n    # Share a X axis with each column of subplots\n    plt.subplots(2, 2, sharex='col')\n\n    # Share a Y axis with each row of subplots\n    plt.subplots(2, 2, sharey='row')\n\n    # Share both X and Y axes with all subplots\n    plt.subplots(2, 2, sharex='all', sharey='all')\n\n    # Note that this is the same as\n    plt.subplots(2, 2, sharex=True, sharey=True)\n\n    # Create figure number 10 with a single subplot\n    # and clears it if it already exists.\n    fig, ax = plt.subplots(num=10, clear=True)", "parameters": {"type": "object", "properties": {"nrows": {"type": "integer", "default": 1}, "ncols": {"type": "integer", "default": 1}}}}}
{"task_id": "BigCodeBench/916", "data": {"name": "seaborn.boxplot", "type": "callable", "signature": "(x=None, ax=None, **kwargs)", "description": "Draw a box plot to show distributions with respect to categories.\n\nA box plot (or box-and-whisker plot) shows the distribution of quantitative\ndata in a way that facilitates comparisons between variables or across\nlevels of a categorical variable. The box shows the quartiles of the\ndataset while the whiskers extend to show the rest of the distribution,\nexcept for points that are determined to be \"outliers\" using a method\nthat is a function of the inter-quartile range.\n\nSee the :ref:`tutorial <categorical_tutorial>` for more information.\n\n.. note::\n    By default, this function treats one of the variables as categorical\n    and draws data at ordinal positions (0, 1, ... n) on the relevant axis.\n    As of version 0.13.0, this can be disabled by setting `native_scale=True`.\n\n\nParameters\n----------\ndata : DataFrame, Series, dict, array, or list of arrays\n    Dataset for plotting. If `x` and `y` are absent, this is\n    interpreted as wide-form. Otherwise it is expected to be long-form.    \nx, y, hue : names of variables in `data` or vector data\n    Inputs for plotting long-form data. See examples for interpretation.    \norder, hue_order : lists of strings\n    Order to plot the categorical levels in; otherwise the levels are\n    inferred from the data objects.    \norient : \"v\" | \"h\" | \"x\" | \"y\"\n    Orientation of the plot (vertical or horizontal). This is usually\n    inferred based on the type of the input variables, but it can be used\n    to resolve ambiguity when both `x` and `y` are numeric or when\n    plotting wide-form data.\n\n    .. versionchanged:: v0.13.0\n        Added 'x'/'y' as options, equivalent to 'v'/'h'.    \ncolor : matplotlib color\n    Single color for the elements in the plot.    \npalette : palette name, list, or dict\n    Colors to use for the different levels of the ``hue`` variable. Should\n    be something that can be interpreted by :func:`color_palette`, or a\n    dictionary mapping hue levels to matplotlib colors.    \nsaturation : float\n    Proportion of the original saturation to draw fill colors in. Large\n    patches often look better with desaturated colors, but set this to\n    `1` if you want the colors to perfectly match the input values.    \nfill : bool\n    If True, use a solid patch. Otherwise, draw as line art.\n\n    .. versionadded:: v0.13.0    \ndodge : \"auto\" or bool\n    When hue mapping is used, whether elements should be narrowed and shifted along\n    the orient axis to eliminate overlap. If `\"auto\"`, set to `True` when the\n    orient variable is crossed with the categorical variable or `False` otherwise.\n\n    .. versionchanged:: 0.13.0\n\n        Added `\"auto\"` mode as a new default.    \nwidth : float\n    Width allotted to each element on the orient axis. When `native_scale=True`,\n    it is relative to the minimum distance between two values in the native scale.    \ngap : float\n    Shrink on the orient axis by this factor to add a gap between dodged elements.\n\n    .. versionadded:: 0.13.0    \nwhis : float or pair of floats\n    Paramater that controls whisker length. If scalar, whiskers are drawn\n    to the farthest datapoint within *whis * IQR* from the nearest hinge.\n    If a tuple, it is interpreted as percentiles that whiskers represent.\nlinecolor : color\n    Color to use for line elements, when `fill` is True.\n\n    .. versionadded:: v0.13.0    \nlinewidth : float\n    Width of the lines that frame the plot elements.    \nfliersize : float\n    Size of the markers used to indicate outlier observations.\nhue_norm : tuple or :class:`matplotlib.colors.Normalize` object\n    Normalization in data units for colormap applied to the `hue`\n    variable when it is numeric. Not relevant if `hue` is categorical.\n\n    .. versionadded:: v0.12.0    \nlog_scale : bool or number, or pair of bools or numbers\n    Set axis scale(s) to log. A single value sets the data axis for any numeric\n    axes in the plot. A pair of values sets each axis independently.\n    Numeric values are interpreted as the desired base (default 10).\n    When `None` or `False`, seaborn defers to the existing Axes scale.\n\n    .. versionadded:: v0.13.0    \nnative_scale : bool\n    When True, numeric or datetime values on the categorical axis will maintain\n    their original scaling rather than being converted to fixed indices.\n\n    .. versionadded:: v0.13.0    \nformatter : callable\n    Function for converting categorical data into strings. Affects both grouping\n    and tick labels.\n\n    .. versionadded:: v0.13.0    \nlegend : \"auto\", \"brief\", \"full\", or False\n    How to draw the legend. If \"brief\", numeric `hue` and `size`\n    variables will be represented with a sample of evenly spaced values.\n    If \"full\", every group will get an entry in the legend. If \"auto\",\n    choose between brief or full representation based on number of levels.\n    If `False`, no legend data is added and no legend is drawn.\n\n    .. versionadded:: v0.13.0    \nax : matplotlib Axes\n    Axes object to draw the plot onto, otherwise uses the current Axes.    \nkwargs : key, value mappings\n    Other keyword arguments are passed through to\n    :meth:`matplotlib.axes.Axes.boxplot`.\n\nReturns\n-------\nax : matplotlib Axes\n    Returns the Axes object with the plot drawn onto it.    \n\nSee Also\n--------\nviolinplot : A combination of boxplot and kernel density estimation.    \nstripplot : A scatterplot where one variable is categorical. Can be used\n            in conjunction with other plots to show each observation.    \nswarmplot : A categorical scatterplot where the points do not overlap. Can\n            be used with other plots to show each observation.    \ncatplot : Combine a categorical plot with a :class:`FacetGrid`.    \n\nExamples\n--------\n.. include:: ../docstrings/boxplot.rst", "parameters": {"type": "object", "properties": {"x": {"type": "NoneType", "default": null}, "ax": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/916", "data": {"name": "seaborn.boxplot.set_title", "type": "callable", "signature": "(data=None)", "description": "Draw a box plot to show distributions with respect to categories.\n\nA box plot (or box-and-whisker plot) shows the distribution of quantitative\ndata in a way that facilitates comparisons between variables or across\nlevels of a categorical variable. The box shows the quartiles of the\ndataset while the whiskers extend to show the rest of the distribution,\nexcept for points that are determined to be \"outliers\" using a method\nthat is a function of the inter-quartile range.\n\nSee the :ref:`tutorial <categorical_tutorial>` for more information.\n\n.. note::\n    By default, this function treats one of the variables as categorical\n    and draws data at ordinal positions (0, 1, ... n) on the relevant axis.\n    As of version 0.13.0, this can be disabled by setting `native_scale=True`.\n\n\nParameters\n----------\ndata : DataFrame, Series, dict, array, or list of arrays\n    Dataset for plotting. If `x` and `y` are absent, this is\n    interpreted as wide-form. Otherwise it is expected to be long-form.    \nx, y, hue : names of variables in `data` or vector data\n    Inputs for plotting long-form data. See examples for interpretation.    \norder, hue_order : lists of strings\n    Order to plot the categorical levels in; otherwise the levels are\n    inferred from the data objects.    \norient : \"v\" | \"h\" | \"x\" | \"y\"\n    Orientation of the plot (vertical or horizontal). This is usually\n    inferred based on the type of the input variables, but it can be used\n    to resolve ambiguity when both `x` and `y` are numeric or when\n    plotting wide-form data.\n\n    .. versionchanged:: v0.13.0\n        Added 'x'/'y' as options, equivalent to 'v'/'h'.    \ncolor : matplotlib color\n    Single color for the elements in the plot.    \npalette : palette name, list, or dict\n    Colors to use for the different levels of the ``hue`` variable. Should\n    be something that can be interpreted by :func:`color_palette`, or a\n    dictionary mapping hue levels to matplotlib colors.    \nsaturation : float\n    Proportion of the original saturation to draw fill colors in. Large\n    patches often look better with desaturated colors, but set this to\n    `1` if you want the colors to perfectly match the input values.    \nfill : bool\n    If True, use a solid patch. Otherwise, draw as line art.\n\n    .. versionadded:: v0.13.0    \ndodge : \"auto\" or bool\n    When hue mapping is used, whether elements should be narrowed and shifted along\n    the orient axis to eliminate overlap. If `\"auto\"`, set to `True` when the\n    orient variable is crossed with the categorical variable or `False` otherwise.\n\n    .. versionchanged:: 0.13.0\n\n        Added `\"auto\"` mode as a new default.    \nwidth : float\n    Width allotted to each element on the orient axis. When `native_scale=True`,\n    it is relative to the minimum distance between two values in the native scale.    \ngap : float\n    Shrink on the orient axis by this factor to add a gap between dodged elements.\n\n    .. versionadded:: 0.13.0    \nwhis : float or pair of floats\n    Paramater that controls whisker length. If scalar, whiskers are drawn\n    to the farthest datapoint within *whis * IQR* from the nearest hinge.\n    If a tuple, it is interpreted as percentiles that whiskers represent.\nlinecolor : color\n    Color to use for line elements, when `fill` is True.\n\n    .. versionadded:: v0.13.0    \nlinewidth : float\n    Width of the lines that frame the plot elements.    \nfliersize : float\n    Size of the markers used to indicate outlier observations.\nhue_norm : tuple or :class:`matplotlib.colors.Normalize` object\n    Normalization in data units for colormap applied to the `hue`\n    variable when it is numeric. Not relevant if `hue` is categorical.\n\n    .. versionadded:: v0.12.0    \nlog_scale : bool or number, or pair of bools or numbers\n    Set axis scale(s) to log. A single value sets the data axis for any numeric\n    axes in the plot. A pair of values sets each axis independently.\n    Numeric values are interpreted as the desired base (default 10).\n    When `None` or `False`, seaborn defers to the existing Axes scale.\n\n    .. versionadded:: v0.13.0    \nnative_scale : bool\n    When True, numeric or datetime values on the categorical axis will maintain\n    their original scaling rather than being converted to fixed indices.\n\n    .. versionadded:: v0.13.0    \nformatter : callable\n    Function for converting categorical data into strings. Affects both grouping\n    and tick labels.\n\n    .. versionadded:: v0.13.0    \nlegend : \"auto\", \"brief\", \"full\", or False\n    How to draw the legend. If \"brief\", numeric `hue` and `size`\n    variables will be represented with a sample of evenly spaced values.\n    If \"full\", every group will get an entry in the legend. If \"auto\",\n    choose between brief or full representation based on number of levels.\n    If `False`, no legend data is added and no legend is drawn.\n\n    .. versionadded:: v0.13.0    \nax : matplotlib Axes\n    Axes object to draw the plot onto, otherwise uses the current Axes.    \nkwargs : key, value mappings\n    Other keyword arguments are passed through to\n    :meth:`matplotlib.axes.Axes.boxplot`.\n\nReturns\n-------\nax : matplotlib Axes\n    Returns the Axes object with the plot drawn onto it.    \n\nSee Also\n--------\nviolinplot : A combination of boxplot and kernel density estimation.    \nstripplot : A scatterplot where one variable is categorical. Can be used\n            in conjunction with other plots to show each observation.    \nswarmplot : A categorical scatterplot where the points do not overlap. Can\n            be used with other plots to show each observation.    \ncatplot : Combine a categorical plot with a :class:`FacetGrid`.    \n\nExamples\n--------\n.. include:: ../docstrings/boxplot.rst", "parameters": {"type": "object", "properties": {"data": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/916", "data": {"name": "seaborn.histplot", "type": "callable", "signature": "(data=None, kde=False, ax=None, **kwargs)", "description": "Plot univariate or bivariate histograms to show distributions of datasets.\n\nA histogram is a classic visualization tool that represents the distribution\nof one or more variables by counting the number of observations that fall within\ndiscrete bins.\n\nThis function can normalize the statistic computed within each bin to estimate\nfrequency, density or probability mass, and it can add a smooth curve obtained\nusing a kernel density estimate, similar to :func:`kdeplot`.\n\nMore information is provided in the :ref:`user guide <tutorial_hist>`.\n\nParameters\n----------\ndata : :class:`pandas.DataFrame`, :class:`numpy.ndarray`, mapping, or sequence\n    Input data structure. Either a long-form collection of vectors that can be\n    assigned to named variables or a wide-form dataset that will be internally\n    reshaped.\nx, y : vectors or keys in ``data``\n    Variables that specify positions on the x and y axes.\nhue : vector or key in ``data``\n    Semantic variable that is mapped to determine the color of plot elements.\nweights : vector or key in ``data``\n    If provided, weight the contribution of the corresponding data points\n    towards the count in each bin by these factors.\nstat : str\n    Aggregate statistic to compute in each bin.\n    \n    - `count`: show the number of observations in each bin\n    - `frequency`: show the number of observations divided by the bin width\n    - `probability` or `proportion`: normalize such that bar heights sum to 1\n    - `percent`: normalize such that bar heights sum to 100\n    - `density`: normalize such that the total area of the histogram equals 1\nbins : str, number, vector, or a pair of such values\n    Generic bin parameter that can be the name of a reference rule,\n    the number of bins, or the breaks of the bins.\n    Passed to :func:`numpy.histogram_bin_edges`.\nbinwidth : number or pair of numbers\n    Width of each bin, overrides ``bins`` but can be used with\n    ``binrange``.\nbinrange : pair of numbers or a pair of pairs\n    Lowest and highest value for bin edges; can be used either\n    with ``bins`` or ``binwidth``. Defaults to data extremes.\ndiscrete : bool\n    If True, default to ``binwidth=1`` and draw the bars so that they are\n    centered on their corresponding data points. This avoids \"gaps\" that may\n    otherwise appear when using discrete (integer) data.\ncumulative : bool\n    If True, plot the cumulative counts as bins increase.\ncommon_bins : bool\n    If True, use the same bins when semantic variables produce multiple\n    plots. If using a reference rule to determine the bins, it will be computed\n    with the full dataset.\ncommon_norm : bool\n    If True and using a normalized statistic, the normalization will apply over\n    the full dataset. Otherwise, normalize each histogram independently.\nmultiple : {\"layer\", \"dodge\", \"stack\", \"fill\"}\n    Approach to resolving multiple elements when semantic mapping creates subsets.\n    Only relevant with univariate data.\nelement : {\"bars\", \"step\", \"poly\"}\n    Visual representation of the histogram statistic.\n    Only relevant with univariate data.\nfill : bool\n    If True, fill in the space under the histogram.\n    Only relevant with univariate data.\nshrink : number\n    Scale the width of each bar relative to the binwidth by this factor.\n    Only relevant with univariate data.\nkde : bool\n    If True, compute a kernel density estimate to smooth the distribution\n    and show on the plot as (one or more) line(s).\n    Only relevant with univariate data.\nkde_kws : dict\n    Parameters that control the KDE computation, as in :func:`kdeplot`.\nline_kws : dict\n    Parameters that control the KDE visualization, passed to\n    :meth:`matplotlib.axes.Axes.plot`.\nthresh : number or None\n    Cells with a statistic less than or equal to this value will be transparent.\n    Only relevant with bivariate data.\npthresh : number or None\n    Like ``thresh``, but a value in [0, 1] such that cells with aggregate counts\n    (or other statistics, when used) up to this proportion of the total will be\n    transparent.\npmax : number or None\n    A value in [0, 1] that sets that saturation point for the colormap at a value\n    such that cells below constitute this proportion of the total count (or\n    other statistic, when used).\ncbar : bool\n    If True, add a colorbar to annotate the color mapping in a bivariate plot.\n    Note: Does not currently support plots with a ``hue`` variable well.\ncbar_ax : :class:`matplotlib.axes.Axes`\n    Pre-existing axes for the colorbar.\ncbar_kws : dict\n    Additional parameters passed to :meth:`matplotlib.figure.Figure.colorbar`.\npalette : string, list, dict, or :class:`matplotlib.colors.Colormap`\n    Method for choosing the colors to use when mapping the ``hue`` semantic.\n    String values are passed to :func:`color_palette`. List or dict values\n    imply categorical mapping, while a colormap object implies numeric mapping.\nhue_order : vector of strings\n    Specify the order of processing and plotting for categorical levels of the\n    ``hue`` semantic.\nhue_norm : tuple or :class:`matplotlib.colors.Normalize`\n    Either a pair of values that set the normalization range in data units\n    or an object that will map from data units into a [0, 1] interval. Usage\n    implies numeric mapping.\ncolor : :mod:`matplotlib color <matplotlib.colors>`\n    Single color specification for when hue mapping is not used. Otherwise, the\n    plot will try to hook into the matplotlib property cycle.\nlog_scale : bool or number, or pair of bools or numbers\n    Set axis scale(s) to log. A single value sets the data axis for any numeric\n    axes in the plot. A pair of values sets each axis independently.\n    Numeric values are interpreted as the desired base (default 10).\n    When `None` or `False`, seaborn defers to the existing Axes scale.\nlegend : bool\n    If False, suppress the legend for semantic variables.\nax : :class:`matplotlib.axes.Axes`\n    Pre-existing axes for the plot. Otherwise, call :func:`matplotlib.pyplot.gca`\n    internally.\nkwargs\n    Other keyword arguments are passed to one of the following matplotlib\n    functions:\n\n    - :meth:`matplotlib.axes.Axes.bar` (univariate, element=\"bars\")\n    - :meth:`matplotlib.axes.Axes.fill_between` (univariate, other element, fill=True)\n    - :meth:`matplotlib.axes.Axes.plot` (univariate, other element, fill=False)\n    - :meth:`matplotlib.axes.Axes.pcolormesh` (bivariate)\n\nReturns\n-------\n:class:`matplotlib.axes.Axes`\n    The matplotlib axes containing the plot.\n\nSee Also\n--------\ndisplot : Figure-level interface to distribution plot functions.\nkdeplot : Plot univariate or bivariate distributions using kernel density estimation.\nrugplot : Plot a tick at each observation value along the x and/or y axes.\necdfplot : Plot empirical cumulative distribution functions.\njointplot : Draw a bivariate plot with univariate marginal distributions.\n\nNotes\n-----\n\nThe choice of bins for computing and plotting a histogram can exert\nsubstantial influence on the insights that one is able to draw from the\nvisualization. If the bins are too large, they may erase important features.\nOn the other hand, bins that are too small may be dominated by random\nvariability, obscuring the shape of the true underlying distribution. The\ndefault bin size is determined using a reference rule that depends on the\nsample size and variance. This works well in many cases, (i.e., with\n\"well-behaved\" data) but it fails in others. It is always a good to try\ndifferent bin sizes to be sure that you are not missing something important.\nThis function allows you to specify bins in several different ways, such as\nby setting the total number of bins to use, the width of each bin, or the\nspecific locations where the bins should break.\n\nExamples\n--------\n\n.. include:: ../docstrings/histplot.rst", "parameters": {"type": "object", "properties": {"data": {"type": "NoneType", "default": null}, "kde": {"type": "bool", "default": false}, "ax": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/916", "data": {"name": "seaborn.histplot.set_title", "type": "callable", "signature": "(data=None)", "description": "Plot univariate or bivariate histograms to show distributions of datasets.\n\nA histogram is a classic visualization tool that represents the distribution\nof one or more variables by counting the number of observations that fall within\ndiscrete bins.\n\nThis function can normalize the statistic computed within each bin to estimate\nfrequency, density or probability mass, and it can add a smooth curve obtained\nusing a kernel density estimate, similar to :func:`kdeplot`.\n\nMore information is provided in the :ref:`user guide <tutorial_hist>`.\n\nParameters\n----------\ndata : :class:`pandas.DataFrame`, :class:`numpy.ndarray`, mapping, or sequence\n    Input data structure. Either a long-form collection of vectors that can be\n    assigned to named variables or a wide-form dataset that will be internally\n    reshaped.\nx, y : vectors or keys in ``data``\n    Variables that specify positions on the x and y axes.\nhue : vector or key in ``data``\n    Semantic variable that is mapped to determine the color of plot elements.\nweights : vector or key in ``data``\n    If provided, weight the contribution of the corresponding data points\n    towards the count in each bin by these factors.\nstat : str\n    Aggregate statistic to compute in each bin.\n    \n    - `count`: show the number of observations in each bin\n    - `frequency`: show the number of observations divided by the bin width\n    - `probability` or `proportion`: normalize such that bar heights sum to 1\n    - `percent`: normalize such that bar heights sum to 100\n    - `density`: normalize such that the total area of the histogram equals 1\nbins : str, number, vector, or a pair of such values\n    Generic bin parameter that can be the name of a reference rule,\n    the number of bins, or the breaks of the bins.\n    Passed to :func:`numpy.histogram_bin_edges`.\nbinwidth : number or pair of numbers\n    Width of each bin, overrides ``bins`` but can be used with\n    ``binrange``.\nbinrange : pair of numbers or a pair of pairs\n    Lowest and highest value for bin edges; can be used either\n    with ``bins`` or ``binwidth``. Defaults to data extremes.\ndiscrete : bool\n    If True, default to ``binwidth=1`` and draw the bars so that they are\n    centered on their corresponding data points. This avoids \"gaps\" that may\n    otherwise appear when using discrete (integer) data.\ncumulative : bool\n    If True, plot the cumulative counts as bins increase.\ncommon_bins : bool\n    If True, use the same bins when semantic variables produce multiple\n    plots. If using a reference rule to determine the bins, it will be computed\n    with the full dataset.\ncommon_norm : bool\n    If True and using a normalized statistic, the normalization will apply over\n    the full dataset. Otherwise, normalize each histogram independently.\nmultiple : {\"layer\", \"dodge\", \"stack\", \"fill\"}\n    Approach to resolving multiple elements when semantic mapping creates subsets.\n    Only relevant with univariate data.\nelement : {\"bars\", \"step\", \"poly\"}\n    Visual representation of the histogram statistic.\n    Only relevant with univariate data.\nfill : bool\n    If True, fill in the space under the histogram.\n    Only relevant with univariate data.\nshrink : number\n    Scale the width of each bar relative to the binwidth by this factor.\n    Only relevant with univariate data.\nkde : bool\n    If True, compute a kernel density estimate to smooth the distribution\n    and show on the plot as (one or more) line(s).\n    Only relevant with univariate data.\nkde_kws : dict\n    Parameters that control the KDE computation, as in :func:`kdeplot`.\nline_kws : dict\n    Parameters that control the KDE visualization, passed to\n    :meth:`matplotlib.axes.Axes.plot`.\nthresh : number or None\n    Cells with a statistic less than or equal to this value will be transparent.\n    Only relevant with bivariate data.\npthresh : number or None\n    Like ``thresh``, but a value in [0, 1] such that cells with aggregate counts\n    (or other statistics, when used) up to this proportion of the total will be\n    transparent.\npmax : number or None\n    A value in [0, 1] that sets that saturation point for the colormap at a value\n    such that cells below constitute this proportion of the total count (or\n    other statistic, when used).\ncbar : bool\n    If True, add a colorbar to annotate the color mapping in a bivariate plot.\n    Note: Does not currently support plots with a ``hue`` variable well.\ncbar_ax : :class:`matplotlib.axes.Axes`\n    Pre-existing axes for the colorbar.\ncbar_kws : dict\n    Additional parameters passed to :meth:`matplotlib.figure.Figure.colorbar`.\npalette : string, list, dict, or :class:`matplotlib.colors.Colormap`\n    Method for choosing the colors to use when mapping the ``hue`` semantic.\n    String values are passed to :func:`color_palette`. List or dict values\n    imply categorical mapping, while a colormap object implies numeric mapping.\nhue_order : vector of strings\n    Specify the order of processing and plotting for categorical levels of the\n    ``hue`` semantic.\nhue_norm : tuple or :class:`matplotlib.colors.Normalize`\n    Either a pair of values that set the normalization range in data units\n    or an object that will map from data units into a [0, 1] interval. Usage\n    implies numeric mapping.\ncolor : :mod:`matplotlib color <matplotlib.colors>`\n    Single color specification for when hue mapping is not used. Otherwise, the\n    plot will try to hook into the matplotlib property cycle.\nlog_scale : bool or number, or pair of bools or numbers\n    Set axis scale(s) to log. A single value sets the data axis for any numeric\n    axes in the plot. A pair of values sets each axis independently.\n    Numeric values are interpreted as the desired base (default 10).\n    When `None` or `False`, seaborn defers to the existing Axes scale.\nlegend : bool\n    If False, suppress the legend for semantic variables.\nax : :class:`matplotlib.axes.Axes`\n    Pre-existing axes for the plot. Otherwise, call :func:`matplotlib.pyplot.gca`\n    internally.\nkwargs\n    Other keyword arguments are passed to one of the following matplotlib\n    functions:\n\n    - :meth:`matplotlib.axes.Axes.bar` (univariate, element=\"bars\")\n    - :meth:`matplotlib.axes.Axes.fill_between` (univariate, other element, fill=True)\n    - :meth:`matplotlib.axes.Axes.plot` (univariate, other element, fill=False)\n    - :meth:`matplotlib.axes.Axes.pcolormesh` (bivariate)\n\nReturns\n-------\n:class:`matplotlib.axes.Axes`\n    The matplotlib axes containing the plot.\n\nSee Also\n--------\ndisplot : Figure-level interface to distribution plot functions.\nkdeplot : Plot univariate or bivariate distributions using kernel density estimation.\nrugplot : Plot a tick at each observation value along the x and/or y axes.\necdfplot : Plot empirical cumulative distribution functions.\njointplot : Draw a bivariate plot with univariate marginal distributions.\n\nNotes\n-----\n\nThe choice of bins for computing and plotting a histogram can exert\nsubstantial influence on the insights that one is able to draw from the\nvisualization. If the bins are too large, they may erase important features.\nOn the other hand, bins that are too small may be dominated by random\nvariability, obscuring the shape of the true underlying distribution. The\ndefault bin size is determined using a reference rule that depends on the\nsample size and variance. This works well in many cases, (i.e., with\n\"well-behaved\" data) but it fails in others. It is always a good to try\ndifferent bin sizes to be sure that you are not missing something important.\nThis function allows you to specify bins in several different ways, such as\nby setting the total number of bins to use, the width of each bin, or the\nspecific locations where the bins should break.\n\nExamples\n--------\n\n.. include:: ../docstrings/histplot.rst", "parameters": {"type": "object", "properties": {"data": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/917", "data": {"name": "matplotlib.pyplot.subplots", "type": "class", "signature": "()", "description": "Create a figure and a set of subplots.\n\nThis utility wrapper makes it convenient to create common layouts of\nsubplots, including the enclosing figure object, in a single call.\n\nParameters\n----------\nnrows, ncols : int, default: 1\n    Number of rows/columns of the subplot grid.\n\nsharex, sharey : bool or {'none', 'all', 'row', 'col'}, default: False\n    Controls sharing of properties among x (*sharex*) or y (*sharey*)\n    axes:\n\n    - True or 'all': x- or y-axis will be shared among all subplots.\n    - False or 'none': each subplot x- or y-axis will be independent.\n    - 'row': each subplot row will share an x- or y-axis.\n    - 'col': each subplot column will share an x- or y-axis.\n\n    When subplots have a shared x-axis along a column, only the x tick\n    labels of the bottom subplot are created. Similarly, when subplots\n    have a shared y-axis along a row, only the y tick labels of the first\n    column subplot are created. To later turn other subplots' ticklabels\n    on, use `~matplotlib.axes.Axes.tick_params`.\n\n    When subplots have a shared axis that has units, calling\n    `~matplotlib.axis.Axis.set_units` will update each axis with the\n    new units.\n\nsqueeze : bool, default: True\n    - If True, extra dimensions are squeezed out from the returned\n      array of `~matplotlib.axes.Axes`:\n\n      - if only one subplot is constructed (nrows=ncols=1), the\n        resulting single Axes object is returned as a scalar.\n      - for Nx1 or 1xM subplots, the returned object is a 1D numpy\n        object array of Axes objects.\n      - for NxM, subplots with N>1 and M>1 are returned as a 2D array.\n\n    - If False, no squeezing at all is done: the returned Axes object is\n      always a 2D array containing Axes instances, even if it ends up\n      being 1x1.\n\nwidth_ratios : array-like of length *ncols*, optional\n    Defines the relative widths of the columns. Each column gets a\n    relative width of ``width_ratios[i] / sum(width_ratios)``.\n    If not given, all columns will have the same width.  Equivalent\n    to ``gridspec_kw={'width_ratios': [...]}``.\n\nheight_ratios : array-like of length *nrows*, optional\n    Defines the relative heights of the rows. Each row gets a\n    relative height of ``height_ratios[i] / sum(height_ratios)``.\n    If not given, all rows will have the same height. Convenience\n    for ``gridspec_kw={'height_ratios': [...]}``.\n\nsubplot_kw : dict, optional\n    Dict with keywords passed to the\n    `~matplotlib.figure.Figure.add_subplot` call used to create each\n    subplot.\n\ngridspec_kw : dict, optional\n    Dict with keywords passed to the `~matplotlib.gridspec.GridSpec`\n    constructor used to create the grid the subplots are placed on.\n\n**fig_kw\n    All additional keyword arguments are passed to the\n    `.pyplot.figure` call.\n\nReturns\n-------\nfig : `.Figure`\n\nax : `~matplotlib.axes.Axes` or array of Axes\n    *ax* can be either a single `~.axes.Axes` object, or an array of Axes\n    objects if more than one subplot was created.  The dimensions of the\n    resulting array can be controlled with the squeeze keyword, see above.\n\n    Typical idioms for handling the return value are::\n\n        # using the variable ax for single a Axes\n        fig, ax = plt.subplots()\n\n        # using the variable axs for multiple Axes\n        fig, axs = plt.subplots(2, 2)\n\n        # using tuple unpacking for multiple Axes\n        fig, (ax1, ax2) = plt.subplots(1, 2)\n        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n\n    The names ``ax`` and pluralized ``axs`` are preferred over ``axes``\n    because for the latter it's not clear if it refers to a single\n    `~.axes.Axes` instance or a collection of these.\n\nSee Also\n--------\n.pyplot.figure\n.pyplot.subplot\n.pyplot.axes\n.Figure.subplots\n.Figure.add_subplot\n\nExamples\n--------\n::\n\n    # First create some toy data:\n    x = np.linspace(0, 2*np.pi, 400)\n    y = np.sin(x**2)\n\n    # Create just a figure and only one subplot\n    fig, ax = plt.subplots()\n    ax.plot(x, y)\n    ax.set_title('Simple plot')\n\n    # Create two subplots and unpack the output array immediately\n    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n    ax1.plot(x, y)\n    ax1.set_title('Sharing Y axis')\n    ax2.scatter(x, y)\n\n    # Create four polar axes and access them through the returned array\n    fig, axs = plt.subplots(2, 2, subplot_kw=dict(projection=\"polar\"))\n    axs[0, 0].plot(x, y)\n    axs[1, 1].scatter(x, y)\n\n    # Share a X axis with each column of subplots\n    plt.subplots(2, 2, sharex='col')\n\n    # Share a Y axis with each row of subplots\n    plt.subplots(2, 2, sharey='row')\n\n    # Share both X and Y axes with all subplots\n    plt.subplots(2, 2, sharex='all', sharey='all')\n\n    # Note that this is the same as\n    plt.subplots(2, 2, sharex=True, sharey=True)\n\n    # Create figure number 10 with a single subplot\n    # and clears it if it already exists.\n    fig, ax = plt.subplots(num=10, clear=True)", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/917", "data": {"name": "matplotlib.pyplot.subplots[1].legend", "type": "method", "signature": "(*args, **kwargs)", "description": "Place a legend on the Axes.\n\nCall signatures::\n\n    legend()\n    legend(handles, labels)\n    legend(handles=handles)\n    legend(labels)\n\nThe call signatures correspond to the following different ways to use\nthis method:\n\n**1. Automatic detection of elements to be shown in the legend**\n\nThe elements to be added to the legend are automatically determined,\nwhen you do not pass in any extra arguments.\n\nIn this case, the labels are taken from the artist. You can specify\nthem either at artist creation or by calling the\n:meth:`~.Artist.set_label` method on the artist::\n\n    ax.plot([1, 2, 3], label='Inline label')\n    ax.legend()\n\nor::\n\n    line, = ax.plot([1, 2, 3])\n    line.set_label('Label via method')\n    ax.legend()\n\n.. note::\n    Specific artists can be excluded from the automatic legend element\n    selection by using a label starting with an underscore, \"_\".\n    A string starting with an underscore is the default label for all\n    artists, so calling `.Axes.legend` without any arguments and\n    without setting the labels manually will result in no legend being\n    drawn.\n\n\n**2. Explicitly listing the artists and labels in the legend**\n\nFor full control of which artists have a legend entry, it is possible\nto pass an iterable of legend artists followed by an iterable of\nlegend labels respectively::\n\n    ax.legend([line1, line2, line3], ['label1', 'label2', 'label3'])\n\n\n**3. Explicitly listing the artists in the legend**\n\nThis is similar to 2, but the labels are taken from the artists'\nlabel properties. Example::\n\n    line1, = ax.plot([1, 2, 3], label='label1')\n    line2, = ax.plot([1, 2, 3], label='label2')\n    ax.legend(handles=[line1, line2])\n\n\n**4. Labeling existing plot elements**\n\n.. admonition:: Discouraged\n\n    This call signature is discouraged, because the relation between\n    plot elements and labels is only implicit by their order and can\n    easily be mixed up.\n\nTo make a legend for all artists on an Axes, call this function with\nan iterable of strings, one for each legend item. For example::\n\n    ax.plot([1, 2, 3])\n    ax.plot([5, 6, 7])\n    ax.legend(['First line', 'Second line'])\n\n\nParameters\n----------\nhandles : list of (`.Artist` or tuple of `.Artist`), optional\n    A list of Artists (lines, patches) to be added to the legend.\n    Use this together with *labels*, if you need full control on what\n    is shown in the legend and the automatic mechanism described above\n    is not sufficient.\n\n    The length of handles and labels should be the same in this\n    case. If they are not, they are truncated to the smaller length.\n\n    If an entry contains a tuple, then the legend handler for all Artists in the\n    tuple will be placed alongside a single label.\n\nlabels : list of str, optional\n    A list of labels to show next to the artists.\n    Use this together with *handles*, if you need full control on what\n    is shown in the legend and the automatic mechanism described above\n    is not sufficient.\n\nReturns\n-------\n`~matplotlib.legend.Legend`\n\nOther Parameters\n----------------\n\nloc : str or pair of floats, default: :rc:`legend.loc`\n    The location of the legend.\n\n    The strings ``'upper left'``, ``'upper right'``, ``'lower left'``,\n    ``'lower right'`` place the legend at the corresponding corner of the\n    axes.\n\n    The strings ``'upper center'``, ``'lower center'``, ``'center left'``,\n    ``'center right'`` place the legend at the center of the corresponding edge\n    of the axes.\n\n    The string ``'center'`` places the legend at the center of the axes.\n\n    The string ``'best'`` places the legend at the location, among the nine\n    locations defined so far, with the minimum overlap with other drawn\n    artists.  This option can be quite slow for plots with large amounts of\n    data; your plotting speed may benefit from providing a specific location.\n\n    The location can also be a 2-tuple giving the coordinates of the lower-left\n    corner of the legend in axes coordinates (in which case *bbox_to_anchor*\n    will be ignored).\n\n    For back-compatibility, ``'center right'`` (but no other location) can also\n    be spelled ``'right'``, and each \"string\" location can also be given as a\n    numeric value:\n\n    ==================   =============\n    Location String      Location Code\n    ==================   =============\n    'best' (Axes only)   0\n    'upper right'        1\n    'upper left'         2\n    'lower left'         3\n    'lower right'        4\n    'right'              5\n    'center left'        6\n    'center right'       7\n    'lower center'       8\n    'upper center'       9\n    'center'             10\n    ==================   =============\n    \nbbox_to_anchor : `.BboxBase`, 2-tuple, or 4-tuple of floats\n    Box that is used to position the legend in conjunction with *loc*.\n    Defaults to `axes.bbox` (if called as a method to `.Axes.legend`) or\n    `figure.bbox` (if `.Figure.legend`).  This argument allows arbitrary\n    placement of the legend.\n\n    Bbox coordinates are interpreted in the coordinate system given by\n    *bbox_transform*, with the default transform\n    Axes or Figure coordinates, depending on which ``legend`` is called.\n\n    If a 4-tuple or `.BboxBase` is given, then it specifies the bbox\n    ``(x, y, width, height)`` that the legend is placed in.\n    To put the legend in the best location in the bottom right\n    quadrant of the axes (or figure)::\n\n        loc='best', bbox_to_anchor=(0.5, 0., 0.5, 0.5)\n\n    A 2-tuple ``(x, y)`` places the corner of the legend specified by *loc* at\n    x, y.  For example, to put the legend's upper right-hand corner in the\n    center of the axes (or figure) the following keywords can be used::\n\n        loc='upper right', bbox_to_anchor=(0.5, 0.5)\n\nncols : int, default: 1\n    The number of columns that the legend has.\n\n    For backward compatibility, the spelling *ncol* is also supported\n    but it is discouraged. If both are given, *ncols* takes precedence.\n\nprop : None or `~matplotlib.font_manager.FontProperties` or dict\n    The font properties of the legend. If None (default), the current\n    :data:`matplotlib.rcParams` will be used.\n\nfontsize : int or {'xx-small', 'x-small', 'small', 'medium', 'large', 'x-large', 'xx-large'}\n    The font size of the legend. If the value is numeric the size will be the\n    absolute font size in points. String values are relative to the current\n    default font size. This argument is only used if *prop* is not specified.\n\nlabelcolor : str or list, default: :rc:`legend.labelcolor`\n    The color of the text in the legend. Either a valid color string\n    (for example, 'red'), or a list of color strings. The labelcolor can\n    also be made to match the color of the line or marker using 'linecolor',\n    'markerfacecolor' (or 'mfc'), or 'markeredgecolor' (or 'mec').\n\n    Labelcolor can be set globally using :rc:`legend.labelcolor`. If None,\n    use :rc:`text.color`.\n\nnumpoints : int, default: :rc:`legend.numpoints`\n    The number of marker points in the legend when creating a legend\n    entry for a `.Line2D` (line).\n\nscatterpoints : int, default: :rc:`legend.scatterpoints`\n    The number of marker points in the legend when creating\n    a legend entry for a `.PathCollection` (scatter plot).\n\nscatteryoffsets : iterable of floats, default: ``[0.375, 0.5, 0.3125]``\n    The vertical offset (relative to the font size) for the markers\n    created for a scatter plot legend entry. 0.0 is at the base the\n    legend text, and 1.0 is at the top. To draw all markers at the\n    same height, set to ``[0.5]``.\n\nmarkerscale : float, default: :rc:`legend.markerscale`\n    The relative size of legend markers compared to the originally drawn ones.\n\nmarkerfirst : bool, default: True\n    If *True*, legend marker is placed to the left of the legend label.\n    If *False*, legend marker is placed to the right of the legend label.\n\nreverse : bool, default: False\n    If *True*, the legend labels are displayed in reverse order from the input.\n    If *False*, the legend labels are displayed in the same order as the input.\n\n    .. versionadded:: 3.7\n\nframeon : bool, default: :rc:`legend.frameon`\n    Whether the legend should be drawn on a patch (frame).\n\nfancybox : bool, default: :rc:`legend.fancybox`\n    Whether round edges should be enabled around the `.FancyBboxPatch` which\n    makes up the legend's background.\n\nshadow : None, bool or dict, default: :rc:`legend.shadow`\n    Whether to draw a shadow behind the legend.\n    The shadow can be configured using `.Patch` keywords.\n    Customization via :rc:`legend.shadow` is currently not supported.\n\nframealpha : float, default: :rc:`legend.framealpha`\n    The alpha transparency of the legend's background.\n    If *shadow* is activated and *framealpha* is ``None``, the default value is\n    ignored.\n\nfacecolor : \"inherit\" or color, default: :rc:`legend.facecolor`\n    The legend's background color.\n    If ``\"inherit\"``, use :rc:`axes.facecolor`.\n\nedgecolor : \"inherit\" or color, default: :rc:`legend.edgecolor`\n    The legend's background patch edge color.\n    If ``\"inherit\"``, use take :rc:`axes.edgecolor`.\n\nmode : {\"expand\", None}\n    If *mode* is set to ``\"expand\"`` the legend will be horizontally\n    expanded to fill the axes area (or *bbox_to_anchor* if defines\n    the legend's size).\n\nbbox_transform : None or `~matplotlib.transforms.Transform`\n    The transform for the bounding box (*bbox_to_anchor*). For a value\n    of ``None`` (default) the Axes'\n    :data:`~matplotlib.axes.Axes.transAxes` transform will be used.\n\ntitle : str or None\n    The legend's title. Default is no title (``None``).\n\ntitle_fontproperties : None or `~matplotlib.font_manager.FontProperties` or dict\n    The font properties of the legend's title. If None (default), the\n    *title_fontsize* argument will be used if present; if *title_fontsize* is\n    also None, the current :rc:`legend.title_fontsize` will be used.\n\ntitle_fontsize : int or {'xx-small', 'x-small', 'small', 'medium', 'large', 'x-large', 'xx-large'}, default: :rc:`legend.title_fontsize`\n    The font size of the legend's title.\n    Note: This cannot be combined with *title_fontproperties*. If you want\n    to set the fontsize alongside other font properties, use the *size*\n    parameter in *title_fontproperties*.\n\nalignment : {'center', 'left', 'right'}, default: 'center'\n    The alignment of the legend title and the box of entries. The entries\n    are aligned as a single block, so that markers always lined up.\n\nborderpad : float, default: :rc:`legend.borderpad`\n    The fractional whitespace inside the legend border, in font-size units.\n\nlabelspacing : float, default: :rc:`legend.labelspacing`\n    The vertical space between the legend entries, in font-size units.\n\nhandlelength : float, default: :rc:`legend.handlelength`\n    The length of the legend handles, in font-size units.\n\nhandleheight : float, default: :rc:`legend.handleheight`\n    The height of the legend handles, in font-size units.\n\nhandletextpad : float, default: :rc:`legend.handletextpad`\n    The pad between the legend handle and text, in font-size units.\n\nborderaxespad : float, default: :rc:`legend.borderaxespad`\n    The pad between the axes and legend border, in font-size units.\n\ncolumnspacing : float, default: :rc:`legend.columnspacing`\n    The spacing between columns, in font-size units.\n\nhandler_map : dict or None\n    The custom dictionary mapping instances or types to a legend\n    handler. This *handler_map* updates the default handler map\n    found at `matplotlib.legend.Legend.get_legend_handler_map`.\n\ndraggable : bool, default: False\n    Whether the legend can be dragged with the mouse.\n\n\nSee Also\n--------\n.Figure.legend\n\nNotes\n-----\nSome artists are not supported by this function.  See\n:ref:`legend_guide` for details.\n\nExamples\n--------\n.. plot:: gallery/text_labels_and_annotations/legend.py", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/917", "data": {"name": "matplotlib.pyplot.subplots[1].plot", "type": "method", "signature": "(*args, scalex=True, scaley=True, data=None, **kwargs)", "description": "Plot y versus x as lines and/or markers.\n\nCall signatures::\n\n    plot([x], y, [fmt], *, data=None, **kwargs)\n    plot([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n\nThe coordinates of the points or line nodes are given by *x*, *y*.\n\nThe optional parameter *fmt* is a convenient way for defining basic\nformatting like color, marker and linestyle. It's a shortcut string\nnotation described in the *Notes* section below.\n\n>>> plot(x, y)        # plot x and y using default line style and color\n>>> plot(x, y, 'bo')  # plot x and y using blue circle markers\n>>> plot(y)           # plot y using x as index array 0..N-1\n>>> plot(y, 'r+')     # ditto, but with red plusses\n\nYou can use `.Line2D` properties as keyword arguments for more\ncontrol on the appearance. Line properties and *fmt* can be mixed.\nThe following two calls yield identical results:\n\n>>> plot(x, y, 'go--', linewidth=2, markersize=12)\n>>> plot(x, y, color='green', marker='o', linestyle='dashed',\n...      linewidth=2, markersize=12)\n\nWhen conflicting with *fmt*, keyword arguments take precedence.\n\n\n**Plotting labelled data**\n\nThere's a convenient way for plotting objects with labelled data (i.e.\ndata that can be accessed by index ``obj['y']``). Instead of giving\nthe data in *x* and *y*, you can provide the object in the *data*\nparameter and just give the labels for *x* and *y*::\n\n>>> plot('xlabel', 'ylabel', data=obj)\n\nAll indexable objects are supported. This could e.g. be a `dict`, a\n`pandas.DataFrame` or a structured numpy array.\n\n\n**Plotting multiple sets of data**\n\nThere are various ways to plot multiple sets of data.\n\n- The most straight forward way is just to call `plot` multiple times.\n  Example:\n\n  >>> plot(x1, y1, 'bo')\n  >>> plot(x2, y2, 'go')\n\n- If *x* and/or *y* are 2D arrays a separate data set will be drawn\n  for every column. If both *x* and *y* are 2D, they must have the\n  same shape. If only one of them is 2D with shape (N, m) the other\n  must have length N and will be used for every data set m.\n\n  Example:\n\n  >>> x = [1, 2, 3]\n  >>> y = np.array([[1, 2], [3, 4], [5, 6]])\n  >>> plot(x, y)\n\n  is equivalent to:\n\n  >>> for col in range(y.shape[1]):\n  ...     plot(x, y[:, col])\n\n- The third way is to specify multiple sets of *[x]*, *y*, *[fmt]*\n  groups::\n\n  >>> plot(x1, y1, 'g^', x2, y2, 'g-')\n\n  In this case, any additional keyword argument applies to all\n  datasets. Also, this syntax cannot be combined with the *data*\n  parameter.\n\nBy default, each line is assigned a different style specified by a\n'style cycle'. The *fmt* and line property parameters are only\nnecessary if you want explicit deviations from these defaults.\nAlternatively, you can also change the style cycle using\n:rc:`axes.prop_cycle`.\n\n\nParameters\n----------\nx, y : array-like or scalar\n    The horizontal / vertical coordinates of the data points.\n    *x* values are optional and default to ``range(len(y))``.\n\n    Commonly, these parameters are 1D arrays.\n\n    They can also be scalars, or two-dimensional (in that case, the\n    columns represent separate data sets).\n\n    These arguments cannot be passed as keywords.\n\nfmt : str, optional\n    A format string, e.g. 'ro' for red circles. See the *Notes*\n    section for a full description of the format strings.\n\n    Format strings are just an abbreviation for quickly setting\n    basic line properties. All of these and more can also be\n    controlled by keyword arguments.\n\n    This argument cannot be passed as keyword.\n\ndata : indexable object, optional\n    An object with labelled data. If given, provide the label names to\n    plot in *x* and *y*.\n\n    .. note::\n        Technically there's a slight ambiguity in calls where the\n        second label is a valid *fmt*. ``plot('n', 'o', data=obj)``\n        could be ``plt(x, y)`` or ``plt(y, fmt)``. In such cases,\n        the former interpretation is chosen, but a warning is issued.\n        You may suppress the warning by adding an empty format string\n        ``plot('n', 'o', '', data=obj)``.\n\nReturns\n-------\nlist of `.Line2D`\n    A list of lines representing the plotted data.\n\nOther Parameters\n----------------\nscalex, scaley : bool, default: True\n    These parameters determine if the view limits are adapted to the\n    data limits. The values are passed on to\n    `~.axes.Axes.autoscale_view`.\n\n**kwargs : `~matplotlib.lines.Line2D` properties, optional\n    *kwargs* are used to specify properties like a line label (for\n    auto legends), linewidth, antialiasing, marker face color.\n    Example::\n\n    >>> plot([1, 2, 3], [1, 2, 3], 'go-', label='line 1', linewidth=2)\n    >>> plot([1, 2, 3], [1, 4, 9], 'rs', label='line 2')\n\n    If you specify multiple lines with one plot call, the kwargs apply\n    to all those lines. In case the label object is iterable, each\n    element is used as labels for each set of data.\n\n    Here is a list of available `.Line2D` properties:\n\n    Properties:\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: scalar or None\n    animated: bool\n    antialiased or aa: bool\n    clip_box: `~matplotlib.transforms.BboxBase` or None\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    color or c: color\n    dash_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    dash_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    dashes: sequence of floats (on/off ink in points) or (None, None)\n    data: (2, N) array or two 1D arrays\n    drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n    figure: `~matplotlib.figure.Figure`\n    fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n    gapcolor: color or None\n    gid: str\n    in_layout: bool\n    label: object\n    linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n    linewidth or lw: float\n    marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n    markeredgecolor or mec: color\n    markeredgewidth or mew: float\n    markerfacecolor or mfc: color\n    markerfacecoloralt or mfcalt: color\n    markersize or ms: float\n    markevery: None or int or (int, int) or slice or list[int] or float or (float, float) or list[bool]\n    mouseover: bool\n    path_effects: list of `.AbstractPathEffect`\n    picker: float or callable[[Artist, Event], tuple[bool, dict]]\n    pickradius: float\n    rasterized: bool\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    solid_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    solid_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    transform: unknown\n    url: str\n    visible: bool\n    xdata: 1D array\n    ydata: 1D array\n    zorder: float\n\nSee Also\n--------\nscatter : XY scatter plot with markers of varying size and/or color (\n    sometimes also called bubble chart).\n\nNotes\n-----\n**Format Strings**\n\nA format string consists of a part for color, marker and line::\n\n    fmt = '[marker][line][color]'\n\nEach of them is optional. If not provided, the value from the style\ncycle is used. Exception: If ``line`` is given, but no ``marker``,\nthe data will be a line without markers.\n\nOther combinations such as ``[color][marker][line]`` are also\nsupported, but note that their parsing may be ambiguous.\n\n**Markers**\n\n=============   ===============================\ncharacter       description\n=============   ===============================\n``'.'``         point marker\n``','``         pixel marker\n``'o'``         circle marker\n``'v'``         triangle_down marker\n``'^'``         triangle_up marker\n``'<'``         triangle_left marker\n``'>'``         triangle_right marker\n``'1'``         tri_down marker\n``'2'``         tri_up marker\n``'3'``         tri_left marker\n``'4'``         tri_right marker\n``'8'``         octagon marker\n``'s'``         square marker\n``'p'``         pentagon marker\n``'P'``         plus (filled) marker\n``'*'``         star marker\n``'h'``         hexagon1 marker\n``'H'``         hexagon2 marker\n``'+'``         plus marker\n``'x'``         x marker\n``'X'``         x (filled) marker\n``'D'``         diamond marker\n``'d'``         thin_diamond marker\n``'|'``         vline marker\n``'_'``         hline marker\n=============   ===============================\n\n**Line Styles**\n\n=============    ===============================\ncharacter        description\n=============    ===============================\n``'-'``          solid line style\n``'--'``         dashed line style\n``'-.'``         dash-dot line style\n``':'``          dotted line style\n=============    ===============================\n\nExample format strings::\n\n    'b'    # blue markers with default shape\n    'or'   # red circles\n    '-g'   # green solid line\n    '--'   # dashed line with default color\n    '^k:'  # black triangle_up markers connected by a dotted line\n\n**Colors**\n\nThe supported color abbreviations are the single letter codes\n\n=============    ===============================\ncharacter        color\n=============    ===============================\n``'b'``          blue\n``'g'``          green\n``'r'``          red\n``'c'``          cyan\n``'m'``          magenta\n``'y'``          yellow\n``'k'``          black\n``'w'``          white\n=============    ===============================\n\nand the ``'CN'`` colors that index into the default property cycle.\n\nIf the color is the only part of the format string, you can\nadditionally use any  `matplotlib.colors` spec, e.g. full names\n(``'green'``) or hex strings (``'#008000'``).", "parameters": {"type": "object", "properties": {"scalex": {"type": "bool", "default": true}, "scaley": {"type": "bool", "default": true}, "data": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/917", "data": {"name": "model_fit.forecast(steps=7).tolist()"}}
{"task_id": "BigCodeBench/917", "data": {"name": "pandas.date_range", "type": "callable", "signature": "(start=None, periods=None, **kwargs) -> 'DatetimeIndex)", "description": "Return a fixed frequency DatetimeIndex.\n\nReturns the range of equally spaced time points (where the difference between any\ntwo adjacent points is specified by the given frequency) such that they all\nsatisfy `start <[=] x <[=] end`, where the first one and the last one are, resp.,\nthe first and last time points in that range that fall on the boundary of ``freq``\n(if given as a frequency string) or that are valid for ``freq`` (if given as a\n:class:`pandas.tseries.offsets.DateOffset`). (If exactly one of ``start``,\n``end``, or ``freq`` is *not* specified, this missing parameter can be computed\ngiven ``periods``, the number of timesteps in the range. See the note below.)\n\nParameters\n----------\nstart : str or datetime-like, optional\n    Left bound for generating dates.\nend : str or datetime-like, optional\n    Right bound for generating dates.\nperiods : int, optional\n    Number of periods to generate.\nfreq : str, Timedelta, datetime.timedelta, or DateOffset, default 'D'\n    Frequency strings can have multiples, e.g. '5h'. See\n    :ref:`here <timeseries.offset_aliases>` for a list of\n    frequency aliases.\ntz : str or tzinfo, optional\n    Time zone name for returning localized DatetimeIndex, for example\n    'Asia/Hong_Kong'. By default, the resulting DatetimeIndex is\n    timezone-naive unless timezone-aware datetime-likes are passed.\nnormalize : bool, default False\n    Normalize start/end dates to midnight before generating date range.\nname : str, default None\n    Name of the resulting DatetimeIndex.\ninclusive : {\"both\", \"neither\", \"left\", \"right\"}, default \"both\"\n    Include boundaries; Whether to set each bound as closed or open.\n\n    .. versionadded:: 1.4.0\nunit : str, default None\n    Specify the desired resolution of the result.\n\n    .. versionadded:: 2.0.0\n**kwargs\n    For compatibility. Has no effect on the result.\n\nReturns\n-------\nDatetimeIndex\n\nSee Also\n--------\nDatetimeIndex : An immutable container for datetimes.\ntimedelta_range : Return a fixed frequency TimedeltaIndex.\nperiod_range : Return a fixed frequency PeriodIndex.\ninterval_range : Return a fixed frequency IntervalIndex.\n\nNotes\n-----\nOf the four parameters ``start``, ``end``, ``periods``, and ``freq``,\nexactly three must be specified. If ``freq`` is omitted, the resulting\n``DatetimeIndex`` will have ``periods`` linearly spaced elements between\n``start`` and ``end`` (closed on both sides).\n\nTo learn more about the frequency strings, please see `this link\n<https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases>`__.\n\nExamples\n--------\n**Specifying the values**\n\nThe next four examples generate the same `DatetimeIndex`, but vary\nthe combination of `start`, `end` and `periods`.\n\nSpecify `start` and `end`, with the default daily frequency.\n\n>>> pd.date_range(start='1/1/2018', end='1/08/2018')\nDatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03', '2018-01-04',\n               '2018-01-05', '2018-01-06', '2018-01-07', '2018-01-08'],\n              dtype='datetime64[ns]', freq='D')\n\nSpecify timezone-aware `start` and `end`, with the default daily frequency.\n\n>>> pd.date_range(\n...     start=pd.to_datetime(\"1/1/2018\").tz_localize(\"Europe/Berlin\"),\n...     end=pd.to_datetime(\"1/08/2018\").tz_localize(\"Europe/Berlin\"),\n... )\nDatetimeIndex(['2018-01-01 00:00:00+01:00', '2018-01-02 00:00:00+01:00',\n               '2018-01-03 00:00:00+01:00', '2018-01-04 00:00:00+01:00',\n               '2018-01-05 00:00:00+01:00', '2018-01-06 00:00:00+01:00',\n               '2018-01-07 00:00:00+01:00', '2018-01-08 00:00:00+01:00'],\n              dtype='datetime64[ns, Europe/Berlin]', freq='D')\n\nSpecify `start` and `periods`, the number of periods (days).\n\n>>> pd.date_range(start='1/1/2018', periods=8)\nDatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03', '2018-01-04',\n               '2018-01-05', '2018-01-06', '2018-01-07', '2018-01-08'],\n              dtype='datetime64[ns]', freq='D')\n\nSpecify `end` and `periods`, the number of periods (days).\n\n>>> pd.date_range(end='1/1/2018', periods=8)\nDatetimeIndex(['2017-12-25', '2017-12-26', '2017-12-27', '2017-12-28',\n               '2017-12-29', '2017-12-30', '2017-12-31', '2018-01-01'],\n              dtype='datetime64[ns]', freq='D')\n\nSpecify `start`, `end`, and `periods`; the frequency is generated\nautomatically (linearly spaced).\n\n>>> pd.date_range(start='2018-04-24', end='2018-04-27', periods=3)\nDatetimeIndex(['2018-04-24 00:00:00', '2018-04-25 12:00:00',\n               '2018-04-27 00:00:00'],\n              dtype='datetime64[ns]', freq=None)\n\n**Other Parameters**\n\nChanged the `freq` (frequency) to ``'ME'`` (month end frequency).\n\n>>> pd.date_range(start='1/1/2018', periods=5, freq='ME')\nDatetimeIndex(['2018-01-31', '2018-02-28', '2018-03-31', '2018-04-30',\n               '2018-05-31'],\n              dtype='datetime64[ns]', freq='ME')\n\nMultiples are allowed\n\n>>> pd.date_range(start='1/1/2018', periods=5, freq='3ME')\nDatetimeIndex(['2018-01-31', '2018-04-30', '2018-07-31', '2018-10-31',\n               '2019-01-31'],\n              dtype='datetime64[ns]', freq='3ME')\n\n`freq` can also be specified as an Offset object.\n\n>>> pd.date_range(start='1/1/2018', periods=5, freq=pd.offsets.MonthEnd(3))\nDatetimeIndex(['2018-01-31', '2018-04-30', '2018-07-31', '2018-10-31',\n               '2019-01-31'],\n              dtype='datetime64[ns]', freq='3ME')\n\nSpecify `tz` to set the timezone.\n\n>>> pd.date_range(start='1/1/2018', periods=5, tz='Asia/Tokyo')\nDatetimeIndex(['2018-01-01 00:00:00+09:00', '2018-01-02 00:00:00+09:00',\n               '2018-01-03 00:00:00+09:00', '2018-01-04 00:00:00+09:00',\n               '2018-01-05 00:00:00+09:00'],\n              dtype='datetime64[ns, Asia/Tokyo]', freq='D')\n\n`inclusive` controls whether to include `start` and `end` that are on the\nboundary. The default, \"both\", includes boundary points on either end.\n\n>>> pd.date_range(start='2017-01-01', end='2017-01-04', inclusive=\"both\")\nDatetimeIndex(['2017-01-01', '2017-01-02', '2017-01-03', '2017-01-04'],\n              dtype='datetime64[ns]', freq='D')\n\nUse ``inclusive='left'`` to exclude `end` if it falls on the boundary.\n\n>>> pd.date_range(start='2017-01-01', end='2017-01-04', inclusive='left')\nDatetimeIndex(['2017-01-01', '2017-01-02', '2017-01-03'],\n              dtype='datetime64[ns]', freq='D')\n\nUse ``inclusive='right'`` to exclude `start` if it falls on the boundary, and\nsimilarly ``inclusive='neither'`` will exclude both `start` and `end`.\n\n>>> pd.date_range(start='2017-01-01', end='2017-01-04', inclusive='right')\nDatetimeIndex(['2017-01-02', '2017-01-03', '2017-01-04'],\n              dtype='datetime64[ns]', freq='D')\n\n**Specify a unit**\n\n>>> pd.date_range(start=\"2017-01-01\", periods=10, freq=\"100YS\", unit=\"s\")\nDatetimeIndex(['2017-01-01', '2117-01-01', '2217-01-01', '2317-01-01',\n               '2417-01-01', '2517-01-01', '2617-01-01', '2717-01-01',\n               '2817-01-01', '2917-01-01'],\n              dtype='datetime64[s]', freq='100YS-JAN')", "parameters": {"type": "object", "properties": {"start": {"type": "NoneType", "default": null}, "periods": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/917", "data": {"name": "statsmodels.tsa.arima.model.ARIMA", "type": "class", "signature": "(endog, order=(0)", "description": "Autoregressive Integrated Moving Average (ARIMA) model, and extensions\n\nThis model is the basic interface for ARIMA-type models, including those\nwith exogenous regressors and those with seasonal components. The most\ngeneral form of the model is SARIMAX(p, d, q)x(P, D, Q, s). It also allows\nall specialized cases, including\n\n- autoregressive models: AR(p)\n- moving average models: MA(q)\n- mixed autoregressive moving average models: ARMA(p, q)\n- integration models: ARIMA(p, d, q)\n- seasonal models: SARIMA(P, D, Q, s)\n- regression with errors that follow one of the above ARIMA-type models\n\nParameters\n----------\nendog : array_like, optional\n    The observed time-series process :math:`y`.\nexog : array_like, optional\n    Array of exogenous regressors.\norder : tuple, optional\n    The (p,d,q) order of the model for the autoregressive, differences, and\n    moving average components. d is always an integer, while p and q may\n    either be integers or lists of integers.\nseasonal_order : tuple, optional\n    The (P,D,Q,s) order of the seasonal component of the model for the\n    AR parameters, differences, MA parameters, and periodicity. Default\n    is (0, 0, 0, 0). D and s are always integers, while P and Q\n    may either be integers or lists of positive integers.\ntrend : str{'n','c','t','ct'} or iterable, optional\n    Parameter controlling the deterministic trend. Can be specified as a\n    string where 'c' indicates a constant term, 't' indicates a\n    linear trend in time, and 'ct' includes both. Can also be specified as\n    an iterable defining a polynomial, as in `numpy.poly1d`, where\n    `[1,1,0,1]` would denote :math:`a + bt + ct^3`. Default is 'c' for\n    models without integration, and no trend for models with integration.\n    Note that all trend terms are included in the model as exogenous\n    regressors, which differs from how trends are included in ``SARIMAX``\n    models.  See the Notes section for a precise definition of the\n    treatment of trend terms.\nenforce_stationarity : bool, optional\n    Whether or not to require the autoregressive parameters to correspond\n    to a stationarity process.\nenforce_invertibility : bool, optional\n    Whether or not to require the moving average parameters to correspond\n    to an invertible process.\nconcentrate_scale : bool, optional\n    Whether or not to concentrate the scale (variance of the error term)\n    out of the likelihood. This reduces the number of parameters by one.\n    This is only applicable when considering estimation by numerical\n    maximum likelihood.\ntrend_offset : int, optional\n    The offset at which to start time trend values. Default is 1, so that\n    if `trend='t'` the trend is equal to 1, 2, ..., nobs. Typically is only\n    set when the model created by extending a previous dataset.\ndates : array_like of datetime, optional\n    If no index is given by `endog` or `exog`, an array-like object of\n    datetime objects can be provided.\nfreq : str, optional\n    If no index is given by `endog` or `exog`, the frequency of the\n    time-series may be specified here as a Pandas offset or offset string.\nmissing : str\n    Available options are 'none', 'drop', and 'raise'. If 'none', no nan\n    checking is done. If 'drop', any observations with nans are dropped.\n    If 'raise', an error is raised. Default is 'none'.\n\nNotes\n-----\nThis model incorporates both exogenous regressors and trend components\nthrough \"regression with ARIMA errors\". This differs from the\nspecification estimated using ``SARIMAX`` which treats the trend\ncomponents separately from any included exogenous regressors. The full\nspecification of the model estimated here is:\n\n.. math::\n\n    Y_{t}-\\delta_{0}-\\delta_{1}t-\\ldots-\\delta_{k}t^{k}-X_{t}\\beta\n        & =\\epsilon_{t} \\\\\n    \\left(1-L\\right)^{d}\\left(1-L^{s}\\right)^{D}\\Phi\\left(L\\right)\n    \\Phi_{s}\\left(L\\right)\\epsilon_{t}\n        & =\\Theta\\left(L\\right)\\Theta_{s}\\left(L\\right)\\eta_{t}\n\nwhere :math:`\\eta_t \\sim WN(0,\\sigma^2)` is a white noise process, L\nis the lag operator, and :math:`G(L)` are lag polynomials corresponding\nto the autoregressive (:math:`\\Phi`), seasonal autoregressive\n(:math:`\\Phi_s`), moving average (:math:`\\Theta`), and seasonal moving\naverage components (:math:`\\Theta_s`).\n\n`enforce_stationarity` and `enforce_invertibility` are specified in the\nconstructor because they affect loglikelihood computations, and so should\nnot be changed on the fly. This is why they are not instead included as\narguments to the `fit` method.\n\nSee the notebook `ARMA: Sunspots Data\n<../examples/notebooks/generated/tsa_arma_0.html>`__ and\n`ARMA: Artificial Data <../examples/notebooks/generated/tsa_arma_1.html>`__\nfor an overview.\n\n.. todo:: should concentrate_scale=True by default\n\nExamples\n--------\n>>> mod = sm.tsa.arima.ARIMA(endog, order=(1, 0, 0))\n>>> res = mod.fit()\n>>> print(res.summary())", "parameters": {"type": "object", "properties": {"endog": {}, "order": {"type": "str", "default": "(0"}}}}}
{"task_id": "BigCodeBench/917", "data": {"name": "statsmodels.tsa.arima.model.ARIMA.fit", "type": "callable", "signature": "(self)", "description": "Fit (estimate) the parameters of the model.\n\nParameters\n----------\nstart_params : array_like, optional\n    Initial guess of the solution for the loglikelihood maximization.\n    If None, the default is given by Model.start_params.\ntransformed : bool, optional\n    Whether or not `start_params` is already transformed. Default is\n    True.\nincludes_fixed : bool, optional\n    If parameters were previously fixed with the `fix_params` method,\n    this argument describes whether or not `start_params` also includes\n    the fixed parameters, in addition to the free parameters. Default\n    is False.\nmethod : str, optional\n    The method used for estimating the parameters of the model. Valid\n    options include 'statespace', 'innovations_mle', 'hannan_rissanen',\n    'burg', 'innovations', and 'yule_walker'. Not all options are\n    available for every specification (for example 'yule_walker' can\n    only be used with AR(p) models).\nmethod_kwargs : dict, optional\n    Arguments to pass to the fit function for the parameter estimator\n    described by the `method` argument.\ngls : bool, optional\n    Whether or not to use generalized least squares (GLS) to estimate\n    regression effects. The default is False if `method='statespace'`\n    and is True otherwise.\ngls_kwargs : dict, optional\n    Arguments to pass to the GLS estimation fit method. Only applicable\n    if GLS estimation is used (see `gls` argument for details).\ncov_type : str, optional\n    The `cov_type` keyword governs the method for calculating the\n    covariance matrix of parameter estimates. Can be one of:\n\n    - 'opg' for the outer product of gradient estimator\n    - 'oim' for the observed information matrix estimator, calculated\n      using the method of Harvey (1989)\n    - 'approx' for the observed information matrix estimator,\n      calculated using a numerical approximation of the Hessian matrix.\n    - 'robust' for an approximate (quasi-maximum likelihood) covariance\n      matrix that may be valid even in the presence of some\n      misspecifications. Intermediate calculations use the 'oim'\n      method.\n    - 'robust_approx' is the same as 'robust' except that the\n      intermediate calculations use the 'approx' method.\n    - 'none' for no covariance matrix calculation.\n\n    Default is 'opg' unless memory conservation is used to avoid\n    computing the loglikelihood values for each observation, in which\n    case the default is 'oim'.\ncov_kwds : dict or None, optional\n    A dictionary of arguments affecting covariance matrix computation.\n\n    **opg, oim, approx, robust, robust_approx**\n\n    - 'approx_complex_step' : bool, optional - If True, numerical\n      approximations are computed using complex-step methods. If False,\n      numerical approximations are computed using finite difference\n      methods. Default is True.\n    - 'approx_centered' : bool, optional - If True, numerical\n      approximations computed using finite difference methods use a\n      centered approximation. Default is False.\nreturn_params : bool, optional\n    Whether or not to return only the array of maximizing parameters.\n    Default is False.\nlow_memory : bool, optional\n    If set to True, techniques are applied to substantially reduce\n    memory usage. If used, some features of the results object will\n    not be available (including smoothed results and in-sample\n    prediction), although out-of-sample forecasting is possible.\n    Default is False.\n\nReturns\n-------\nARIMAResults\n\nExamples\n--------\n>>> mod = sm.tsa.arima.ARIMA(endog, order=(1, 0, 0))\n>>> res = mod.fit()\n>>> print(res.summary())", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/917", "data": {"name": "typing.List", "type": "callable", "signature": "(*args, **kwargs)", "description": "A generic version of list.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/917", "data": {"name": "typing.Tuple", "type": "callable", "signature": "(*args, **kwargs)", "description": "Tuple type; Tuple[X, Y] is the cross-product type of X and Y.\n\nExample: Tuple[T1, T2] is a tuple of two elements corresponding\nto type variables T1 and T2.  Tuple[int, float, str] is a tuple\nof an int, a float and a string.\n\nTo specify a variable-length tuple of homogeneous type, use Tuple[T, ...].", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/928", "data": {"name": "collections.Counter", "type": "class", "signature": "(iterable=None, /, **kwds)", "description": "Dict subclass for counting hashable items.  Sometimes called a bag\nor multiset.  Elements are stored as dictionary keys and their counts\nare stored as dictionary values.\n\n>>> c = Counter('abcdeabcdabcaba')  # count elements from a string\n\n>>> c.most_common(3)                # three most common elements\n[('a', 5), ('b', 4), ('c', 3)]\n>>> sorted(c)                       # list all unique elements\n['a', 'b', 'c', 'd', 'e']\n>>> ''.join(sorted(c.elements()))   # list elements with repetitions\n'aaaaabbbbcccdde'\n>>> sum(c.values())                 # total of all counts\n15\n\n>>> c['a']                          # count of letter 'a'\n5\n>>> for elem in 'shazam':           # update counts from an iterable\n...     c[elem] += 1                # by adding 1 to each element's count\n>>> c['a']                          # now there are seven 'a'\n7\n>>> del c['b']                      # remove all 'b'\n>>> c['b']                          # now there are zero 'b'\n0\n\n>>> d = Counter('simsalabim')       # make another counter\n>>> c.update(d)                     # add in the second counter\n>>> c['a']                          # now there are nine 'a'\n9\n\n>>> c.clear()                       # empty the counter\n>>> c\nCounter()\n\nNote:  If a count is set to zero or reduced to zero, it will remain\nin the counter until the entry is deleted or the counter is cleared:\n\n>>> c = Counter('aaabbc')\n>>> c['b'] -= 2                     # reduce the count of 'b' by two\n>>> c.most_common()                 # 'b' is still in, but its count is zero\n[('a', 3), ('c', 1), ('b', 0)]", "parameters": {"type": "object", "properties": {"iterable": {"type": "NoneType", "default": null}, "/": {}}}}}
{"task_id": "BigCodeBench/928", "data": {"name": "collections.Counter.get", "type": "callable", "signature": "(self, key, default=None)", "description": "Return the value for key if key is in the dictionary, else default.", "parameters": {"type": "object", "properties": {"key": {}, "default": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/928", "data": {"name": "string.ascii_lowercase", "type": "constant", "signature": null, "description": "str(object='') -> str\nstr(bytes_or_buffer[, encoding[, errors]]) -> str\n\nCreate a new string object from the given object. If encoding or\nerrors is specified, then the object must expose a data buffer\nthat will be decoded using the given encoding and error handler.\nOtherwise, returns the result of object.__str__() (if defined)\nor repr(object).\nencoding defaults to sys.getdefaultencoding().\nerrors defaults to 'strict'.", "value": "abcdefghijklmnopqrstuvwxyz", "parameters": {"type": "constant"}}}
{"task_id": "BigCodeBench/942", "data": {"name": "pandas.DataFrame", "type": "class", "signature": "(data=None, index: 'Axes | None' = None, columns: 'Axes | None' = None, dtype: 'Dtype | None' = None, copy: 'bool | None' = None) -> 'None'", "description": "Two-dimensional, size-mutable, potentially heterogeneous tabular data.\n\nData structure also contains labeled axes (rows and columns).\nArithmetic operations align on both row and column labels. Can be\nthought of as a dict-like container for Series objects. The primary\npandas data structure.\n\nParameters\n----------\ndata : ndarray (structured or homogeneous), Iterable, dict, or DataFrame\n    Dict can contain Series, arrays, constants, dataclass or list-like objects. If\n    data is a dict, column order follows insertion-order. If a dict contains Series\n    which have an index defined, it is aligned by its index. This alignment also\n    occurs if data is a Series or a DataFrame itself. Alignment is done on\n    Series/DataFrame inputs.\n\n    If data is a list of dicts, column order follows insertion-order.\n\nindex : Index or array-like\n    Index to use for resulting frame. Will default to RangeIndex if\n    no indexing information part of input data and no index provided.\ncolumns : Index or array-like\n    Column labels to use for resulting frame when data does not have them,\n    defaulting to RangeIndex(0, 1, 2, ..., n). If data contains column labels,\n    will perform column selection instead.\ndtype : dtype, default None\n    Data type to force. Only a single dtype is allowed. If None, infer.\ncopy : bool or None, default None\n    Copy data from inputs.\n    For dict data, the default of None behaves like ``copy=True``.  For DataFrame\n    or 2d ndarray input, the default of None behaves like ``copy=False``.\n    If data is a dict containing one or more Series (possibly of different dtypes),\n    ``copy=False`` will ensure that these inputs are not copied.\n\n    .. versionchanged:: 1.3.0\n\nSee Also\n--------\nDataFrame.from_records : Constructor from tuples, also record arrays.\nDataFrame.from_dict : From dicts of Series, arrays, or dicts.\nread_csv : Read a comma-separated values (csv) file into DataFrame.\nread_table : Read general delimited file into DataFrame.\nread_clipboard : Read text from clipboard into DataFrame.\n\nNotes\n-----\nPlease reference the :ref:`User Guide <basics.dataframe>` for more information.\n\nExamples\n--------\nConstructing DataFrame from a dictionary.\n\n>>> d = {'col1': [1, 2], 'col2': [3, 4]}\n>>> df = pd.DataFrame(data=d)\n>>> df\n   col1  col2\n0     1     3\n1     2     4\n\nNotice that the inferred dtype is int64.\n\n>>> df.dtypes\ncol1    int64\ncol2    int64\ndtype: object\n\nTo enforce a single dtype:\n\n>>> df = pd.DataFrame(data=d, dtype=np.int8)\n>>> df.dtypes\ncol1    int8\ncol2    int8\ndtype: object\n\nConstructing DataFrame from a dictionary including Series:\n\n>>> d = {'col1': [0, 1, 2, 3], 'col2': pd.Series([2, 3], index=[2, 3])}\n>>> pd.DataFrame(data=d, index=[0, 1, 2, 3])\n   col1  col2\n0     0   NaN\n1     1   NaN\n2     2   2.0\n3     3   3.0\n\nConstructing DataFrame from numpy ndarray:\n\n>>> df2 = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),\n...                    columns=['a', 'b', 'c'])\n>>> df2\n   a  b  c\n0  1  2  3\n1  4  5  6\n2  7  8  9\n\nConstructing DataFrame from a numpy ndarray that has labeled columns:\n\n>>> data = np.array([(1, 2, 3), (4, 5, 6), (7, 8, 9)],\n...                 dtype=[(\"a\", \"i4\"), (\"b\", \"i4\"), (\"c\", \"i4\")])\n>>> df3 = pd.DataFrame(data, columns=['c', 'a'])\n...\n>>> df3\n   c  a\n0  3  1\n1  6  4\n2  9  7\n\nConstructing DataFrame from dataclass:\n\n>>> from dataclasses import make_dataclass\n>>> Point = make_dataclass(\"Point\", [(\"x\", int), (\"y\", int)])\n>>> pd.DataFrame([Point(0, 0), Point(0, 3), Point(2, 3)])\n   x  y\n0  0  0\n1  0  3\n2  2  3\n\nConstructing DataFrame from Series/DataFrame:\n\n>>> ser = pd.Series([1, 2, 3], index=[\"a\", \"b\", \"c\"])\n>>> df = pd.DataFrame(data=ser, index=[\"a\", \"c\"])\n>>> df\n   0\na  1\nc  3\n\n>>> df1 = pd.DataFrame([1, 2, 3], index=[\"a\", \"b\", \"c\"], columns=[\"x\"])\n>>> df2 = pd.DataFrame(data=df1, index=[\"a\", \"c\"])\n>>> df2\n   x\na  1\nc  3", "parameters": {"type": "object", "properties": {"data": {"type": "NoneType", "default": null}, "index": {"type": ["axes", "null"], "default": null}, "columns": {"type": ["axes", "null"], "default": null}, "dtype": {"type": ["dtype", "null"], "default": null}, "copy": {"type": ["bool", "null"], "default": null}}}}}
{"task_id": "BigCodeBench/942", "data": {"name": "pandas.DataFrame.pivot", "type": "callable", "signature": "(self, columns, index=<no_default>, values=<no_default>) -> 'DataFrame)", "description": "Return reshaped DataFrame organized by given index / column values.\n\nReshape data (produce a \"pivot\" table) based on column values. Uses\nunique values from specified `index` / `columns` to form axes of the\nresulting DataFrame. This function does not support data\naggregation, multiple values will result in a MultiIndex in the\ncolumns. See the :ref:`User Guide <reshaping>` for more on reshaping.\n\nParameters\n----------\ncolumns : str or object or a list of str\n    Column to use to make new frame's columns.\nindex : str or object or a list of str, optional\n    Column to use to make new frame's index. If not given, uses existing index.\nvalues : str, object or a list of the previous, optional\n    Column(s) to use for populating new frame's values. If not\n    specified, all remaining columns will be used and the result will\n    have hierarchically indexed columns.\n\nReturns\n-------\nDataFrame\n    Returns reshaped DataFrame.\n\nRaises\n------\nValueError:\n    When there are any `index`, `columns` combinations with multiple\n    values. `DataFrame.pivot_table` when you need to aggregate.\n\nSee Also\n--------\nDataFrame.pivot_table : Generalization of pivot that can handle\n    duplicate values for one index/column pair.\nDataFrame.unstack : Pivot based on the index values instead of a\n    column.\nwide_to_long : Wide panel to long format. Less flexible but more\n    user-friendly than melt.\n\nNotes\n-----\nFor finer-tuned control, see hierarchical indexing documentation along\nwith the related stack/unstack methods.\n\nReference :ref:`the user guide <reshaping.pivot>` for more examples.\n\nExamples\n--------\n>>> df = pd.DataFrame({'foo': ['one', 'one', 'one', 'two', 'two',\n...                            'two'],\n...                    'bar': ['A', 'B', 'C', 'A', 'B', 'C'],\n...                    'baz': [1, 2, 3, 4, 5, 6],\n...                    'zoo': ['x', 'y', 'z', 'q', 'w', 't']})\n>>> df\n    foo   bar  baz  zoo\n0   one   A    1    x\n1   one   B    2    y\n2   one   C    3    z\n3   two   A    4    q\n4   two   B    5    w\n5   two   C    6    t\n\n>>> df.pivot(index='foo', columns='bar', values='baz')\nbar  A   B   C\nfoo\none  1   2   3\ntwo  4   5   6\n\n>>> df.pivot(index='foo', columns='bar')['baz']\nbar  A   B   C\nfoo\none  1   2   3\ntwo  4   5   6\n\n>>> df.pivot(index='foo', columns='bar', values=['baz', 'zoo'])\n      baz       zoo\nbar   A  B  C   A  B  C\nfoo\none   1  2  3   x  y  z\ntwo   4  5  6   q  w  t\n\nYou could also assign a list of column names or a list of index names.\n\n>>> df = pd.DataFrame({\n...        \"lev1\": [1, 1, 1, 2, 2, 2],\n...        \"lev2\": [1, 1, 2, 1, 1, 2],\n...        \"lev3\": [1, 2, 1, 2, 1, 2],\n...        \"lev4\": [1, 2, 3, 4, 5, 6],\n...        \"values\": [0, 1, 2, 3, 4, 5]})\n>>> df\n    lev1 lev2 lev3 lev4 values\n0   1    1    1    1    0\n1   1    1    2    2    1\n2   1    2    1    3    2\n3   2    1    2    4    3\n4   2    1    1    5    4\n5   2    2    2    6    5\n\n>>> df.pivot(index=\"lev1\", columns=[\"lev2\", \"lev3\"], values=\"values\")\nlev2    1         2\nlev3    1    2    1    2\nlev1\n1     0.0  1.0  2.0  NaN\n2     4.0  3.0  NaN  5.0\n\n>>> df.pivot(index=[\"lev1\", \"lev2\"], columns=[\"lev3\"], values=\"values\")\n      lev3    1    2\nlev1  lev2\n   1     1  0.0  1.0\n         2  2.0  NaN\n   2     1  4.0  3.0\n         2  NaN  5.0\n\nA ValueError is raised if there are any duplicates.\n\n>>> df = pd.DataFrame({\"foo\": ['one', 'one', 'two', 'two'],\n...                    \"bar\": ['A', 'A', 'B', 'C'],\n...                    \"baz\": [1, 2, 3, 4]})\n>>> df\n   foo bar  baz\n0  one   A    1\n1  one   A    2\n2  two   B    3\n3  two   C    4\n\nNotice that the first two rows are the same for our `index`\nand `columns` arguments.\n\n>>> df.pivot(index='foo', columns='bar', values='baz')\nTraceback (most recent call last):\n   ...\nValueError: Index contains duplicate entries, cannot reshape", "parameters": {"type": "object", "properties": {"columns": {}, "index": {"type": "str", "default": "<no_default>"}, "values": {"type": "str", "default": "<no_default>"}}}}}
{"task_id": "BigCodeBench/942", "data": {"name": "matplotlib.pyplot.subplots", "type": "class", "signature": "(**fig_kw) -> 'tuple[Figure)", "description": "Create a figure and a set of subplots.\n\nThis utility wrapper makes it convenient to create common layouts of\nsubplots, including the enclosing figure object, in a single call.\n\nParameters\n----------\nnrows, ncols : int, default: 1\n    Number of rows/columns of the subplot grid.\n\nsharex, sharey : bool or {'none', 'all', 'row', 'col'}, default: False\n    Controls sharing of properties among x (*sharex*) or y (*sharey*)\n    axes:\n\n    - True or 'all': x- or y-axis will be shared among all subplots.\n    - False or 'none': each subplot x- or y-axis will be independent.\n    - 'row': each subplot row will share an x- or y-axis.\n    - 'col': each subplot column will share an x- or y-axis.\n\n    When subplots have a shared x-axis along a column, only the x tick\n    labels of the bottom subplot are created. Similarly, when subplots\n    have a shared y-axis along a row, only the y tick labels of the first\n    column subplot are created. To later turn other subplots' ticklabels\n    on, use `~matplotlib.axes.Axes.tick_params`.\n\n    When subplots have a shared axis that has units, calling\n    `~matplotlib.axis.Axis.set_units` will update each axis with the\n    new units.\n\nsqueeze : bool, default: True\n    - If True, extra dimensions are squeezed out from the returned\n      array of `~matplotlib.axes.Axes`:\n\n      - if only one subplot is constructed (nrows=ncols=1), the\n        resulting single Axes object is returned as a scalar.\n      - for Nx1 or 1xM subplots, the returned object is a 1D numpy\n        object array of Axes objects.\n      - for NxM, subplots with N>1 and M>1 are returned as a 2D array.\n\n    - If False, no squeezing at all is done: the returned Axes object is\n      always a 2D array containing Axes instances, even if it ends up\n      being 1x1.\n\nwidth_ratios : array-like of length *ncols*, optional\n    Defines the relative widths of the columns. Each column gets a\n    relative width of ``width_ratios[i] / sum(width_ratios)``.\n    If not given, all columns will have the same width.  Equivalent\n    to ``gridspec_kw={'width_ratios': [...]}``.\n\nheight_ratios : array-like of length *nrows*, optional\n    Defines the relative heights of the rows. Each row gets a\n    relative height of ``height_ratios[i] / sum(height_ratios)``.\n    If not given, all rows will have the same height. Convenience\n    for ``gridspec_kw={'height_ratios': [...]}``.\n\nsubplot_kw : dict, optional\n    Dict with keywords passed to the\n    `~matplotlib.figure.Figure.add_subplot` call used to create each\n    subplot.\n\ngridspec_kw : dict, optional\n    Dict with keywords passed to the `~matplotlib.gridspec.GridSpec`\n    constructor used to create the grid the subplots are placed on.\n\n**fig_kw\n    All additional keyword arguments are passed to the\n    `.pyplot.figure` call.\n\nReturns\n-------\nfig : `.Figure`\n\nax : `~matplotlib.axes.Axes` or array of Axes\n    *ax* can be either a single `~.axes.Axes` object, or an array of Axes\n    objects if more than one subplot was created.  The dimensions of the\n    resulting array can be controlled with the squeeze keyword, see above.\n\n    Typical idioms for handling the return value are::\n\n        # using the variable ax for single a Axes\n        fig, ax = plt.subplots()\n\n        # using the variable axs for multiple Axes\n        fig, axs = plt.subplots(2, 2)\n\n        # using tuple unpacking for multiple Axes\n        fig, (ax1, ax2) = plt.subplots(1, 2)\n        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n\n    The names ``ax`` and pluralized ``axs`` are preferred over ``axes``\n    because for the latter it's not clear if it refers to a single\n    `~.axes.Axes` instance or a collection of these.\n\nSee Also\n--------\n.pyplot.figure\n.pyplot.subplot\n.pyplot.axes\n.Figure.subplots\n.Figure.add_subplot\n\nExamples\n--------\n::\n\n    # First create some toy data:\n    x = np.linspace(0, 2*np.pi, 400)\n    y = np.sin(x**2)\n\n    # Create just a figure and only one subplot\n    fig, ax = plt.subplots()\n    ax.plot(x, y)\n    ax.set_title('Simple plot')\n\n    # Create two subplots and unpack the output array immediately\n    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n    ax1.plot(x, y)\n    ax1.set_title('Sharing Y axis')\n    ax2.scatter(x, y)\n\n    # Create four polar axes and access them through the returned array\n    fig, axs = plt.subplots(2, 2, subplot_kw=dict(projection=\"polar\"))\n    axs[0, 0].plot(x, y)\n    axs[1, 1].scatter(x, y)\n\n    # Share a X axis with each column of subplots\n    plt.subplots(2, 2, sharex='col')\n\n    # Share a Y axis with each row of subplots\n    plt.subplots(2, 2, sharey='row')\n\n    # Share both X and Y axes with all subplots\n    plt.subplots(2, 2, sharex='all', sharey='all')\n\n    # Note that this is the same as\n    plt.subplots(2, 2, sharex=True, sharey=True)\n\n    # Create figure number 10 with a single subplot\n    # and clears it if it already exists.\n    fig, ax = plt.subplots(num=10, clear=True)", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/942", "data": {"name": "matplotlib.pyplot.subplots[1].grid", "type": "method", "signature": "(visible=None, which='major', axis='both', **kwargs)", "description": "Configure the grid lines.\n\nParameters\n----------\nvisible : bool or None, optional\n    Whether to show the grid lines.  If any *kwargs* are supplied, it\n    is assumed you want the grid on and *visible* will be set to True.\n\n    If *visible* is *None* and there are no *kwargs*, this toggles the\n    visibility of the lines.\n\nwhich : {'major', 'minor', 'both'}, optional\n    The grid lines to apply the changes on.\n\naxis : {'both', 'x', 'y'}, optional\n    The axis to apply the changes on.\n\n**kwargs : `~matplotlib.lines.Line2D` properties\n    Define the line properties of the grid, e.g.::\n\n        grid(color='r', linestyle='-', linewidth=2)\n\n    Valid keyword arguments are:\n\n    Properties:\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: scalar or None\n    animated: bool\n    antialiased or aa: bool\n    clip_box: `~matplotlib.transforms.BboxBase` or None\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    color or c: color\n    dash_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    dash_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    dashes: sequence of floats (on/off ink in points) or (None, None)\n    data: (2, N) array or two 1D arrays\n    drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n    figure: `~matplotlib.figure.Figure`\n    fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n    gapcolor: color or None\n    gid: str\n    in_layout: bool\n    label: object\n    linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n    linewidth or lw: float\n    marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n    markeredgecolor or mec: color\n    markeredgewidth or mew: float\n    markerfacecolor or mfc: color\n    markerfacecoloralt or mfcalt: color\n    markersize or ms: float\n    markevery: None or int or (int, int) or slice or list[int] or float or (float, float) or list[bool]\n    mouseover: bool\n    path_effects: list of `.AbstractPathEffect`\n    picker: float or callable[[Artist, Event], tuple[bool, dict]]\n    pickradius: float\n    rasterized: bool\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    solid_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    solid_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    transform: unknown\n    url: str\n    visible: bool\n    xdata: 1D array\n    ydata: 1D array\n    zorder: float\n\nNotes\n-----\nThe axis is drawn as a unit, so the effective zorder for drawing the\ngrid is determined by the zorder of each axis, not by the zorder of the\n`.Line2D` objects comprising the grid.  Therefore, to set grid zorder,\nuse `.set_axisbelow` or, for more control, call the\n`~.Artist.set_zorder` method of each axis.", "parameters": {"type": "object", "properties": {"visible": {"type": "NoneType", "default": null}, "which": {"type": "str", "default": "major"}, "axis": {"type": "str", "default": "both"}}}}}
{"task_id": "BigCodeBench/942", "data": {"name": "matplotlib.pyplot.subplots[1].set_title", "type": "method", "signature": "(label, fontdict=None, loc=None, pad=None, *, y=None, **kwargs)", "description": "Set a title for the Axes.\n\nSet one of the three available Axes titles. The available titles\nare positioned above the Axes in the center, flush with the left\nedge, and flush with the right edge.\n\nParameters\n----------\nlabel : str\n    Text to use for the title\n\nfontdict : dict\n\n    .. admonition:: Discouraged\n\n       The use of *fontdict* is discouraged. Parameters should be passed as\n       individual keyword arguments or using dictionary-unpacking\n       ``set_title(..., **fontdict)``.\n\n    A dictionary controlling the appearance of the title text,\n    the default *fontdict* is::\n\n       {'fontsize': rcParams['axes.titlesize'],\n        'fontweight': rcParams['axes.titleweight'],\n        'color': rcParams['axes.titlecolor'],\n        'verticalalignment': 'baseline',\n        'horizontalalignment': loc}\n\nloc : {'center', 'left', 'right'}, default: :rc:`axes.titlelocation`\n    Which title to set.\n\ny : float, default: :rc:`axes.titley`\n    Vertical Axes location for the title (1.0 is the top).  If\n    None (the default) and :rc:`axes.titley` is also None, y is\n    determined automatically to avoid decorators on the Axes.\n\npad : float, default: :rc:`axes.titlepad`\n    The offset of the title from the top of the Axes, in points.\n\nReturns\n-------\n`.Text`\n    The matplotlib text instance representing the title\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    Other keyword arguments are text properties, see `.Text` for a list\n    of valid text properties.", "parameters": {"type": "object", "properties": {"label": {}, "fontdict": {"type": "NoneType", "default": null}, "loc": {"type": "NoneType", "default": null}, "pad": {"type": "NoneType", "default": null}, "y": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/942", "data": {"name": "numpy.random.seed", "type": "callable", "signature": "(*args, **kwargs)", "description": "seed(self, seed=None)\n\nReseed a legacy MT19937 BitGenerator\n\nNotes\n-----\nThis is a convenience, legacy function.\n\nThe best practice is to **not** reseed a BitGenerator, rather to\nrecreate a new one. This method is here for legacy reasons.\nThis example demonstrates best practice.\n\n>>> from numpy.random import MT19937\n>>> from numpy.random import RandomState, SeedSequence\n>>> rs = RandomState(MT19937(SeedSequence(123456789)))\n# Later, you want to restart the stream\n>>> rs = RandomState(MT19937(SeedSequence(987654321)))", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/942", "data": {"name": "numpy.random.randint", "type": "callable", "signature": "(**kwargs)", "description": "randint(low, high=None, size=None, dtype=int)\n\nReturn random integers from `low` (inclusive) to `high` (exclusive).\n\nReturn random integers from the \"discrete uniform\" distribution of\nthe specified dtype in the \"half-open\" interval [`low`, `high`). If\n`high` is None (the default), then results are from [0, `low`).\n\n.. note::\n    New code should use the ``integers`` method of a ``default_rng()``\n    instance instead; please see the :ref:`random-quick-start`.\n\nParameters\n----------\nlow : int or array-like of ints\n    Lowest (signed) integers to be drawn from the distribution (unless\n    ``high=None``, in which case this parameter is one above the\n    *highest* such integer).\nhigh : int or array-like of ints, optional\n    If provided, one above the largest (signed) integer to be drawn\n    from the distribution (see above for behavior if ``high=None``).\n    If array-like, must contain integer values\nsize : int or tuple of ints, optional\n    Output shape.  If the given shape is, e.g., ``(m, n, k)``, then\n    ``m * n * k`` samples are drawn.  Default is None, in which case a\n    single value is returned.\ndtype : dtype, optional\n    Desired dtype of the result. Byteorder must be native.\n    The default value is int.\n\n    .. versionadded:: 1.11.0\n\nReturns\n-------\nout : int or ndarray of ints\n    `size`-shaped array of random integers from the appropriate\n    distribution, or a single such random int if `size` not provided.\n\nSee Also\n--------\nrandom_integers : similar to `randint`, only for the closed\n    interval [`low`, `high`], and 1 is the lowest value if `high` is\n    omitted.\nrandom.Generator.integers: which should be used for new code.\n\nExamples\n--------\n>>> np.random.randint(2, size=10)\narray([1, 0, 0, 0, 1, 1, 0, 0, 1, 0]) # random\n>>> np.random.randint(1, size=10)\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n\nGenerate a 2 x 4 array of ints between 0 and 4, inclusive:\n\n>>> np.random.randint(5, size=(2, 4))\narray([[4, 0, 2, 1], # random\n       [3, 2, 2, 0]])\n\nGenerate a 1 x 3 array with 3 different upper bounds\n\n>>> np.random.randint(1, [3, 5, 10])\narray([2, 2, 9]) # random\n\nGenerate a 1 by 3 array with 3 different lower bounds\n\n>>> np.random.randint([1, 5, 7], 10)\narray([9, 8, 7]) # random\n\nGenerate a 2 by 4 array using broadcasting with dtype of uint8\n\n>>> np.random.randint([1, 3, 5, 7], [[10], [20]], dtype=np.uint8)\narray([[ 8,  6,  9,  7], # random\n       [ 1, 16,  9, 12]], dtype=uint8)", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/942", "data": {"name": "pandas.date_range", "type": "callable", "signature": "(start=None, periods=None, freq=None, **kwargs) -> 'DatetimeIndex)", "description": "Return a fixed frequency DatetimeIndex.\n\nReturns the range of equally spaced time points (where the difference between any\ntwo adjacent points is specified by the given frequency) such that they all\nsatisfy `start <[=] x <[=] end`, where the first one and the last one are, resp.,\nthe first and last time points in that range that fall on the boundary of ``freq``\n(if given as a frequency string) or that are valid for ``freq`` (if given as a\n:class:`pandas.tseries.offsets.DateOffset`). (If exactly one of ``start``,\n``end``, or ``freq`` is *not* specified, this missing parameter can be computed\ngiven ``periods``, the number of timesteps in the range. See the note below.)\n\nParameters\n----------\nstart : str or datetime-like, optional\n    Left bound for generating dates.\nend : str or datetime-like, optional\n    Right bound for generating dates.\nperiods : int, optional\n    Number of periods to generate.\nfreq : str, Timedelta, datetime.timedelta, or DateOffset, default 'D'\n    Frequency strings can have multiples, e.g. '5h'. See\n    :ref:`here <timeseries.offset_aliases>` for a list of\n    frequency aliases.\ntz : str or tzinfo, optional\n    Time zone name for returning localized DatetimeIndex, for example\n    'Asia/Hong_Kong'. By default, the resulting DatetimeIndex is\n    timezone-naive unless timezone-aware datetime-likes are passed.\nnormalize : bool, default False\n    Normalize start/end dates to midnight before generating date range.\nname : str, default None\n    Name of the resulting DatetimeIndex.\ninclusive : {\"both\", \"neither\", \"left\", \"right\"}, default \"both\"\n    Include boundaries; Whether to set each bound as closed or open.\n\n    .. versionadded:: 1.4.0\nunit : str, default None\n    Specify the desired resolution of the result.\n\n    .. versionadded:: 2.0.0\n**kwargs\n    For compatibility. Has no effect on the result.\n\nReturns\n-------\nDatetimeIndex\n\nSee Also\n--------\nDatetimeIndex : An immutable container for datetimes.\ntimedelta_range : Return a fixed frequency TimedeltaIndex.\nperiod_range : Return a fixed frequency PeriodIndex.\ninterval_range : Return a fixed frequency IntervalIndex.\n\nNotes\n-----\nOf the four parameters ``start``, ``end``, ``periods``, and ``freq``,\nexactly three must be specified. If ``freq`` is omitted, the resulting\n``DatetimeIndex`` will have ``periods`` linearly spaced elements between\n``start`` and ``end`` (closed on both sides).\n\nTo learn more about the frequency strings, please see `this link\n<https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases>`__.\n\nExamples\n--------\n**Specifying the values**\n\nThe next four examples generate the same `DatetimeIndex`, but vary\nthe combination of `start`, `end` and `periods`.\n\nSpecify `start` and `end`, with the default daily frequency.\n\n>>> pd.date_range(start='1/1/2018', end='1/08/2018')\nDatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03', '2018-01-04',\n               '2018-01-05', '2018-01-06', '2018-01-07', '2018-01-08'],\n              dtype='datetime64[ns]', freq='D')\n\nSpecify timezone-aware `start` and `end`, with the default daily frequency.\n\n>>> pd.date_range(\n...     start=pd.to_datetime(\"1/1/2018\").tz_localize(\"Europe/Berlin\"),\n...     end=pd.to_datetime(\"1/08/2018\").tz_localize(\"Europe/Berlin\"),\n... )\nDatetimeIndex(['2018-01-01 00:00:00+01:00', '2018-01-02 00:00:00+01:00',\n               '2018-01-03 00:00:00+01:00', '2018-01-04 00:00:00+01:00',\n               '2018-01-05 00:00:00+01:00', '2018-01-06 00:00:00+01:00',\n               '2018-01-07 00:00:00+01:00', '2018-01-08 00:00:00+01:00'],\n              dtype='datetime64[ns, Europe/Berlin]', freq='D')\n\nSpecify `start` and `periods`, the number of periods (days).\n\n>>> pd.date_range(start='1/1/2018', periods=8)\nDatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03', '2018-01-04',\n               '2018-01-05', '2018-01-06', '2018-01-07', '2018-01-08'],\n              dtype='datetime64[ns]', freq='D')\n\nSpecify `end` and `periods`, the number of periods (days).\n\n>>> pd.date_range(end='1/1/2018', periods=8)\nDatetimeIndex(['2017-12-25', '2017-12-26', '2017-12-27', '2017-12-28',\n               '2017-12-29', '2017-12-30', '2017-12-31', '2018-01-01'],\n              dtype='datetime64[ns]', freq='D')\n\nSpecify `start`, `end`, and `periods`; the frequency is generated\nautomatically (linearly spaced).\n\n>>> pd.date_range(start='2018-04-24', end='2018-04-27', periods=3)\nDatetimeIndex(['2018-04-24 00:00:00', '2018-04-25 12:00:00',\n               '2018-04-27 00:00:00'],\n              dtype='datetime64[ns]', freq=None)\n\n**Other Parameters**\n\nChanged the `freq` (frequency) to ``'ME'`` (month end frequency).\n\n>>> pd.date_range(start='1/1/2018', periods=5, freq='ME')\nDatetimeIndex(['2018-01-31', '2018-02-28', '2018-03-31', '2018-04-30',\n               '2018-05-31'],\n              dtype='datetime64[ns]', freq='ME')\n\nMultiples are allowed\n\n>>> pd.date_range(start='1/1/2018', periods=5, freq='3ME')\nDatetimeIndex(['2018-01-31', '2018-04-30', '2018-07-31', '2018-10-31',\n               '2019-01-31'],\n              dtype='datetime64[ns]', freq='3ME')\n\n`freq` can also be specified as an Offset object.\n\n>>> pd.date_range(start='1/1/2018', periods=5, freq=pd.offsets.MonthEnd(3))\nDatetimeIndex(['2018-01-31', '2018-04-30', '2018-07-31', '2018-10-31',\n               '2019-01-31'],\n              dtype='datetime64[ns]', freq='3ME')\n\nSpecify `tz` to set the timezone.\n\n>>> pd.date_range(start='1/1/2018', periods=5, tz='Asia/Tokyo')\nDatetimeIndex(['2018-01-01 00:00:00+09:00', '2018-01-02 00:00:00+09:00',\n               '2018-01-03 00:00:00+09:00', '2018-01-04 00:00:00+09:00',\n               '2018-01-05 00:00:00+09:00'],\n              dtype='datetime64[ns, Asia/Tokyo]', freq='D')\n\n`inclusive` controls whether to include `start` and `end` that are on the\nboundary. The default, \"both\", includes boundary points on either end.\n\n>>> pd.date_range(start='2017-01-01', end='2017-01-04', inclusive=\"both\")\nDatetimeIndex(['2017-01-01', '2017-01-02', '2017-01-03', '2017-01-04'],\n              dtype='datetime64[ns]', freq='D')\n\nUse ``inclusive='left'`` to exclude `end` if it falls on the boundary.\n\n>>> pd.date_range(start='2017-01-01', end='2017-01-04', inclusive='left')\nDatetimeIndex(['2017-01-01', '2017-01-02', '2017-01-03'],\n              dtype='datetime64[ns]', freq='D')\n\nUse ``inclusive='right'`` to exclude `start` if it falls on the boundary, and\nsimilarly ``inclusive='neither'`` will exclude both `start` and `end`.\n\n>>> pd.date_range(start='2017-01-01', end='2017-01-04', inclusive='right')\nDatetimeIndex(['2017-01-02', '2017-01-03', '2017-01-04'],\n              dtype='datetime64[ns]', freq='D')\n\n**Specify a unit**\n\n>>> pd.date_range(start=\"2017-01-01\", periods=10, freq=\"100YS\", unit=\"s\")\nDatetimeIndex(['2017-01-01', '2117-01-01', '2217-01-01', '2317-01-01',\n               '2417-01-01', '2517-01-01', '2617-01-01', '2717-01-01',\n               '2817-01-01', '2917-01-01'],\n              dtype='datetime64[s]', freq='100YS-JAN')", "parameters": {"type": "object", "properties": {"start": {"type": "NoneType", "default": null}, "periods": {"type": "NoneType", "default": null}, "freq": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/942", "data": {"name": "report_data.append([date, category, sales])"}}
{"task_id": "BigCodeBench/945", "data": {"name": "numpy.arange", "type": "callable", "signature": "(*args, **kwargs)", "description": "arange([start,] stop[, step,], dtype=None, *, like=None)\n\nReturn evenly spaced values within a given interval.\n\n``arange`` can be called with a varying number of positional arguments:\n\n* ``arange(stop)``: Values are generated within the half-open interval\n  ``[0, stop)`` (in other words, the interval including `start` but\n  excluding `stop`).\n* ``arange(start, stop)``: Values are generated within the half-open\n  interval ``[start, stop)``.\n* ``arange(start, stop, step)`` Values are generated within the half-open\n  interval ``[start, stop)``, with spacing between values given by\n  ``step``.\n\nFor integer arguments the function is roughly equivalent to the Python\nbuilt-in :py:class:`range`, but returns an ndarray rather than a ``range``\ninstance.\n\nWhen using a non-integer step, such as 0.1, it is often better to use\n`numpy.linspace`.\n\nSee the Warning sections below for more information.\n\nParameters\n----------\nstart : integer or real, optional\n    Start of interval.  The interval includes this value.  The default\n    start value is 0.\nstop : integer or real\n    End of interval.  The interval does not include this value, except\n    in some cases where `step` is not an integer and floating point\n    round-off affects the length of `out`.\nstep : integer or real, optional\n    Spacing between values.  For any output `out`, this is the distance\n    between two adjacent values, ``out[i+1] - out[i]``.  The default\n    step size is 1.  If `step` is specified as a position argument,\n    `start` must also be given.\ndtype : dtype, optional\n    The type of the output array.  If `dtype` is not given, infer the data\n    type from the other input arguments.\nlike : array_like, optional\n    Reference object to allow the creation of arrays which are not\n    NumPy arrays. If an array-like passed in as ``like`` supports\n    the ``__array_function__`` protocol, the result will be defined\n    by it. In this case, it ensures the creation of an array object\n    compatible with that passed in via this argument.\n\n    .. versionadded:: 1.20.0\n\nReturns\n-------\narange : ndarray\n    Array of evenly spaced values.\n\n    For floating point arguments, the length of the result is\n    ``ceil((stop - start)/step)``.  Because of floating point overflow,\n    this rule may result in the last element of `out` being greater\n    than `stop`.\n\nWarnings\n--------\nThe length of the output might not be numerically stable.\n\nAnother stability issue is due to the internal implementation of\n`numpy.arange`.\nThe actual step value used to populate the array is\n``dtype(start + step) - dtype(start)`` and not `step`. Precision loss\ncan occur here, due to casting or due to using floating points when\n`start` is much larger than `step`. This can lead to unexpected\nbehaviour. For example::\n\n  >>> np.arange(0, 5, 0.5, dtype=int)\n  array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n  >>> np.arange(-3, 3, 0.5, dtype=int)\n  array([-3, -2, -1,  0,  1,  2,  3,  4,  5,  6,  7,  8])\n\nIn such cases, the use of `numpy.linspace` should be preferred.\n\nThe built-in :py:class:`range` generates :std:doc:`Python built-in integers\nthat have arbitrary size <c-api/long>`, while `numpy.arange` produces\n`numpy.int32` or `numpy.int64` numbers. This may result in incorrect\nresults for large integer values::\n\n  >>> power = 40\n  >>> modulo = 10000\n  >>> x1 = [(n ** power) % modulo for n in range(8)]\n  >>> x2 = [(n ** power) % modulo for n in np.arange(8)]\n  >>> print(x1)\n  [0, 1, 7776, 8801, 6176, 625, 6576, 4001]  # correct\n  >>> print(x2)\n  [0, 1, 7776, 7185, 0, 5969, 4816, 3361]  # incorrect\n\nSee Also\n--------\nnumpy.linspace : Evenly spaced numbers with careful handling of endpoints.\nnumpy.ogrid: Arrays of evenly spaced numbers in N-dimensions.\nnumpy.mgrid: Grid-shaped arrays of evenly spaced numbers in N-dimensions.\n\nExamples\n--------\n>>> np.arange(3)\narray([0, 1, 2])\n>>> np.arange(3.0)\narray([ 0.,  1.,  2.])\n>>> np.arange(3,7)\narray([3, 4, 5, 6])\n>>> np.arange(3,7,2)\narray([3, 5])", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/945", "data": {"name": "numpy.random.randint", "type": "callable", "signature": "(**kwargs)", "description": "randint(low, high=None, size=None, dtype=int)\n\nReturn random integers from `low` (inclusive) to `high` (exclusive).\n\nReturn random integers from the \"discrete uniform\" distribution of\nthe specified dtype in the \"half-open\" interval [`low`, `high`). If\n`high` is None (the default), then results are from [0, `low`).\n\n.. note::\n    New code should use the ``integers`` method of a ``default_rng()``\n    instance instead; please see the :ref:`random-quick-start`.\n\nParameters\n----------\nlow : int or array-like of ints\n    Lowest (signed) integers to be drawn from the distribution (unless\n    ``high=None``, in which case this parameter is one above the\n    *highest* such integer).\nhigh : int or array-like of ints, optional\n    If provided, one above the largest (signed) integer to be drawn\n    from the distribution (see above for behavior if ``high=None``).\n    If array-like, must contain integer values\nsize : int or tuple of ints, optional\n    Output shape.  If the given shape is, e.g., ``(m, n, k)``, then\n    ``m * n * k`` samples are drawn.  Default is None, in which case a\n    single value is returned.\ndtype : dtype, optional\n    Desired dtype of the result. Byteorder must be native.\n    The default value is int.\n\n    .. versionadded:: 1.11.0\n\nReturns\n-------\nout : int or ndarray of ints\n    `size`-shaped array of random integers from the appropriate\n    distribution, or a single such random int if `size` not provided.\n\nSee Also\n--------\nrandom_integers : similar to `randint`, only for the closed\n    interval [`low`, `high`], and 1 is the lowest value if `high` is\n    omitted.\nrandom.Generator.integers: which should be used for new code.\n\nExamples\n--------\n>>> np.random.randint(2, size=10)\narray([1, 0, 0, 0, 1, 1, 0, 0, 1, 0]) # random\n>>> np.random.randint(1, size=10)\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n\nGenerate a 2 x 4 array of ints between 0 and 4, inclusive:\n\n>>> np.random.randint(5, size=(2, 4))\narray([[4, 0, 2, 1], # random\n       [3, 2, 2, 0]])\n\nGenerate a 1 x 3 array with 3 different upper bounds\n\n>>> np.random.randint(1, [3, 5, 10])\narray([2, 2, 9]) # random\n\nGenerate a 1 by 3 array with 3 different lower bounds\n\n>>> np.random.randint([1, 5, 7], 10)\narray([9, 8, 7]) # random\n\nGenerate a 2 by 4 array using broadcasting with dtype of uint8\n\n>>> np.random.randint([1, 3, 5, 7], [[10], [20]], dtype=np.uint8)\narray([[ 8,  6,  9,  7], # random\n       [ 1, 16,  9, 12]], dtype=uint8)", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/945", "data": {"name": "pandas.date_range", "type": "callable", "signature": "(start=None, periods=None, freq=None, **kwargs) -> 'DatetimeIndex)", "description": "Return a fixed frequency DatetimeIndex.\n\nReturns the range of equally spaced time points (where the difference between any\ntwo adjacent points is specified by the given frequency) such that they all\nsatisfy `start <[=] x <[=] end`, where the first one and the last one are, resp.,\nthe first and last time points in that range that fall on the boundary of ``freq``\n(if given as a frequency string) or that are valid for ``freq`` (if given as a\n:class:`pandas.tseries.offsets.DateOffset`). (If exactly one of ``start``,\n``end``, or ``freq`` is *not* specified, this missing parameter can be computed\ngiven ``periods``, the number of timesteps in the range. See the note below.)\n\nParameters\n----------\nstart : str or datetime-like, optional\n    Left bound for generating dates.\nend : str or datetime-like, optional\n    Right bound for generating dates.\nperiods : int, optional\n    Number of periods to generate.\nfreq : str, Timedelta, datetime.timedelta, or DateOffset, default 'D'\n    Frequency strings can have multiples, e.g. '5h'. See\n    :ref:`here <timeseries.offset_aliases>` for a list of\n    frequency aliases.\ntz : str or tzinfo, optional\n    Time zone name for returning localized DatetimeIndex, for example\n    'Asia/Hong_Kong'. By default, the resulting DatetimeIndex is\n    timezone-naive unless timezone-aware datetime-likes are passed.\nnormalize : bool, default False\n    Normalize start/end dates to midnight before generating date range.\nname : str, default None\n    Name of the resulting DatetimeIndex.\ninclusive : {\"both\", \"neither\", \"left\", \"right\"}, default \"both\"\n    Include boundaries; Whether to set each bound as closed or open.\n\n    .. versionadded:: 1.4.0\nunit : str, default None\n    Specify the desired resolution of the result.\n\n    .. versionadded:: 2.0.0\n**kwargs\n    For compatibility. Has no effect on the result.\n\nReturns\n-------\nDatetimeIndex\n\nSee Also\n--------\nDatetimeIndex : An immutable container for datetimes.\ntimedelta_range : Return a fixed frequency TimedeltaIndex.\nperiod_range : Return a fixed frequency PeriodIndex.\ninterval_range : Return a fixed frequency IntervalIndex.\n\nNotes\n-----\nOf the four parameters ``start``, ``end``, ``periods``, and ``freq``,\nexactly three must be specified. If ``freq`` is omitted, the resulting\n``DatetimeIndex`` will have ``periods`` linearly spaced elements between\n``start`` and ``end`` (closed on both sides).\n\nTo learn more about the frequency strings, please see `this link\n<https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases>`__.\n\nExamples\n--------\n**Specifying the values**\n\nThe next four examples generate the same `DatetimeIndex`, but vary\nthe combination of `start`, `end` and `periods`.\n\nSpecify `start` and `end`, with the default daily frequency.\n\n>>> pd.date_range(start='1/1/2018', end='1/08/2018')\nDatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03', '2018-01-04',\n               '2018-01-05', '2018-01-06', '2018-01-07', '2018-01-08'],\n              dtype='datetime64[ns]', freq='D')\n\nSpecify timezone-aware `start` and `end`, with the default daily frequency.\n\n>>> pd.date_range(\n...     start=pd.to_datetime(\"1/1/2018\").tz_localize(\"Europe/Berlin\"),\n...     end=pd.to_datetime(\"1/08/2018\").tz_localize(\"Europe/Berlin\"),\n... )\nDatetimeIndex(['2018-01-01 00:00:00+01:00', '2018-01-02 00:00:00+01:00',\n               '2018-01-03 00:00:00+01:00', '2018-01-04 00:00:00+01:00',\n               '2018-01-05 00:00:00+01:00', '2018-01-06 00:00:00+01:00',\n               '2018-01-07 00:00:00+01:00', '2018-01-08 00:00:00+01:00'],\n              dtype='datetime64[ns, Europe/Berlin]', freq='D')\n\nSpecify `start` and `periods`, the number of periods (days).\n\n>>> pd.date_range(start='1/1/2018', periods=8)\nDatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03', '2018-01-04',\n               '2018-01-05', '2018-01-06', '2018-01-07', '2018-01-08'],\n              dtype='datetime64[ns]', freq='D')\n\nSpecify `end` and `periods`, the number of periods (days).\n\n>>> pd.date_range(end='1/1/2018', periods=8)\nDatetimeIndex(['2017-12-25', '2017-12-26', '2017-12-27', '2017-12-28',\n               '2017-12-29', '2017-12-30', '2017-12-31', '2018-01-01'],\n              dtype='datetime64[ns]', freq='D')\n\nSpecify `start`, `end`, and `periods`; the frequency is generated\nautomatically (linearly spaced).\n\n>>> pd.date_range(start='2018-04-24', end='2018-04-27', periods=3)\nDatetimeIndex(['2018-04-24 00:00:00', '2018-04-25 12:00:00',\n               '2018-04-27 00:00:00'],\n              dtype='datetime64[ns]', freq=None)\n\n**Other Parameters**\n\nChanged the `freq` (frequency) to ``'ME'`` (month end frequency).\n\n>>> pd.date_range(start='1/1/2018', periods=5, freq='ME')\nDatetimeIndex(['2018-01-31', '2018-02-28', '2018-03-31', '2018-04-30',\n               '2018-05-31'],\n              dtype='datetime64[ns]', freq='ME')\n\nMultiples are allowed\n\n>>> pd.date_range(start='1/1/2018', periods=5, freq='3ME')\nDatetimeIndex(['2018-01-31', '2018-04-30', '2018-07-31', '2018-10-31',\n               '2019-01-31'],\n              dtype='datetime64[ns]', freq='3ME')\n\n`freq` can also be specified as an Offset object.\n\n>>> pd.date_range(start='1/1/2018', periods=5, freq=pd.offsets.MonthEnd(3))\nDatetimeIndex(['2018-01-31', '2018-04-30', '2018-07-31', '2018-10-31',\n               '2019-01-31'],\n              dtype='datetime64[ns]', freq='3ME')\n\nSpecify `tz` to set the timezone.\n\n>>> pd.date_range(start='1/1/2018', periods=5, tz='Asia/Tokyo')\nDatetimeIndex(['2018-01-01 00:00:00+09:00', '2018-01-02 00:00:00+09:00',\n               '2018-01-03 00:00:00+09:00', '2018-01-04 00:00:00+09:00',\n               '2018-01-05 00:00:00+09:00'],\n              dtype='datetime64[ns, Asia/Tokyo]', freq='D')\n\n`inclusive` controls whether to include `start` and `end` that are on the\nboundary. The default, \"both\", includes boundary points on either end.\n\n>>> pd.date_range(start='2017-01-01', end='2017-01-04', inclusive=\"both\")\nDatetimeIndex(['2017-01-01', '2017-01-02', '2017-01-03', '2017-01-04'],\n              dtype='datetime64[ns]', freq='D')\n\nUse ``inclusive='left'`` to exclude `end` if it falls on the boundary.\n\n>>> pd.date_range(start='2017-01-01', end='2017-01-04', inclusive='left')\nDatetimeIndex(['2017-01-01', '2017-01-02', '2017-01-03'],\n              dtype='datetime64[ns]', freq='D')\n\nUse ``inclusive='right'`` to exclude `start` if it falls on the boundary, and\nsimilarly ``inclusive='neither'`` will exclude both `start` and `end`.\n\n>>> pd.date_range(start='2017-01-01', end='2017-01-04', inclusive='right')\nDatetimeIndex(['2017-01-02', '2017-01-03', '2017-01-04'],\n              dtype='datetime64[ns]', freq='D')\n\n**Specify a unit**\n\n>>> pd.date_range(start=\"2017-01-01\", periods=10, freq=\"100YS\", unit=\"s\")\nDatetimeIndex(['2017-01-01', '2117-01-01', '2217-01-01', '2317-01-01',\n               '2417-01-01', '2517-01-01', '2617-01-01', '2717-01-01',\n               '2817-01-01', '2917-01-01'],\n              dtype='datetime64[s]', freq='100YS-JAN')", "parameters": {"type": "object", "properties": {"start": {"type": "NoneType", "default": null}, "periods": {"type": "NoneType", "default": null}, "freq": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/945", "data": {"name": "sklearn.linear_model.LinearRegression", "type": "class", "signature": "()", "description": "Ordinary least squares Linear Regression.\n\nLinearRegression fits a linear model with coefficients w = (w1, ..., wp)\nto minimize the residual sum of squares between the observed targets in\nthe dataset, and the targets predicted by the linear approximation.\n\nParameters\n----------\nfit_intercept : bool, default=True\n    Whether to calculate the intercept for this model. If set\n    to False, no intercept will be used in calculations\n    (i.e. data is expected to be centered).\n\ncopy_X : bool, default=True\n    If True, X will be copied; else, it may be overwritten.\n\nn_jobs : int, default=None\n    The number of jobs to use for the computation. This will only provide\n    speedup in case of sufficiently large problems, that is if firstly\n    `n_targets > 1` and secondly `X` is sparse or if `positive` is set\n    to `True`. ``None`` means 1 unless in a\n    :obj:`joblib.parallel_backend` context. ``-1`` means using all\n    processors. See :term:`Glossary <n_jobs>` for more details.\n\npositive : bool, default=False\n    When set to ``True``, forces the coefficients to be positive. This\n    option is only supported for dense arrays.\n\n    .. versionadded:: 0.24\n\nAttributes\n----------\ncoef_ : array of shape (n_features, ) or (n_targets, n_features)\n    Estimated coefficients for the linear regression problem.\n    If multiple targets are passed during the fit (y 2D), this\n    is a 2D array of shape (n_targets, n_features), while if only\n    one target is passed, this is a 1D array of length n_features.\n\nrank_ : int\n    Rank of matrix `X`. Only available when `X` is dense.\n\nsingular_ : array of shape (min(X, y),)\n    Singular values of `X`. Only available when `X` is dense.\n\nintercept_ : float or array of shape (n_targets,)\n    Independent term in the linear model. Set to 0.0 if\n    `fit_intercept = False`.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nRidge : Ridge regression addresses some of the\n    problems of Ordinary Least Squares by imposing a penalty on the\n    size of the coefficients with l2 regularization.\nLasso : The Lasso is a linear model that estimates\n    sparse coefficients with l1 regularization.\nElasticNet : Elastic-Net is a linear regression\n    model trained with both l1 and l2 -norm regularization of the\n    coefficients.\n\nNotes\n-----\nFrom the implementation point of view, this is just plain Ordinary\nLeast Squares (scipy.linalg.lstsq) or Non Negative Least Squares\n(scipy.optimize.nnls) wrapped as a predictor object.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.linear_model import LinearRegression\n>>> X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n>>> # y = 1 * x_0 + 2 * x_1 + 3\n>>> y = np.dot(X, np.array([1, 2])) + 3\n>>> reg = LinearRegression().fit(X, y)\n>>> reg.score(X, y)\n1.0\n>>> reg.coef_\narray([1., 2.])\n>>> reg.intercept_\n3.0...\n>>> reg.predict(np.array([[3, 5]]))\narray([16.])", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/945", "data": {"name": "sklearn.linear_model.LinearRegression.fit", "type": "callable", "signature": "(self, X, y)", "description": "Fit linear model.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training data.\n\ny : array-like of shape (n_samples,) or (n_samples, n_targets)\n    Target values. Will be cast to X's dtype if necessary.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Individual weights for each sample.\n\n    .. versionadded:: 0.17\n       parameter *sample_weight* support to LinearRegression.\n\nReturns\n-------\nself : object\n    Fitted Estimator.", "parameters": {"type": "object", "properties": {"X": {}, "y": {}}}}}
{"task_id": "BigCodeBench/945", "data": {"name": "sklearn.linear_model.LinearRegression.predict", "type": "callable", "signature": "(self, X)", "description": "Predict using the linear model.\n\nParameters\n----------\nX : array-like or sparse matrix, shape (n_samples, n_features)\n    Samples.\n\nReturns\n-------\nC : array, shape (n_samples,)\n    Returns predicted values.", "parameters": {"type": "object", "properties": {"X": {}}}}}
{"task_id": "BigCodeBench/952", "data": {"name": "datetime.datetime", "type": "class", "signature": "(self, /, *args, **kwargs)", "description": "datetime(year, month, day[, hour[, minute[, second[, microsecond[,tzinfo]]]]])\n\nThe year, month and day arguments are required. tzinfo may be None, or an\ninstance of a tzinfo subclass. The remaining arguments may be ints.", "parameters": {"type": "object", "properties": {"/": {}}}}}
{"task_id": "BigCodeBench/952", "data": {"name": "datetime.datetime.today", "type": "callable", "signature": "()", "description": "Current date or datetime:  same as self.__class__.fromtimestamp(time.time()).", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/952", "data": {"name": "assignment_data.append([task_name, employee, due_date])"}}
{"task_id": "BigCodeBench/952", "data": {"name": "random.seed", "type": "callable", "signature": "(a=None)", "description": "Initialize internal state from a seed.\n\nThe only supported seed types are None, int, float,\nstr, bytes, and bytearray.\n\nNone or no argument seeds from current time or from an operating\nsystem specific randomness source if available.\n\nIf *a* is an int, all bits are used.\n\nFor version 2 (the default), all of the bits are used if *a* is a str,\nbytes, or bytearray.  For version 1 (provided for reproducing random\nsequences from older versions of Python), the algorithm for str and\nbytes generates a narrower range of seeds.", "parameters": {"type": "object", "properties": {"a": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/952", "data": {"name": "random.choice", "type": "callable", "signature": "(seq)", "description": "Choose a random element from a non-empty sequence.", "parameters": {"type": "object", "properties": {"seq": {}}}}}
{"task_id": "BigCodeBench/955", "data": {"name": "collections.Counter", "type": "class", "signature": "(iterable=None, /, **kwds)", "description": "Dict subclass for counting hashable items.  Sometimes called a bag\nor multiset.  Elements are stored as dictionary keys and their counts\nare stored as dictionary values.\n\n>>> c = Counter('abcdeabcdabcaba')  # count elements from a string\n\n>>> c.most_common(3)                # three most common elements\n[('a', 5), ('b', 4), ('c', 3)]\n>>> sorted(c)                       # list all unique elements\n['a', 'b', 'c', 'd', 'e']\n>>> ''.join(sorted(c.elements()))   # list elements with repetitions\n'aaaaabbbbcccdde'\n>>> sum(c.values())                 # total of all counts\n15\n\n>>> c['a']                          # count of letter 'a'\n5\n>>> for elem in 'shazam':           # update counts from an iterable\n...     c[elem] += 1                # by adding 1 to each element's count\n>>> c['a']                          # now there are seven 'a'\n7\n>>> del c['b']                      # remove all 'b'\n>>> c['b']                          # now there are zero 'b'\n0\n\n>>> d = Counter('simsalabim')       # make another counter\n>>> c.update(d)                     # add in the second counter\n>>> c['a']                          # now there are nine 'a'\n9\n\n>>> c.clear()                       # empty the counter\n>>> c\nCounter()\n\nNote:  If a count is set to zero or reduced to zero, it will remain\nin the counter until the entry is deleted or the counter is cleared:\n\n>>> c = Counter('aaabbc')\n>>> c['b'] -= 2                     # reduce the count of 'b' by two\n>>> c.most_common()                 # 'b' is still in, but its count is zero\n[('a', 3), ('c', 1), ('b', 0)]", "parameters": {"type": "object", "properties": {"iterable": {"type": "NoneType", "default": null}, "/": {}}}}}
{"task_id": "BigCodeBench/955", "data": {"name": "collections.Counter.items", "type": "callable", "signature": "()", "description": "D.items() -> a set-like object providing a view on D's items", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/955", "data": {"name": "matplotlib.pyplot.subplots", "type": "class", "signature": "()", "description": "Create a figure and a set of subplots.\n\nThis utility wrapper makes it convenient to create common layouts of\nsubplots, including the enclosing figure object, in a single call.\n\nParameters\n----------\nnrows, ncols : int, default: 1\n    Number of rows/columns of the subplot grid.\n\nsharex, sharey : bool or {'none', 'all', 'row', 'col'}, default: False\n    Controls sharing of properties among x (*sharex*) or y (*sharey*)\n    axes:\n\n    - True or 'all': x- or y-axis will be shared among all subplots.\n    - False or 'none': each subplot x- or y-axis will be independent.\n    - 'row': each subplot row will share an x- or y-axis.\n    - 'col': each subplot column will share an x- or y-axis.\n\n    When subplots have a shared x-axis along a column, only the x tick\n    labels of the bottom subplot are created. Similarly, when subplots\n    have a shared y-axis along a row, only the y tick labels of the first\n    column subplot are created. To later turn other subplots' ticklabels\n    on, use `~matplotlib.axes.Axes.tick_params`.\n\n    When subplots have a shared axis that has units, calling\n    `~matplotlib.axis.Axis.set_units` will update each axis with the\n    new units.\n\nsqueeze : bool, default: True\n    - If True, extra dimensions are squeezed out from the returned\n      array of `~matplotlib.axes.Axes`:\n\n      - if only one subplot is constructed (nrows=ncols=1), the\n        resulting single Axes object is returned as a scalar.\n      - for Nx1 or 1xM subplots, the returned object is a 1D numpy\n        object array of Axes objects.\n      - for NxM, subplots with N>1 and M>1 are returned as a 2D array.\n\n    - If False, no squeezing at all is done: the returned Axes object is\n      always a 2D array containing Axes instances, even if it ends up\n      being 1x1.\n\nwidth_ratios : array-like of length *ncols*, optional\n    Defines the relative widths of the columns. Each column gets a\n    relative width of ``width_ratios[i] / sum(width_ratios)``.\n    If not given, all columns will have the same width.  Equivalent\n    to ``gridspec_kw={'width_ratios': [...]}``.\n\nheight_ratios : array-like of length *nrows*, optional\n    Defines the relative heights of the rows. Each row gets a\n    relative height of ``height_ratios[i] / sum(height_ratios)``.\n    If not given, all rows will have the same height. Convenience\n    for ``gridspec_kw={'height_ratios': [...]}``.\n\nsubplot_kw : dict, optional\n    Dict with keywords passed to the\n    `~matplotlib.figure.Figure.add_subplot` call used to create each\n    subplot.\n\ngridspec_kw : dict, optional\n    Dict with keywords passed to the `~matplotlib.gridspec.GridSpec`\n    constructor used to create the grid the subplots are placed on.\n\n**fig_kw\n    All additional keyword arguments are passed to the\n    `.pyplot.figure` call.\n\nReturns\n-------\nfig : `.Figure`\n\nax : `~matplotlib.axes.Axes` or array of Axes\n    *ax* can be either a single `~.axes.Axes` object, or an array of Axes\n    objects if more than one subplot was created.  The dimensions of the\n    resulting array can be controlled with the squeeze keyword, see above.\n\n    Typical idioms for handling the return value are::\n\n        # using the variable ax for single a Axes\n        fig, ax = plt.subplots()\n\n        # using the variable axs for multiple Axes\n        fig, axs = plt.subplots(2, 2)\n\n        # using tuple unpacking for multiple Axes\n        fig, (ax1, ax2) = plt.subplots(1, 2)\n        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n\n    The names ``ax`` and pluralized ``axs`` are preferred over ``axes``\n    because for the latter it's not clear if it refers to a single\n    `~.axes.Axes` instance or a collection of these.\n\nSee Also\n--------\n.pyplot.figure\n.pyplot.subplot\n.pyplot.axes\n.Figure.subplots\n.Figure.add_subplot\n\nExamples\n--------\n::\n\n    # First create some toy data:\n    x = np.linspace(0, 2*np.pi, 400)\n    y = np.sin(x**2)\n\n    # Create just a figure and only one subplot\n    fig, ax = plt.subplots()\n    ax.plot(x, y)\n    ax.set_title('Simple plot')\n\n    # Create two subplots and unpack the output array immediately\n    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n    ax1.plot(x, y)\n    ax1.set_title('Sharing Y axis')\n    ax2.scatter(x, y)\n\n    # Create four polar axes and access them through the returned array\n    fig, axs = plt.subplots(2, 2, subplot_kw=dict(projection=\"polar\"))\n    axs[0, 0].plot(x, y)\n    axs[1, 1].scatter(x, y)\n\n    # Share a X axis with each column of subplots\n    plt.subplots(2, 2, sharex='col')\n\n    # Share a Y axis with each row of subplots\n    plt.subplots(2, 2, sharey='row')\n\n    # Share both X and Y axes with all subplots\n    plt.subplots(2, 2, sharex='all', sharey='all')\n\n    # Note that this is the same as\n    plt.subplots(2, 2, sharex=True, sharey=True)\n\n    # Create figure number 10 with a single subplot\n    # and clears it if it already exists.\n    fig, ax = plt.subplots(num=10, clear=True)", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/955", "data": {"name": "matplotlib.pyplot.subplots[1].set_xticks", "type": "method", "signature": "(ticks, labels=None, *, minor=False, **kwargs)", "description": "Set the xaxis' tick locations and optionally tick labels.\n\nIf necessary, the view limits of the Axis are expanded so that all\ngiven ticks are visible.\n\nParameters\n----------\nticks : 1D array-like\n    Array of tick locations.  The axis `.Locator` is replaced by a\n    `~.ticker.FixedLocator`.\n\n    The values may be either floats or in axis units.\n\n    Pass an empty list to remove all ticks::\n\n        set_xticks([])\n\n    Some tick formatters will not label arbitrary tick positions;\n    e.g. log formatters only label decade ticks by default. In\n    such a case you can set a formatter explicitly on the axis\n    using `.Axis.set_major_formatter` or provide formatted\n    *labels* yourself.\nlabels : list of str, optional\n    Tick labels for each location in *ticks*. *labels* must be of the same\n    length as *ticks*. If not set, the labels are generate using the axis\n    tick `.Formatter`.\nminor : bool, default: False\n    If ``False``, set the major ticks; if ``True``, the minor ticks.\n**kwargs\n    `.Text` properties for the labels. Using these is only allowed if\n    you pass *labels*. In other cases, please use `~.Axes.tick_params`.\n\nNotes\n-----\nThe mandatory expansion of the view limits is an intentional design\nchoice to prevent the surprise of a non-visible tick. If you need\nother limits, you should set the limits explicitly after setting the\nticks.", "parameters": {"type": "object", "properties": {"ticks": {}, "labels": {"type": "NoneType", "default": null}, "minor": {"type": "bool", "default": false}}}}}
{"task_id": "BigCodeBench/955", "data": {"name": "matplotlib.pyplot.subplots[1].set_xticklabels", "type": "method", "signature": "(labels, *, minor=False, fontdict=None, **kwargs)", "description": "[*Discouraged*] Set the xaxis' tick labels with list of string labels.\n\n.. admonition:: Discouraged\n\n    The use of this method is discouraged, because of the dependency on\n    tick positions. In most cases, you'll want to use\n    ``Axes.set_[x/y/z]ticks(positions, labels)`` or ``Axes.set_xticks``\n    instead.\n\n    If you are using this method, you should always fix the tick\n    positions before, e.g. by using `.Axes.set_xticks` or by explicitly\n    setting a `~.ticker.FixedLocator`. Otherwise, ticks are free to\n    move and the labels may end up in unexpected positions.\n\nParameters\n----------\nlabels : sequence of str or of `.Text`\\s\n    Texts for labeling each tick location in the sequence set by\n    `.Axes.set_xticks`; the number of labels must match the number of\n    locations.\n\nminor : bool\n    If True, set minor ticks instead of major ticks.\n\nfontdict : dict, optional\n\n    .. admonition:: Discouraged\n\n       The use of *fontdict* is discouraged. Parameters should be passed as\n       individual keyword arguments or using dictionary-unpacking\n       ``set_ticklabels(..., **fontdict)``.\n\n    A dictionary controlling the appearance of the ticklabels.\n    The default *fontdict* is::\n\n       {'fontsize': rcParams['axes.titlesize'],\n        'fontweight': rcParams['axes.titleweight'],\n        'verticalalignment': 'baseline',\n        'horizontalalignment': loc}\n\n**kwargs\n    Text properties.\n\n    .. warning::\n\n        This only sets the properties of the current ticks.\n        Ticks are not guaranteed to be persistent. Various operations\n        can create, delete and modify the Tick instances. There is an\n        imminent risk that these settings can get lost if you work on\n        the figure further (including also panning/zooming on a\n        displayed figure).\n\n        Use `.set_tick_params` instead if possible.\n\nReturns\n-------\nlist of `.Text`\\s\n    For each tick, includes ``tick.label1`` if it is visible, then\n    ``tick.label2`` if it is visible, in that order.", "parameters": {"type": "object", "properties": {"labels": {}, "minor": {"type": "bool", "default": false}, "fontdict": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/955", "data": {"name": "matplotlib.pyplot.subplots[1].bar", "type": "method", "signature": "(x, height, width=0.8, bottom=None, *, align='center', data=None, **kwargs)", "description": "Make a bar plot.\n\nThe bars are positioned at *x* with the given *align*\\ment. Their\ndimensions are given by *height* and *width*. The vertical baseline\nis *bottom* (default 0).\n\nMany parameters can take either a single value applying to all bars\nor a sequence of values, one for each bar.\n\nParameters\n----------\nx : float or array-like\n    The x coordinates of the bars. See also *align* for the\n    alignment of the bars to the coordinates.\n\nheight : float or array-like\n    The height(s) of the bars.\n\n    Note that if *bottom* has units (e.g. datetime), *height* should be in\n    units that are a difference from the value of *bottom* (e.g. timedelta).\n\nwidth : float or array-like, default: 0.8\n    The width(s) of the bars.\n\n    Note that if *x* has units (e.g. datetime), then *width* should be in\n    units that are a difference (e.g. timedelta) around the *x* values.\n\nbottom : float or array-like, default: 0\n    The y coordinate(s) of the bottom side(s) of the bars.\n\n    Note that if *bottom* has units, then the y-axis will get a Locator and\n    Formatter appropriate for the units (e.g. dates, or categorical).\n\nalign : {'center', 'edge'}, default: 'center'\n    Alignment of the bars to the *x* coordinates:\n\n    - 'center': Center the base on the *x* positions.\n    - 'edge': Align the left edges of the bars with the *x* positions.\n\n    To align the bars on the right edge pass a negative *width* and\n    ``align='edge'``.\n\nReturns\n-------\n`.BarContainer`\n    Container with all the bars and optionally errorbars.\n\nOther Parameters\n----------------\ncolor : color or list of color, optional\n    The colors of the bar faces.\n\nedgecolor : color or list of color, optional\n    The colors of the bar edges.\n\nlinewidth : float or array-like, optional\n    Width of the bar edge(s). If 0, don't draw edges.\n\ntick_label : str or list of str, optional\n    The tick labels of the bars.\n    Default: None (Use default numeric labels.)\n\nlabel : str or list of str, optional\n    A single label is attached to the resulting `.BarContainer` as a\n    label for the whole dataset.\n    If a list is provided, it must be the same length as *x* and\n    labels the individual bars. Repeated labels are not de-duplicated\n    and will cause repeated label entries, so this is best used when\n    bars also differ in style (e.g., by passing a list to *color*.)\n\nxerr, yerr : float or array-like of shape(N,) or shape(2, N), optional\n    If not *None*, add horizontal / vertical errorbars to the bar tips.\n    The values are +/- sizes relative to the data:\n\n    - scalar: symmetric +/- values for all bars\n    - shape(N,): symmetric +/- values for each bar\n    - shape(2, N): Separate - and + values for each bar. First row\n      contains the lower errors, the second row contains the upper\n      errors.\n    - *None*: No errorbar. (Default)\n\n    See :doc:`/gallery/statistics/errorbar_features` for an example on\n    the usage of *xerr* and *yerr*.\n\necolor : color or list of color, default: 'black'\n    The line color of the errorbars.\n\ncapsize : float, default: :rc:`errorbar.capsize`\n   The length of the error bar caps in points.\n\nerror_kw : dict, optional\n    Dictionary of keyword arguments to be passed to the\n    `~.Axes.errorbar` method. Values of *ecolor* or *capsize* defined\n    here take precedence over the independent keyword arguments.\n\nlog : bool, default: False\n    If *True*, set the y-axis to be log scale.\n\ndata : indexable object, optional\n    If given, all parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` (unless this raises an exception).\n\n**kwargs : `.Rectangle` properties\n\nProperties:\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: scalar or None\n    angle: unknown\n    animated: bool\n    antialiased or aa: bool or None\n    bounds: (left, bottom, width, height)\n    capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    clip_box: `~matplotlib.transforms.BboxBase` or None\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    color: color\n    edgecolor or ec: color or None\n    facecolor or fc: color or None\n    figure: `~matplotlib.figure.Figure`\n    fill: bool\n    gid: str\n    hatch: {'/', '\\\\', '|', '-', '+', 'x', 'o', 'O', '.', '*'}\n    height: unknown\n    in_layout: bool\n    joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    label: object\n    linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n    linewidth or lw: float or None\n    mouseover: bool\n    path_effects: list of `.AbstractPathEffect`\n    picker: None or bool or float or callable\n    rasterized: bool\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    transform: `~matplotlib.transforms.Transform`\n    url: str\n    visible: bool\n    width: unknown\n    x: unknown\n    xy: (float, float)\n    y: unknown\n    zorder: float\n\nSee Also\n--------\nbarh : Plot a horizontal bar plot.\n\nNotes\n-----\nStacked bars can be achieved by passing individual *bottom* values per\nbar. See :doc:`/gallery/lines_bars_and_markers/bar_stacked`.", "parameters": {"type": "object", "properties": {"x": {}, "height": {}, "width": {"type": "float", "default": 0.8}, "bottom": {"type": "NoneType", "default": null}, "align": {"type": "str", "default": "center"}, "data": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/955", "data": {"name": "numpy.arange", "type": "callable", "signature": "(*args, **kwargs)", "description": "arange([start,] stop[, step,], dtype=None, *, like=None)\n\nReturn evenly spaced values within a given interval.\n\n``arange`` can be called with a varying number of positional arguments:\n\n* ``arange(stop)``: Values are generated within the half-open interval\n  ``[0, stop)`` (in other words, the interval including `start` but\n  excluding `stop`).\n* ``arange(start, stop)``: Values are generated within the half-open\n  interval ``[start, stop)``.\n* ``arange(start, stop, step)`` Values are generated within the half-open\n  interval ``[start, stop)``, with spacing between values given by\n  ``step``.\n\nFor integer arguments the function is roughly equivalent to the Python\nbuilt-in :py:class:`range`, but returns an ndarray rather than a ``range``\ninstance.\n\nWhen using a non-integer step, such as 0.1, it is often better to use\n`numpy.linspace`.\n\nSee the Warning sections below for more information.\n\nParameters\n----------\nstart : integer or real, optional\n    Start of interval.  The interval includes this value.  The default\n    start value is 0.\nstop : integer or real\n    End of interval.  The interval does not include this value, except\n    in some cases where `step` is not an integer and floating point\n    round-off affects the length of `out`.\nstep : integer or real, optional\n    Spacing between values.  For any output `out`, this is the distance\n    between two adjacent values, ``out[i+1] - out[i]``.  The default\n    step size is 1.  If `step` is specified as a position argument,\n    `start` must also be given.\ndtype : dtype, optional\n    The type of the output array.  If `dtype` is not given, infer the data\n    type from the other input arguments.\nlike : array_like, optional\n    Reference object to allow the creation of arrays which are not\n    NumPy arrays. If an array-like passed in as ``like`` supports\n    the ``__array_function__`` protocol, the result will be defined\n    by it. In this case, it ensures the creation of an array object\n    compatible with that passed in via this argument.\n\n    .. versionadded:: 1.20.0\n\nReturns\n-------\narange : ndarray\n    Array of evenly spaced values.\n\n    For floating point arguments, the length of the result is\n    ``ceil((stop - start)/step)``.  Because of floating point overflow,\n    this rule may result in the last element of `out` being greater\n    than `stop`.\n\nWarnings\n--------\nThe length of the output might not be numerically stable.\n\nAnother stability issue is due to the internal implementation of\n`numpy.arange`.\nThe actual step value used to populate the array is\n``dtype(start + step) - dtype(start)`` and not `step`. Precision loss\ncan occur here, due to casting or due to using floating points when\n`start` is much larger than `step`. This can lead to unexpected\nbehaviour. For example::\n\n  >>> np.arange(0, 5, 0.5, dtype=int)\n  array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n  >>> np.arange(-3, 3, 0.5, dtype=int)\n  array([-3, -2, -1,  0,  1,  2,  3,  4,  5,  6,  7,  8])\n\nIn such cases, the use of `numpy.linspace` should be preferred.\n\nThe built-in :py:class:`range` generates :std:doc:`Python built-in integers\nthat have arbitrary size <c-api/long>`, while `numpy.arange` produces\n`numpy.int32` or `numpy.int64` numbers. This may result in incorrect\nresults for large integer values::\n\n  >>> power = 40\n  >>> modulo = 10000\n  >>> x1 = [(n ** power) % modulo for n in range(8)]\n  >>> x2 = [(n ** power) % modulo for n in np.arange(8)]\n  >>> print(x1)\n  [0, 1, 7776, 8801, 6176, 625, 6576, 4001]  # correct\n  >>> print(x2)\n  [0, 1, 7776, 7185, 0, 5969, 4816, 3361]  # incorrect\n\nSee Also\n--------\nnumpy.linspace : Evenly spaced numbers with careful handling of endpoints.\nnumpy.ogrid: Arrays of evenly spaced numbers in N-dimensions.\nnumpy.mgrid: Grid-shaped arrays of evenly spaced numbers in N-dimensions.\n\nExamples\n--------\n>>> np.arange(3)\narray([0, 1, 2])\n>>> np.arange(3.0)\narray([ 0.,  1.,  2.])\n>>> np.arange(3,7)\narray([3, 4, 5, 6])\n>>> np.arange(3,7,2)\narray([3, 5])", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/955", "data": {"name": "re.IGNORECASE", "type": "constant", "signature": null, "description": "An enumeration.", "value": "re.IGNORECASE", "parameters": {"type": "constant"}}}
{"task_id": "BigCodeBench/955", "data": {"name": "re.sub", "type": "callable", "signature": "(pattern, repl, string, flags=0)", "description": "Return the string obtained by replacing the leftmost\nnon-overlapping occurrences of the pattern in string by the\nreplacement repl.  repl can be either a string or a callable;\nif a string, backslash escapes in it are processed.  If it is\na callable, it's passed the Match object and must return\na replacement string to be used.", "parameters": {"type": "object", "properties": {"pattern": {}, "repl": {}, "string": {}, "flags": {"type": "integer", "default": 0}}}}}
{"task_id": "BigCodeBench/955", "data": {"name": "re.sub.split", "type": "callable", "signature": "()", "description": "Return the string obtained by replacing the leftmost\nnon-overlapping occurrences of the pattern in string by the\nreplacement repl.  repl can be either a string or a callable;\nif a string, backslash escapes in it are processed.  If it is\na callable, it's passed the Match object and must return\na replacement string to be used.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/955", "data": {"name": "word.replace(' ', '_')"}}
{"task_id": "BigCodeBench/963", "data": {"name": "zipfile.ZipFile", "type": "class", "signature": "(file, mode='r', compression=0, allowZip64=True, compresslevel=None, *, strict_timestamps=True)", "description": "Class with methods to open, read, write, close, list zip files.\n\nz = ZipFile(file, mode=\"r\", compression=ZIP_STORED, allowZip64=True,\n            compresslevel=None)\n\nfile: Either the path to the file, or a file-like object.\n      If it is a path, the file will be opened and closed by ZipFile.\nmode: The mode can be either read 'r', write 'w', exclusive create 'x',\n      or append 'a'.\ncompression: ZIP_STORED (no compression), ZIP_DEFLATED (requires zlib),\n             ZIP_BZIP2 (requires bz2) or ZIP_LZMA (requires lzma).\nallowZip64: if True ZipFile will create files with ZIP64 extensions when\n            needed, otherwise it will raise an exception when this would\n            be necessary.\ncompresslevel: None (default for the given compression type) or an integer\n               specifying the level to pass to the compressor.\n               When using ZIP_STORED or ZIP_LZMA this keyword has no effect.\n               When using ZIP_DEFLATED integers 0 through 9 are accepted.\n               When using ZIP_BZIP2 integers 1 through 9 are accepted.", "parameters": {"type": "object", "properties": {"file": {}, "mode": {"type": "str", "default": "r"}, "compression": {"type": "integer", "default": 0}, "allowZip64": {"type": "bool", "default": true}, "compresslevel": {"type": "NoneType", "default": null}, "strict_timestamps": {"type": "bool", "default": true}}}}}
{"task_id": "BigCodeBench/963", "data": {"name": "zipfile.ZipFile.write", "type": "callable", "signature": "(self, filename, arcname=None)", "description": "Put the bytes from filename into the archive under the name\narcname.", "parameters": {"type": "object", "properties": {"filename": {}, "arcname": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/963", "data": {"name": "glob.glob", "type": "callable", "signature": "(pathname, recursive=False)", "description": "Return a list of paths matching a pathname pattern.\n\nThe pattern may contain simple shell-style wildcards a la\nfnmatch. However, unlike fnmatch, filenames starting with a\ndot are special cases that are not matched by '*' and '?'\npatterns.\n\nIf recursive is true, the pattern '**' will match any files and\nzero or more directories and subdirectories.", "parameters": {"type": "object", "properties": {"pathname": {}, "recursive": {"type": "bool", "default": false}}}}}
{"task_id": "BigCodeBench/963", "data": {"name": "os.path.abspath", "type": "callable", "signature": "(path)", "description": "Return an absolute path.", "parameters": {"type": "object", "properties": {"path": {}}}}}
{"task_id": "BigCodeBench/963", "data": {"name": "os.path.exists", "type": "callable", "signature": "(path)", "description": "Test whether a path exists.  Returns False for broken symbolic links", "parameters": {"type": "object", "properties": {"path": {}}}}}
{"task_id": "BigCodeBench/963", "data": {"name": "os.makedirs", "type": "callable", "signature": "(name, exist_ok=False)", "description": "makedirs(name [, mode=0o777][, exist_ok=False])\n\nSuper-mkdir; create a leaf directory and all intermediate ones.  Works like\nmkdir, except that any intermediate path segment (not just the rightmost)\nwill be created if it does not exist. If the target directory already\nexists, raise an OSError if exist_ok is False. Otherwise no exception is\nraised.  This is recursive.", "parameters": {"type": "object", "properties": {"name": {}, "exist_ok": {"type": "bool", "default": false}}}}}
{"task_id": "BigCodeBench/963", "data": {"name": "os.path.join", "type": "callable", "signature": "(a, *p)", "description": "Join two or more pathname components, inserting '/' as needed.\nIf any component is an absolute path, all previous path components\nwill be discarded.  An empty last part will result in a path that\nends with a separator.", "parameters": {"type": "object", "properties": {"a": {}}}}}
{"task_id": "BigCodeBench/963", "data": {"name": "zip_name.strip()"}}
{"task_id": "BigCodeBench/964", "data": {"name": "pandas.DataFrame", "type": "class", "signature": "(data=None, index: 'Axes | None' = None, columns: 'Axes | None' = None, dtype: 'Dtype | None' = None, copy: 'bool | None' = None) -> 'None'", "description": "Two-dimensional, size-mutable, potentially heterogeneous tabular data.\n\nData structure also contains labeled axes (rows and columns).\nArithmetic operations align on both row and column labels. Can be\nthought of as a dict-like container for Series objects. The primary\npandas data structure.\n\nParameters\n----------\ndata : ndarray (structured or homogeneous), Iterable, dict, or DataFrame\n    Dict can contain Series, arrays, constants, dataclass or list-like objects. If\n    data is a dict, column order follows insertion-order. If a dict contains Series\n    which have an index defined, it is aligned by its index. This alignment also\n    occurs if data is a Series or a DataFrame itself. Alignment is done on\n    Series/DataFrame inputs.\n\n    If data is a list of dicts, column order follows insertion-order.\n\nindex : Index or array-like\n    Index to use for resulting frame. Will default to RangeIndex if\n    no indexing information part of input data and no index provided.\ncolumns : Index or array-like\n    Column labels to use for resulting frame when data does not have them,\n    defaulting to RangeIndex(0, 1, 2, ..., n). If data contains column labels,\n    will perform column selection instead.\ndtype : dtype, default None\n    Data type to force. Only a single dtype is allowed. If None, infer.\ncopy : bool or None, default None\n    Copy data from inputs.\n    For dict data, the default of None behaves like ``copy=True``.  For DataFrame\n    or 2d ndarray input, the default of None behaves like ``copy=False``.\n    If data is a dict containing one or more Series (possibly of different dtypes),\n    ``copy=False`` will ensure that these inputs are not copied.\n\n    .. versionchanged:: 1.3.0\n\nSee Also\n--------\nDataFrame.from_records : Constructor from tuples, also record arrays.\nDataFrame.from_dict : From dicts of Series, arrays, or dicts.\nread_csv : Read a comma-separated values (csv) file into DataFrame.\nread_table : Read general delimited file into DataFrame.\nread_clipboard : Read text from clipboard into DataFrame.\n\nNotes\n-----\nPlease reference the :ref:`User Guide <basics.dataframe>` for more information.\n\nExamples\n--------\nConstructing DataFrame from a dictionary.\n\n>>> d = {'col1': [1, 2], 'col2': [3, 4]}\n>>> df = pd.DataFrame(data=d)\n>>> df\n   col1  col2\n0     1     3\n1     2     4\n\nNotice that the inferred dtype is int64.\n\n>>> df.dtypes\ncol1    int64\ncol2    int64\ndtype: object\n\nTo enforce a single dtype:\n\n>>> df = pd.DataFrame(data=d, dtype=np.int8)\n>>> df.dtypes\ncol1    int8\ncol2    int8\ndtype: object\n\nConstructing DataFrame from a dictionary including Series:\n\n>>> d = {'col1': [0, 1, 2, 3], 'col2': pd.Series([2, 3], index=[2, 3])}\n>>> pd.DataFrame(data=d, index=[0, 1, 2, 3])\n   col1  col2\n0     0   NaN\n1     1   NaN\n2     2   2.0\n3     3   3.0\n\nConstructing DataFrame from numpy ndarray:\n\n>>> df2 = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),\n...                    columns=['a', 'b', 'c'])\n>>> df2\n   a  b  c\n0  1  2  3\n1  4  5  6\n2  7  8  9\n\nConstructing DataFrame from a numpy ndarray that has labeled columns:\n\n>>> data = np.array([(1, 2, 3), (4, 5, 6), (7, 8, 9)],\n...                 dtype=[(\"a\", \"i4\"), (\"b\", \"i4\"), (\"c\", \"i4\")])\n>>> df3 = pd.DataFrame(data, columns=['c', 'a'])\n...\n>>> df3\n   c  a\n0  3  1\n1  6  4\n2  9  7\n\nConstructing DataFrame from dataclass:\n\n>>> from dataclasses import make_dataclass\n>>> Point = make_dataclass(\"Point\", [(\"x\", int), (\"y\", int)])\n>>> pd.DataFrame([Point(0, 0), Point(0, 3), Point(2, 3)])\n   x  y\n0  0  0\n1  0  3\n2  2  3\n\nConstructing DataFrame from Series/DataFrame:\n\n>>> ser = pd.Series([1, 2, 3], index=[\"a\", \"b\", \"c\"])\n>>> df = pd.DataFrame(data=ser, index=[\"a\", \"c\"])\n>>> df\n   0\na  1\nc  3\n\n>>> df1 = pd.DataFrame([1, 2, 3], index=[\"a\", \"b\", \"c\"], columns=[\"x\"])\n>>> df2 = pd.DataFrame(data=df1, index=[\"a\", \"c\"])\n>>> df2\n   x\na  1\nc  3", "parameters": {"type": "object", "properties": {"data": {"type": "NoneType", "default": null}, "index": {"type": ["axes", "null"], "default": null}, "columns": {"type": ["axes", "null"], "default": null}, "dtype": {"type": ["dtype", "null"], "default": null}, "copy": {"type": ["bool", "null"], "default": null}}}}}
{"task_id": "BigCodeBench/964", "data": {"name": "pandas.DataFrame.to_csv", "type": "callable", "signature": "(self, path_or_buf: 'FilePath | WriteBuffer[bytes] | WriteBuffer[str] | None' = None, index: 'bool_t' = True)", "description": "Write object to a comma-separated values (csv) file.\n\nParameters\n----------\npath_or_buf : str, path object, file-like object, or None, default None\n    String, path object (implementing os.PathLike[str]), or file-like\n    object implementing a write() function. If None, the result is\n    returned as a string. If a non-binary file object is passed, it should\n    be opened with `newline=''`, disabling universal newlines. If a binary\n    file object is passed, `mode` might need to contain a `'b'`.\nsep : str, default ','\n    String of length 1. Field delimiter for the output file.\nna_rep : str, default ''\n    Missing data representation.\nfloat_format : str, Callable, default None\n    Format string for floating point numbers. If a Callable is given, it takes\n    precedence over other numeric formatting parameters, like decimal.\ncolumns : sequence, optional\n    Columns to write.\nheader : bool or list of str, default True\n    Write out the column names. If a list of strings is given it is\n    assumed to be aliases for the column names.\nindex : bool, default True\n    Write row names (index).\nindex_label : str or sequence, or False, default None\n    Column label for index column(s) if desired. If None is given, and\n    `header` and `index` are True, then the index names are used. A\n    sequence should be given if the object uses MultiIndex. If\n    False do not print fields for index names. Use index_label=False\n    for easier importing in R.\nmode : {'w', 'x', 'a'}, default 'w'\n    Forwarded to either `open(mode=)` or `fsspec.open(mode=)` to control\n    the file opening. Typical values include:\n\n    - 'w', truncate the file first.\n    - 'x', exclusive creation, failing if the file already exists.\n    - 'a', append to the end of file if it exists.\n\nencoding : str, optional\n    A string representing the encoding to use in the output file,\n    defaults to 'utf-8'. `encoding` is not supported if `path_or_buf`\n    is a non-binary file object.\ncompression : str or dict, default 'infer'\n    For on-the-fly compression of the output data. If 'infer' and 'path_or_buf' is\n    path-like, then detect compression from the following extensions: '.gz',\n    '.bz2', '.zip', '.xz', '.zst', '.tar', '.tar.gz', '.tar.xz' or '.tar.bz2'\n    (otherwise no compression).\n    Set to ``None`` for no compression.\n    Can also be a dict with key ``'method'`` set\n    to one of {``'zip'``, ``'gzip'``, ``'bz2'``, ``'zstd'``, ``'xz'``, ``'tar'``} and\n    other key-value pairs are forwarded to\n    ``zipfile.ZipFile``, ``gzip.GzipFile``,\n    ``bz2.BZ2File``, ``zstandard.ZstdCompressor``, ``lzma.LZMAFile`` or\n    ``tarfile.TarFile``, respectively.\n    As an example, the following could be passed for faster compression and to create\n    a reproducible gzip archive:\n    ``compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}``.\n\n    .. versionadded:: 1.5.0\n        Added support for `.tar` files.\n\n       May be a dict with key 'method' as compression mode\n       and other entries as additional compression options if\n       compression mode is 'zip'.\n\n       Passing compression options as keys in dict is\n       supported for compression modes 'gzip', 'bz2', 'zstd', and 'zip'.\nquoting : optional constant from csv module\n    Defaults to csv.QUOTE_MINIMAL. If you have set a `float_format`\n    then floats are converted to strings and thus csv.QUOTE_NONNUMERIC\n    will treat them as non-numeric.\nquotechar : str, default '\\\"'\n    String of length 1. Character used to quote fields.\nlineterminator : str, optional\n    The newline character or character sequence to use in the output\n    file. Defaults to `os.linesep`, which depends on the OS in which\n    this method is called ('\\\\n' for linux, '\\\\r\\\\n' for Windows, i.e.).\n\n    .. versionchanged:: 1.5.0\n\n        Previously was line_terminator, changed for consistency with\n        read_csv and the standard library 'csv' module.\n\nchunksize : int or None\n    Rows to write at a time.\ndate_format : str, default None\n    Format string for datetime objects.\ndoublequote : bool, default True\n    Control quoting of `quotechar` inside a field.\nescapechar : str, default None\n    String of length 1. Character used to escape `sep` and `quotechar`\n    when appropriate.\ndecimal : str, default '.'\n    Character recognized as decimal separator. E.g. use ',' for\n    European data.\nerrors : str, default 'strict'\n    Specifies how encoding and decoding errors are to be handled.\n    See the errors argument for :func:`open` for a full list\n    of options.\n\nstorage_options : dict, optional\n    Extra options that make sense for a particular storage connection, e.g.\n    host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n    are forwarded to ``urllib.request.Request`` as header options. For other\n    URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n    forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n    details, and for more examples on storage options refer `here\n    <https://pandas.pydata.org/docs/user_guide/io.html?\n    highlight=storage_options#reading-writing-remote-files>`_.\n\nReturns\n-------\nNone or str\n    If path_or_buf is None, returns the resulting csv format as a\n    string. Otherwise returns None.\n\nSee Also\n--------\nread_csv : Load a CSV file into a DataFrame.\nto_excel : Write DataFrame to an Excel file.\n\nExamples\n--------\nCreate 'out.csv' containing 'df' without indices\n\n>>> df = pd.DataFrame({'name': ['Raphael', 'Donatello'],\n...                    'mask': ['red', 'purple'],\n...                    'weapon': ['sai', 'bo staff']})\n>>> df.to_csv('out.csv', index=False)  # doctest: +SKIP\n\nCreate 'out.zip' containing 'out.csv'\n\n>>> df.to_csv(index=False)\n'name,mask,weapon\\nRaphael,red,sai\\nDonatello,purple,bo staff\\n'\n>>> compression_opts = dict(method='zip',\n...                         archive_name='out.csv')  # doctest: +SKIP\n>>> df.to_csv('out.zip', index=False,\n...           compression=compression_opts)  # doctest: +SKIP\n\nTo write a csv file to a new folder or nested folder you will first\nneed to create it using either Pathlib or os:\n\n>>> from pathlib import Path  # doctest: +SKIP\n>>> filepath = Path('folder/subfolder/out.csv')  # doctest: +SKIP\n>>> filepath.parent.mkdir(parents=True, exist_ok=True)  # doctest: +SKIP\n>>> df.to_csv(filepath)  # doctest: +SKIP\n\n>>> import os  # doctest: +SKIP\n>>> os.makedirs('folder/subfolder', exist_ok=True)  # doctest: +SKIP\n>>> df.to_csv('folder/subfolder/out.csv')  # doctest: +SKIP", "parameters": {"type": "object", "properties": {"path_or_buf": {"type": ["writebuffer[bytes]", "null", "writebuffer[str]", "filepath"], "default": null}, "index": {"type": "bool_t", "default": true}}}}}
{"task_id": "BigCodeBench/964", "data": {"name": "docx.Document", "type": "callable", "signature": "(docx: 'str | IO[bytes] | None' = None)", "description": "Return a |Document| object loaded from `docx`, where `docx` can be either a path\nto a ``.docx`` file (a string) or a file-like object.\n\nIf `docx` is missing or ``None``, the built-in default document \"template\" is\nloaded.", "parameters": {"type": "object", "properties": {"docx": {"type": ["io[bytes]", "str", "null"], "default": null}}}}}
{"task_id": "BigCodeBench/964", "data": {"name": "docx.Document.paragraphs", "type": "callable", "signature": "(docx: 'str | IO[bytes] | None' = None)", "description": "Return a |Document| object loaded from `docx`, where `docx` can be either a path\nto a ``.docx`` file (a string) or a file-like object.\n\nIf `docx` is missing or ``None``, the built-in default document \"template\" is\nloaded.", "parameters": {"type": "object", "properties": {"docx": {"type": ["io[bytes]", "str", "null"], "default": null}}}}}
{"task_id": "BigCodeBench/964", "data": {"name": "open.readlines()"}}
{"task_id": "BigCodeBench/964", "data": {"name": "os.path.join", "type": "callable", "signature": "(a, *p)", "description": "Join two or more pathname components, inserting '/' as needed.\nIf any component is an absolute path, all previous path components\nwill be discarded.  An empty last part will result in a path that\nends with a separator.", "parameters": {"type": "object", "properties": {"a": {}}}}}
{"task_id": "BigCodeBench/964", "data": {"name": "os.walk", "type": "callable", "signature": "(top)", "description": "Directory tree generator.\n\nFor each directory in the directory tree rooted at top (including top\nitself, but excluding '.' and '..'), yields a 3-tuple\n\n    dirpath, dirnames, filenames\n\ndirpath is a string, the path to the directory.  dirnames is a list of\nthe names of the subdirectories in dirpath (excluding '.' and '..').\nfilenames is a list of the names of the non-directory files in dirpath.\nNote that the names in the lists are just names, with no path components.\nTo get a full path (which begins with top) to a file or directory in\ndirpath, do os.path.join(dirpath, name).\n\nIf optional arg 'topdown' is true or not specified, the triple for a\ndirectory is generated before the triples for any of its subdirectories\n(directories are generated top down).  If topdown is false, the triple\nfor a directory is generated after the triples for all of its\nsubdirectories (directories are generated bottom up).\n\nWhen topdown is true, the caller can modify the dirnames list in-place\n(e.g., via del or slice assignment), and walk will only recurse into the\nsubdirectories whose names remain in dirnames; this can be used to prune the\nsearch, or to impose a specific order of visiting.  Modifying dirnames when\ntopdown is false has no effect on the behavior of os.walk(), since the\ndirectories in dirnames have already been generated by the time dirnames\nitself is generated. No matter the value of topdown, the list of\nsubdirectories is retrieved before the tuples for the directory and its\nsubdirectories are generated.\n\nBy default errors from the os.scandir() call are ignored.  If\noptional arg 'onerror' is specified, it should be a function; it\nwill be called with one argument, an OSError instance.  It can\nreport the error to continue with the walk, or raise the exception\nto abort the walk.  Note that the filename is available as the\nfilename attribute of the exception object.\n\nBy default, os.walk does not follow symbolic links to subdirectories on\nsystems that support them.  In order to get this functionality, set the\noptional argument 'followlinks' to true.\n\nCaution:  if you pass a relative pathname for top, don't change the\ncurrent working directory between resumptions of walk.  walk never\nchanges the current directory, and assumes that the client doesn't\neither.\n\nExample:\n\nimport os\nfrom os.path import join, getsize\nfor root, dirs, files in os.walk('python/Lib/email'):\n    print(root, \"consumes\", end=\"\")\n    print(sum(getsize(join(root, name)) for name in files), end=\"\")\n    print(\"bytes in\", len(files), \"non-directory files\")\n    if 'CVS' in dirs:\n        dirs.remove('CVS')  # don't visit CVS directories", "parameters": {"type": "object", "properties": {"top": {}}}}}
{"task_id": "BigCodeBench/964", "data": {"name": "os.path.exists", "type": "callable", "signature": "(path)", "description": "Test whether a path exists.  Returns False for broken symbolic links", "parameters": {"type": "object", "properties": {"path": {}}}}}
{"task_id": "BigCodeBench/964", "data": {"name": "os.makedirs", "type": "callable", "signature": "(name, exist_ok=False)", "description": "makedirs(name [, mode=0o777][, exist_ok=False])\n\nSuper-mkdir; create a leaf directory and all intermediate ones.  Works like\nmkdir, except that any intermediate path segment (not just the rightmost)\nwill be created if it does not exist. If the target directory already\nexists, raise an OSError if exist_ok is False. Otherwise no exception is\nraised.  This is recursive.", "parameters": {"type": "object", "properties": {"name": {}, "exist_ok": {"type": "bool", "default": false}}}}}
{"task_id": "BigCodeBench/964", "data": {"name": "p.text"}}
{"task_id": "BigCodeBench/964", "data": {"name": "pandas.read_csv", "type": "callable", "signature": "(filepath_or_buffer: 'FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str]')", "description": "Read a comma-separated values (csv) file into DataFrame.\n\nAlso supports optionally iterating or breaking of the file\ninto chunks.\n\nAdditional help can be found in the online docs for\n`IO Tools <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html>`_.\n\nParameters\n----------\nfilepath_or_buffer : str, path object or file-like object\n    Any valid string path is acceptable. The string could be a URL. Valid\n    URL schemes include http, ftp, s3, gs, and file. For file URLs, a host is\n    expected. A local file could be: file://localhost/path/to/table.csv.\n\n    If you want to pass in a path object, pandas accepts any ``os.PathLike``.\n\n    By file-like object, we refer to objects with a ``read()`` method, such as\n    a file handle (e.g. via builtin ``open`` function) or ``StringIO``.\nsep : str, default ','\n    Character or regex pattern to treat as the delimiter. If ``sep=None``, the\n    C engine cannot automatically detect\n    the separator, but the Python parsing engine can, meaning the latter will\n    be used and automatically detect the separator from only the first valid\n    row of the file by Python's builtin sniffer tool, ``csv.Sniffer``.\n    In addition, separators longer than 1 character and different from\n    ``'\\s+'`` will be interpreted as regular expressions and will also force\n    the use of the Python parsing engine. Note that regex delimiters are prone\n    to ignoring quoted data. Regex example: ``'\\r\\t'``.\ndelimiter : str, optional\n    Alias for ``sep``.\nheader : int, Sequence of int, 'infer' or None, default 'infer'\n    Row number(s) containing column labels and marking the start of the\n    data (zero-indexed). Default behavior is to infer the column names: if no ``names``\n    are passed the behavior is identical to ``header=0`` and column\n    names are inferred from the first line of the file, if column\n    names are passed explicitly to ``names`` then the behavior is identical to\n    ``header=None``. Explicitly pass ``header=0`` to be able to\n    replace existing names. The header can be a list of integers that\n    specify row locations for a :class:`~pandas.MultiIndex` on the columns\n    e.g. ``[0, 1, 3]``. Intervening rows that are not specified will be\n    skipped (e.g. 2 in this example is skipped). Note that this\n    parameter ignores commented lines and empty lines if\n    ``skip_blank_lines=True``, so ``header=0`` denotes the first line of\n    data rather than the first line of the file.\nnames : Sequence of Hashable, optional\n    Sequence of column labels to apply. If the file contains a header row,\n    then you should explicitly pass ``header=0`` to override the column names.\n    Duplicates in this list are not allowed.\nindex_col : Hashable, Sequence of Hashable or False, optional\n  Column(s) to use as row label(s), denoted either by column labels or column\n  indices.  If a sequence of labels or indices is given, :class:`~pandas.MultiIndex`\n  will be formed for the row labels.\n\n  Note: ``index_col=False`` can be used to force pandas to *not* use the first\n  column as the index, e.g., when you have a malformed file with delimiters at\n  the end of each line.\nusecols : Sequence of Hashable or Callable, optional\n    Subset of columns to select, denoted either by column labels or column indices.\n    If list-like, all elements must either\n    be positional (i.e. integer indices into the document columns) or strings\n    that correspond to column names provided either by the user in ``names`` or\n    inferred from the document header row(s). If ``names`` are given, the document\n    header row(s) are not taken into account. For example, a valid list-like\n    ``usecols`` parameter would be ``[0, 1, 2]`` or ``['foo', 'bar', 'baz']``.\n    Element order is ignored, so ``usecols=[0, 1]`` is the same as ``[1, 0]``.\n    To instantiate a :class:`~pandas.DataFrame` from ``data`` with element order\n    preserved use ``pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']]``\n    for columns in ``['foo', 'bar']`` order or\n    ``pd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']]``\n    for ``['bar', 'foo']`` order.\n\n    If callable, the callable function will be evaluated against the column\n    names, returning names where the callable function evaluates to ``True``. An\n    example of a valid callable argument would be ``lambda x: x.upper() in\n    ['AAA', 'BBB', 'DDD']``. Using this parameter results in much faster\n    parsing time and lower memory usage.\ndtype : dtype or dict of {Hashable : dtype}, optional\n    Data type(s) to apply to either the whole dataset or individual columns.\n    E.g., ``{'a': np.float64, 'b': np.int32, 'c': 'Int64'}``\n    Use ``str`` or ``object`` together with suitable ``na_values`` settings\n    to preserve and not interpret ``dtype``.\n    If ``converters`` are specified, they will be applied INSTEAD\n    of ``dtype`` conversion.\n\n    .. versionadded:: 1.5.0\n\n        Support for ``defaultdict`` was added. Specify a ``defaultdict`` as input where\n        the default determines the ``dtype`` of the columns which are not explicitly\n        listed.\nengine : {'c', 'python', 'pyarrow'}, optional\n    Parser engine to use. The C and pyarrow engines are faster, while the python engine\n    is currently more feature-complete. Multithreading is currently only supported by\n    the pyarrow engine.\n\n    .. versionadded:: 1.4.0\n\n        The 'pyarrow' engine was added as an *experimental* engine, and some features\n        are unsupported, or may not work correctly, with this engine.\nconverters : dict of {Hashable : Callable}, optional\n    Functions for converting values in specified columns. Keys can either\n    be column labels or column indices.\ntrue_values : list, optional\n    Values to consider as ``True`` in addition to case-insensitive variants of 'True'.\nfalse_values : list, optional\n    Values to consider as ``False`` in addition to case-insensitive variants of 'False'.\nskipinitialspace : bool, default False\n    Skip spaces after delimiter.\nskiprows : int, list of int or Callable, optional\n    Line numbers to skip (0-indexed) or number of lines to skip (``int``)\n    at the start of the file.\n\n    If callable, the callable function will be evaluated against the row\n    indices, returning ``True`` if the row should be skipped and ``False`` otherwise.\n    An example of a valid callable argument would be ``lambda x: x in [0, 2]``.\nskipfooter : int, default 0\n    Number of lines at bottom of file to skip (Unsupported with ``engine='c'``).\nnrows : int, optional\n    Number of rows of file to read. Useful for reading pieces of large files.\nna_values : Hashable, Iterable of Hashable or dict of {Hashable : Iterable}, optional\n    Additional strings to recognize as ``NA``/``NaN``. If ``dict`` passed, specific\n    per-column ``NA`` values.  By default the following values are interpreted as\n    ``NaN``: \" \", \"#N/A\", \"#N/A N/A\", \"#NA\", \"-1.#IND\", \"-1.#QNAN\", \"-NaN\", \"-nan\",\n    \"1.#IND\", \"1.#QNAN\", \"<NA>\", \"N/A\", \"NA\", \"NULL\", \"NaN\", \"None\",\n    \"n/a\", \"nan\", \"null \".\n\nkeep_default_na : bool, default True\n    Whether or not to include the default ``NaN`` values when parsing the data.\n    Depending on whether ``na_values`` is passed in, the behavior is as follows:\n\n    * If ``keep_default_na`` is ``True``, and ``na_values`` are specified, ``na_values``\n      is appended to the default ``NaN`` values used for parsing.\n    * If ``keep_default_na`` is ``True``, and ``na_values`` are not specified, only\n      the default ``NaN`` values are used for parsing.\n    * If ``keep_default_na`` is ``False``, and ``na_values`` are specified, only\n      the ``NaN`` values specified ``na_values`` are used for parsing.\n    * If ``keep_default_na`` is ``False``, and ``na_values`` are not specified, no\n      strings will be parsed as ``NaN``.\n\n    Note that if ``na_filter`` is passed in as ``False``, the ``keep_default_na`` and\n    ``na_values`` parameters will be ignored.\nna_filter : bool, default True\n    Detect missing value markers (empty strings and the value of ``na_values``). In\n    data without any ``NA`` values, passing ``na_filter=False`` can improve the\n    performance of reading a large file.\nverbose : bool, default False\n    Indicate number of ``NA`` values placed in non-numeric columns.\n\n    .. deprecated:: 2.2.0\nskip_blank_lines : bool, default True\n    If ``True``, skip over blank lines rather than interpreting as ``NaN`` values.\nparse_dates : bool, list of Hashable, list of lists or dict of {Hashable : list}, default False\n    The behavior is as follows:\n\n    * ``bool``. If ``True`` -> try parsing the index. Note: Automatically set to\n      ``True`` if ``date_format`` or ``date_parser`` arguments have been passed.\n    * ``list`` of ``int`` or names. e.g. If ``[1, 2, 3]`` -> try parsing columns 1, 2, 3\n      each as a separate date column.\n    * ``list`` of ``list``. e.g.  If ``[[1, 3]]`` -> combine columns 1 and 3 and parse\n      as a single date column. Values are joined with a space before parsing.\n    * ``dict``, e.g. ``{'foo' : [1, 3]}`` -> parse columns 1, 3 as date and call\n      result 'foo'. Values are joined with a space before parsing.\n\n    If a column or index cannot be represented as an array of ``datetime``,\n    say because of an unparsable value or a mixture of timezones, the column\n    or index will be returned unaltered as an ``object`` data type. For\n    non-standard ``datetime`` parsing, use :func:`~pandas.to_datetime` after\n    :func:`~pandas.read_csv`.\n\n    Note: A fast-path exists for iso8601-formatted dates.\ninfer_datetime_format : bool, default False\n    If ``True`` and ``parse_dates`` is enabled, pandas will attempt to infer the\n    format of the ``datetime`` strings in the columns, and if it can be inferred,\n    switch to a faster method of parsing them. In some cases this can increase\n    the parsing speed by 5-10x.\n\n    .. deprecated:: 2.0.0\n        A strict version of this argument is now the default, passing it has no effect.\n\nkeep_date_col : bool, default False\n    If ``True`` and ``parse_dates`` specifies combining multiple columns then\n    keep the original columns.\ndate_parser : Callable, optional\n    Function to use for converting a sequence of string columns to an array of\n    ``datetime`` instances. The default uses ``dateutil.parser.parser`` to do the\n    conversion. pandas will try to call ``date_parser`` in three different ways,\n    advancing to the next if an exception occurs: 1) Pass one or more arrays\n    (as defined by ``parse_dates``) as arguments; 2) concatenate (row-wise) the\n    string values from the columns defined by ``parse_dates`` into a single array\n    and pass that; and 3) call ``date_parser`` once for each row using one or\n    more strings (corresponding to the columns defined by ``parse_dates``) as\n    arguments.\n\n    .. deprecated:: 2.0.0\n       Use ``date_format`` instead, or read in as ``object`` and then apply\n       :func:`~pandas.to_datetime` as-needed.\ndate_format : str or dict of column -> format, optional\n    Format to use for parsing dates when used in conjunction with ``parse_dates``.\n    The strftime to parse time, e.g. :const:`\"%d/%m/%Y\"`. See\n    `strftime documentation\n    <https://docs.python.org/3/library/datetime.html\n    #strftime-and-strptime-behavior>`_ for more information on choices, though\n    note that :const:`\"%f\"` will parse all the way up to nanoseconds.\n    You can also pass:\n\n    - \"ISO8601\", to parse any `ISO8601 <https://en.wikipedia.org/wiki/ISO_8601>`_\n        time string (not necessarily in exactly the same format);\n    - \"mixed\", to infer the format for each element individually. This is risky,\n        and you should probably use it along with `dayfirst`.\n\n    .. versionadded:: 2.0.0\ndayfirst : bool, default False\n    DD/MM format dates, international and European format.\ncache_dates : bool, default True\n    If ``True``, use a cache of unique, converted dates to apply the ``datetime``\n    conversion. May produce significant speed-up when parsing duplicate\n    date strings, especially ones with timezone offsets.\n\niterator : bool, default False\n    Return ``TextFileReader`` object for iteration or getting chunks with\n    ``get_chunk()``.\nchunksize : int, optional\n    Number of lines to read from the file per chunk. Passing a value will cause the\n    function to return a ``TextFileReader`` object for iteration.\n    See the `IO Tools docs\n    <https://pandas.pydata.org/pandas-docs/stable/io.html#io-chunking>`_\n    for more information on ``iterator`` and ``chunksize``.\n\ncompression : str or dict, default 'infer'\n    For on-the-fly decompression of on-disk data. If 'infer' and 'filepath_or_buffer' is\n    path-like, then detect compression from the following extensions: '.gz',\n    '.bz2', '.zip', '.xz', '.zst', '.tar', '.tar.gz', '.tar.xz' or '.tar.bz2'\n    (otherwise no compression).\n    If using 'zip' or 'tar', the ZIP file must contain only one data file to be read in.\n    Set to ``None`` for no decompression.\n    Can also be a dict with key ``'method'`` set\n    to one of {``'zip'``, ``'gzip'``, ``'bz2'``, ``'zstd'``, ``'xz'``, ``'tar'``} and\n    other key-value pairs are forwarded to\n    ``zipfile.ZipFile``, ``gzip.GzipFile``,\n    ``bz2.BZ2File``, ``zstandard.ZstdDecompressor``, ``lzma.LZMAFile`` or\n    ``tarfile.TarFile``, respectively.\n    As an example, the following could be passed for Zstandard decompression using a\n    custom compression dictionary:\n    ``compression={'method': 'zstd', 'dict_data': my_compression_dict}``.\n\n    .. versionadded:: 1.5.0\n        Added support for `.tar` files.\n\n    .. versionchanged:: 1.4.0 Zstandard support.\n\nthousands : str (length 1), optional\n    Character acting as the thousands separator in numerical values.\ndecimal : str (length 1), default '.'\n    Character to recognize as decimal point (e.g., use ',' for European data).\nlineterminator : str (length 1), optional\n    Character used to denote a line break. Only valid with C parser.\nquotechar : str (length 1), optional\n    Character used to denote the start and end of a quoted item. Quoted\n    items can include the ``delimiter`` and it will be ignored.\nquoting : {0 or csv.QUOTE_MINIMAL, 1 or csv.QUOTE_ALL, 2 or csv.QUOTE_NONNUMERIC, 3 or csv.QUOTE_NONE}, default csv.QUOTE_MINIMAL\n    Control field quoting behavior per ``csv.QUOTE_*`` constants. Default is\n    ``csv.QUOTE_MINIMAL`` (i.e., 0) which implies that only fields containing special\n    characters are quoted (e.g., characters defined in ``quotechar``, ``delimiter``,\n    or ``lineterminator``.\ndoublequote : bool, default True\n   When ``quotechar`` is specified and ``quoting`` is not ``QUOTE_NONE``, indicate\n   whether or not to interpret two consecutive ``quotechar`` elements INSIDE a\n   field as a single ``quotechar`` element.\nescapechar : str (length 1), optional\n    Character used to escape other characters.\ncomment : str (length 1), optional\n    Character indicating that the remainder of line should not be parsed.\n    If found at the beginning\n    of a line, the line will be ignored altogether. This parameter must be a\n    single character. Like empty lines (as long as ``skip_blank_lines=True``),\n    fully commented lines are ignored by the parameter ``header`` but not by\n    ``skiprows``. For example, if ``comment='#'``, parsing\n    ``#empty\\na,b,c\\n1,2,3`` with ``header=0`` will result in ``'a,b,c'`` being\n    treated as the header.\nencoding : str, optional, default 'utf-8'\n    Encoding to use for UTF when reading/writing (ex. ``'utf-8'``). `List of Python\n    standard encodings\n    <https://docs.python.org/3/library/codecs.html#standard-encodings>`_ .\n\nencoding_errors : str, optional, default 'strict'\n    How encoding errors are treated. `List of possible values\n    <https://docs.python.org/3/library/codecs.html#error-handlers>`_ .\n\n    .. versionadded:: 1.3.0\n\ndialect : str or csv.Dialect, optional\n    If provided, this parameter will override values (default or not) for the\n    following parameters: ``delimiter``, ``doublequote``, ``escapechar``,\n    ``skipinitialspace``, ``quotechar``, and ``quoting``. If it is necessary to\n    override values, a ``ParserWarning`` will be issued. See ``csv.Dialect``\n    documentation for more details.\non_bad_lines : {'error', 'warn', 'skip'} or Callable, default 'error'\n    Specifies what to do upon encountering a bad line (a line with too many fields).\n    Allowed values are :\n\n    - ``'error'``, raise an Exception when a bad line is encountered.\n    - ``'warn'``, raise a warning when a bad line is encountered and skip that line.\n    - ``'skip'``, skip bad lines without raising or warning when they are encountered.\n\n    .. versionadded:: 1.3.0\n\n    .. versionadded:: 1.4.0\n\n        - Callable, function with signature\n          ``(bad_line: list[str]) -> list[str] | None`` that will process a single\n          bad line. ``bad_line`` is a list of strings split by the ``sep``.\n          If the function returns ``None``, the bad line will be ignored.\n          If the function returns a new ``list`` of strings with more elements than\n          expected, a ``ParserWarning`` will be emitted while dropping extra elements.\n          Only supported when ``engine='python'``\n\n    .. versionchanged:: 2.2.0\n\n        - Callable, function with signature\n          as described in `pyarrow documentation\n          <https://arrow.apache.org/docs/python/generated/pyarrow.csv.ParseOptions.html\n          #pyarrow.csv.ParseOptions.invalid_row_handler>`_ when ``engine='pyarrow'``\n\ndelim_whitespace : bool, default False\n    Specifies whether or not whitespace (e.g. ``' '`` or ``'\\t'``) will be\n    used as the ``sep`` delimiter. Equivalent to setting ``sep='\\s+'``. If this option\n    is set to ``True``, nothing should be passed in for the ``delimiter``\n    parameter.\n\n    .. deprecated:: 2.2.0\n        Use ``sep=\"\\s+\"`` instead.\nlow_memory : bool, default True\n    Internally process the file in chunks, resulting in lower memory use\n    while parsing, but possibly mixed type inference.  To ensure no mixed\n    types either set ``False``, or specify the type with the ``dtype`` parameter.\n    Note that the entire file is read into a single :class:`~pandas.DataFrame`\n    regardless, use the ``chunksize`` or ``iterator`` parameter to return the data in\n    chunks. (Only valid with C parser).\nmemory_map : bool, default False\n    If a filepath is provided for ``filepath_or_buffer``, map the file object\n    directly onto memory and access the data directly from there. Using this\n    option can improve performance because there is no longer any I/O overhead.\nfloat_precision : {'high', 'legacy', 'round_trip'}, optional\n    Specifies which converter the C engine should use for floating-point\n    values. The options are ``None`` or ``'high'`` for the ordinary converter,\n    ``'legacy'`` for the original lower precision pandas converter, and\n    ``'round_trip'`` for the round-trip converter.\n\nstorage_options : dict, optional\n    Extra options that make sense for a particular storage connection, e.g.\n    host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n    are forwarded to ``urllib.request.Request`` as header options. For other\n    URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n    forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n    details, and for more examples on storage options refer `here\n    <https://pandas.pydata.org/docs/user_guide/io.html?\n    highlight=storage_options#reading-writing-remote-files>`_.\n\ndtype_backend : {'numpy_nullable', 'pyarrow'}, default 'numpy_nullable'\n    Back-end data type applied to the resultant :class:`DataFrame`\n    (still experimental). Behaviour is as follows:\n\n    * ``\"numpy_nullable\"``: returns nullable-dtype-backed :class:`DataFrame`\n      (default).\n    * ``\"pyarrow\"``: returns pyarrow-backed nullable :class:`ArrowDtype`\n      DataFrame.\n\n    .. versionadded:: 2.0\n\nReturns\n-------\nDataFrame or TextFileReader\n    A comma-separated values (csv) file is returned as two-dimensional\n    data structure with labeled axes.\n\nSee Also\n--------\nDataFrame.to_csv : Write DataFrame to a comma-separated values (csv) file.\nread_table : Read general delimited file into DataFrame.\nread_fwf : Read a table of fixed-width formatted lines into DataFrame.\n\nExamples\n--------\n>>> pd.read_csv('data.csv')  # doctest: +SKIP", "parameters": {"type": "object", "properties": {"filepath_or_buffer": {"type": ["readcsvbuffer[bytes]", "readcsvbuffer[str]", "filepath"]}}}}}
{"task_id": "BigCodeBench/964", "data": {"name": "pandas.read_excel", "type": "callable", "signature": "(io, engine: \"Literal['xlrd')", "description": "Read an Excel file into a ``pandas`` ``DataFrame``.\n\nSupports `xls`, `xlsx`, `xlsm`, `xlsb`, `odf`, `ods` and `odt` file extensions\nread from a local filesystem or URL. Supports an option to read\na single sheet or a list of sheets.\n\nParameters\n----------\nio : str, bytes, ExcelFile, xlrd.Book, path object, or file-like object\n    Any valid string path is acceptable. The string could be a URL. Valid\n    URL schemes include http, ftp, s3, and file. For file URLs, a host is\n    expected. A local file could be: ``file://localhost/path/to/table.xlsx``.\n\n    If you want to pass in a path object, pandas accepts any ``os.PathLike``.\n\n    By file-like object, we refer to objects with a ``read()`` method,\n    such as a file handle (e.g. via builtin ``open`` function)\n    or ``StringIO``.\n\n    .. deprecated:: 2.1.0\n        Passing byte strings is deprecated. To read from a\n        byte string, wrap it in a ``BytesIO`` object.\nsheet_name : str, int, list, or None, default 0\n    Strings are used for sheet names. Integers are used in zero-indexed\n    sheet positions (chart sheets do not count as a sheet position).\n    Lists of strings/integers are used to request multiple sheets.\n    Specify ``None`` to get all worksheets.\n\n    Available cases:\n\n    * Defaults to ``0``: 1st sheet as a `DataFrame`\n    * ``1``: 2nd sheet as a `DataFrame`\n    * ``\"Sheet1\"``: Load sheet with name \"Sheet1\"\n    * ``[0, 1, \"Sheet5\"]``: Load first, second and sheet named \"Sheet5\"\n      as a dict of `DataFrame`\n    * ``None``: All worksheets.\n\nheader : int, list of int, default 0\n    Row (0-indexed) to use for the column labels of the parsed\n    DataFrame. If a list of integers is passed those row positions will\n    be combined into a ``MultiIndex``. Use None if there is no header.\nnames : array-like, default None\n    List of column names to use. If file contains no header row,\n    then you should explicitly pass header=None.\nindex_col : int, str, list of int, default None\n    Column (0-indexed) to use as the row labels of the DataFrame.\n    Pass None if there is no such column.  If a list is passed,\n    those columns will be combined into a ``MultiIndex``.  If a\n    subset of data is selected with ``usecols``, index_col\n    is based on the subset.\n\n    Missing values will be forward filled to allow roundtripping with\n    ``to_excel`` for ``merged_cells=True``. To avoid forward filling the\n    missing values use ``set_index`` after reading the data instead of\n    ``index_col``.\nusecols : str, list-like, or callable, default None\n    * If None, then parse all columns.\n    * If str, then indicates comma separated list of Excel column letters\n      and column ranges (e.g. \"A:E\" or \"A,C,E:F\"). Ranges are inclusive of\n      both sides.\n    * If list of int, then indicates list of column numbers to be parsed\n      (0-indexed).\n    * If list of string, then indicates list of column names to be parsed.\n    * If callable, then evaluate each column name against it and parse the\n      column if the callable returns ``True``.\n\n    Returns a subset of the columns according to behavior above.\ndtype : Type name or dict of column -> type, default None\n    Data type for data or columns. E.g. {'a': np.float64, 'b': np.int32}\n    Use ``object`` to preserve data as stored in Excel and not interpret dtype,\n    which will necessarily result in ``object`` dtype.\n    If converters are specified, they will be applied INSTEAD\n    of dtype conversion.\n    If you use ``None``, it will infer the dtype of each column based on the data.\nengine : {'openpyxl', 'calamine', 'odf', 'pyxlsb', 'xlrd'}, default None\n    If io is not a buffer or path, this must be set to identify io.\n    Engine compatibility :\n\n    - ``openpyxl`` supports newer Excel file formats.\n    - ``calamine`` supports Excel (.xls, .xlsx, .xlsm, .xlsb)\n      and OpenDocument (.ods) file formats.\n    - ``odf`` supports OpenDocument file formats (.odf, .ods, .odt).\n    - ``pyxlsb`` supports Binary Excel files.\n    - ``xlrd`` supports old-style Excel files (.xls).\n\n    When ``engine=None``, the following logic will be used to determine the engine:\n\n    - If ``path_or_buffer`` is an OpenDocument format (.odf, .ods, .odt),\n      then `odf <https://pypi.org/project/odfpy/>`_ will be used.\n    - Otherwise if ``path_or_buffer`` is an xls format, ``xlrd`` will be used.\n    - Otherwise if ``path_or_buffer`` is in xlsb format, ``pyxlsb`` will be used.\n    - Otherwise ``openpyxl`` will be used.\nconverters : dict, default None\n    Dict of functions for converting values in certain columns. Keys can\n    either be integers or column labels, values are functions that take one\n    input argument, the Excel cell content, and return the transformed\n    content.\ntrue_values : list, default None\n    Values to consider as True.\nfalse_values : list, default None\n    Values to consider as False.\nskiprows : list-like, int, or callable, optional\n    Line numbers to skip (0-indexed) or number of lines to skip (int) at the\n    start of the file. If callable, the callable function will be evaluated\n    against the row indices, returning True if the row should be skipped and\n    False otherwise. An example of a valid callable argument would be ``lambda\n    x: x in [0, 2]``.\nnrows : int, default None\n    Number of rows to parse.\nna_values : scalar, str, list-like, or dict, default None\n    Additional strings to recognize as NA/NaN. If dict passed, specific\n    per-column NA values. By default the following values are interpreted\n    as NaN: '', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan',\n    '1.#IND', '1.#QNAN', '<NA>', 'N/A', 'NA', 'NULL', 'NaN', 'None',\n    'n/a', 'nan', 'null'.\nkeep_default_na : bool, default True\n    Whether or not to include the default NaN values when parsing the data.\n    Depending on whether ``na_values`` is passed in, the behavior is as follows:\n\n    * If ``keep_default_na`` is True, and ``na_values`` are specified,\n      ``na_values`` is appended to the default NaN values used for parsing.\n    * If ``keep_default_na`` is True, and ``na_values`` are not specified, only\n      the default NaN values are used for parsing.\n    * If ``keep_default_na`` is False, and ``na_values`` are specified, only\n      the NaN values specified ``na_values`` are used for parsing.\n    * If ``keep_default_na`` is False, and ``na_values`` are not specified, no\n      strings will be parsed as NaN.\n\n    Note that if `na_filter` is passed in as False, the ``keep_default_na`` and\n    ``na_values`` parameters will be ignored.\nna_filter : bool, default True\n    Detect missing value markers (empty strings and the value of na_values). In\n    data without any NAs, passing ``na_filter=False`` can improve the\n    performance of reading a large file.\nverbose : bool, default False\n    Indicate number of NA values placed in non-numeric columns.\nparse_dates : bool, list-like, or dict, default False\n    The behavior is as follows:\n\n    * ``bool``. If True -> try parsing the index.\n    * ``list`` of int or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3\n      each as a separate date column.\n    * ``list`` of lists. e.g.  If [[1, 3]] -> combine columns 1 and 3 and parse as\n      a single date column.\n    * ``dict``, e.g. {'foo' : [1, 3]} -> parse columns 1, 3 as date and call\n      result 'foo'\n\n    If a column or index contains an unparsable date, the entire column or\n    index will be returned unaltered as an object data type. If you don`t want to\n    parse some cells as date just change their type in Excel to \"Text\".\n    For non-standard datetime parsing, use ``pd.to_datetime`` after ``pd.read_excel``.\n\n    Note: A fast-path exists for iso8601-formatted dates.\ndate_parser : function, optional\n    Function to use for converting a sequence of string columns to an array of\n    datetime instances. The default uses ``dateutil.parser.parser`` to do the\n    conversion. Pandas will try to call `date_parser` in three different ways,\n    advancing to the next if an exception occurs: 1) Pass one or more arrays\n    (as defined by `parse_dates`) as arguments; 2) concatenate (row-wise) the\n    string values from the columns defined by `parse_dates` into a single array\n    and pass that; and 3) call `date_parser` once for each row using one or\n    more strings (corresponding to the columns defined by `parse_dates`) as\n    arguments.\n\n    .. deprecated:: 2.0.0\n       Use ``date_format`` instead, or read in as ``object`` and then apply\n       :func:`to_datetime` as-needed.\ndate_format : str or dict of column -> format, default ``None``\n   If used in conjunction with ``parse_dates``, will parse dates according to this\n   format. For anything more complex,\n   please read in as ``object`` and then apply :func:`to_datetime` as-needed.\n\n   .. versionadded:: 2.0.0\nthousands : str, default None\n    Thousands separator for parsing string columns to numeric.  Note that\n    this parameter is only necessary for columns stored as TEXT in Excel,\n    any numeric columns will automatically be parsed, regardless of display\n    format.\ndecimal : str, default '.'\n    Character to recognize as decimal point for parsing string columns to numeric.\n    Note that this parameter is only necessary for columns stored as TEXT in Excel,\n    any numeric columns will automatically be parsed, regardless of display\n    format.(e.g. use ',' for European data).\n\n    .. versionadded:: 1.4.0\n\ncomment : str, default None\n    Comments out remainder of line. Pass a character or characters to this\n    argument to indicate comments in the input file. Any data between the\n    comment string and the end of the current line is ignored.\nskipfooter : int, default 0\n    Rows at the end to skip (0-indexed).\nstorage_options : dict, optional\n    Extra options that make sense for a particular storage connection, e.g.\n    host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n    are forwarded to ``urllib.request.Request`` as header options. For other\n    URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n    forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n    details, and for more examples on storage options refer `here\n    <https://pandas.pydata.org/docs/user_guide/io.html?\n    highlight=storage_options#reading-writing-remote-files>`_.\n\ndtype_backend : {'numpy_nullable', 'pyarrow'}, default 'numpy_nullable'\n    Back-end data type applied to the resultant :class:`DataFrame`\n    (still experimental). Behaviour is as follows:\n\n    * ``\"numpy_nullable\"``: returns nullable-dtype-backed :class:`DataFrame`\n      (default).\n    * ``\"pyarrow\"``: returns pyarrow-backed nullable :class:`ArrowDtype`\n      DataFrame.\n\n    .. versionadded:: 2.0\n\nengine_kwargs : dict, optional\n    Arbitrary keyword arguments passed to excel engine.\n\nReturns\n-------\nDataFrame or dict of DataFrames\n    DataFrame from the passed in Excel file. See notes in sheet_name\n    argument for more information on when a dict of DataFrames is returned.\n\nSee Also\n--------\nDataFrame.to_excel : Write DataFrame to an Excel file.\nDataFrame.to_csv : Write DataFrame to a comma-separated values (csv) file.\nread_csv : Read a comma-separated values (csv) file into DataFrame.\nread_fwf : Read a table of fixed-width formatted lines into DataFrame.\n\nNotes\n-----\nFor specific information on the methods used for each Excel engine, refer to the pandas\n:ref:`user guide <io.excel_reader>`\n\nExamples\n--------\nThe file can be read using the file name as string or an open file object:\n\n>>> pd.read_excel('tmp.xlsx', index_col=0)  # doctest: +SKIP\n       Name  Value\n0   string1      1\n1   string2      2\n2  #Comment      3\n\n>>> pd.read_excel(open('tmp.xlsx', 'rb'),\n...               sheet_name='Sheet3')  # doctest: +SKIP\n   Unnamed: 0      Name  Value\n0           0   string1      1\n1           1   string2      2\n2           2  #Comment      3\n\nIndex and header can be specified via the `index_col` and `header` arguments\n\n>>> pd.read_excel('tmp.xlsx', index_col=None, header=None)  # doctest: +SKIP\n     0         1      2\n0  NaN      Name  Value\n1  0.0   string1      1\n2  1.0   string2      2\n3  2.0  #Comment      3\n\nColumn types are inferred but can be explicitly specified\n\n>>> pd.read_excel('tmp.xlsx', index_col=0,\n...               dtype={'Name': str, 'Value': float})  # doctest: +SKIP\n       Name  Value\n0   string1    1.0\n1   string2    2.0\n2  #Comment    3.0\n\nTrue, False, and NA values, and thousands separators have defaults,\nbut can be explicitly specified, too. Supply the values you would like\nas strings or lists of strings!\n\n>>> pd.read_excel('tmp.xlsx', index_col=0,\n...               na_values=['string1', 'string2'])  # doctest: +SKIP\n       Name  Value\n0       NaN      1\n1       NaN      2\n2  #Comment      3\n\nComment lines in the excel input file can be skipped using the\n``comment`` kwarg.\n\n>>> pd.read_excel('tmp.xlsx', index_col=0, comment='#')  # doctest: +SKIP\n      Name  Value\n0  string1    1.0\n1  string2    2.0\n2     None    NaN", "parameters": {"type": "object", "properties": {"io": {}, "engine": {"type": "\"literal['xlrd"}}}}}
{"task_id": "BigCodeBench/969", "data": {"name": "df.shape"}}
{"task_id": "BigCodeBench/969", "data": {"name": "df.empty"}}
{"task_id": "BigCodeBench/969", "data": {"name": "df.columns"}}
{"task_id": "BigCodeBench/969", "data": {"name": "df.isnull()"}}
{"task_id": "BigCodeBench/969", "data": {"name": "df.cumsum()"}}
{"task_id": "BigCodeBench/969", "data": {"name": "df.select_dtypes(include=np.number)"}}
{"task_id": "BigCodeBench/969", "data": {"name": "sklearn.preprocessing.MinMaxScaler", "type": "class", "signature": "()", "description": "Transform features by scaling each feature to a given range.\n\nThis estimator scales and translates each feature individually such\nthat it is in the given range on the training set, e.g. between\nzero and one.\n\nThe transformation is given by::\n\n    X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n    X_scaled = X_std * (max - min) + min\n\nwhere min, max = feature_range.\n\nThis transformation is often used as an alternative to zero mean,\nunit variance scaling.\n\n`MinMaxScaler` doesn't reduce the effect of outliers, but it linearly\nscales them down into a fixed range, where the largest occurring data point\ncorresponds to the maximum value and the smallest one corresponds to the\nminimum value. For an example visualization, refer to :ref:`Compare\nMinMaxScaler with other scalers <plot_all_scaling_minmax_scaler_section>`.\n\nRead more in the :ref:`User Guide <preprocessing_scaler>`.\n\nParameters\n----------\nfeature_range : tuple (min, max), default=(0, 1)\n    Desired range of transformed data.\n\ncopy : bool, default=True\n    Set to False to perform inplace row normalization and avoid a\n    copy (if the input is already a numpy array).\n\nclip : bool, default=False\n    Set to True to clip transformed values of held-out data to\n    provided `feature range`.\n\n    .. versionadded:: 0.24\n\nAttributes\n----------\nmin_ : ndarray of shape (n_features,)\n    Per feature adjustment for minimum. Equivalent to\n    ``min - X.min(axis=0) * self.scale_``\n\nscale_ : ndarray of shape (n_features,)\n    Per feature relative scaling of the data. Equivalent to\n    ``(max - min) / (X.max(axis=0) - X.min(axis=0))``\n\n    .. versionadded:: 0.17\n       *scale_* attribute.\n\ndata_min_ : ndarray of shape (n_features,)\n    Per feature minimum seen in the data\n\n    .. versionadded:: 0.17\n       *data_min_*\n\ndata_max_ : ndarray of shape (n_features,)\n    Per feature maximum seen in the data\n\n    .. versionadded:: 0.17\n       *data_max_*\n\ndata_range_ : ndarray of shape (n_features,)\n    Per feature range ``(data_max_ - data_min_)`` seen in the data\n\n    .. versionadded:: 0.17\n       *data_range_*\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nn_samples_seen_ : int\n    The number of samples processed by the estimator.\n    It will be reset on new calls to fit, but increments across\n    ``partial_fit`` calls.\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nminmax_scale : Equivalent function without the estimator API.\n\nNotes\n-----\nNaNs are treated as missing values: disregarded in fit, and maintained in\ntransform.\n\nExamples\n--------\n>>> from sklearn.preprocessing import MinMaxScaler\n>>> data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n>>> scaler = MinMaxScaler()\n>>> print(scaler.fit(data))\nMinMaxScaler()\n>>> print(scaler.data_max_)\n[ 1. 18.]\n>>> print(scaler.transform(data))\n[[0.   0.  ]\n [0.25 0.25]\n [0.5  0.5 ]\n [1.   1.  ]]\n>>> print(scaler.transform([[2, 2]]))\n[[1.5 0. ]]", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/969", "data": {"name": "sklearn.preprocessing.MinMaxScaler.fit_transform", "type": "callable", "signature": "(self, X)", "description": "Fit to data, then transform it.\n\nFits transformer to `X` and `y` with optional parameters `fit_params`\nand returns a transformed version of `X`.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Input samples.\n\ny :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n    Target values (None for unsupervised transformations).\n\n**fit_params : dict\n    Additional fit parameters.\n\nReturns\n-------\nX_new : ndarray array of shape (n_samples, n_features_new)\n    Transformed array.", "parameters": {"type": "object", "properties": {"X": {}}}}}
{"task_id": "BigCodeBench/971", "data": {"name": "datetime.timezone", "type": "class", "signature": "(self, /, *args, **kwargs)", "description": "Fixed offset from UTC implementation of tzinfo.", "parameters": {"type": "object", "properties": {"/": {}}}}}
{"task_id": "BigCodeBench/971", "data": {"name": "datetime.timezone.utc", "type": "constant", "signature": null, "description": "Fixed offset from UTC implementation of tzinfo.", "value": "UTC", "parameters": {"type": "constant"}}}
{"task_id": "BigCodeBench/971", "data": {"name": "datetime.datetime", "type": "class", "signature": "(self, /, *args, **kwargs)", "description": "datetime(year, month, day[, hour[, minute[, second[, microsecond[,tzinfo]]]]])\n\nThe year, month and day arguments are required. tzinfo may be None, or an\ninstance of a tzinfo subclass. The remaining arguments may be ints.", "parameters": {"type": "object", "properties": {"/": {}}}}}
{"task_id": "BigCodeBench/971", "data": {"name": "datetime.datetime.fromtimestamp", "type": "callable", "signature": "(*args, **kwargs)", "description": "timestamp[, tz] -> tz's local time from POSIX timestamp.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/971", "data": {"name": "entry.path"}}
{"task_id": "BigCodeBench/971", "data": {"name": "entry.name"}}
{"task_id": "BigCodeBench/971", "data": {"name": "entry.is_file()"}}
{"task_id": "BigCodeBench/971", "data": {"name": "file_details.append((entry.name, file_size, creation_time, modification_time))"}}
{"task_id": "BigCodeBench/971", "data": {"name": "os.stat", "type": "callable", "signature": "(path)", "description": "Perform a stat system call on the given path.\n\n  path\n    Path to be examined; can be string, bytes, a path-like object or\n    open-file-descriptor int.\n  dir_fd\n    If not None, it should be a file descriptor open to a directory,\n    and path should be a relative string; path will then be relative to\n    that directory.\n  follow_symlinks\n    If False, and the last element of the path is a symbolic link,\n    stat will examine the symbolic link itself instead of the file\n    the link points to.\n\ndir_fd and follow_symlinks may not be implemented\n  on your platform.  If they are unavailable, using them will raise a\n  NotImplementedError.\n\nIt's an error to use dir_fd or follow_symlinks when specifying path as\n  an open file descriptor.", "parameters": {"type": "object", "properties": {"path": {}}}}}
{"task_id": "BigCodeBench/971", "data": {"name": "os.stat.st_size", "type": "callable", "signature": "(path, *, dir_fd=None, follow_symlinks=True)", "description": "Perform a stat system call on the given path.\n\n  path\n    Path to be examined; can be string, bytes, a path-like object or\n    open-file-descriptor int.\n  dir_fd\n    If not None, it should be a file descriptor open to a directory,\n    and path should be a relative string; path will then be relative to\n    that directory.\n  follow_symlinks\n    If False, and the last element of the path is a symbolic link,\n    stat will examine the symbolic link itself instead of the file\n    the link points to.\n\ndir_fd and follow_symlinks may not be implemented\n  on your platform.  If they are unavailable, using them will raise a\n  NotImplementedError.\n\nIt's an error to use dir_fd or follow_symlinks when specifying path as\n  an open file descriptor.", "parameters": {"type": "object", "properties": {"path": {}, "dir_fd": {"type": "NoneType", "default": null}, "follow_symlinks": {"type": "bool", "default": true}}}}}
{"task_id": "BigCodeBench/971", "data": {"name": "os.stat.st_ctime", "type": "callable", "signature": "(path, *, dir_fd=None, follow_symlinks=True)", "description": "Perform a stat system call on the given path.\n\n  path\n    Path to be examined; can be string, bytes, a path-like object or\n    open-file-descriptor int.\n  dir_fd\n    If not None, it should be a file descriptor open to a directory,\n    and path should be a relative string; path will then be relative to\n    that directory.\n  follow_symlinks\n    If False, and the last element of the path is a symbolic link,\n    stat will examine the symbolic link itself instead of the file\n    the link points to.\n\ndir_fd and follow_symlinks may not be implemented\n  on your platform.  If they are unavailable, using them will raise a\n  NotImplementedError.\n\nIt's an error to use dir_fd or follow_symlinks when specifying path as\n  an open file descriptor.", "parameters": {"type": "object", "properties": {"path": {}, "dir_fd": {"type": "NoneType", "default": null}, "follow_symlinks": {"type": "bool", "default": true}}}}}
{"task_id": "BigCodeBench/971", "data": {"name": "os.stat.st_mtime", "type": "callable", "signature": "(path, *, dir_fd=None, follow_symlinks=True)", "description": "Perform a stat system call on the given path.\n\n  path\n    Path to be examined; can be string, bytes, a path-like object or\n    open-file-descriptor int.\n  dir_fd\n    If not None, it should be a file descriptor open to a directory,\n    and path should be a relative string; path will then be relative to\n    that directory.\n  follow_symlinks\n    If False, and the last element of the path is a symbolic link,\n    stat will examine the symbolic link itself instead of the file\n    the link points to.\n\ndir_fd and follow_symlinks may not be implemented\n  on your platform.  If they are unavailable, using them will raise a\n  NotImplementedError.\n\nIt's an error to use dir_fd or follow_symlinks when specifying path as\n  an open file descriptor.", "parameters": {"type": "object", "properties": {"path": {}, "dir_fd": {"type": "NoneType", "default": null}, "follow_symlinks": {"type": "bool", "default": true}}}}}
{"task_id": "BigCodeBench/971", "data": {"name": "os.scandir", "type": "callable", "signature": "(path=None)", "description": "Return an iterator of DirEntry objects for given path.\n\npath can be specified as either str, bytes, or a path-like object.  If path\nis bytes, the names of yielded DirEntry objects will also be bytes; in\nall other circumstances they will be str.\n\nIf path is None, uses the path='.'.", "parameters": {"type": "object", "properties": {"path": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/985", "data": {"name": "pandas.DataFrame", "type": "class", "signature": "(data=None, index: 'Axes | None' = None, columns: 'Axes | None' = None, dtype: 'Dtype | None' = None, copy: 'bool | None' = None) -> 'None'", "description": "Two-dimensional, size-mutable, potentially heterogeneous tabular data.\n\nData structure also contains labeled axes (rows and columns).\nArithmetic operations align on both row and column labels. Can be\nthought of as a dict-like container for Series objects. The primary\npandas data structure.\n\nParameters\n----------\ndata : ndarray (structured or homogeneous), Iterable, dict, or DataFrame\n    Dict can contain Series, arrays, constants, dataclass or list-like objects. If\n    data is a dict, column order follows insertion-order. If a dict contains Series\n    which have an index defined, it is aligned by its index. This alignment also\n    occurs if data is a Series or a DataFrame itself. Alignment is done on\n    Series/DataFrame inputs.\n\n    If data is a list of dicts, column order follows insertion-order.\n\nindex : Index or array-like\n    Index to use for resulting frame. Will default to RangeIndex if\n    no indexing information part of input data and no index provided.\ncolumns : Index or array-like\n    Column labels to use for resulting frame when data does not have them,\n    defaulting to RangeIndex(0, 1, 2, ..., n). If data contains column labels,\n    will perform column selection instead.\ndtype : dtype, default None\n    Data type to force. Only a single dtype is allowed. If None, infer.\ncopy : bool or None, default None\n    Copy data from inputs.\n    For dict data, the default of None behaves like ``copy=True``.  For DataFrame\n    or 2d ndarray input, the default of None behaves like ``copy=False``.\n    If data is a dict containing one or more Series (possibly of different dtypes),\n    ``copy=False`` will ensure that these inputs are not copied.\n\n    .. versionchanged:: 1.3.0\n\nSee Also\n--------\nDataFrame.from_records : Constructor from tuples, also record arrays.\nDataFrame.from_dict : From dicts of Series, arrays, or dicts.\nread_csv : Read a comma-separated values (csv) file into DataFrame.\nread_table : Read general delimited file into DataFrame.\nread_clipboard : Read text from clipboard into DataFrame.\n\nNotes\n-----\nPlease reference the :ref:`User Guide <basics.dataframe>` for more information.\n\nExamples\n--------\nConstructing DataFrame from a dictionary.\n\n>>> d = {'col1': [1, 2], 'col2': [3, 4]}\n>>> df = pd.DataFrame(data=d)\n>>> df\n   col1  col2\n0     1     3\n1     2     4\n\nNotice that the inferred dtype is int64.\n\n>>> df.dtypes\ncol1    int64\ncol2    int64\ndtype: object\n\nTo enforce a single dtype:\n\n>>> df = pd.DataFrame(data=d, dtype=np.int8)\n>>> df.dtypes\ncol1    int8\ncol2    int8\ndtype: object\n\nConstructing DataFrame from a dictionary including Series:\n\n>>> d = {'col1': [0, 1, 2, 3], 'col2': pd.Series([2, 3], index=[2, 3])}\n>>> pd.DataFrame(data=d, index=[0, 1, 2, 3])\n   col1  col2\n0     0   NaN\n1     1   NaN\n2     2   2.0\n3     3   3.0\n\nConstructing DataFrame from numpy ndarray:\n\n>>> df2 = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),\n...                    columns=['a', 'b', 'c'])\n>>> df2\n   a  b  c\n0  1  2  3\n1  4  5  6\n2  7  8  9\n\nConstructing DataFrame from a numpy ndarray that has labeled columns:\n\n>>> data = np.array([(1, 2, 3), (4, 5, 6), (7, 8, 9)],\n...                 dtype=[(\"a\", \"i4\"), (\"b\", \"i4\"), (\"c\", \"i4\")])\n>>> df3 = pd.DataFrame(data, columns=['c', 'a'])\n...\n>>> df3\n   c  a\n0  3  1\n1  6  4\n2  9  7\n\nConstructing DataFrame from dataclass:\n\n>>> from dataclasses import make_dataclass\n>>> Point = make_dataclass(\"Point\", [(\"x\", int), (\"y\", int)])\n>>> pd.DataFrame([Point(0, 0), Point(0, 3), Point(2, 3)])\n   x  y\n0  0  0\n1  0  3\n2  2  3\n\nConstructing DataFrame from Series/DataFrame:\n\n>>> ser = pd.Series([1, 2, 3], index=[\"a\", \"b\", \"c\"])\n>>> df = pd.DataFrame(data=ser, index=[\"a\", \"c\"])\n>>> df\n   0\na  1\nc  3\n\n>>> df1 = pd.DataFrame([1, 2, 3], index=[\"a\", \"b\", \"c\"], columns=[\"x\"])\n>>> df2 = pd.DataFrame(data=df1, index=[\"a\", \"c\"])\n>>> df2\n   x\na  1\nc  3", "parameters": {"type": "object", "properties": {"data": {"type": "NoneType", "default": null}, "index": {"type": ["axes", "null"], "default": null}, "columns": {"type": ["axes", "null"], "default": null}, "dtype": {"type": ["dtype", "null"], "default": null}, "copy": {"type": ["bool", "null"], "default": null}}}}}
{"task_id": "BigCodeBench/985", "data": {"name": "pandas.DataFrame.to_csv", "type": "callable", "signature": "(self, path_or_buf: 'FilePath | WriteBuffer[bytes] | WriteBuffer[str] | None' = None, index: 'bool_t' = True)", "description": "Write object to a comma-separated values (csv) file.\n\nParameters\n----------\npath_or_buf : str, path object, file-like object, or None, default None\n    String, path object (implementing os.PathLike[str]), or file-like\n    object implementing a write() function. If None, the result is\n    returned as a string. If a non-binary file object is passed, it should\n    be opened with `newline=''`, disabling universal newlines. If a binary\n    file object is passed, `mode` might need to contain a `'b'`.\nsep : str, default ','\n    String of length 1. Field delimiter for the output file.\nna_rep : str, default ''\n    Missing data representation.\nfloat_format : str, Callable, default None\n    Format string for floating point numbers. If a Callable is given, it takes\n    precedence over other numeric formatting parameters, like decimal.\ncolumns : sequence, optional\n    Columns to write.\nheader : bool or list of str, default True\n    Write out the column names. If a list of strings is given it is\n    assumed to be aliases for the column names.\nindex : bool, default True\n    Write row names (index).\nindex_label : str or sequence, or False, default None\n    Column label for index column(s) if desired. If None is given, and\n    `header` and `index` are True, then the index names are used. A\n    sequence should be given if the object uses MultiIndex. If\n    False do not print fields for index names. Use index_label=False\n    for easier importing in R.\nmode : {'w', 'x', 'a'}, default 'w'\n    Forwarded to either `open(mode=)` or `fsspec.open(mode=)` to control\n    the file opening. Typical values include:\n\n    - 'w', truncate the file first.\n    - 'x', exclusive creation, failing if the file already exists.\n    - 'a', append to the end of file if it exists.\n\nencoding : str, optional\n    A string representing the encoding to use in the output file,\n    defaults to 'utf-8'. `encoding` is not supported if `path_or_buf`\n    is a non-binary file object.\ncompression : str or dict, default 'infer'\n    For on-the-fly compression of the output data. If 'infer' and 'path_or_buf' is\n    path-like, then detect compression from the following extensions: '.gz',\n    '.bz2', '.zip', '.xz', '.zst', '.tar', '.tar.gz', '.tar.xz' or '.tar.bz2'\n    (otherwise no compression).\n    Set to ``None`` for no compression.\n    Can also be a dict with key ``'method'`` set\n    to one of {``'zip'``, ``'gzip'``, ``'bz2'``, ``'zstd'``, ``'xz'``, ``'tar'``} and\n    other key-value pairs are forwarded to\n    ``zipfile.ZipFile``, ``gzip.GzipFile``,\n    ``bz2.BZ2File``, ``zstandard.ZstdCompressor``, ``lzma.LZMAFile`` or\n    ``tarfile.TarFile``, respectively.\n    As an example, the following could be passed for faster compression and to create\n    a reproducible gzip archive:\n    ``compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}``.\n\n    .. versionadded:: 1.5.0\n        Added support for `.tar` files.\n\n       May be a dict with key 'method' as compression mode\n       and other entries as additional compression options if\n       compression mode is 'zip'.\n\n       Passing compression options as keys in dict is\n       supported for compression modes 'gzip', 'bz2', 'zstd', and 'zip'.\nquoting : optional constant from csv module\n    Defaults to csv.QUOTE_MINIMAL. If you have set a `float_format`\n    then floats are converted to strings and thus csv.QUOTE_NONNUMERIC\n    will treat them as non-numeric.\nquotechar : str, default '\\\"'\n    String of length 1. Character used to quote fields.\nlineterminator : str, optional\n    The newline character or character sequence to use in the output\n    file. Defaults to `os.linesep`, which depends on the OS in which\n    this method is called ('\\\\n' for linux, '\\\\r\\\\n' for Windows, i.e.).\n\n    .. versionchanged:: 1.5.0\n\n        Previously was line_terminator, changed for consistency with\n        read_csv and the standard library 'csv' module.\n\nchunksize : int or None\n    Rows to write at a time.\ndate_format : str, default None\n    Format string for datetime objects.\ndoublequote : bool, default True\n    Control quoting of `quotechar` inside a field.\nescapechar : str, default None\n    String of length 1. Character used to escape `sep` and `quotechar`\n    when appropriate.\ndecimal : str, default '.'\n    Character recognized as decimal separator. E.g. use ',' for\n    European data.\nerrors : str, default 'strict'\n    Specifies how encoding and decoding errors are to be handled.\n    See the errors argument for :func:`open` for a full list\n    of options.\n\nstorage_options : dict, optional\n    Extra options that make sense for a particular storage connection, e.g.\n    host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n    are forwarded to ``urllib.request.Request`` as header options. For other\n    URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n    forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n    details, and for more examples on storage options refer `here\n    <https://pandas.pydata.org/docs/user_guide/io.html?\n    highlight=storage_options#reading-writing-remote-files>`_.\n\nReturns\n-------\nNone or str\n    If path_or_buf is None, returns the resulting csv format as a\n    string. Otherwise returns None.\n\nSee Also\n--------\nread_csv : Load a CSV file into a DataFrame.\nto_excel : Write DataFrame to an Excel file.\n\nExamples\n--------\nCreate 'out.csv' containing 'df' without indices\n\n>>> df = pd.DataFrame({'name': ['Raphael', 'Donatello'],\n...                    'mask': ['red', 'purple'],\n...                    'weapon': ['sai', 'bo staff']})\n>>> df.to_csv('out.csv', index=False)  # doctest: +SKIP\n\nCreate 'out.zip' containing 'out.csv'\n\n>>> df.to_csv(index=False)\n'name,mask,weapon\\nRaphael,red,sai\\nDonatello,purple,bo staff\\n'\n>>> compression_opts = dict(method='zip',\n...                         archive_name='out.csv')  # doctest: +SKIP\n>>> df.to_csv('out.zip', index=False,\n...           compression=compression_opts)  # doctest: +SKIP\n\nTo write a csv file to a new folder or nested folder you will first\nneed to create it using either Pathlib or os:\n\n>>> from pathlib import Path  # doctest: +SKIP\n>>> filepath = Path('folder/subfolder/out.csv')  # doctest: +SKIP\n>>> filepath.parent.mkdir(parents=True, exist_ok=True)  # doctest: +SKIP\n>>> df.to_csv(filepath)  # doctest: +SKIP\n\n>>> import os  # doctest: +SKIP\n>>> os.makedirs('folder/subfolder', exist_ok=True)  # doctest: +SKIP\n>>> df.to_csv('folder/subfolder/out.csv')  # doctest: +SKIP", "parameters": {"type": "object", "properties": {"path_or_buf": {"type": ["writebuffer[bytes]", "null", "writebuffer[str]", "filepath"], "default": null}, "index": {"type": "bool_t", "default": true}}}}}
{"task_id": "BigCodeBench/985", "data": {"name": "data.get('Countries').items()"}}
{"task_id": "BigCodeBench/985", "data": {"name": "json.loads", "type": "callable", "signature": "(s)", "description": "Deserialize ``s`` (a ``str``, ``bytes`` or ``bytearray`` instance\ncontaining a JSON document) to a Python object.\n\n``object_hook`` is an optional function that will be called with the\nresult of any object literal decode (a ``dict``). The return value of\n``object_hook`` will be used instead of the ``dict``. This feature\ncan be used to implement custom decoders (e.g. JSON-RPC class hinting).\n\n``object_pairs_hook`` is an optional function that will be called with the\nresult of any object literal decoded with an ordered list of pairs.  The\nreturn value of ``object_pairs_hook`` will be used instead of the ``dict``.\nThis feature can be used to implement custom decoders.  If ``object_hook``\nis also defined, the ``object_pairs_hook`` takes priority.\n\n``parse_float``, if specified, will be called with the string\nof every JSON float to be decoded. By default this is equivalent to\nfloat(num_str). This can be used to use another datatype or parser\nfor JSON floats (e.g. decimal.Decimal).\n\n``parse_int``, if specified, will be called with the string\nof every JSON int to be decoded. By default this is equivalent to\nint(num_str). This can be used to use another datatype or parser\nfor JSON integers (e.g. float).\n\n``parse_constant``, if specified, will be called with one of the\nfollowing strings: -Infinity, Infinity, NaN.\nThis can be used to raise an exception if invalid JSON numbers\nare encountered.\n\nTo use a custom ``JSONDecoder`` subclass, specify it with the ``cls``\nkwarg; otherwise ``JSONDecoder`` is used.", "parameters": {"type": "object", "properties": {"s": {}}}}}
{"task_id": "BigCodeBench/985", "data": {"name": "json.loads.get", "type": "callable", "signature": "(s)", "description": "Deserialize ``s`` (a ``str``, ``bytes`` or ``bytearray`` instance\ncontaining a JSON document) to a Python object.\n\n``object_hook`` is an optional function that will be called with the\nresult of any object literal decode (a ``dict``). The return value of\n``object_hook`` will be used instead of the ``dict``. This feature\ncan be used to implement custom decoders (e.g. JSON-RPC class hinting).\n\n``object_pairs_hook`` is an optional function that will be called with the\nresult of any object literal decoded with an ordered list of pairs.  The\nreturn value of ``object_pairs_hook`` will be used instead of the ``dict``.\nThis feature can be used to implement custom decoders.  If ``object_hook``\nis also defined, the ``object_pairs_hook`` takes priority.\n\n``parse_float``, if specified, will be called with the string\nof every JSON float to be decoded. By default this is equivalent to\nfloat(num_str). This can be used to use another datatype or parser\nfor JSON floats (e.g. decimal.Decimal).\n\n``parse_int``, if specified, will be called with the string\nof every JSON int to be decoded. By default this is equivalent to\nint(num_str). This can be used to use another datatype or parser\nfor JSON integers (e.g. float).\n\n``parse_constant``, if specified, will be called with one of the\nfollowing strings: -Infinity, Infinity, NaN.\nThis can be used to raise an exception if invalid JSON numbers\nare encountered.\n\nTo use a custom ``JSONDecoder`` subclass, specify it with the ``cls``\nkwarg; otherwise ``JSONDecoder`` is used.", "parameters": {"type": "object", "properties": {"s": {}}}}}
{"task_id": "BigCodeBench/985", "data": {"name": "math.floor", "type": "callable", "signature": "(x)", "description": "Return the floor of x as an Integral.\n\nThis is the largest integer <= x.", "parameters": {"type": "object", "properties": {"x": {}}}}}
{"task_id": "BigCodeBench/985", "data": {"name": "os.path.join", "type": "callable", "signature": "(a, *p)", "description": "Join two or more pathname components, inserting '/' as needed.\nIf any component is an absolute path, all previous path components\nwill be discarded.  An empty last part will result in a path that\nends with a separator.", "parameters": {"type": "object", "properties": {"a": {}}}}}
{"task_id": "BigCodeBench/985", "data": {"name": "os.makedirs", "type": "callable", "signature": "(name, exist_ok=False)", "description": "makedirs(name [, mode=0o777][, exist_ok=False])\n\nSuper-mkdir; create a leaf directory and all intermediate ones.  Works like\nmkdir, except that any intermediate path segment (not just the rightmost)\nwill be created if it does not exist. If the target directory already\nexists, raise an OSError if exist_ok is False. Otherwise no exception is\nraised.  This is recursive.", "parameters": {"type": "object", "properties": {"name": {}, "exist_ok": {"type": "bool", "default": false}}}}}
{"task_id": "BigCodeBench/988", "data": {"name": "os.listdir", "type": "callable", "signature": "(path=None)", "description": "Return a list containing the names of the files in the directory.\n\npath can be specified as either str, bytes, or a path-like object.  If path is bytes,\n  the filenames returned will also be bytes; in all other circumstances\n  the filenames returned will be str.\nIf path is None, uses the path='.'.\nOn some platforms, path may also be specified as an open file descriptor;\\\n  the file descriptor must refer to a directory.\n  If this functionality is unavailable, using it raises NotImplementedError.\n\nThe list is in arbitrary order.  It does not include the special\nentries '.' and '..' even if they are present in the directory.", "parameters": {"type": "object", "properties": {"path": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/988", "data": {"name": "os.path.isdir", "type": "callable", "signature": "(s)", "description": "Return true if the pathname refers to an existing directory.", "parameters": {"type": "object", "properties": {"s": {}}}}}
{"task_id": "BigCodeBench/988", "data": {"name": "os.path.exists", "type": "callable", "signature": "(path)", "description": "Test whether a path exists.  Returns False for broken symbolic links", "parameters": {"type": "object", "properties": {"path": {}}}}}
{"task_id": "BigCodeBench/988", "data": {"name": "os.path.join", "type": "callable", "signature": "(a, *p)", "description": "Join two or more pathname components, inserting '/' as needed.\nIf any component is an absolute path, all previous path components\nwill be discarded.  An empty last part will result in a path that\nends with a separator.", "parameters": {"type": "object", "properties": {"a": {}}}}}
{"task_id": "BigCodeBench/988", "data": {"name": "predicate_functions.items()"}}
{"task_id": "BigCodeBench/988", "data": {"name": "re.search", "type": "callable", "signature": "(pattern, string)", "description": "Scan through string looking for a match to the pattern, returning\na Match object, or None if no match was found.", "parameters": {"type": "object", "properties": {"pattern": {}, "string": {}}}}}
{"task_id": "BigCodeBench/988", "data": {"name": "x.stem"}}
{"task_id": "BigCodeBench/988", "data": {"name": "x.name"}}
{"task_id": "BigCodeBench/988", "data": {"name": "x.is_dir()"}}
{"task_id": "BigCodeBench/988", "data": {"name": "x.is_file()"}}
{"task_id": "BigCodeBench/990", "data": {"name": "base64.b64encode", "type": "callable", "signature": "(s)", "description": "Encode the bytes-like object s using Base64 and return a bytes object.\n\nOptional altchars should be a byte string of length 2 which specifies an\nalternative alphabet for the '+' and '/' characters.  This allows an\napplication to e.g. generate url or filesystem safe Base64 strings.", "parameters": {"type": "object", "properties": {"s": {}}}}}
{"task_id": "BigCodeBench/990", "data": {"name": "binascii.hexlify", "type": "callable", "signature": "(*args, **kwargs)", "description": "Hexadecimal representation of binary data.\n\n  sep\n    An optional single character or byte to separate hex bytes.\n  bytes_per_sep\n    How many bytes between separators.  Positive values count from the\n    right, negative values count from the left.\n\nThe return value is a bytes object.  This function is also\navailable as \"b2a_hex()\".", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/990", "data": {"name": "bytes.fromhex(hex_string)"}}
{"task_id": "BigCodeBench/990", "data": {"name": "bytes.fromhex.decode('utf-8').encode()"}}
{"task_id": "BigCodeBench/990", "data": {"name": "bytes.fromhex.decode('utf-8').encode('utf-8')"}}
{"task_id": "BigCodeBench/990", "data": {"name": "bytes.fromhex.decode('utf-8').encode('ascii')"}}
{"task_id": "BigCodeBench/990", "data": {"name": "bytes.fromhex.decode('utf-8').encode('utf-16')"}}
{"task_id": "BigCodeBench/990", "data": {"name": "bytes.fromhex.decode('utf-8').encode('utf-32')"}}
{"task_id": "BigCodeBench/990", "data": {"name": "codecs.encode", "type": "callable", "signature": "(obj, encoding='utf-8')", "description": "Encodes obj using the codec registered for encoding.\n\nThe default encoding is 'utf-8'.  errors may be given to set a\ndifferent error handling scheme.  Default is 'strict' meaning that encoding\nerrors raise a ValueError.  Other possible values are 'ignore', 'replace'\nand 'backslashreplace' as well as any other name registered with\ncodecs.register_error that can handle ValueErrors.", "parameters": {"type": "object", "properties": {"obj": {}, "encoding": {"type": "str", "default": "utf-8"}}}}}
{"task_id": "BigCodeBench/990", "data": {"name": "urllib.parse.quote", "type": "callable", "signature": "(string)", "description": "quote('abc def') -> 'abc%20def'\n\nEach part of a URL, e.g. the path info, the query, etc., has a\ndifferent set of reserved characters that must be quoted. The\nquote function offers a cautious (not minimal) way to quote a\nstring for most of these parts.\n\nRFC 3986 Uniform Resource Identifier (URI): Generic Syntax lists\nthe following (un)reserved characters.\n\nunreserved    = ALPHA / DIGIT / \"-\" / \".\" / \"_\" / \"~\"\nreserved      = gen-delims / sub-delims\ngen-delims    = \":\" / \"/\" / \"?\" / \"#\" / \"[\" / \"]\" / \"@\"\nsub-delims    = \"!\" / \"$\" / \"&\" / \"'\" / \"(\" / \")\"\n              / \"*\" / \"+\" / \",\" / \";\" / \"=\"\n\nEach of the reserved characters is reserved in some component of a URL,\nbut not necessarily in all of them.\n\nThe quote function %-escapes all characters that are neither in the\nunreserved chars (\"always safe\") nor the additional chars set via the\nsafe arg.\n\nThe default for the safe arg is '/'. The character is reserved, but in\ntypical usage the quote function is being called on a path where the\nexisting slash characters are to be preserved.\n\nPython 3.7 updates from using RFC 2396 to RFC 3986 to quote URL strings.\nNow, \"~\" is included in the set of unreserved characters.\n\nstring and safe may be either str or bytes objects. encoding and errors\nmust not be specified if string is a bytes object.\n\nThe optional encoding and errors parameters specify how to deal with\nnon-ASCII characters, as accepted by the str.encode method.\nBy default, encoding='utf-8' (characters are encoded with UTF-8), and\nerrors='strict' (unsupported characters raise a UnicodeEncodeError).", "parameters": {"type": "object", "properties": {"string": {}}}}}
{"task_id": "BigCodeBench/998", "data": {"name": "hashlib.md5", "type": "callable", "signature": "()", "description": "Returns a md5 hash object; optionally initialized with a string", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/998", "data": {"name": "hashlib.md5.hexdigest", "type": "callable", "signature": "()", "description": "Returns a md5 hash object; optionally initialized with a string", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/998", "data": {"name": "hashlib.md5.update", "type": "callable", "signature": "(string=b'')", "description": "Returns a md5 hash object; optionally initialized with a string", "parameters": {"type": "object", "properties": {"string": {"type": "str", "default": "b''"}}}}}
{"task_id": "BigCodeBench/998", "data": {"name": "open.read(4096)"}}
{"task_id": "BigCodeBench/998", "data": {"name": "os.remove", "type": "callable", "signature": "(path)", "description": "Remove a file (same as unlink()).\n\nIf dir_fd is not None, it should be a file descriptor open to a directory,\n  and path should be relative; path will then be relative to that directory.\ndir_fd may not be implemented on your platform.\n  If it is unavailable, using it will raise a NotImplementedError.", "parameters": {"type": "object", "properties": {"path": {}}}}}
{"task_id": "BigCodeBench/998", "data": {"name": "tarfile.open", "type": "callable", "signature": "(name=None, mode='r')", "description": "Open a tar archive for reading, writing or appending. Return\nan appropriate TarFile class.\n\nmode:\n'r' or 'r:*' open for reading with transparent compression\n'r:'         open for reading exclusively uncompressed\n'r:gz'       open for reading with gzip compression\n'r:bz2'      open for reading with bzip2 compression\n'r:xz'       open for reading with lzma compression\n'a' or 'a:'  open for appending, creating the file if necessary\n'w' or 'w:'  open for writing without compression\n'w:gz'       open for writing with gzip compression\n'w:bz2'      open for writing with bzip2 compression\n'w:xz'       open for writing with lzma compression\n\n'x' or 'x:'  create a tarfile exclusively without compression, raise\n             an exception if the file is already created\n'x:gz'       create a gzip compressed tarfile, raise an exception\n             if the file is already created\n'x:bz2'      create a bzip2 compressed tarfile, raise an exception\n             if the file is already created\n'x:xz'       create an lzma compressed tarfile, raise an exception\n             if the file is already created\n\n'r|*'        open a stream of tar blocks with transparent compression\n'r|'         open an uncompressed stream of tar blocks for reading\n'r|gz'       open a gzip compressed stream of tar blocks\n'r|bz2'      open a bzip2 compressed stream of tar blocks\n'r|xz'       open an lzma compressed stream of tar blocks\n'w|'         open an uncompressed stream for writing\n'w|gz'       open a gzip compressed stream for writing\n'w|bz2'      open a bzip2 compressed stream for writing\n'w|xz'       open an lzma compressed stream for writing", "parameters": {"type": "object", "properties": {"name": {"type": "NoneType", "default": null}, "mode": {"type": "str", "default": "r"}}}}}
{"task_id": "BigCodeBench/998", "data": {"name": "tarfile.open.extractall", "type": "callable", "signature": "()", "description": "Open a tar archive for reading, writing or appending. Return\nan appropriate TarFile class.\n\nmode:\n'r' or 'r:*' open for reading with transparent compression\n'r:'         open for reading exclusively uncompressed\n'r:gz'       open for reading with gzip compression\n'r:bz2'      open for reading with bzip2 compression\n'r:xz'       open for reading with lzma compression\n'a' or 'a:'  open for appending, creating the file if necessary\n'w' or 'w:'  open for writing without compression\n'w:gz'       open for writing with gzip compression\n'w:bz2'      open for writing with bzip2 compression\n'w:xz'       open for writing with lzma compression\n\n'x' or 'x:'  create a tarfile exclusively without compression, raise\n             an exception if the file is already created\n'x:gz'       create a gzip compressed tarfile, raise an exception\n             if the file is already created\n'x:bz2'      create a bzip2 compressed tarfile, raise an exception\n             if the file is already created\n'x:xz'       create an lzma compressed tarfile, raise an exception\n             if the file is already created\n\n'r|*'        open a stream of tar blocks with transparent compression\n'r|'         open an uncompressed stream of tar blocks for reading\n'r|gz'       open a gzip compressed stream of tar blocks\n'r|bz2'      open a bzip2 compressed stream of tar blocks\n'r|xz'       open an lzma compressed stream of tar blocks\n'w|'         open an uncompressed stream for writing\n'w|gz'       open a gzip compressed stream for writing\n'w|bz2'      open a bzip2 compressed stream for writing\n'w|xz'       open an lzma compressed stream for writing", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/998", "data": {"name": "urllib.request.urlretrieve", "type": "callable", "signature": "(url, filename=None)", "description": "Retrieve a URL into a temporary location on disk.\n\nRequires a URL argument. If a filename is passed, it is used as\nthe temporary file location. The reporthook argument should be\na callable that accepts a block number, a read size, and the\ntotal file size of the URL target. The data argument should be\nvalid URL encoded data.\n\nIf a filename is passed and the URL points to a local resource,\nthe result is a copy from local file to new file.\n\nReturns a tuple containing the path to the newly created\ndata file as well as the resulting HTTPMessage object.", "parameters": {"type": "object", "properties": {"url": {}, "filename": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/999", "data": {"name": "csv.DictReader", "type": "class", "signature": "(f, fieldnames=None, restkey=None, restval=None, dialect='excel', *args, **kwds)", "description": "", "parameters": {"type": "object", "properties": {"f": {}, "fieldnames": {"type": "NoneType", "default": null}, "restkey": {"type": "NoneType", "default": null}, "restval": {"type": "NoneType", "default": null}, "dialect": {"type": "str", "default": "excel"}}}}}
{"task_id": "BigCodeBench/999", "data": {"name": "csv.DictReader.fieldnames", "type": "constant", "signature": null, "description": "", "value": "<property object at 0x7fa572f2af20>", "parameters": {"type": "constant"}}}
{"task_id": "BigCodeBench/999", "data": {"name": "os.remove", "type": "callable", "signature": "(path)", "description": "Remove a file (same as unlink()).\n\nIf dir_fd is not None, it should be a file descriptor open to a directory,\n  and path should be relative; path will then be relative to that directory.\ndir_fd may not be implemented on your platform.\n  If it is unavailable, using it will raise a NotImplementedError.", "parameters": {"type": "object", "properties": {"path": {}}}}}
{"task_id": "BigCodeBench/999", "data": {"name": "urllib.request.urlretrieve", "type": "callable", "signature": "(url, filename=None)", "description": "Retrieve a URL into a temporary location on disk.\n\nRequires a URL argument. If a filename is passed, it is used as\nthe temporary file location. The reporthook argument should be\na callable that accepts a block number, a read size, and the\ntotal file size of the URL target. The data argument should be\nvalid URL encoded data.\n\nIf a filename is passed and the URL points to a local resource,\nthe result is a copy from local file to new file.\n\nReturns a tuple containing the path to the newly created\ndata file as well as the resulting HTTPMessage object.", "parameters": {"type": "object", "properties": {"url": {}, "filename": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/1003", "data": {"name": "child.tag"}}
{"task_id": "BigCodeBench/1003", "data": {"name": "child.text"}}
{"task_id": "BigCodeBench/1003", "data": {"name": "data.append(data_item)"}}
{"task_id": "BigCodeBench/1003", "data": {"name": "lxml.etree.XML", "type": "callable", "signature": "(text)", "description": "XML(text, parser=None, base_url=None)\n\nParses an XML document or fragment from a string constant.\nReturns the root node (or the result returned by a parser target).\nThis function can be used to embed \"XML literals\" in Python code,\nlike in\n\n   >>> root = XML(\"<root><test/></root>\")\n   >>> print(root.tag)\n   root\n\nTo override the parser with a different ``XMLParser`` you can pass it to\nthe ``parser`` keyword argument.\n\nThe ``base_url`` keyword argument allows to set the original base URL of\nthe document to support relative Paths when looking up external entities\n(DTD, XInclude, ...).", "parameters": {"type": "object", "properties": {"text": {}}}}}
{"task_id": "BigCodeBench/1003", "data": {"name": "lxml.etree.XML.findall", "type": "callable", "signature": "(text)", "description": "XML(text, parser=None, base_url=None)\n\nParses an XML document or fragment from a string constant.\nReturns the root node (or the result returned by a parser target).\nThis function can be used to embed \"XML literals\" in Python code,\nlike in\n\n   >>> root = XML(\"<root><test/></root>\")\n   >>> print(root.tag)\n   root\n\nTo override the parser with a different ``XMLParser`` you can pass it to\nthe ``parser`` keyword argument.\n\nThe ``base_url`` keyword argument allows to set the original base URL of\nthe document to support relative Paths when looking up external entities\n(DTD, XInclude, ...).", "parameters": {"type": "object", "properties": {"text": {}}}}}
{"task_id": "BigCodeBench/1003", "data": {"name": "lxml.etree.XMLSyntaxError", "type": "class", "signature": "(message, code, line, column, filename=None)", "description": "Syntax error while parsing an XML document.\n    ", "parameters": {"type": "object", "properties": {"message": {}, "code": {}, "line": {}, "column": {}, "filename": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/1003", "data": {"name": "urllib.request.urlopen", "type": "callable", "signature": "(url)", "description": "Open the URL url, which can be either a string or a Request object.\n\n*data* must be an object specifying additional data to be sent to\nthe server, or None if no such data is needed.  See Request for\ndetails.\n\nurllib.request module uses HTTP/1.1 and includes a \"Connection:close\"\nheader in its HTTP requests.\n\nThe optional *timeout* parameter specifies a timeout in seconds for\nblocking operations like the connection attempt (if not specified, the\nglobal default timeout setting will be used). This only works for HTTP,\nHTTPS and FTP connections.\n\nIf *context* is specified, it must be a ssl.SSLContext instance describing\nthe various SSL options. See HTTPSConnection for more details.\n\nThe optional *cafile* and *capath* parameters specify a set of trusted CA\ncertificates for HTTPS requests. cafile should point to a single file\ncontaining a bundle of CA certificates, whereas capath should point to a\ndirectory of hashed certificate files. More information can be found in\nssl.SSLContext.load_verify_locations().\n\nThe *cadefault* parameter is ignored.\n\n\nThis function always returns an object which can work as a\ncontext manager and has the properties url, headers, and status.\nSee urllib.response.addinfourl for more detail on these properties.\n\nFor HTTP and HTTPS URLs, this function returns a http.client.HTTPResponse\nobject slightly modified. In addition to the three new methods above, the\nmsg attribute contains the same information as the reason attribute ---\nthe reason phrase returned by the server --- instead of the response\nheaders as it is specified in the documentation for HTTPResponse.\n\nFor FTP, file, and data URLs and requests explicitly handled by legacy\nURLopener and FancyURLopener classes, this function returns a\nurllib.response.addinfourl object.\n\nNote that None may be returned if no handler handles the request (though\nthe default installed global OpenerDirector uses UnknownHandler to ensure\nthis never happens).\n\nIn addition, if proxy settings are detected (for example, when a *_proxy\nenvironment variable like http_proxy is set), ProxyHandler is default\ninstalled and makes sure the requests are handled through the proxy.", "parameters": {"type": "object", "properties": {"url": {}}}}}
{"task_id": "BigCodeBench/1003", "data": {"name": "urllib.request.urlopen.read", "type": "callable", "signature": "()", "description": "Open the URL url, which can be either a string or a Request object.\n\n*data* must be an object specifying additional data to be sent to\nthe server, or None if no such data is needed.  See Request for\ndetails.\n\nurllib.request module uses HTTP/1.1 and includes a \"Connection:close\"\nheader in its HTTP requests.\n\nThe optional *timeout* parameter specifies a timeout in seconds for\nblocking operations like the connection attempt (if not specified, the\nglobal default timeout setting will be used). This only works for HTTP,\nHTTPS and FTP connections.\n\nIf *context* is specified, it must be a ssl.SSLContext instance describing\nthe various SSL options. See HTTPSConnection for more details.\n\nThe optional *cafile* and *capath* parameters specify a set of trusted CA\ncertificates for HTTPS requests. cafile should point to a single file\ncontaining a bundle of CA certificates, whereas capath should point to a\ndirectory of hashed certificate files. More information can be found in\nssl.SSLContext.load_verify_locations().\n\nThe *cadefault* parameter is ignored.\n\n\nThis function always returns an object which can work as a\ncontext manager and has the properties url, headers, and status.\nSee urllib.response.addinfourl for more detail on these properties.\n\nFor HTTP and HTTPS URLs, this function returns a http.client.HTTPResponse\nobject slightly modified. In addition to the three new methods above, the\nmsg attribute contains the same information as the reason attribute ---\nthe reason phrase returned by the server --- instead of the response\nheaders as it is specified in the documentation for HTTPResponse.\n\nFor FTP, file, and data URLs and requests explicitly handled by legacy\nURLopener and FancyURLopener classes, this function returns a\nurllib.response.addinfourl object.\n\nNote that None may be returned if no handler handles the request (though\nthe default installed global OpenerDirector uses UnknownHandler to ensure\nthis never happens).\n\nIn addition, if proxy settings are detected (for example, when a *_proxy\nenvironment variable like http_proxy is set), ProxyHandler is default\ninstalled and makes sure the requests are handled through the proxy.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/1004", "data": {"name": "collections.Counter", "type": "class", "signature": "(iterable=None, /, **kwds)", "description": "Dict subclass for counting hashable items.  Sometimes called a bag\nor multiset.  Elements are stored as dictionary keys and their counts\nare stored as dictionary values.\n\n>>> c = Counter('abcdeabcdabcaba')  # count elements from a string\n\n>>> c.most_common(3)                # three most common elements\n[('a', 5), ('b', 4), ('c', 3)]\n>>> sorted(c)                       # list all unique elements\n['a', 'b', 'c', 'd', 'e']\n>>> ''.join(sorted(c.elements()))   # list elements with repetitions\n'aaaaabbbbcccdde'\n>>> sum(c.values())                 # total of all counts\n15\n\n>>> c['a']                          # count of letter 'a'\n5\n>>> for elem in 'shazam':           # update counts from an iterable\n...     c[elem] += 1                # by adding 1 to each element's count\n>>> c['a']                          # now there are seven 'a'\n7\n>>> del c['b']                      # remove all 'b'\n>>> c['b']                          # now there are zero 'b'\n0\n\n>>> d = Counter('simsalabim')       # make another counter\n>>> c.update(d)                     # add in the second counter\n>>> c['a']                          # now there are nine 'a'\n9\n\n>>> c.clear()                       # empty the counter\n>>> c\nCounter()\n\nNote:  If a count is set to zero or reduced to zero, it will remain\nin the counter until the entry is deleted or the counter is cleared:\n\n>>> c = Counter('aaabbc')\n>>> c['b'] -= 2                     # reduce the count of 'b' by two\n>>> c.most_common()                 # 'b' is still in, but its count is zero\n[('a', 3), ('c', 1), ('b', 0)]", "parameters": {"type": "object", "properties": {"iterable": {"type": "NoneType", "default": null}, "/": {}}}}}
{"task_id": "BigCodeBench/1004", "data": {"name": "collections.Counter.most_common", "type": "callable", "signature": "(self, n=None)", "description": "List the n most common elements and their counts from the most\ncommon to the least.  If n is None, then list all element counts.\n\n>>> Counter('abracadabra').most_common(3)\n[('a', 5), ('b', 2), ('r', 2)]", "parameters": {"type": "object", "properties": {"n": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/1004", "data": {"name": "matplotlib.pyplot.subplots", "type": "class", "signature": "()", "description": "Create a figure and a set of subplots.\n\nThis utility wrapper makes it convenient to create common layouts of\nsubplots, including the enclosing figure object, in a single call.\n\nParameters\n----------\nnrows, ncols : int, default: 1\n    Number of rows/columns of the subplot grid.\n\nsharex, sharey : bool or {'none', 'all', 'row', 'col'}, default: False\n    Controls sharing of properties among x (*sharex*) or y (*sharey*)\n    axes:\n\n    - True or 'all': x- or y-axis will be shared among all subplots.\n    - False or 'none': each subplot x- or y-axis will be independent.\n    - 'row': each subplot row will share an x- or y-axis.\n    - 'col': each subplot column will share an x- or y-axis.\n\n    When subplots have a shared x-axis along a column, only the x tick\n    labels of the bottom subplot are created. Similarly, when subplots\n    have a shared y-axis along a row, only the y tick labels of the first\n    column subplot are created. To later turn other subplots' ticklabels\n    on, use `~matplotlib.axes.Axes.tick_params`.\n\n    When subplots have a shared axis that has units, calling\n    `~matplotlib.axis.Axis.set_units` will update each axis with the\n    new units.\n\nsqueeze : bool, default: True\n    - If True, extra dimensions are squeezed out from the returned\n      array of `~matplotlib.axes.Axes`:\n\n      - if only one subplot is constructed (nrows=ncols=1), the\n        resulting single Axes object is returned as a scalar.\n      - for Nx1 or 1xM subplots, the returned object is a 1D numpy\n        object array of Axes objects.\n      - for NxM, subplots with N>1 and M>1 are returned as a 2D array.\n\n    - If False, no squeezing at all is done: the returned Axes object is\n      always a 2D array containing Axes instances, even if it ends up\n      being 1x1.\n\nwidth_ratios : array-like of length *ncols*, optional\n    Defines the relative widths of the columns. Each column gets a\n    relative width of ``width_ratios[i] / sum(width_ratios)``.\n    If not given, all columns will have the same width.  Equivalent\n    to ``gridspec_kw={'width_ratios': [...]}``.\n\nheight_ratios : array-like of length *nrows*, optional\n    Defines the relative heights of the rows. Each row gets a\n    relative height of ``height_ratios[i] / sum(height_ratios)``.\n    If not given, all rows will have the same height. Convenience\n    for ``gridspec_kw={'height_ratios': [...]}``.\n\nsubplot_kw : dict, optional\n    Dict with keywords passed to the\n    `~matplotlib.figure.Figure.add_subplot` call used to create each\n    subplot.\n\ngridspec_kw : dict, optional\n    Dict with keywords passed to the `~matplotlib.gridspec.GridSpec`\n    constructor used to create the grid the subplots are placed on.\n\n**fig_kw\n    All additional keyword arguments are passed to the\n    `.pyplot.figure` call.\n\nReturns\n-------\nfig : `.Figure`\n\nax : `~matplotlib.axes.Axes` or array of Axes\n    *ax* can be either a single `~.axes.Axes` object, or an array of Axes\n    objects if more than one subplot was created.  The dimensions of the\n    resulting array can be controlled with the squeeze keyword, see above.\n\n    Typical idioms for handling the return value are::\n\n        # using the variable ax for single a Axes\n        fig, ax = plt.subplots()\n\n        # using the variable axs for multiple Axes\n        fig, axs = plt.subplots(2, 2)\n\n        # using tuple unpacking for multiple Axes\n        fig, (ax1, ax2) = plt.subplots(1, 2)\n        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n\n    The names ``ax`` and pluralized ``axs`` are preferred over ``axes``\n    because for the latter it's not clear if it refers to a single\n    `~.axes.Axes` instance or a collection of these.\n\nSee Also\n--------\n.pyplot.figure\n.pyplot.subplot\n.pyplot.axes\n.Figure.subplots\n.Figure.add_subplot\n\nExamples\n--------\n::\n\n    # First create some toy data:\n    x = np.linspace(0, 2*np.pi, 400)\n    y = np.sin(x**2)\n\n    # Create just a figure and only one subplot\n    fig, ax = plt.subplots()\n    ax.plot(x, y)\n    ax.set_title('Simple plot')\n\n    # Create two subplots and unpack the output array immediately\n    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n    ax1.plot(x, y)\n    ax1.set_title('Sharing Y axis')\n    ax2.scatter(x, y)\n\n    # Create four polar axes and access them through the returned array\n    fig, axs = plt.subplots(2, 2, subplot_kw=dict(projection=\"polar\"))\n    axs[0, 0].plot(x, y)\n    axs[1, 1].scatter(x, y)\n\n    # Share a X axis with each column of subplots\n    plt.subplots(2, 2, sharex='col')\n\n    # Share a Y axis with each row of subplots\n    plt.subplots(2, 2, sharey='row')\n\n    # Share both X and Y axes with all subplots\n    plt.subplots(2, 2, sharex='all', sharey='all')\n\n    # Note that this is the same as\n    plt.subplots(2, 2, sharex=True, sharey=True)\n\n    # Create figure number 10 with a single subplot\n    # and clears it if it already exists.\n    fig, ax = plt.subplots(num=10, clear=True)", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/1004", "data": {"name": "matplotlib.pyplot.subplots[1].set_xlabel", "type": "method", "signature": "(xlabel, fontdict=None, labelpad=None, *, loc=None, **kwargs)", "description": "Set the label for the x-axis.\n\nParameters\n----------\nxlabel : str\n    The label text.\n\nlabelpad : float, default: :rc:`axes.labelpad`\n    Spacing in points from the Axes bounding box including ticks\n    and tick labels.  If None, the previous value is left as is.\n\nloc : {'left', 'center', 'right'}, default: :rc:`xaxis.labellocation`\n    The label position. This is a high-level alternative for passing\n    parameters *x* and *horizontalalignment*.\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    `.Text` properties control the appearance of the label.\n\nSee Also\n--------\ntext : Documents the properties supported by `.Text`.", "parameters": {"type": "object", "properties": {"xlabel": {}, "fontdict": {"type": "NoneType", "default": null}, "labelpad": {"type": "NoneType", "default": null}, "loc": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/1004", "data": {"name": "matplotlib.pyplot.subplots[1].set_ylabel", "type": "method", "signature": "(ylabel, fontdict=None, labelpad=None, *, loc=None, **kwargs)", "description": "Set the label for the y-axis.\n\nParameters\n----------\nylabel : str\n    The label text.\n\nlabelpad : float, default: :rc:`axes.labelpad`\n    Spacing in points from the Axes bounding box including ticks\n    and tick labels.  If None, the previous value is left as is.\n\nloc : {'bottom', 'center', 'top'}, default: :rc:`yaxis.labellocation`\n    The label position. This is a high-level alternative for passing\n    parameters *y* and *horizontalalignment*.\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    `.Text` properties control the appearance of the label.\n\nSee Also\n--------\ntext : Documents the properties supported by `.Text`.", "parameters": {"type": "object", "properties": {"ylabel": {}, "fontdict": {"type": "NoneType", "default": null}, "labelpad": {"type": "NoneType", "default": null}, "loc": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/1004", "data": {"name": "matplotlib.pyplot.subplots[1].set_title", "type": "method", "signature": "(label, fontdict=None, loc=None, pad=None, *, y=None, **kwargs)", "description": "Set a title for the Axes.\n\nSet one of the three available Axes titles. The available titles\nare positioned above the Axes in the center, flush with the left\nedge, and flush with the right edge.\n\nParameters\n----------\nlabel : str\n    Text to use for the title\n\nfontdict : dict\n\n    .. admonition:: Discouraged\n\n       The use of *fontdict* is discouraged. Parameters should be passed as\n       individual keyword arguments or using dictionary-unpacking\n       ``set_title(..., **fontdict)``.\n\n    A dictionary controlling the appearance of the title text,\n    the default *fontdict* is::\n\n       {'fontsize': rcParams['axes.titlesize'],\n        'fontweight': rcParams['axes.titleweight'],\n        'color': rcParams['axes.titlecolor'],\n        'verticalalignment': 'baseline',\n        'horizontalalignment': loc}\n\nloc : {'center', 'left', 'right'}, default: :rc:`axes.titlelocation`\n    Which title to set.\n\ny : float, default: :rc:`axes.titley`\n    Vertical Axes location for the title (1.0 is the top).  If\n    None (the default) and :rc:`axes.titley` is also None, y is\n    determined automatically to avoid decorators on the Axes.\n\npad : float, default: :rc:`axes.titlepad`\n    The offset of the title from the top of the Axes, in points.\n\nReturns\n-------\n`.Text`\n    The matplotlib text instance representing the title\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    Other keyword arguments are text properties, see `.Text` for a list\n    of valid text properties.", "parameters": {"type": "object", "properties": {"label": {}, "fontdict": {"type": "NoneType", "default": null}, "loc": {"type": "NoneType", "default": null}, "pad": {"type": "NoneType", "default": null}, "y": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/1004", "data": {"name": "matplotlib.pyplot.subplots[1].bar", "type": "method", "signature": "(x, height, width=0.8, bottom=None, *, align='center', data=None, **kwargs)", "description": "Make a bar plot.\n\nThe bars are positioned at *x* with the given *align*\\ment. Their\ndimensions are given by *height* and *width*. The vertical baseline\nis *bottom* (default 0).\n\nMany parameters can take either a single value applying to all bars\nor a sequence of values, one for each bar.\n\nParameters\n----------\nx : float or array-like\n    The x coordinates of the bars. See also *align* for the\n    alignment of the bars to the coordinates.\n\nheight : float or array-like\n    The height(s) of the bars.\n\n    Note that if *bottom* has units (e.g. datetime), *height* should be in\n    units that are a difference from the value of *bottom* (e.g. timedelta).\n\nwidth : float or array-like, default: 0.8\n    The width(s) of the bars.\n\n    Note that if *x* has units (e.g. datetime), then *width* should be in\n    units that are a difference (e.g. timedelta) around the *x* values.\n\nbottom : float or array-like, default: 0\n    The y coordinate(s) of the bottom side(s) of the bars.\n\n    Note that if *bottom* has units, then the y-axis will get a Locator and\n    Formatter appropriate for the units (e.g. dates, or categorical).\n\nalign : {'center', 'edge'}, default: 'center'\n    Alignment of the bars to the *x* coordinates:\n\n    - 'center': Center the base on the *x* positions.\n    - 'edge': Align the left edges of the bars with the *x* positions.\n\n    To align the bars on the right edge pass a negative *width* and\n    ``align='edge'``.\n\nReturns\n-------\n`.BarContainer`\n    Container with all the bars and optionally errorbars.\n\nOther Parameters\n----------------\ncolor : color or list of color, optional\n    The colors of the bar faces.\n\nedgecolor : color or list of color, optional\n    The colors of the bar edges.\n\nlinewidth : float or array-like, optional\n    Width of the bar edge(s). If 0, don't draw edges.\n\ntick_label : str or list of str, optional\n    The tick labels of the bars.\n    Default: None (Use default numeric labels.)\n\nlabel : str or list of str, optional\n    A single label is attached to the resulting `.BarContainer` as a\n    label for the whole dataset.\n    If a list is provided, it must be the same length as *x* and\n    labels the individual bars. Repeated labels are not de-duplicated\n    and will cause repeated label entries, so this is best used when\n    bars also differ in style (e.g., by passing a list to *color*.)\n\nxerr, yerr : float or array-like of shape(N,) or shape(2, N), optional\n    If not *None*, add horizontal / vertical errorbars to the bar tips.\n    The values are +/- sizes relative to the data:\n\n    - scalar: symmetric +/- values for all bars\n    - shape(N,): symmetric +/- values for each bar\n    - shape(2, N): Separate - and + values for each bar. First row\n      contains the lower errors, the second row contains the upper\n      errors.\n    - *None*: No errorbar. (Default)\n\n    See :doc:`/gallery/statistics/errorbar_features` for an example on\n    the usage of *xerr* and *yerr*.\n\necolor : color or list of color, default: 'black'\n    The line color of the errorbars.\n\ncapsize : float, default: :rc:`errorbar.capsize`\n   The length of the error bar caps in points.\n\nerror_kw : dict, optional\n    Dictionary of keyword arguments to be passed to the\n    `~.Axes.errorbar` method. Values of *ecolor* or *capsize* defined\n    here take precedence over the independent keyword arguments.\n\nlog : bool, default: False\n    If *True*, set the y-axis to be log scale.\n\ndata : indexable object, optional\n    If given, all parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` (unless this raises an exception).\n\n**kwargs : `.Rectangle` properties\n\nProperties:\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: scalar or None\n    angle: unknown\n    animated: bool\n    antialiased or aa: bool or None\n    bounds: (left, bottom, width, height)\n    capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    clip_box: `~matplotlib.transforms.BboxBase` or None\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    color: color\n    edgecolor or ec: color or None\n    facecolor or fc: color or None\n    figure: `~matplotlib.figure.Figure`\n    fill: bool\n    gid: str\n    hatch: {'/', '\\\\', '|', '-', '+', 'x', 'o', 'O', '.', '*'}\n    height: unknown\n    in_layout: bool\n    joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    label: object\n    linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n    linewidth or lw: float or None\n    mouseover: bool\n    path_effects: list of `.AbstractPathEffect`\n    picker: None or bool or float or callable\n    rasterized: bool\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    transform: `~matplotlib.transforms.Transform`\n    url: str\n    visible: bool\n    width: unknown\n    x: unknown\n    xy: (float, float)\n    y: unknown\n    zorder: float\n\nSee Also\n--------\nbarh : Plot a horizontal bar plot.\n\nNotes\n-----\nStacked bars can be achieved by passing individual *bottom* values per\nbar. See :doc:`/gallery/lines_bars_and_markers/bar_stacked`.", "parameters": {"type": "object", "properties": {"x": {}, "height": {}, "width": {"type": "float", "default": 0.8}, "bottom": {"type": "NoneType", "default": null}, "align": {"type": "str", "default": "center"}, "data": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/1004", "data": {"name": "re.findall", "type": "callable", "signature": "(pattern, string)", "description": "Return a list of all non-overlapping matches in the string.\n\nIf one or more capturing groups are present in the pattern, return\na list of groups; this will be a list of tuples if the pattern\nhas more than one group.\n\nEmpty matches are included in the result.", "parameters": {"type": "object", "properties": {"pattern": {}, "string": {}}}}}
{"task_id": "BigCodeBench/1004", "data": {"name": "urllib.request.urlopen", "type": "callable", "signature": "(url)", "description": "Open the URL url, which can be either a string or a Request object.\n\n*data* must be an object specifying additional data to be sent to\nthe server, or None if no such data is needed.  See Request for\ndetails.\n\nurllib.request module uses HTTP/1.1 and includes a \"Connection:close\"\nheader in its HTTP requests.\n\nThe optional *timeout* parameter specifies a timeout in seconds for\nblocking operations like the connection attempt (if not specified, the\nglobal default timeout setting will be used). This only works for HTTP,\nHTTPS and FTP connections.\n\nIf *context* is specified, it must be a ssl.SSLContext instance describing\nthe various SSL options. See HTTPSConnection for more details.\n\nThe optional *cafile* and *capath* parameters specify a set of trusted CA\ncertificates for HTTPS requests. cafile should point to a single file\ncontaining a bundle of CA certificates, whereas capath should point to a\ndirectory of hashed certificate files. More information can be found in\nssl.SSLContext.load_verify_locations().\n\nThe *cadefault* parameter is ignored.\n\n\nThis function always returns an object which can work as a\ncontext manager and has the properties url, headers, and status.\nSee urllib.response.addinfourl for more detail on these properties.\n\nFor HTTP and HTTPS URLs, this function returns a http.client.HTTPResponse\nobject slightly modified. In addition to the three new methods above, the\nmsg attribute contains the same information as the reason attribute ---\nthe reason phrase returned by the server --- instead of the response\nheaders as it is specified in the documentation for HTTPResponse.\n\nFor FTP, file, and data URLs and requests explicitly handled by legacy\nURLopener and FancyURLopener classes, this function returns a\nurllib.response.addinfourl object.\n\nNote that None may be returned if no handler handles the request (though\nthe default installed global OpenerDirector uses UnknownHandler to ensure\nthis never happens).\n\nIn addition, if proxy settings are detected (for example, when a *_proxy\nenvironment variable like http_proxy is set), ProxyHandler is default\ninstalled and makes sure the requests are handled through the proxy.", "parameters": {"type": "object", "properties": {"url": {}}}}}
{"task_id": "BigCodeBench/1004", "data": {"name": "urllib.request.urlopen.read", "type": "callable", "signature": "()", "description": "Open the URL url, which can be either a string or a Request object.\n\n*data* must be an object specifying additional data to be sent to\nthe server, or None if no such data is needed.  See Request for\ndetails.\n\nurllib.request module uses HTTP/1.1 and includes a \"Connection:close\"\nheader in its HTTP requests.\n\nThe optional *timeout* parameter specifies a timeout in seconds for\nblocking operations like the connection attempt (if not specified, the\nglobal default timeout setting will be used). This only works for HTTP,\nHTTPS and FTP connections.\n\nIf *context* is specified, it must be a ssl.SSLContext instance describing\nthe various SSL options. See HTTPSConnection for more details.\n\nThe optional *cafile* and *capath* parameters specify a set of trusted CA\ncertificates for HTTPS requests. cafile should point to a single file\ncontaining a bundle of CA certificates, whereas capath should point to a\ndirectory of hashed certificate files. More information can be found in\nssl.SSLContext.load_verify_locations().\n\nThe *cadefault* parameter is ignored.\n\n\nThis function always returns an object which can work as a\ncontext manager and has the properties url, headers, and status.\nSee urllib.response.addinfourl for more detail on these properties.\n\nFor HTTP and HTTPS URLs, this function returns a http.client.HTTPResponse\nobject slightly modified. In addition to the three new methods above, the\nmsg attribute contains the same information as the reason attribute ---\nthe reason phrase returned by the server --- instead of the response\nheaders as it is specified in the documentation for HTTPResponse.\n\nFor FTP, file, and data URLs and requests explicitly handled by legacy\nURLopener and FancyURLopener classes, this function returns a\nurllib.response.addinfourl object.\n\nNote that None may be returned if no handler handles the request (though\nthe default installed global OpenerDirector uses UnknownHandler to ensure\nthis never happens).\n\nIn addition, if proxy settings are detected (for example, when a *_proxy\nenvironment variable like http_proxy is set), ProxyHandler is default\ninstalled and makes sure the requests are handled through the proxy.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/1006", "data": {"name": "zipfile.ZipFile", "type": "class", "signature": "(file, mode='r', compression=0, allowZip64=True, compresslevel=None, *, strict_timestamps=True)", "description": "Class with methods to open, read, write, close, list zip files.\n\nz = ZipFile(file, mode=\"r\", compression=ZIP_STORED, allowZip64=True,\n            compresslevel=None)\n\nfile: Either the path to the file, or a file-like object.\n      If it is a path, the file will be opened and closed by ZipFile.\nmode: The mode can be either read 'r', write 'w', exclusive create 'x',\n      or append 'a'.\ncompression: ZIP_STORED (no compression), ZIP_DEFLATED (requires zlib),\n             ZIP_BZIP2 (requires bz2) or ZIP_LZMA (requires lzma).\nallowZip64: if True ZipFile will create files with ZIP64 extensions when\n            needed, otherwise it will raise an exception when this would\n            be necessary.\ncompresslevel: None (default for the given compression type) or an integer\n               specifying the level to pass to the compressor.\n               When using ZIP_STORED or ZIP_LZMA this keyword has no effect.\n               When using ZIP_DEFLATED integers 0 through 9 are accepted.\n               When using ZIP_BZIP2 integers 1 through 9 are accepted.", "parameters": {"type": "object", "properties": {"file": {}, "mode": {"type": "str", "default": "r"}, "compression": {"type": "integer", "default": 0}, "allowZip64": {"type": "bool", "default": true}, "compresslevel": {"type": "NoneType", "default": null}, "strict_timestamps": {"type": "bool", "default": true}}}}}
{"task_id": "BigCodeBench/1006", "data": {"name": "zipfile.ZipFile.extractall", "type": "callable", "signature": "(self, path=None)", "description": "Extract all members from the archive to the current working\ndirectory. `path' specifies a different directory to extract to.\n`members' is optional and must be a subset of the list returned\nby namelist().", "parameters": {"type": "object", "properties": {"path": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/1006", "data": {"name": "open.write(response.content)"}}
{"task_id": "BigCodeBench/1006", "data": {"name": "os.path.basename", "type": "callable", "signature": "(p)", "description": "Returns the final component of a pathname", "parameters": {"type": "object", "properties": {"p": {}}}}}
{"task_id": "BigCodeBench/1006", "data": {"name": "os.makedirs", "type": "callable", "signature": "(name)", "description": "makedirs(name [, mode=0o777][, exist_ok=False])\n\nSuper-mkdir; create a leaf directory and all intermediate ones.  Works like\nmkdir, except that any intermediate path segment (not just the rightmost)\nwill be created if it does not exist. If the target directory already\nexists, raise an OSError if exist_ok is False. Otherwise no exception is\nraised.  This is recursive.", "parameters": {"type": "object", "properties": {"name": {}}}}}
{"task_id": "BigCodeBench/1006", "data": {"name": "os.path.splitext", "type": "callable", "signature": "(p)", "description": "Split the extension from a pathname.\n\nExtension is everything from the last dot to the end, ignoring\nleading dots.  Returns \"(root, ext)\"; ext may be empty.", "parameters": {"type": "object", "properties": {"p": {}}}}}
{"task_id": "BigCodeBench/1006", "data": {"name": "os.path.exists", "type": "callable", "signature": "(path)", "description": "Test whether a path exists.  Returns False for broken symbolic links", "parameters": {"type": "object", "properties": {"path": {}}}}}
{"task_id": "BigCodeBench/1006", "data": {"name": "os.path.join", "type": "callable", "signature": "(a, *p)", "description": "Join two or more pathname components, inserting '/' as needed.\nIf any component is an absolute path, all previous path components\nwill be discarded.  An empty last part will result in a path that\nends with a separator.", "parameters": {"type": "object", "properties": {"a": {}}}}}
{"task_id": "BigCodeBench/1006", "data": {"name": "requests.get", "type": "callable", "signature": "(url, **kwargs)", "description": "Sends a GET request.\n\n:param url: URL for the new :class:`Request` object.\n:param params: (optional) Dictionary, list of tuples or bytes to send\n    in the query string for the :class:`Request`.\n:param \\*\\*kwargs: Optional arguments that ``request`` takes.\n:return: :class:`Response <Response>` object\n:rtype: requests.Response", "parameters": {"type": "object", "properties": {"url": {}}}}}
{"task_id": "BigCodeBench/1006", "data": {"name": "requests.get.headers", "type": "callable", "signature": "(url, params=None, **kwargs)", "description": "Sends a GET request.\n\n:param url: URL for the new :class:`Request` object.\n:param params: (optional) Dictionary, list of tuples or bytes to send\n    in the query string for the :class:`Request`.\n:param \\*\\*kwargs: Optional arguments that ``request`` takes.\n:return: :class:`Response <Response>` object\n:rtype: requests.Response"}}
{"task_id": "BigCodeBench/1006", "data": {"name": "requests.get.headers.get", "type": "callable", "signature": "(url, params=None)", "description": "Sends a GET request.\n\n:param url: URL for the new :class:`Request` object.\n:param params: (optional) Dictionary, list of tuples or bytes to send\n    in the query string for the :class:`Request`.\n:param \\*\\*kwargs: Optional arguments that ``request`` takes.\n:return: :class:`Response <Response>` object\n:rtype: requests.Response", "parameters": {"type": "object", "properties": {"url": {}, "params": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/1006", "data": {"name": "requests.get.content", "type": "callable", "signature": "(url, params=None, **kwargs)", "description": "Sends a GET request.\n\n:param url: URL for the new :class:`Request` object.\n:param params: (optional) Dictionary, list of tuples or bytes to send\n    in the query string for the :class:`Request`.\n:param \\*\\*kwargs: Optional arguments that ``request`` takes.\n:return: :class:`Response <Response>` object\n:rtype: requests.Response", "parameters": {"type": "object", "properties": {"url": {}, "params": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/1006", "data": {"name": "requests.get.raise_for_status", "type": "callable", "signature": "()", "description": "Sends a GET request.\n\n:param url: URL for the new :class:`Request` object.\n:param params: (optional) Dictionary, list of tuples or bytes to send\n    in the query string for the :class:`Request`.\n:param \\*\\*kwargs: Optional arguments that ``request`` takes.\n:return: :class:`Response <Response>` object\n:rtype: requests.Response", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/1008", "data": {"name": "bs4.BeautifulSoup", "type": "class", "signature": "(markup='', features=None, builder=None, parse_only=None, from_encoding=None, exclude_encodings=None, element_classes=None, **kwargs)", "description": "A data structure representing a parsed HTML or XML document.\n\nMost of the methods you'll call on a BeautifulSoup object are inherited from\nPageElement or Tag.\n\nInternally, this class defines the basic interface called by the\ntree builders when converting an HTML/XML document into a data\nstructure. The interface abstracts away the differences between\nparsers. To write a new tree builder, you'll need to understand\nthese methods as a whole.\n\nThese methods will be called by the BeautifulSoup constructor:\n  * reset()\n  * feed(markup)\n\nThe tree builder may call these methods from its feed() implementation:\n  * handle_starttag(name, attrs) # See note about return value\n  * handle_endtag(name)\n  * handle_data(data) # Appends to the current data node\n  * endData(containerClass) # Ends the current data node\n\nNo matter how complicated the underlying parser is, you should be\nable to build a tree using 'start tag' events, 'end tag' events,\n'data' events, and \"done with data\" events.\n\nIf you encounter an empty-element tag (aka a self-closing tag,\nlike HTML's <br> tag), call handle_starttag and then\nhandle_endtag.", "parameters": {"type": "object", "properties": {"markup": {"type": "str", "default": ""}, "features": {"type": "NoneType", "default": null}, "builder": {"type": "NoneType", "default": null}, "parse_only": {"type": "NoneType", "default": null}, "from_encoding": {"type": "NoneType", "default": null}, "exclude_encodings": {"type": "NoneType", "default": null}, "element_classes": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/1008", "data": {"name": "bs4.BeautifulSoup.find", "type": "callable", "signature": "(self, name=None, attrs={})", "description": "Look in the children of this PageElement and find the first\nPageElement that matches the given criteria.\n\nAll find_* methods take a common set of arguments. See the online\ndocumentation for detailed explanations.\n\n:param name: A filter on tag name.\n:param attrs: A dictionary of filters on attribute values.\n:param recursive: If this is True, find() will perform a\n    recursive search of this PageElement's children. Otherwise,\n    only the direct children will be considered.\n:param limit: Stop looking after finding this many results.\n:kwargs: A dictionary of filters on attribute values.\n:return: A PageElement.\n:rtype: bs4.element.PageElement", "parameters": {"type": "object", "properties": {"name": {"type": "NoneType", "default": null}, "attrs": {"type": "str", "default": "{}"}}}}}
{"task_id": "BigCodeBench/1008", "data": {"name": "pandas.read_html", "type": "callable", "signature": "(io: 'FilePath | ReadBuffer[str]')", "description": "Read HTML tables into a ``list`` of ``DataFrame`` objects.\n\nParameters\n----------\nio : str, path object, or file-like object\n    String, path object (implementing ``os.PathLike[str]``), or file-like\n    object implementing a string ``read()`` function.\n    The string can represent a URL or the HTML itself. Note that\n    lxml only accepts the http, ftp and file url protocols. If you have a\n    URL that starts with ``'https'`` you might try removing the ``'s'``.\n\n    .. deprecated:: 2.1.0\n        Passing html literal strings is deprecated.\n        Wrap literal string/bytes input in ``io.StringIO``/``io.BytesIO`` instead.\n\nmatch : str or compiled regular expression, optional\n    The set of tables containing text matching this regex or string will be\n    returned. Unless the HTML is extremely simple you will probably need to\n    pass a non-empty string here. Defaults to '.+' (match any non-empty\n    string). The default value will return all tables contained on a page.\n    This value is converted to a regular expression so that there is\n    consistent behavior between Beautiful Soup and lxml.\n\nflavor : {\"lxml\", \"html5lib\", \"bs4\"} or list-like, optional\n    The parsing engine (or list of parsing engines) to use. 'bs4' and\n    'html5lib' are synonymous with each other, they are both there for\n    backwards compatibility. The default of ``None`` tries to use ``lxml``\n    to parse and if that fails it falls back on ``bs4`` + ``html5lib``.\n\nheader : int or list-like, optional\n    The row (or list of rows for a :class:`~pandas.MultiIndex`) to use to\n    make the columns headers.\n\nindex_col : int or list-like, optional\n    The column (or list of columns) to use to create the index.\n\nskiprows : int, list-like or slice, optional\n    Number of rows to skip after parsing the column integer. 0-based. If a\n    sequence of integers or a slice is given, will skip the rows indexed by\n    that sequence.  Note that a single element sequence means 'skip the nth\n    row' whereas an integer means 'skip n rows'.\n\nattrs : dict, optional\n    This is a dictionary of attributes that you can pass to use to identify\n    the table in the HTML. These are not checked for validity before being\n    passed to lxml or Beautiful Soup. However, these attributes must be\n    valid HTML table attributes to work correctly. For example, ::\n\n        attrs = {'id': 'table'}\n\n    is a valid attribute dictionary because the 'id' HTML tag attribute is\n    a valid HTML attribute for *any* HTML tag as per `this document\n    <https://html.spec.whatwg.org/multipage/dom.html#global-attributes>`__. ::\n\n        attrs = {'asdf': 'table'}\n\n    is *not* a valid attribute dictionary because 'asdf' is not a valid\n    HTML attribute even if it is a valid XML attribute.  Valid HTML 4.01\n    table attributes can be found `here\n    <http://www.w3.org/TR/REC-html40/struct/tables.html#h-11.2>`__. A\n    working draft of the HTML 5 spec can be found `here\n    <https://html.spec.whatwg.org/multipage/tables.html>`__. It contains the\n    latest information on table attributes for the modern web.\n\nparse_dates : bool, optional\n    See :func:`~read_csv` for more details.\n\nthousands : str, optional\n    Separator to use to parse thousands. Defaults to ``','``.\n\nencoding : str, optional\n    The encoding used to decode the web page. Defaults to ``None``.``None``\n    preserves the previous encoding behavior, which depends on the\n    underlying parser library (e.g., the parser library will try to use\n    the encoding provided by the document).\n\ndecimal : str, default '.'\n    Character to recognize as decimal point (e.g. use ',' for European\n    data).\n\nconverters : dict, default None\n    Dict of functions for converting values in certain columns. Keys can\n    either be integers or column labels, values are functions that take one\n    input argument, the cell (not column) content, and return the\n    transformed content.\n\nna_values : iterable, default None\n    Custom NA values.\n\nkeep_default_na : bool, default True\n    If na_values are specified and keep_default_na is False the default NaN\n    values are overridden, otherwise they're appended to.\n\ndisplayed_only : bool, default True\n    Whether elements with \"display: none\" should be parsed.\n\nextract_links : {None, \"all\", \"header\", \"body\", \"footer\"}\n    Table elements in the specified section(s) with <a> tags will have their\n    href extracted.\n\n    .. versionadded:: 1.5.0\n\ndtype_backend : {'numpy_nullable', 'pyarrow'}, default 'numpy_nullable'\n    Back-end data type applied to the resultant :class:`DataFrame`\n    (still experimental). Behaviour is as follows:\n\n    * ``\"numpy_nullable\"``: returns nullable-dtype-backed :class:`DataFrame`\n      (default).\n    * ``\"pyarrow\"``: returns pyarrow-backed nullable :class:`ArrowDtype`\n      DataFrame.\n\n    .. versionadded:: 2.0\n\nstorage_options : dict, optional\n    Extra options that make sense for a particular storage connection, e.g.\n    host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n    are forwarded to ``urllib.request.Request`` as header options. For other\n    URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n    forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n    details, and for more examples on storage options refer `here\n    <https://pandas.pydata.org/docs/user_guide/io.html?\n    highlight=storage_options#reading-writing-remote-files>`_.\n\n    .. versionadded:: 2.1.0\n\nReturns\n-------\ndfs\n    A list of DataFrames.\n\nSee Also\n--------\nread_csv : Read a comma-separated values (csv) file into DataFrame.\n\nNotes\n-----\nBefore using this function you should read the :ref:`gotchas about the\nHTML parsing libraries <io.html.gotchas>`.\n\nExpect to do some cleanup after you call this function. For example, you\nmight need to manually assign column names if the column names are\nconverted to NaN when you pass the `header=0` argument. We try to assume as\nlittle as possible about the structure of the table and push the\nidiosyncrasies of the HTML contained in the table to the user.\n\nThis function searches for ``<table>`` elements and only for ``<tr>``\nand ``<th>`` rows and ``<td>`` elements within each ``<tr>`` or ``<th>``\nelement in the table. ``<td>`` stands for \"table data\". This function\nattempts to properly handle ``colspan`` and ``rowspan`` attributes.\nIf the function has a ``<thead>`` argument, it is used to construct\nthe header, otherwise the function attempts to find the header within\nthe body (by putting rows with only ``<th>`` elements into the header).\n\nSimilar to :func:`~read_csv` the `header` argument is applied\n**after** `skiprows` is applied.\n\nThis function will *always* return a list of :class:`DataFrame` *or*\nit will fail, e.g., it will *not* return an empty list.\n\nExamples\n--------\nSee the :ref:`read_html documentation in the IO section of the docs\n<io.read_html>` for some examples of reading in HTML tables.", "parameters": {"type": "object", "properties": {"io": {"type": ["readbuffer[str]", "filepath"]}}}}}
{"task_id": "BigCodeBench/1008", "data": {"name": "requests.get", "type": "callable", "signature": "(url, **kwargs)", "description": "Sends a GET request.\n\n:param url: URL for the new :class:`Request` object.\n:param params: (optional) Dictionary, list of tuples or bytes to send\n    in the query string for the :class:`Request`.\n:param \\*\\*kwargs: Optional arguments that ``request`` takes.\n:return: :class:`Response <Response>` object\n:rtype: requests.Response", "parameters": {"type": "object", "properties": {"url": {}}}}}
{"task_id": "BigCodeBench/1008", "data": {"name": "requests.get.text", "type": "callable", "signature": "(url, params=None, **kwargs)", "description": "Sends a GET request.\n\n:param url: URL for the new :class:`Request` object.\n:param params: (optional) Dictionary, list of tuples or bytes to send\n    in the query string for the :class:`Request`.\n:param \\*\\*kwargs: Optional arguments that ``request`` takes.\n:return: :class:`Response <Response>` object\n:rtype: requests.Response", "parameters": {"type": "object", "properties": {"url": {}, "params": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/1008", "data": {"name": "requests.get.raise_for_status", "type": "callable", "signature": "()", "description": "Sends a GET request.\n\n:param url: URL for the new :class:`Request` object.\n:param params: (optional) Dictionary, list of tuples or bytes to send\n    in the query string for the :class:`Request`.\n:param \\*\\*kwargs: Optional arguments that ``request`` takes.\n:return: :class:`Response <Response>` object\n:rtype: requests.Response", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/1008", "data": {"name": "requests.exceptions.HTTPError", "type": "class", "signature": "(*args, **kwargs)", "description": "An HTTP error occurred.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/1008", "data": {"name": "soup.find('table', {'id': table_id}).find_all('tr')"}}
{"task_id": "BigCodeBench/1012", "data": {"name": "zipfile.ZipFile", "type": "class", "signature": "(file, mode='r', compression=0, allowZip64=True, compresslevel=None, *, strict_timestamps=True)", "description": "Class with methods to open, read, write, close, list zip files.\n\nz = ZipFile(file, mode=\"r\", compression=ZIP_STORED, allowZip64=True,\n            compresslevel=None)\n\nfile: Either the path to the file, or a file-like object.\n      If it is a path, the file will be opened and closed by ZipFile.\nmode: The mode can be either read 'r', write 'w', exclusive create 'x',\n      or append 'a'.\ncompression: ZIP_STORED (no compression), ZIP_DEFLATED (requires zlib),\n             ZIP_BZIP2 (requires bz2) or ZIP_LZMA (requires lzma).\nallowZip64: if True ZipFile will create files with ZIP64 extensions when\n            needed, otherwise it will raise an exception when this would\n            be necessary.\ncompresslevel: None (default for the given compression type) or an integer\n               specifying the level to pass to the compressor.\n               When using ZIP_STORED or ZIP_LZMA this keyword has no effect.\n               When using ZIP_DEFLATED integers 0 through 9 are accepted.\n               When using ZIP_BZIP2 integers 1 through 9 are accepted.", "parameters": {"type": "object", "properties": {"file": {}, "mode": {"type": "str", "default": "r"}, "compression": {"type": "integer", "default": 0}, "allowZip64": {"type": "bool", "default": true}, "compresslevel": {"type": "NoneType", "default": null}, "strict_timestamps": {"type": "bool", "default": true}}}}}
{"task_id": "BigCodeBench/1012", "data": {"name": "zipfile.ZipFile.extractall", "type": "callable", "signature": "(self, path=None)", "description": "Extract all members from the archive to the current working\ndirectory. `path' specifies a different directory to extract to.\n`members' is optional and must be a subset of the list returned\nby namelist().", "parameters": {"type": "object", "properties": {"path": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/1012", "data": {"name": "file.name"}}
{"task_id": "BigCodeBench/1012", "data": {"name": "filepath.parent"}}
{"task_id": "BigCodeBench/1012", "data": {"name": "filepath.parent.mkdir(parents=True, exist_ok=True)"}}
{"task_id": "BigCodeBench/1012", "data": {"name": "open.write(data)"}}
{"task_id": "BigCodeBench/1012", "data": {"name": "requests.exceptions.RequestException", "type": "class", "signature": "(*args, **kwargs)", "description": "There was an ambiguous exception that occurred while handling your\nrequest.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/1012", "data": {"name": "requests.get", "type": "callable", "signature": "(url, **kwargs)", "description": "Sends a GET request.\n\n:param url: URL for the new :class:`Request` object.\n:param params: (optional) Dictionary, list of tuples or bytes to send\n    in the query string for the :class:`Request`.\n:param \\*\\*kwargs: Optional arguments that ``request`` takes.\n:return: :class:`Response <Response>` object\n:rtype: requests.Response", "parameters": {"type": "object", "properties": {"url": {}}}}}
{"task_id": "BigCodeBench/1012", "data": {"name": "requests.get.status_code", "type": "callable", "signature": "(url, params=None, **kwargs)", "description": "Sends a GET request.\n\n:param url: URL for the new :class:`Request` object.\n:param params: (optional) Dictionary, list of tuples or bytes to send\n    in the query string for the :class:`Request`.\n:param \\*\\*kwargs: Optional arguments that ``request`` takes.\n:return: :class:`Response <Response>` object\n:rtype: requests.Response", "parameters": {"type": "object", "properties": {"url": {}, "params": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/1012", "data": {"name": "requests.get.iter_content", "type": "callable", "signature": "()", "description": "Sends a GET request.\n\n:param url: URL for the new :class:`Request` object.\n:param params: (optional) Dictionary, list of tuples or bytes to send\n    in the query string for the :class:`Request`.\n:param \\*\\*kwargs: Optional arguments that ``request`` takes.\n:return: :class:`Response <Response>` object\n:rtype: requests.Response", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/1012", "data": {"name": "zip_dir.iterdir()"}}
{"task_id": "BigCodeBench/1012", "data": {"name": "zip_dir.mkdir(parents=True, exist_ok=True)"}}
{"task_id": "BigCodeBench/1013", "data": {"name": "bs4.BeautifulSoup", "type": "class", "signature": "(markup='', features=None, builder=None, parse_only=None, from_encoding=None, exclude_encodings=None, element_classes=None, **kwargs)", "description": "A data structure representing a parsed HTML or XML document.\n\nMost of the methods you'll call on a BeautifulSoup object are inherited from\nPageElement or Tag.\n\nInternally, this class defines the basic interface called by the\ntree builders when converting an HTML/XML document into a data\nstructure. The interface abstracts away the differences between\nparsers. To write a new tree builder, you'll need to understand\nthese methods as a whole.\n\nThese methods will be called by the BeautifulSoup constructor:\n  * reset()\n  * feed(markup)\n\nThe tree builder may call these methods from its feed() implementation:\n  * handle_starttag(name, attrs) # See note about return value\n  * handle_endtag(name)\n  * handle_data(data) # Appends to the current data node\n  * endData(containerClass) # Ends the current data node\n\nNo matter how complicated the underlying parser is, you should be\nable to build a tree using 'start tag' events, 'end tag' events,\n'data' events, and \"done with data\" events.\n\nIf you encounter an empty-element tag (aka a self-closing tag,\nlike HTML's <br> tag), call handle_starttag and then\nhandle_endtag.", "parameters": {"type": "object", "properties": {"markup": {"type": "str", "default": ""}, "features": {"type": "NoneType", "default": null}, "builder": {"type": "NoneType", "default": null}, "parse_only": {"type": "NoneType", "default": null}, "from_encoding": {"type": "NoneType", "default": null}, "exclude_encodings": {"type": "NoneType", "default": null}, "element_classes": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/1013", "data": {"name": "bs4.BeautifulSoup.find_all", "type": "callable", "signature": "(self, name=None, **kwargs)", "description": "Look in the children of this PageElement and find all\nPageElements that match the given criteria.\n\nAll find_* methods take a common set of arguments. See the online\ndocumentation for detailed explanations.\n\n:param name: A filter on tag name.\n:param attrs: A dictionary of filters on attribute values.\n:param recursive: If this is True, find_all() will perform a\n    recursive search of this PageElement's children. Otherwise,\n    only the direct children will be considered.\n:param limit: Stop looking after finding this many results.\n:kwargs: A dictionary of filters on attribute values.\n:return: A ResultSet of PageElements.\n:rtype: bs4.element.ResultSet", "parameters": {"type": "object", "properties": {"name": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/1013", "data": {"name": "csv.writer", "type": "callable", "signature": "(*args, **kwargs)", "description": "csv_writer = csv.writer(fileobj [, dialect='excel']\n                            [optional keyword args])\n    for row in sequence:\n        csv_writer.writerow(row)\n\n    [or]\n\n    csv_writer = csv.writer(fileobj [, dialect='excel']\n                            [optional keyword args])\n    csv_writer.writerows(rows)\n\nThe \"fileobj\" argument can be any object that supports the file API.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/1013", "data": {"name": "csv.writer.writerow", "type": "callable", "signature": "(*args, **kwargs)", "description": "csv_writer = csv.writer(fileobj [, dialect='excel']\n                            [optional keyword args])\n    for row in sequence:\n        csv_writer.writerow(row)\n\n    [or]\n\n    csv_writer = csv.writer(fileobj [, dialect='excel']\n                            [optional keyword args])\n    csv_writer.writerows(rows)\n\nThe \"fileobj\" argument can be any object that supports the file API.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/1013", "data": {"name": "requests.get", "type": "callable", "signature": "(url)", "description": "Sends a GET request.\n\n:param url: URL for the new :class:`Request` object.\n:param params: (optional) Dictionary, list of tuples or bytes to send\n    in the query string for the :class:`Request`.\n:param \\*\\*kwargs: Optional arguments that ``request`` takes.\n:return: :class:`Response <Response>` object\n:rtype: requests.Response", "parameters": {"type": "object", "properties": {"url": {}}}}}
{"task_id": "BigCodeBench/1013", "data": {"name": "requests.get.text", "type": "callable", "signature": "(url, params=None, **kwargs)", "description": "Sends a GET request.\n\n:param url: URL for the new :class:`Request` object.\n:param params: (optional) Dictionary, list of tuples or bytes to send\n    in the query string for the :class:`Request`.\n:param \\*\\*kwargs: Optional arguments that ``request`` takes.\n:return: :class:`Response <Response>` object\n:rtype: requests.Response", "parameters": {"type": "object", "properties": {"url": {}, "params": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/1013", "data": {"name": "urllib.parse.urljoin", "type": "callable", "signature": "(base, url)", "description": "Join a base URL and a possibly relative URL to form an absolute\ninterpretation of the latter.", "parameters": {"type": "object", "properties": {"base": {}, "url": {}}}}}
{"task_id": "BigCodeBench/1015", "data": {"name": "pandas.DataFrame", "type": "class", "signature": "(data=None, index: 'Axes | None' = None, columns: 'Axes | None' = None, dtype: 'Dtype | None' = None, copy: 'bool | None' = None) -> 'None'", "description": "Two-dimensional, size-mutable, potentially heterogeneous tabular data.\n\nData structure also contains labeled axes (rows and columns).\nArithmetic operations align on both row and column labels. Can be\nthought of as a dict-like container for Series objects. The primary\npandas data structure.\n\nParameters\n----------\ndata : ndarray (structured or homogeneous), Iterable, dict, or DataFrame\n    Dict can contain Series, arrays, constants, dataclass or list-like objects. If\n    data is a dict, column order follows insertion-order. If a dict contains Series\n    which have an index defined, it is aligned by its index. This alignment also\n    occurs if data is a Series or a DataFrame itself. Alignment is done on\n    Series/DataFrame inputs.\n\n    If data is a list of dicts, column order follows insertion-order.\n\nindex : Index or array-like\n    Index to use for resulting frame. Will default to RangeIndex if\n    no indexing information part of input data and no index provided.\ncolumns : Index or array-like\n    Column labels to use for resulting frame when data does not have them,\n    defaulting to RangeIndex(0, 1, 2, ..., n). If data contains column labels,\n    will perform column selection instead.\ndtype : dtype, default None\n    Data type to force. Only a single dtype is allowed. If None, infer.\ncopy : bool or None, default None\n    Copy data from inputs.\n    For dict data, the default of None behaves like ``copy=True``.  For DataFrame\n    or 2d ndarray input, the default of None behaves like ``copy=False``.\n    If data is a dict containing one or more Series (possibly of different dtypes),\n    ``copy=False`` will ensure that these inputs are not copied.\n\n    .. versionchanged:: 1.3.0\n\nSee Also\n--------\nDataFrame.from_records : Constructor from tuples, also record arrays.\nDataFrame.from_dict : From dicts of Series, arrays, or dicts.\nread_csv : Read a comma-separated values (csv) file into DataFrame.\nread_table : Read general delimited file into DataFrame.\nread_clipboard : Read text from clipboard into DataFrame.\n\nNotes\n-----\nPlease reference the :ref:`User Guide <basics.dataframe>` for more information.\n\nExamples\n--------\nConstructing DataFrame from a dictionary.\n\n>>> d = {'col1': [1, 2], 'col2': [3, 4]}\n>>> df = pd.DataFrame(data=d)\n>>> df\n   col1  col2\n0     1     3\n1     2     4\n\nNotice that the inferred dtype is int64.\n\n>>> df.dtypes\ncol1    int64\ncol2    int64\ndtype: object\n\nTo enforce a single dtype:\n\n>>> df = pd.DataFrame(data=d, dtype=np.int8)\n>>> df.dtypes\ncol1    int8\ncol2    int8\ndtype: object\n\nConstructing DataFrame from a dictionary including Series:\n\n>>> d = {'col1': [0, 1, 2, 3], 'col2': pd.Series([2, 3], index=[2, 3])}\n>>> pd.DataFrame(data=d, index=[0, 1, 2, 3])\n   col1  col2\n0     0   NaN\n1     1   NaN\n2     2   2.0\n3     3   3.0\n\nConstructing DataFrame from numpy ndarray:\n\n>>> df2 = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),\n...                    columns=['a', 'b', 'c'])\n>>> df2\n   a  b  c\n0  1  2  3\n1  4  5  6\n2  7  8  9\n\nConstructing DataFrame from a numpy ndarray that has labeled columns:\n\n>>> data = np.array([(1, 2, 3), (4, 5, 6), (7, 8, 9)],\n...                 dtype=[(\"a\", \"i4\"), (\"b\", \"i4\"), (\"c\", \"i4\")])\n>>> df3 = pd.DataFrame(data, columns=['c', 'a'])\n...\n>>> df3\n   c  a\n0  3  1\n1  6  4\n2  9  7\n\nConstructing DataFrame from dataclass:\n\n>>> from dataclasses import make_dataclass\n>>> Point = make_dataclass(\"Point\", [(\"x\", int), (\"y\", int)])\n>>> pd.DataFrame([Point(0, 0), Point(0, 3), Point(2, 3)])\n   x  y\n0  0  0\n1  0  3\n2  2  3\n\nConstructing DataFrame from Series/DataFrame:\n\n>>> ser = pd.Series([1, 2, 3], index=[\"a\", \"b\", \"c\"])\n>>> df = pd.DataFrame(data=ser, index=[\"a\", \"c\"])\n>>> df\n   0\na  1\nc  3\n\n>>> df1 = pd.DataFrame([1, 2, 3], index=[\"a\", \"b\", \"c\"], columns=[\"x\"])\n>>> df2 = pd.DataFrame(data=df1, index=[\"a\", \"c\"])\n>>> df2\n   x\na  1\nc  3", "parameters": {"type": "object", "properties": {"data": {"type": "NoneType", "default": null}, "index": {"type": ["axes", "null"], "default": null}, "columns": {"type": ["axes", "null"], "default": null}, "dtype": {"type": ["dtype", "null"], "default": null}, "copy": {"type": ["bool", "null"], "default": null}}}}}
{"task_id": "BigCodeBench/1015", "data": {"name": "pandas.DataFrame.empty", "type": "constant", "signature": null, "description": "Indicator whether Series/DataFrame is empty.\n\nTrue if Series/DataFrame is entirely empty (no items), meaning any of the\naxes are of length 0.\n\nReturns\n-------\nbool\n    If Series/DataFrame is empty, return True, if not return False.\n\nSee Also\n--------\nSeries.dropna : Return series without null values.\nDataFrame.dropna : Return DataFrame with labels on given axis omitted\n    where (all or any) data are missing.\n\nNotes\n-----\nIf Series/DataFrame contains only NaNs, it is still not considered empty. See\nthe example below.\n\nExamples\n--------\nAn example of an actual empty DataFrame. Notice the index is empty:\n\n>>> df_empty = pd.DataFrame({'A' : []})\n>>> df_empty\nEmpty DataFrame\nColumns: [A]\nIndex: []\n>>> df_empty.empty\nTrue\n\nIf we only have NaNs in our DataFrame, it is not considered empty! We\nwill need to drop the NaNs to make the DataFrame empty:\n\n>>> df = pd.DataFrame({'A' : [np.nan]})\n>>> df\n    A\n0 NaN\n>>> df.empty\nFalse\n>>> df.dropna().empty\nTrue\n\n>>> ser_empty = pd.Series({'A' : []})\n>>> ser_empty\nA    []\ndtype: object\n>>> ser_empty.empty\nFalse\n>>> ser_empty = pd.Series()\n>>> ser_empty.empty\nTrue", "value": "<property object at 0x7fa4c5615210>", "parameters": {"type": "constant"}}}
{"task_id": "BigCodeBench/1015", "data": {"name": "pandas.DataFrame.to_sql", "type": "callable", "signature": "(self, name: 'str', con, if_exists: \"Literal['fail', index: 'bool_t' = True)", "description": "Write records stored in a DataFrame to a SQL database.\n\nDatabases supported by SQLAlchemy [1]_ are supported. Tables can be\nnewly created, appended to, or overwritten.\n\nParameters\n----------\nname : str\n    Name of SQL table.\ncon : sqlalchemy.engine.(Engine or Connection) or sqlite3.Connection\n    Using SQLAlchemy makes it possible to use any DB supported by that\n    library. Legacy support is provided for sqlite3.Connection objects. The user\n    is responsible for engine disposal and connection closure for the SQLAlchemy\n    connectable. See `here                 <https://docs.sqlalchemy.org/en/20/core/connections.html>`_.\n    If passing a sqlalchemy.engine.Connection which is already in a transaction,\n    the transaction will not be committed.  If passing a sqlite3.Connection,\n    it will not be possible to roll back the record insertion.\n\nschema : str, optional\n    Specify the schema (if database flavor supports this). If None, use\n    default schema.\nif_exists : {'fail', 'replace', 'append'}, default 'fail'\n    How to behave if the table already exists.\n\n    * fail: Raise a ValueError.\n    * replace: Drop the table before inserting new values.\n    * append: Insert new values to the existing table.\n\nindex : bool, default True\n    Write DataFrame index as a column. Uses `index_label` as the column\n    name in the table. Creates a table index for this column.\nindex_label : str or sequence, default None\n    Column label for index column(s). If None is given (default) and\n    `index` is True, then the index names are used.\n    A sequence should be given if the DataFrame uses MultiIndex.\nchunksize : int, optional\n    Specify the number of rows in each batch to be written at a time.\n    By default, all rows will be written at once.\ndtype : dict or scalar, optional\n    Specifying the datatype for columns. If a dictionary is used, the\n    keys should be the column names and the values should be the\n    SQLAlchemy types or strings for the sqlite3 legacy mode. If a\n    scalar is provided, it will be applied to all columns.\nmethod : {None, 'multi', callable}, optional\n    Controls the SQL insertion clause used:\n\n    * None : Uses standard SQL ``INSERT`` clause (one per row).\n    * 'multi': Pass multiple values in a single ``INSERT`` clause.\n    * callable with signature ``(pd_table, conn, keys, data_iter)``.\n\n    Details and a sample callable implementation can be found in the\n    section :ref:`insert method <io.sql.method>`.\n\nReturns\n-------\nNone or int\n    Number of rows affected by to_sql. None is returned if the callable\n    passed into ``method`` does not return an integer number of rows.\n\n    The number of returned rows affected is the sum of the ``rowcount``\n    attribute of ``sqlite3.Cursor`` or SQLAlchemy connectable which may not\n    reflect the exact number of written rows as stipulated in the\n    `sqlite3 <https://docs.python.org/3/library/sqlite3.html#sqlite3.Cursor.rowcount>`__ or\n    `SQLAlchemy <https://docs.sqlalchemy.org/en/20/core/connections.html#sqlalchemy.engine.CursorResult.rowcount>`__.\n\n    .. versionadded:: 1.4.0\n\nRaises\n------\nValueError\n    When the table already exists and `if_exists` is 'fail' (the\n    default).\n\nSee Also\n--------\nread_sql : Read a DataFrame from a table.\n\nNotes\n-----\nTimezone aware datetime columns will be written as\n``Timestamp with timezone`` type with SQLAlchemy if supported by the\ndatabase. Otherwise, the datetimes will be stored as timezone unaware\ntimestamps local to the original timezone.\n\nNot all datastores support ``method=\"multi\"``. Oracle, for example,\ndoes not support multi-value insert.\n\nReferences\n----------\n.. [1] https://docs.sqlalchemy.org\n.. [2] https://www.python.org/dev/peps/pep-0249/\n\nExamples\n--------\nCreate an in-memory SQLite database.\n\n>>> from sqlalchemy import create_engine\n>>> engine = create_engine('sqlite://', echo=False)\n\nCreate a table from scratch with 3 rows.\n\n>>> df = pd.DataFrame({'name' : ['User 1', 'User 2', 'User 3']})\n>>> df\n     name\n0  User 1\n1  User 2\n2  User 3\n\n>>> df.to_sql(name='users', con=engine)\n3\n>>> from sqlalchemy import text\n>>> with engine.connect() as conn:\n...    conn.execute(text(\"SELECT * FROM users\")).fetchall()\n[(0, 'User 1'), (1, 'User 2'), (2, 'User 3')]\n\nAn `sqlalchemy.engine.Connection` can also be passed to `con`:\n\n>>> with engine.begin() as connection:\n...     df1 = pd.DataFrame({'name' : ['User 4', 'User 5']})\n...     df1.to_sql(name='users', con=connection, if_exists='append')\n2\n\nThis is allowed to support operations that require that the same\nDBAPI connection is used for the entire operation.\n\n>>> df2 = pd.DataFrame({'name' : ['User 6', 'User 7']})\n>>> df2.to_sql(name='users', con=engine, if_exists='append')\n2\n>>> with engine.connect() as conn:\n...    conn.execute(text(\"SELECT * FROM users\")).fetchall()\n[(0, 'User 1'), (1, 'User 2'), (2, 'User 3'),\n (0, 'User 4'), (1, 'User 5'), (0, 'User 6'),\n (1, 'User 7')]\n\nOverwrite the table with just ``df2``.\n\n>>> df2.to_sql(name='users', con=engine, if_exists='replace',\n...            index_label='id')\n2\n>>> with engine.connect() as conn:\n...    conn.execute(text(\"SELECT * FROM users\")).fetchall()\n[(0, 'User 6'), (1, 'User 7')]\n\nUse ``method`` to define a callable insertion method to do nothing\nif there's a primary key conflict on a table in a PostgreSQL database.\n\n>>> from sqlalchemy.dialects.postgresql import insert\n>>> def insert_on_conflict_nothing(table, conn, keys, data_iter):\n...     # \"a\" is the primary key in \"conflict_table\"\n...     data = [dict(zip(keys, row)) for row in data_iter]\n...     stmt = insert(table.table).values(data).on_conflict_do_nothing(index_elements=[\"a\"])\n...     result = conn.execute(stmt)\n...     return result.rowcount\n>>> df_conflict.to_sql(name=\"conflict_table\", con=conn, if_exists=\"append\", method=insert_on_conflict_nothing)  # doctest: +SKIP\n0\n\nFor MySQL, a callable to update columns ``b`` and ``c`` if there's a conflict\non a primary key.\n\n>>> from sqlalchemy.dialects.mysql import insert\n>>> def insert_on_conflict_update(table, conn, keys, data_iter):\n...     # update columns \"b\" and \"c\" on primary key conflict\n...     data = [dict(zip(keys, row)) for row in data_iter]\n...     stmt = (\n...         insert(table.table)\n...         .values(data)\n...     )\n...     stmt = stmt.on_duplicate_key_update(b=stmt.inserted.b, c=stmt.inserted.c)\n...     result = conn.execute(stmt)\n...     return result.rowcount\n>>> df_conflict.to_sql(name=\"conflict_table\", con=conn, if_exists=\"append\", method=insert_on_conflict_update)  # doctest: +SKIP\n2\n\nSpecify the dtype (especially useful for integers with missing values).\nNotice that while pandas is forced to store the data as floating point,\nthe database supports nullable integers. When fetching the data with\nPython, we get back integer scalars.\n\n>>> df = pd.DataFrame({\"A\": [1, None, 2]})\n>>> df\n     A\n0  1.0\n1  NaN\n2  2.0\n\n>>> from sqlalchemy.types import Integer\n>>> df.to_sql(name='integers', con=engine, index=False,\n...           dtype={\"A\": Integer()})\n3\n\n>>> with engine.connect() as conn:\n...   conn.execute(text(\"SELECT * FROM integers\")).fetchall()\n[(1,), (None,), (2,)]", "parameters": {"type": "object", "properties": {"name": {"type": "str"}, "con": {}, "if_exists": {"type": "\"literal['fail"}, "index": {"type": "bool_t", "default": true}}}}}
{"task_id": "BigCodeBench/1015", "data": {"name": "cell.text_content()"}}
{"task_id": "BigCodeBench/1015", "data": {"name": "lxml.html.fromstring", "type": "callable", "signature": "(html)", "description": "Parse the html, returning a single element/document.\n\nThis tries to minimally parse the chunk of text, without knowing if it\nis a fragment or a document.\n\nbase_url will set the document's base_url attribute (and the tree's docinfo.URL)", "parameters": {"type": "object", "properties": {"html": {}}}}}
{"task_id": "BigCodeBench/1015", "data": {"name": "lxml.html.fromstring.xpath", "type": "callable", "signature": "(html)", "description": "Parse the html, returning a single element/document.\n\nThis tries to minimally parse the chunk of text, without knowing if it\nis a fragment or a document.\n\nbase_url will set the document's base_url attribute (and the tree's docinfo.URL)", "parameters": {"type": "object", "properties": {"html": {}}}}}
{"task_id": "BigCodeBench/1015", "data": {"name": "open.read()"}}
{"task_id": "BigCodeBench/1015", "data": {"name": "requests.get", "type": "callable", "signature": "(url, **kwargs)", "description": "Sends a GET request.\n\n:param url: URL for the new :class:`Request` object.\n:param params: (optional) Dictionary, list of tuples or bytes to send\n    in the query string for the :class:`Request`.\n:param \\*\\*kwargs: Optional arguments that ``request`` takes.\n:return: :class:`Response <Response>` object\n:rtype: requests.Response", "parameters": {"type": "object", "properties": {"url": {}}}}}
{"task_id": "BigCodeBench/1015", "data": {"name": "requests.get.content", "type": "callable", "signature": "(url, params=None, **kwargs)", "description": "Sends a GET request.\n\n:param url: URL for the new :class:`Request` object.\n:param params: (optional) Dictionary, list of tuples or bytes to send\n    in the query string for the :class:`Request`.\n:param \\*\\*kwargs: Optional arguments that ``request`` takes.\n:return: :class:`Response <Response>` object\n:rtype: requests.Response", "parameters": {"type": "object", "properties": {"url": {}, "params": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/1015", "data": {"name": "requests.get.raise_for_status", "type": "callable", "signature": "()", "description": "Sends a GET request.\n\n:param url: URL for the new :class:`Request` object.\n:param params: (optional) Dictionary, list of tuples or bytes to send\n    in the query string for the :class:`Request`.\n:param \\*\\*kwargs: Optional arguments that ``request`` takes.\n:return: :class:`Response <Response>` object\n:rtype: requests.Response", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/1015", "data": {"name": "row.xpath('.//td')"}}
{"task_id": "BigCodeBench/1015", "data": {"name": "sqlite3.connect", "type": "callable", "signature": "(*args, **kwargs)", "description": "connect(database[, timeout, detect_types, isolation_level,\n        check_same_thread, factory, cached_statements, uri])\n\nOpens a connection to the SQLite database file *database*. You can use\n\":memory:\" to open a database connection to a database that resides in\nRAM instead of on disk.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/1015", "data": {"name": "sqlite3.connect.close", "type": "callable", "signature": "()", "description": "connect(database[, timeout, detect_types, isolation_level,\n        check_same_thread, factory, cached_statements, uri])\n\nOpens a connection to the SQLite database file *database*. You can use\n\":memory:\" to open a database connection to a database that resides in\nRAM instead of on disk.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/1015", "data": {"name": "webpage_url.startswith('file://')"}}
{"task_id": "BigCodeBench/1019", "data": {"name": "PIL.Image.open", "type": "callable", "signature": "(fp)", "description": "Opens and identifies the given image file.\n\nThis is a lazy operation; this function identifies the file, but\nthe file remains open and the actual image data is not read from\nthe file until you try to process the data (or call the\n:py:meth:`~PIL.Image.Image.load` method).  See\n:py:func:`~PIL.Image.new`. See :ref:`file-handling`.\n\n:param fp: A filename (string), os.PathLike object or a file object.\n   The file object must implement ``file.read``,\n   ``file.seek``, and ``file.tell`` methods,\n   and be opened in binary mode. The file object will also seek to zero\n   before reading.\n:param mode: The mode.  If given, this argument must be \"r\".\n:param formats: A list or tuple of formats to attempt to load the file in.\n   This can be used to restrict the set of formats checked.\n   Pass ``None`` to try all supported formats. You can print the set of\n   available formats by running ``python3 -m PIL`` or using\n   the :py:func:`PIL.features.pilinfo` function.\n:returns: An :py:class:`~PIL.Image.Image` object.\n:exception FileNotFoundError: If the file cannot be found.\n:exception PIL.UnidentifiedImageError: If the image cannot be opened and\n   identified.\n:exception ValueError: If the ``mode`` is not \"r\", or if a ``StringIO``\n   instance is used for ``fp``.\n:exception TypeError: If ``formats`` is not ``None``, a list or a tuple.", "parameters": {"type": "object", "properties": {"fp": {}}}}}
{"task_id": "BigCodeBench/1019", "data": {"name": "PIL.Image.open.info", "type": "callable", "signature": "(fp, mode='r', formats=None) -> 'Image'", "description": "Opens and identifies the given image file.\n\nThis is a lazy operation; this function identifies the file, but\nthe file remains open and the actual image data is not read from\nthe file until you try to process the data (or call the\n:py:meth:`~PIL.Image.Image.load` method).  See\n:py:func:`~PIL.Image.new`. See :ref:`file-handling`.\n\n:param fp: A filename (string), os.PathLike object or a file object.\n   The file object must implement ``file.read``,\n   ``file.seek``, and ``file.tell`` methods,\n   and be opened in binary mode. The file object will also seek to zero\n   before reading.\n:param mode: The mode.  If given, this argument must be \"r\".\n:param formats: A list or tuple of formats to attempt to load the file in.\n   This can be used to restrict the set of formats checked.\n   Pass ``None`` to try all supported formats. You can print the set of\n   available formats by running ``python3 -m PIL`` or using\n   the :py:func:`PIL.features.pilinfo` function.\n:returns: An :py:class:`~PIL.Image.Image` object.\n:exception FileNotFoundError: If the file cannot be found.\n:exception PIL.UnidentifiedImageError: If the image cannot be opened and\n   identified.\n:exception ValueError: If the ``mode`` is not \"r\", or if a ``StringIO``\n   instance is used for ``fp``.\n:exception TypeError: If ``formats`` is not ``None``, a list or a tuple."}}
{"task_id": "BigCodeBench/1019", "data": {"name": "PIL.Image.open.info.get", "type": "callable", "signature": "(fp, mode='r')", "description": "Opens and identifies the given image file.\n\nThis is a lazy operation; this function identifies the file, but\nthe file remains open and the actual image data is not read from\nthe file until you try to process the data (or call the\n:py:meth:`~PIL.Image.Image.load` method).  See\n:py:func:`~PIL.Image.new`. See :ref:`file-handling`.\n\n:param fp: A filename (string), os.PathLike object or a file object.\n   The file object must implement ``file.read``,\n   ``file.seek``, and ``file.tell`` methods,\n   and be opened in binary mode. The file object will also seek to zero\n   before reading.\n:param mode: The mode.  If given, this argument must be \"r\".\n:param formats: A list or tuple of formats to attempt to load the file in.\n   This can be used to restrict the set of formats checked.\n   Pass ``None`` to try all supported formats. You can print the set of\n   available formats by running ``python3 -m PIL`` or using\n   the :py:func:`PIL.features.pilinfo` function.\n:returns: An :py:class:`~PIL.Image.Image` object.\n:exception FileNotFoundError: If the file cannot be found.\n:exception PIL.UnidentifiedImageError: If the image cannot be opened and\n   identified.\n:exception ValueError: If the ``mode`` is not \"r\", or if a ``StringIO``\n   instance is used for ``fp``.\n:exception TypeError: If ``formats`` is not ``None``, a list or a tuple.", "parameters": {"type": "object", "properties": {"fp": {}, "mode": {"type": "str", "default": "r"}}}}}
{"task_id": "BigCodeBench/1019", "data": {"name": "codecs.decode", "type": "callable", "signature": "(obj, encoding='utf-8')", "description": "Decodes obj using the codec registered for encoding.\n\nDefault encoding is 'utf-8'.  errors may be given to set a\ndifferent error handling scheme.  Default is 'strict' meaning that encoding\nerrors raise a ValueError.  Other possible values are 'ignore', 'replace'\nand 'backslashreplace' as well as any other name registered with\ncodecs.register_error that can handle ValueErrors.", "parameters": {"type": "object", "properties": {"obj": {}, "encoding": {"type": "str", "default": "utf-8"}}}}}
{"task_id": "BigCodeBench/1019", "data": {"name": "pytesseract.image_to_string", "type": "callable", "signature": "(image)", "description": "Returns the result of a Tesseract OCR run on the provided image to string", "parameters": {"type": "object", "properties": {"image": {}}}}}
{"task_id": "BigCodeBench/1019", "data": {"name": "pytesseract.image_to_string.encode", "type": "callable", "signature": "(image)", "description": "Returns the result of a Tesseract OCR run on the provided image to string", "parameters": {"type": "object", "properties": {"image": {}}}}}
{"task_id": "BigCodeBench/1020", "data": {"name": "chardet.detect", "type": "callable", "signature": "(byte_str: Union[bytes)", "description": "Detect the encoding of the given byte string.\n\n:param byte_str:     The byte sequence to examine.\n:type byte_str:      ``bytes`` or ``bytearray``\n:param should_rename_legacy:  Should we rename legacy encodings\n                              to their more modern equivalents?\n:type should_rename_legacy:   ``bool``", "parameters": {"type": "object", "properties": {"byte_str": {"type": "union[bytes"}}}}}
{"task_id": "BigCodeBench/1020", "data": {"name": "content.decode(from_encoding).decode(from_encoding)"}}
{"task_id": "BigCodeBench/1020", "data": {"name": "content.decode(detected_encoding).decode(detected_encoding)"}}
{"task_id": "BigCodeBench/1020", "data": {"name": "content.encode(to_encoding).decode(to_encoding).encode(to_encoding)"}}
{"task_id": "BigCodeBench/1020", "data": {"name": "json.loads", "type": "callable", "signature": "(s)", "description": "Deserialize ``s`` (a ``str``, ``bytes`` or ``bytearray`` instance\ncontaining a JSON document) to a Python object.\n\n``object_hook`` is an optional function that will be called with the\nresult of any object literal decode (a ``dict``). The return value of\n``object_hook`` will be used instead of the ``dict``. This feature\ncan be used to implement custom decoders (e.g. JSON-RPC class hinting).\n\n``object_pairs_hook`` is an optional function that will be called with the\nresult of any object literal decoded with an ordered list of pairs.  The\nreturn value of ``object_pairs_hook`` will be used instead of the ``dict``.\nThis feature can be used to implement custom decoders.  If ``object_hook``\nis also defined, the ``object_pairs_hook`` takes priority.\n\n``parse_float``, if specified, will be called with the string\nof every JSON float to be decoded. By default this is equivalent to\nfloat(num_str). This can be used to use another datatype or parser\nfor JSON floats (e.g. decimal.Decimal).\n\n``parse_int``, if specified, will be called with the string\nof every JSON int to be decoded. By default this is equivalent to\nint(num_str). This can be used to use another datatype or parser\nfor JSON integers (e.g. float).\n\n``parse_constant``, if specified, will be called with one of the\nfollowing strings: -Infinity, Infinity, NaN.\nThis can be used to raise an exception if invalid JSON numbers\nare encountered.\n\nTo use a custom ``JSONDecoder`` subclass, specify it with the ``cls``\nkwarg; otherwise ``JSONDecoder`` is used.", "parameters": {"type": "object", "properties": {"s": {}}}}}
{"task_id": "BigCodeBench/1020", "data": {"name": "requests.get", "type": "callable", "signature": "(url, **kwargs)", "description": "Sends a GET request.\n\n:param url: URL for the new :class:`Request` object.\n:param params: (optional) Dictionary, list of tuples or bytes to send\n    in the query string for the :class:`Request`.\n:param \\*\\*kwargs: Optional arguments that ``request`` takes.\n:return: :class:`Response <Response>` object\n:rtype: requests.Response", "parameters": {"type": "object", "properties": {"url": {}}}}}
{"task_id": "BigCodeBench/1020", "data": {"name": "requests.get.content", "type": "callable", "signature": "(url, params=None, **kwargs)", "description": "Sends a GET request.\n\n:param url: URL for the new :class:`Request` object.\n:param params: (optional) Dictionary, list of tuples or bytes to send\n    in the query string for the :class:`Request`.\n:param \\*\\*kwargs: Optional arguments that ``request`` takes.\n:return: :class:`Response <Response>` object\n:rtype: requests.Response", "parameters": {"type": "object", "properties": {"url": {}, "params": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/1022", "data": {"name": "datetime.datetime", "type": "class", "signature": "(self, /, *args, **kwargs)", "description": "datetime(year, month, day[, hour[, minute[, second[, microsecond[,tzinfo]]]]])\n\nThe year, month and day arguments are required. tzinfo may be None, or an\ninstance of a tzinfo subclass. The remaining arguments may be ints.", "parameters": {"type": "object", "properties": {"/": {}}}}}
{"task_id": "BigCodeBench/1022", "data": {"name": "datetime.datetime.now", "type": "callable", "signature": "()", "description": "Returns new datetime object representing current time local to tz.\n\n  tz\n    Timezone object.\n\nIf no tz is specified, uses local timezone.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/1022", "data": {"name": "df.sort_values(by=column_name).sort_values(by=column_name)"}}
{"task_id": "BigCodeBench/1022", "data": {"name": "os.path.isfile", "type": "callable", "signature": "(path)", "description": "Test whether a path is a regular file", "parameters": {"type": "object", "properties": {"path": {}}}}}
{"task_id": "BigCodeBench/1022", "data": {"name": "pandas.read_csv", "type": "callable", "signature": "(filepath_or_buffer: 'FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str]')", "description": "Read a comma-separated values (csv) file into DataFrame.\n\nAlso supports optionally iterating or breaking of the file\ninto chunks.\n\nAdditional help can be found in the online docs for\n`IO Tools <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html>`_.\n\nParameters\n----------\nfilepath_or_buffer : str, path object or file-like object\n    Any valid string path is acceptable. The string could be a URL. Valid\n    URL schemes include http, ftp, s3, gs, and file. For file URLs, a host is\n    expected. A local file could be: file://localhost/path/to/table.csv.\n\n    If you want to pass in a path object, pandas accepts any ``os.PathLike``.\n\n    By file-like object, we refer to objects with a ``read()`` method, such as\n    a file handle (e.g. via builtin ``open`` function) or ``StringIO``.\nsep : str, default ','\n    Character or regex pattern to treat as the delimiter. If ``sep=None``, the\n    C engine cannot automatically detect\n    the separator, but the Python parsing engine can, meaning the latter will\n    be used and automatically detect the separator from only the first valid\n    row of the file by Python's builtin sniffer tool, ``csv.Sniffer``.\n    In addition, separators longer than 1 character and different from\n    ``'\\s+'`` will be interpreted as regular expressions and will also force\n    the use of the Python parsing engine. Note that regex delimiters are prone\n    to ignoring quoted data. Regex example: ``'\\r\\t'``.\ndelimiter : str, optional\n    Alias for ``sep``.\nheader : int, Sequence of int, 'infer' or None, default 'infer'\n    Row number(s) containing column labels and marking the start of the\n    data (zero-indexed). Default behavior is to infer the column names: if no ``names``\n    are passed the behavior is identical to ``header=0`` and column\n    names are inferred from the first line of the file, if column\n    names are passed explicitly to ``names`` then the behavior is identical to\n    ``header=None``. Explicitly pass ``header=0`` to be able to\n    replace existing names. The header can be a list of integers that\n    specify row locations for a :class:`~pandas.MultiIndex` on the columns\n    e.g. ``[0, 1, 3]``. Intervening rows that are not specified will be\n    skipped (e.g. 2 in this example is skipped). Note that this\n    parameter ignores commented lines and empty lines if\n    ``skip_blank_lines=True``, so ``header=0`` denotes the first line of\n    data rather than the first line of the file.\nnames : Sequence of Hashable, optional\n    Sequence of column labels to apply. If the file contains a header row,\n    then you should explicitly pass ``header=0`` to override the column names.\n    Duplicates in this list are not allowed.\nindex_col : Hashable, Sequence of Hashable or False, optional\n  Column(s) to use as row label(s), denoted either by column labels or column\n  indices.  If a sequence of labels or indices is given, :class:`~pandas.MultiIndex`\n  will be formed for the row labels.\n\n  Note: ``index_col=False`` can be used to force pandas to *not* use the first\n  column as the index, e.g., when you have a malformed file with delimiters at\n  the end of each line.\nusecols : Sequence of Hashable or Callable, optional\n    Subset of columns to select, denoted either by column labels or column indices.\n    If list-like, all elements must either\n    be positional (i.e. integer indices into the document columns) or strings\n    that correspond to column names provided either by the user in ``names`` or\n    inferred from the document header row(s). If ``names`` are given, the document\n    header row(s) are not taken into account. For example, a valid list-like\n    ``usecols`` parameter would be ``[0, 1, 2]`` or ``['foo', 'bar', 'baz']``.\n    Element order is ignored, so ``usecols=[0, 1]`` is the same as ``[1, 0]``.\n    To instantiate a :class:`~pandas.DataFrame` from ``data`` with element order\n    preserved use ``pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']]``\n    for columns in ``['foo', 'bar']`` order or\n    ``pd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']]``\n    for ``['bar', 'foo']`` order.\n\n    If callable, the callable function will be evaluated against the column\n    names, returning names where the callable function evaluates to ``True``. An\n    example of a valid callable argument would be ``lambda x: x.upper() in\n    ['AAA', 'BBB', 'DDD']``. Using this parameter results in much faster\n    parsing time and lower memory usage.\ndtype : dtype or dict of {Hashable : dtype}, optional\n    Data type(s) to apply to either the whole dataset or individual columns.\n    E.g., ``{'a': np.float64, 'b': np.int32, 'c': 'Int64'}``\n    Use ``str`` or ``object`` together with suitable ``na_values`` settings\n    to preserve and not interpret ``dtype``.\n    If ``converters`` are specified, they will be applied INSTEAD\n    of ``dtype`` conversion.\n\n    .. versionadded:: 1.5.0\n\n        Support for ``defaultdict`` was added. Specify a ``defaultdict`` as input where\n        the default determines the ``dtype`` of the columns which are not explicitly\n        listed.\nengine : {'c', 'python', 'pyarrow'}, optional\n    Parser engine to use. The C and pyarrow engines are faster, while the python engine\n    is currently more feature-complete. Multithreading is currently only supported by\n    the pyarrow engine.\n\n    .. versionadded:: 1.4.0\n\n        The 'pyarrow' engine was added as an *experimental* engine, and some features\n        are unsupported, or may not work correctly, with this engine.\nconverters : dict of {Hashable : Callable}, optional\n    Functions for converting values in specified columns. Keys can either\n    be column labels or column indices.\ntrue_values : list, optional\n    Values to consider as ``True`` in addition to case-insensitive variants of 'True'.\nfalse_values : list, optional\n    Values to consider as ``False`` in addition to case-insensitive variants of 'False'.\nskipinitialspace : bool, default False\n    Skip spaces after delimiter.\nskiprows : int, list of int or Callable, optional\n    Line numbers to skip (0-indexed) or number of lines to skip (``int``)\n    at the start of the file.\n\n    If callable, the callable function will be evaluated against the row\n    indices, returning ``True`` if the row should be skipped and ``False`` otherwise.\n    An example of a valid callable argument would be ``lambda x: x in [0, 2]``.\nskipfooter : int, default 0\n    Number of lines at bottom of file to skip (Unsupported with ``engine='c'``).\nnrows : int, optional\n    Number of rows of file to read. Useful for reading pieces of large files.\nna_values : Hashable, Iterable of Hashable or dict of {Hashable : Iterable}, optional\n    Additional strings to recognize as ``NA``/``NaN``. If ``dict`` passed, specific\n    per-column ``NA`` values.  By default the following values are interpreted as\n    ``NaN``: \" \", \"#N/A\", \"#N/A N/A\", \"#NA\", \"-1.#IND\", \"-1.#QNAN\", \"-NaN\", \"-nan\",\n    \"1.#IND\", \"1.#QNAN\", \"<NA>\", \"N/A\", \"NA\", \"NULL\", \"NaN\", \"None\",\n    \"n/a\", \"nan\", \"null \".\n\nkeep_default_na : bool, default True\n    Whether or not to include the default ``NaN`` values when parsing the data.\n    Depending on whether ``na_values`` is passed in, the behavior is as follows:\n\n    * If ``keep_default_na`` is ``True``, and ``na_values`` are specified, ``na_values``\n      is appended to the default ``NaN`` values used for parsing.\n    * If ``keep_default_na`` is ``True``, and ``na_values`` are not specified, only\n      the default ``NaN`` values are used for parsing.\n    * If ``keep_default_na`` is ``False``, and ``na_values`` are specified, only\n      the ``NaN`` values specified ``na_values`` are used for parsing.\n    * If ``keep_default_na`` is ``False``, and ``na_values`` are not specified, no\n      strings will be parsed as ``NaN``.\n\n    Note that if ``na_filter`` is passed in as ``False``, the ``keep_default_na`` and\n    ``na_values`` parameters will be ignored.\nna_filter : bool, default True\n    Detect missing value markers (empty strings and the value of ``na_values``). In\n    data without any ``NA`` values, passing ``na_filter=False`` can improve the\n    performance of reading a large file.\nverbose : bool, default False\n    Indicate number of ``NA`` values placed in non-numeric columns.\n\n    .. deprecated:: 2.2.0\nskip_blank_lines : bool, default True\n    If ``True``, skip over blank lines rather than interpreting as ``NaN`` values.\nparse_dates : bool, list of Hashable, list of lists or dict of {Hashable : list}, default False\n    The behavior is as follows:\n\n    * ``bool``. If ``True`` -> try parsing the index. Note: Automatically set to\n      ``True`` if ``date_format`` or ``date_parser`` arguments have been passed.\n    * ``list`` of ``int`` or names. e.g. If ``[1, 2, 3]`` -> try parsing columns 1, 2, 3\n      each as a separate date column.\n    * ``list`` of ``list``. e.g.  If ``[[1, 3]]`` -> combine columns 1 and 3 and parse\n      as a single date column. Values are joined with a space before parsing.\n    * ``dict``, e.g. ``{'foo' : [1, 3]}`` -> parse columns 1, 3 as date and call\n      result 'foo'. Values are joined with a space before parsing.\n\n    If a column or index cannot be represented as an array of ``datetime``,\n    say because of an unparsable value or a mixture of timezones, the column\n    or index will be returned unaltered as an ``object`` data type. For\n    non-standard ``datetime`` parsing, use :func:`~pandas.to_datetime` after\n    :func:`~pandas.read_csv`.\n\n    Note: A fast-path exists for iso8601-formatted dates.\ninfer_datetime_format : bool, default False\n    If ``True`` and ``parse_dates`` is enabled, pandas will attempt to infer the\n    format of the ``datetime`` strings in the columns, and if it can be inferred,\n    switch to a faster method of parsing them. In some cases this can increase\n    the parsing speed by 5-10x.\n\n    .. deprecated:: 2.0.0\n        A strict version of this argument is now the default, passing it has no effect.\n\nkeep_date_col : bool, default False\n    If ``True`` and ``parse_dates`` specifies combining multiple columns then\n    keep the original columns.\ndate_parser : Callable, optional\n    Function to use for converting a sequence of string columns to an array of\n    ``datetime`` instances. The default uses ``dateutil.parser.parser`` to do the\n    conversion. pandas will try to call ``date_parser`` in three different ways,\n    advancing to the next if an exception occurs: 1) Pass one or more arrays\n    (as defined by ``parse_dates``) as arguments; 2) concatenate (row-wise) the\n    string values from the columns defined by ``parse_dates`` into a single array\n    and pass that; and 3) call ``date_parser`` once for each row using one or\n    more strings (corresponding to the columns defined by ``parse_dates``) as\n    arguments.\n\n    .. deprecated:: 2.0.0\n       Use ``date_format`` instead, or read in as ``object`` and then apply\n       :func:`~pandas.to_datetime` as-needed.\ndate_format : str or dict of column -> format, optional\n    Format to use for parsing dates when used in conjunction with ``parse_dates``.\n    The strftime to parse time, e.g. :const:`\"%d/%m/%Y\"`. See\n    `strftime documentation\n    <https://docs.python.org/3/library/datetime.html\n    #strftime-and-strptime-behavior>`_ for more information on choices, though\n    note that :const:`\"%f\"` will parse all the way up to nanoseconds.\n    You can also pass:\n\n    - \"ISO8601\", to parse any `ISO8601 <https://en.wikipedia.org/wiki/ISO_8601>`_\n        time string (not necessarily in exactly the same format);\n    - \"mixed\", to infer the format for each element individually. This is risky,\n        and you should probably use it along with `dayfirst`.\n\n    .. versionadded:: 2.0.0\ndayfirst : bool, default False\n    DD/MM format dates, international and European format.\ncache_dates : bool, default True\n    If ``True``, use a cache of unique, converted dates to apply the ``datetime``\n    conversion. May produce significant speed-up when parsing duplicate\n    date strings, especially ones with timezone offsets.\n\niterator : bool, default False\n    Return ``TextFileReader`` object for iteration or getting chunks with\n    ``get_chunk()``.\nchunksize : int, optional\n    Number of lines to read from the file per chunk. Passing a value will cause the\n    function to return a ``TextFileReader`` object for iteration.\n    See the `IO Tools docs\n    <https://pandas.pydata.org/pandas-docs/stable/io.html#io-chunking>`_\n    for more information on ``iterator`` and ``chunksize``.\n\ncompression : str or dict, default 'infer'\n    For on-the-fly decompression of on-disk data. If 'infer' and 'filepath_or_buffer' is\n    path-like, then detect compression from the following extensions: '.gz',\n    '.bz2', '.zip', '.xz', '.zst', '.tar', '.tar.gz', '.tar.xz' or '.tar.bz2'\n    (otherwise no compression).\n    If using 'zip' or 'tar', the ZIP file must contain only one data file to be read in.\n    Set to ``None`` for no decompression.\n    Can also be a dict with key ``'method'`` set\n    to one of {``'zip'``, ``'gzip'``, ``'bz2'``, ``'zstd'``, ``'xz'``, ``'tar'``} and\n    other key-value pairs are forwarded to\n    ``zipfile.ZipFile``, ``gzip.GzipFile``,\n    ``bz2.BZ2File``, ``zstandard.ZstdDecompressor``, ``lzma.LZMAFile`` or\n    ``tarfile.TarFile``, respectively.\n    As an example, the following could be passed for Zstandard decompression using a\n    custom compression dictionary:\n    ``compression={'method': 'zstd', 'dict_data': my_compression_dict}``.\n\n    .. versionadded:: 1.5.0\n        Added support for `.tar` files.\n\n    .. versionchanged:: 1.4.0 Zstandard support.\n\nthousands : str (length 1), optional\n    Character acting as the thousands separator in numerical values.\ndecimal : str (length 1), default '.'\n    Character to recognize as decimal point (e.g., use ',' for European data).\nlineterminator : str (length 1), optional\n    Character used to denote a line break. Only valid with C parser.\nquotechar : str (length 1), optional\n    Character used to denote the start and end of a quoted item. Quoted\n    items can include the ``delimiter`` and it will be ignored.\nquoting : {0 or csv.QUOTE_MINIMAL, 1 or csv.QUOTE_ALL, 2 or csv.QUOTE_NONNUMERIC, 3 or csv.QUOTE_NONE}, default csv.QUOTE_MINIMAL\n    Control field quoting behavior per ``csv.QUOTE_*`` constants. Default is\n    ``csv.QUOTE_MINIMAL`` (i.e., 0) which implies that only fields containing special\n    characters are quoted (e.g., characters defined in ``quotechar``, ``delimiter``,\n    or ``lineterminator``.\ndoublequote : bool, default True\n   When ``quotechar`` is specified and ``quoting`` is not ``QUOTE_NONE``, indicate\n   whether or not to interpret two consecutive ``quotechar`` elements INSIDE a\n   field as a single ``quotechar`` element.\nescapechar : str (length 1), optional\n    Character used to escape other characters.\ncomment : str (length 1), optional\n    Character indicating that the remainder of line should not be parsed.\n    If found at the beginning\n    of a line, the line will be ignored altogether. This parameter must be a\n    single character. Like empty lines (as long as ``skip_blank_lines=True``),\n    fully commented lines are ignored by the parameter ``header`` but not by\n    ``skiprows``. For example, if ``comment='#'``, parsing\n    ``#empty\\na,b,c\\n1,2,3`` with ``header=0`` will result in ``'a,b,c'`` being\n    treated as the header.\nencoding : str, optional, default 'utf-8'\n    Encoding to use for UTF when reading/writing (ex. ``'utf-8'``). `List of Python\n    standard encodings\n    <https://docs.python.org/3/library/codecs.html#standard-encodings>`_ .\n\nencoding_errors : str, optional, default 'strict'\n    How encoding errors are treated. `List of possible values\n    <https://docs.python.org/3/library/codecs.html#error-handlers>`_ .\n\n    .. versionadded:: 1.3.0\n\ndialect : str or csv.Dialect, optional\n    If provided, this parameter will override values (default or not) for the\n    following parameters: ``delimiter``, ``doublequote``, ``escapechar``,\n    ``skipinitialspace``, ``quotechar``, and ``quoting``. If it is necessary to\n    override values, a ``ParserWarning`` will be issued. See ``csv.Dialect``\n    documentation for more details.\non_bad_lines : {'error', 'warn', 'skip'} or Callable, default 'error'\n    Specifies what to do upon encountering a bad line (a line with too many fields).\n    Allowed values are :\n\n    - ``'error'``, raise an Exception when a bad line is encountered.\n    - ``'warn'``, raise a warning when a bad line is encountered and skip that line.\n    - ``'skip'``, skip bad lines without raising or warning when they are encountered.\n\n    .. versionadded:: 1.3.0\n\n    .. versionadded:: 1.4.0\n\n        - Callable, function with signature\n          ``(bad_line: list[str]) -> list[str] | None`` that will process a single\n          bad line. ``bad_line`` is a list of strings split by the ``sep``.\n          If the function returns ``None``, the bad line will be ignored.\n          If the function returns a new ``list`` of strings with more elements than\n          expected, a ``ParserWarning`` will be emitted while dropping extra elements.\n          Only supported when ``engine='python'``\n\n    .. versionchanged:: 2.2.0\n\n        - Callable, function with signature\n          as described in `pyarrow documentation\n          <https://arrow.apache.org/docs/python/generated/pyarrow.csv.ParseOptions.html\n          #pyarrow.csv.ParseOptions.invalid_row_handler>`_ when ``engine='pyarrow'``\n\ndelim_whitespace : bool, default False\n    Specifies whether or not whitespace (e.g. ``' '`` or ``'\\t'``) will be\n    used as the ``sep`` delimiter. Equivalent to setting ``sep='\\s+'``. If this option\n    is set to ``True``, nothing should be passed in for the ``delimiter``\n    parameter.\n\n    .. deprecated:: 2.2.0\n        Use ``sep=\"\\s+\"`` instead.\nlow_memory : bool, default True\n    Internally process the file in chunks, resulting in lower memory use\n    while parsing, but possibly mixed type inference.  To ensure no mixed\n    types either set ``False``, or specify the type with the ``dtype`` parameter.\n    Note that the entire file is read into a single :class:`~pandas.DataFrame`\n    regardless, use the ``chunksize`` or ``iterator`` parameter to return the data in\n    chunks. (Only valid with C parser).\nmemory_map : bool, default False\n    If a filepath is provided for ``filepath_or_buffer``, map the file object\n    directly onto memory and access the data directly from there. Using this\n    option can improve performance because there is no longer any I/O overhead.\nfloat_precision : {'high', 'legacy', 'round_trip'}, optional\n    Specifies which converter the C engine should use for floating-point\n    values. The options are ``None`` or ``'high'`` for the ordinary converter,\n    ``'legacy'`` for the original lower precision pandas converter, and\n    ``'round_trip'`` for the round-trip converter.\n\nstorage_options : dict, optional\n    Extra options that make sense for a particular storage connection, e.g.\n    host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n    are forwarded to ``urllib.request.Request`` as header options. For other\n    URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n    forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n    details, and for more examples on storage options refer `here\n    <https://pandas.pydata.org/docs/user_guide/io.html?\n    highlight=storage_options#reading-writing-remote-files>`_.\n\ndtype_backend : {'numpy_nullable', 'pyarrow'}, default 'numpy_nullable'\n    Back-end data type applied to the resultant :class:`DataFrame`\n    (still experimental). Behaviour is as follows:\n\n    * ``\"numpy_nullable\"``: returns nullable-dtype-backed :class:`DataFrame`\n      (default).\n    * ``\"pyarrow\"``: returns pyarrow-backed nullable :class:`ArrowDtype`\n      DataFrame.\n\n    .. versionadded:: 2.0\n\nReturns\n-------\nDataFrame or TextFileReader\n    A comma-separated values (csv) file is returned as two-dimensional\n    data structure with labeled axes.\n\nSee Also\n--------\nDataFrame.to_csv : Write DataFrame to a comma-separated values (csv) file.\nread_table : Read general delimited file into DataFrame.\nread_fwf : Read a table of fixed-width formatted lines into DataFrame.\n\nExamples\n--------\n>>> pd.read_csv('data.csv')  # doctest: +SKIP", "parameters": {"type": "object", "properties": {"filepath_or_buffer": {"type": ["readcsvbuffer[bytes]", "readcsvbuffer[str]", "filepath"]}}}}}
{"task_id": "BigCodeBench/1022", "data": {"name": "pandas.read_csv.columns", "type": "callable", "signature": "(filepath_or_buffer: 'FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str]', *, sep: 'str | None | lib.NoDefault' = <no_default>, delimiter: 'str | None | lib.NoDefault' = None, header: \"int | Sequence[int] | None | Literal['infer']\" = 'infer', names: 'Sequence[Hashable] | None | lib.NoDefault' = <no_default>, index_col: 'IndexLabel | Literal[False] | None' = None, usecols: 'UsecolsArgType' = None, dtype: 'DtypeArg | None' = None, engine: 'CSVEngine | None' = None, converters: 'Mapping[Hashable, Callable] | None' = None, true_values: 'list | None' = None, false_values: 'list | None' = None, skipinitialspace: 'bool' = False, skiprows: 'list[int] | int | Callable[[Hashable], bool] | None' = None, skipfooter: 'int' = 0, nrows: 'int | None' = None, na_values: 'Hashable | Iterable[Hashable] | Mapping[Hashable, Iterable[Hashable]] | None' = None, keep_default_na: 'bool' = True, na_filter: 'bool' = True, verbose: 'bool | lib.NoDefault' = <no_default>, skip_blank_lines: 'bool' = True, parse_dates: 'bool | Sequence[Hashable] | None' = None, infer_datetime_format: 'bool | lib.NoDefault' = <no_default>, keep_date_col: 'bool | lib.NoDefault' = <no_default>, date_parser: 'Callable | lib.NoDefault' = <no_default>, date_format: 'str | dict[Hashable, str] | None' = None, dayfirst: 'bool' = False, cache_dates: 'bool' = True, iterator: 'bool' = False, chunksize: 'int | None' = None, compression: 'CompressionOptions' = 'infer', thousands: 'str | None' = None, decimal: 'str' = '.', lineterminator: 'str | None' = None, quotechar: 'str' = '\"', quoting: 'int' = 0, doublequote: 'bool' = True, escapechar: 'str | None' = None, comment: 'str | None' = None, encoding: 'str | None' = None, encoding_errors: 'str | None' = 'strict', dialect: 'str | csv.Dialect | None' = None, on_bad_lines: 'str' = 'error', delim_whitespace: 'bool | lib.NoDefault' = <no_default>, low_memory: 'bool' = True, memory_map: 'bool' = False, float_precision: \"Literal['high', 'legacy'] | None\" = None, storage_options: 'StorageOptions | None' = None, dtype_backend: 'DtypeBackend | lib.NoDefault' = <no_default>) -> 'DataFrame | TextFileReader'", "description": "Read a comma-separated values (csv) file into DataFrame.\n\nAlso supports optionally iterating or breaking of the file\ninto chunks.\n\nAdditional help can be found in the online docs for\n`IO Tools <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html>`_.\n\nParameters\n----------\nfilepath_or_buffer : str, path object or file-like object\n    Any valid string path is acceptable. The string could be a URL. Valid\n    URL schemes include http, ftp, s3, gs, and file. For file URLs, a host is\n    expected. A local file could be: file://localhost/path/to/table.csv.\n\n    If you want to pass in a path object, pandas accepts any ``os.PathLike``.\n\n    By file-like object, we refer to objects with a ``read()`` method, such as\n    a file handle (e.g. via builtin ``open`` function) or ``StringIO``.\nsep : str, default ','\n    Character or regex pattern to treat as the delimiter. If ``sep=None``, the\n    C engine cannot automatically detect\n    the separator, but the Python parsing engine can, meaning the latter will\n    be used and automatically detect the separator from only the first valid\n    row of the file by Python's builtin sniffer tool, ``csv.Sniffer``.\n    In addition, separators longer than 1 character and different from\n    ``'\\s+'`` will be interpreted as regular expressions and will also force\n    the use of the Python parsing engine. Note that regex delimiters are prone\n    to ignoring quoted data. Regex example: ``'\\r\\t'``.\ndelimiter : str, optional\n    Alias for ``sep``.\nheader : int, Sequence of int, 'infer' or None, default 'infer'\n    Row number(s) containing column labels and marking the start of the\n    data (zero-indexed). Default behavior is to infer the column names: if no ``names``\n    are passed the behavior is identical to ``header=0`` and column\n    names are inferred from the first line of the file, if column\n    names are passed explicitly to ``names`` then the behavior is identical to\n    ``header=None``. Explicitly pass ``header=0`` to be able to\n    replace existing names. The header can be a list of integers that\n    specify row locations for a :class:`~pandas.MultiIndex` on the columns\n    e.g. ``[0, 1, 3]``. Intervening rows that are not specified will be\n    skipped (e.g. 2 in this example is skipped). Note that this\n    parameter ignores commented lines and empty lines if\n    ``skip_blank_lines=True``, so ``header=0`` denotes the first line of\n    data rather than the first line of the file.\nnames : Sequence of Hashable, optional\n    Sequence of column labels to apply. If the file contains a header row,\n    then you should explicitly pass ``header=0`` to override the column names.\n    Duplicates in this list are not allowed.\nindex_col : Hashable, Sequence of Hashable or False, optional\n  Column(s) to use as row label(s), denoted either by column labels or column\n  indices.  If a sequence of labels or indices is given, :class:`~pandas.MultiIndex`\n  will be formed for the row labels.\n\n  Note: ``index_col=False`` can be used to force pandas to *not* use the first\n  column as the index, e.g., when you have a malformed file with delimiters at\n  the end of each line.\nusecols : Sequence of Hashable or Callable, optional\n    Subset of columns to select, denoted either by column labels or column indices.\n    If list-like, all elements must either\n    be positional (i.e. integer indices into the document columns) or strings\n    that correspond to column names provided either by the user in ``names`` or\n    inferred from the document header row(s). If ``names`` are given, the document\n    header row(s) are not taken into account. For example, a valid list-like\n    ``usecols`` parameter would be ``[0, 1, 2]`` or ``['foo', 'bar', 'baz']``.\n    Element order is ignored, so ``usecols=[0, 1]`` is the same as ``[1, 0]``.\n    To instantiate a :class:`~pandas.DataFrame` from ``data`` with element order\n    preserved use ``pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']]``\n    for columns in ``['foo', 'bar']`` order or\n    ``pd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']]``\n    for ``['bar', 'foo']`` order.\n\n    If callable, the callable function will be evaluated against the column\n    names, returning names where the callable function evaluates to ``True``. An\n    example of a valid callable argument would be ``lambda x: x.upper() in\n    ['AAA', 'BBB', 'DDD']``. Using this parameter results in much faster\n    parsing time and lower memory usage.\ndtype : dtype or dict of {Hashable : dtype}, optional\n    Data type(s) to apply to either the whole dataset or individual columns.\n    E.g., ``{'a': np.float64, 'b': np.int32, 'c': 'Int64'}``\n    Use ``str`` or ``object`` together with suitable ``na_values`` settings\n    to preserve and not interpret ``dtype``.\n    If ``converters`` are specified, they will be applied INSTEAD\n    of ``dtype`` conversion.\n\n    .. versionadded:: 1.5.0\n\n        Support for ``defaultdict`` was added. Specify a ``defaultdict`` as input where\n        the default determines the ``dtype`` of the columns which are not explicitly\n        listed.\nengine : {'c', 'python', 'pyarrow'}, optional\n    Parser engine to use. The C and pyarrow engines are faster, while the python engine\n    is currently more feature-complete. Multithreading is currently only supported by\n    the pyarrow engine.\n\n    .. versionadded:: 1.4.0\n\n        The 'pyarrow' engine was added as an *experimental* engine, and some features\n        are unsupported, or may not work correctly, with this engine.\nconverters : dict of {Hashable : Callable}, optional\n    Functions for converting values in specified columns. Keys can either\n    be column labels or column indices.\ntrue_values : list, optional\n    Values to consider as ``True`` in addition to case-insensitive variants of 'True'.\nfalse_values : list, optional\n    Values to consider as ``False`` in addition to case-insensitive variants of 'False'.\nskipinitialspace : bool, default False\n    Skip spaces after delimiter.\nskiprows : int, list of int or Callable, optional\n    Line numbers to skip (0-indexed) or number of lines to skip (``int``)\n    at the start of the file.\n\n    If callable, the callable function will be evaluated against the row\n    indices, returning ``True`` if the row should be skipped and ``False`` otherwise.\n    An example of a valid callable argument would be ``lambda x: x in [0, 2]``.\nskipfooter : int, default 0\n    Number of lines at bottom of file to skip (Unsupported with ``engine='c'``).\nnrows : int, optional\n    Number of rows of file to read. Useful for reading pieces of large files.\nna_values : Hashable, Iterable of Hashable or dict of {Hashable : Iterable}, optional\n    Additional strings to recognize as ``NA``/``NaN``. If ``dict`` passed, specific\n    per-column ``NA`` values.  By default the following values are interpreted as\n    ``NaN``: \" \", \"#N/A\", \"#N/A N/A\", \"#NA\", \"-1.#IND\", \"-1.#QNAN\", \"-NaN\", \"-nan\",\n    \"1.#IND\", \"1.#QNAN\", \"<NA>\", \"N/A\", \"NA\", \"NULL\", \"NaN\", \"None\",\n    \"n/a\", \"nan\", \"null \".\n\nkeep_default_na : bool, default True\n    Whether or not to include the default ``NaN`` values when parsing the data.\n    Depending on whether ``na_values`` is passed in, the behavior is as follows:\n\n    * If ``keep_default_na`` is ``True``, and ``na_values`` are specified, ``na_values``\n      is appended to the default ``NaN`` values used for parsing.\n    * If ``keep_default_na`` is ``True``, and ``na_values`` are not specified, only\n      the default ``NaN`` values are used for parsing.\n    * If ``keep_default_na`` is ``False``, and ``na_values`` are specified, only\n      the ``NaN`` values specified ``na_values`` are used for parsing.\n    * If ``keep_default_na`` is ``False``, and ``na_values`` are not specified, no\n      strings will be parsed as ``NaN``.\n\n    Note that if ``na_filter`` is passed in as ``False``, the ``keep_default_na`` and\n    ``na_values`` parameters will be ignored.\nna_filter : bool, default True\n    Detect missing value markers (empty strings and the value of ``na_values``). In\n    data without any ``NA`` values, passing ``na_filter=False`` can improve the\n    performance of reading a large file.\nverbose : bool, default False\n    Indicate number of ``NA`` values placed in non-numeric columns.\n\n    .. deprecated:: 2.2.0\nskip_blank_lines : bool, default True\n    If ``True``, skip over blank lines rather than interpreting as ``NaN`` values.\nparse_dates : bool, list of Hashable, list of lists or dict of {Hashable : list}, default False\n    The behavior is as follows:\n\n    * ``bool``. If ``True`` -> try parsing the index. Note: Automatically set to\n      ``True`` if ``date_format`` or ``date_parser`` arguments have been passed.\n    * ``list`` of ``int`` or names. e.g. If ``[1, 2, 3]`` -> try parsing columns 1, 2, 3\n      each as a separate date column.\n    * ``list`` of ``list``. e.g.  If ``[[1, 3]]`` -> combine columns 1 and 3 and parse\n      as a single date column. Values are joined with a space before parsing.\n    * ``dict``, e.g. ``{'foo' : [1, 3]}`` -> parse columns 1, 3 as date and call\n      result 'foo'. Values are joined with a space before parsing.\n\n    If a column or index cannot be represented as an array of ``datetime``,\n    say because of an unparsable value or a mixture of timezones, the column\n    or index will be returned unaltered as an ``object`` data type. For\n    non-standard ``datetime`` parsing, use :func:`~pandas.to_datetime` after\n    :func:`~pandas.read_csv`.\n\n    Note: A fast-path exists for iso8601-formatted dates.\ninfer_datetime_format : bool, default False\n    If ``True`` and ``parse_dates`` is enabled, pandas will attempt to infer the\n    format of the ``datetime`` strings in the columns, and if it can be inferred,\n    switch to a faster method of parsing them. In some cases this can increase\n    the parsing speed by 5-10x.\n\n    .. deprecated:: 2.0.0\n        A strict version of this argument is now the default, passing it has no effect.\n\nkeep_date_col : bool, default False\n    If ``True`` and ``parse_dates`` specifies combining multiple columns then\n    keep the original columns.\ndate_parser : Callable, optional\n    Function to use for converting a sequence of string columns to an array of\n    ``datetime`` instances. The default uses ``dateutil.parser.parser`` to do the\n    conversion. pandas will try to call ``date_parser`` in three different ways,\n    advancing to the next if an exception occurs: 1) Pass one or more arrays\n    (as defined by ``parse_dates``) as arguments; 2) concatenate (row-wise) the\n    string values from the columns defined by ``parse_dates`` into a single array\n    and pass that; and 3) call ``date_parser`` once for each row using one or\n    more strings (corresponding to the columns defined by ``parse_dates``) as\n    arguments.\n\n    .. deprecated:: 2.0.0\n       Use ``date_format`` instead, or read in as ``object`` and then apply\n       :func:`~pandas.to_datetime` as-needed.\ndate_format : str or dict of column -> format, optional\n    Format to use for parsing dates when used in conjunction with ``parse_dates``.\n    The strftime to parse time, e.g. :const:`\"%d/%m/%Y\"`. See\n    `strftime documentation\n    <https://docs.python.org/3/library/datetime.html\n    #strftime-and-strptime-behavior>`_ for more information on choices, though\n    note that :const:`\"%f\"` will parse all the way up to nanoseconds.\n    You can also pass:\n\n    - \"ISO8601\", to parse any `ISO8601 <https://en.wikipedia.org/wiki/ISO_8601>`_\n        time string (not necessarily in exactly the same format);\n    - \"mixed\", to infer the format for each element individually. This is risky,\n        and you should probably use it along with `dayfirst`.\n\n    .. versionadded:: 2.0.0\ndayfirst : bool, default False\n    DD/MM format dates, international and European format.\ncache_dates : bool, default True\n    If ``True``, use a cache of unique, converted dates to apply the ``datetime``\n    conversion. May produce significant speed-up when parsing duplicate\n    date strings, especially ones with timezone offsets.\n\niterator : bool, default False\n    Return ``TextFileReader`` object for iteration or getting chunks with\n    ``get_chunk()``.\nchunksize : int, optional\n    Number of lines to read from the file per chunk. Passing a value will cause the\n    function to return a ``TextFileReader`` object for iteration.\n    See the `IO Tools docs\n    <https://pandas.pydata.org/pandas-docs/stable/io.html#io-chunking>`_\n    for more information on ``iterator`` and ``chunksize``.\n\ncompression : str or dict, default 'infer'\n    For on-the-fly decompression of on-disk data. If 'infer' and 'filepath_or_buffer' is\n    path-like, then detect compression from the following extensions: '.gz',\n    '.bz2', '.zip', '.xz', '.zst', '.tar', '.tar.gz', '.tar.xz' or '.tar.bz2'\n    (otherwise no compression).\n    If using 'zip' or 'tar', the ZIP file must contain only one data file to be read in.\n    Set to ``None`` for no decompression.\n    Can also be a dict with key ``'method'`` set\n    to one of {``'zip'``, ``'gzip'``, ``'bz2'``, ``'zstd'``, ``'xz'``, ``'tar'``} and\n    other key-value pairs are forwarded to\n    ``zipfile.ZipFile``, ``gzip.GzipFile``,\n    ``bz2.BZ2File``, ``zstandard.ZstdDecompressor``, ``lzma.LZMAFile`` or\n    ``tarfile.TarFile``, respectively.\n    As an example, the following could be passed for Zstandard decompression using a\n    custom compression dictionary:\n    ``compression={'method': 'zstd', 'dict_data': my_compression_dict}``.\n\n    .. versionadded:: 1.5.0\n        Added support for `.tar` files.\n\n    .. versionchanged:: 1.4.0 Zstandard support.\n\nthousands : str (length 1), optional\n    Character acting as the thousands separator in numerical values.\ndecimal : str (length 1), default '.'\n    Character to recognize as decimal point (e.g., use ',' for European data).\nlineterminator : str (length 1), optional\n    Character used to denote a line break. Only valid with C parser.\nquotechar : str (length 1), optional\n    Character used to denote the start and end of a quoted item. Quoted\n    items can include the ``delimiter`` and it will be ignored.\nquoting : {0 or csv.QUOTE_MINIMAL, 1 or csv.QUOTE_ALL, 2 or csv.QUOTE_NONNUMERIC, 3 or csv.QUOTE_NONE}, default csv.QUOTE_MINIMAL\n    Control field quoting behavior per ``csv.QUOTE_*`` constants. Default is\n    ``csv.QUOTE_MINIMAL`` (i.e., 0) which implies that only fields containing special\n    characters are quoted (e.g., characters defined in ``quotechar``, ``delimiter``,\n    or ``lineterminator``.\ndoublequote : bool, default True\n   When ``quotechar`` is specified and ``quoting`` is not ``QUOTE_NONE``, indicate\n   whether or not to interpret two consecutive ``quotechar`` elements INSIDE a\n   field as a single ``quotechar`` element.\nescapechar : str (length 1), optional\n    Character used to escape other characters.\ncomment : str (length 1), optional\n    Character indicating that the remainder of line should not be parsed.\n    If found at the beginning\n    of a line, the line will be ignored altogether. This parameter must be a\n    single character. Like empty lines (as long as ``skip_blank_lines=True``),\n    fully commented lines are ignored by the parameter ``header`` but not by\n    ``skiprows``. For example, if ``comment='#'``, parsing\n    ``#empty\\na,b,c\\n1,2,3`` with ``header=0`` will result in ``'a,b,c'`` being\n    treated as the header.\nencoding : str, optional, default 'utf-8'\n    Encoding to use for UTF when reading/writing (ex. ``'utf-8'``). `List of Python\n    standard encodings\n    <https://docs.python.org/3/library/codecs.html#standard-encodings>`_ .\n\nencoding_errors : str, optional, default 'strict'\n    How encoding errors are treated. `List of possible values\n    <https://docs.python.org/3/library/codecs.html#error-handlers>`_ .\n\n    .. versionadded:: 1.3.0\n\ndialect : str or csv.Dialect, optional\n    If provided, this parameter will override values (default or not) for the\n    following parameters: ``delimiter``, ``doublequote``, ``escapechar``,\n    ``skipinitialspace``, ``quotechar``, and ``quoting``. If it is necessary to\n    override values, a ``ParserWarning`` will be issued. See ``csv.Dialect``\n    documentation for more details.\non_bad_lines : {'error', 'warn', 'skip'} or Callable, default 'error'\n    Specifies what to do upon encountering a bad line (a line with too many fields).\n    Allowed values are :\n\n    - ``'error'``, raise an Exception when a bad line is encountered.\n    - ``'warn'``, raise a warning when a bad line is encountered and skip that line.\n    - ``'skip'``, skip bad lines without raising or warning when they are encountered.\n\n    .. versionadded:: 1.3.0\n\n    .. versionadded:: 1.4.0\n\n        - Callable, function with signature\n          ``(bad_line: list[str]) -> list[str] | None`` that will process a single\n          bad line. ``bad_line`` is a list of strings split by the ``sep``.\n          If the function returns ``None``, the bad line will be ignored.\n          If the function returns a new ``list`` of strings with more elements than\n          expected, a ``ParserWarning`` will be emitted while dropping extra elements.\n          Only supported when ``engine='python'``\n\n    .. versionchanged:: 2.2.0\n\n        - Callable, function with signature\n          as described in `pyarrow documentation\n          <https://arrow.apache.org/docs/python/generated/pyarrow.csv.ParseOptions.html\n          #pyarrow.csv.ParseOptions.invalid_row_handler>`_ when ``engine='pyarrow'``\n\ndelim_whitespace : bool, default False\n    Specifies whether or not whitespace (e.g. ``' '`` or ``'\\t'``) will be\n    used as the ``sep`` delimiter. Equivalent to setting ``sep='\\s+'``. If this option\n    is set to ``True``, nothing should be passed in for the ``delimiter``\n    parameter.\n\n    .. deprecated:: 2.2.0\n        Use ``sep=\"\\s+\"`` instead.\nlow_memory : bool, default True\n    Internally process the file in chunks, resulting in lower memory use\n    while parsing, but possibly mixed type inference.  To ensure no mixed\n    types either set ``False``, or specify the type with the ``dtype`` parameter.\n    Note that the entire file is read into a single :class:`~pandas.DataFrame`\n    regardless, use the ``chunksize`` or ``iterator`` parameter to return the data in\n    chunks. (Only valid with C parser).\nmemory_map : bool, default False\n    If a filepath is provided for ``filepath_or_buffer``, map the file object\n    directly onto memory and access the data directly from there. Using this\n    option can improve performance because there is no longer any I/O overhead.\nfloat_precision : {'high', 'legacy', 'round_trip'}, optional\n    Specifies which converter the C engine should use for floating-point\n    values. The options are ``None`` or ``'high'`` for the ordinary converter,\n    ``'legacy'`` for the original lower precision pandas converter, and\n    ``'round_trip'`` for the round-trip converter.\n\nstorage_options : dict, optional\n    Extra options that make sense for a particular storage connection, e.g.\n    host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n    are forwarded to ``urllib.request.Request`` as header options. For other\n    URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n    forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n    details, and for more examples on storage options refer `here\n    <https://pandas.pydata.org/docs/user_guide/io.html?\n    highlight=storage_options#reading-writing-remote-files>`_.\n\ndtype_backend : {'numpy_nullable', 'pyarrow'}, default 'numpy_nullable'\n    Back-end data type applied to the resultant :class:`DataFrame`\n    (still experimental). Behaviour is as follows:\n\n    * ``\"numpy_nullable\"``: returns nullable-dtype-backed :class:`DataFrame`\n      (default).\n    * ``\"pyarrow\"``: returns pyarrow-backed nullable :class:`ArrowDtype`\n      DataFrame.\n\n    .. versionadded:: 2.0\n\nReturns\n-------\nDataFrame or TextFileReader\n    A comma-separated values (csv) file is returned as two-dimensional\n    data structure with labeled axes.\n\nSee Also\n--------\nDataFrame.to_csv : Write DataFrame to a comma-separated values (csv) file.\nread_table : Read general delimited file into DataFrame.\nread_fwf : Read a table of fixed-width formatted lines into DataFrame.\n\nExamples\n--------\n>>> pd.read_csv('data.csv')  # doctest: +SKIP", "parameters": {"type": "object", "properties": {"filepath_or_buffer": {"type": ["readcsvbuffer[bytes]", "readcsvbuffer[str]", "filepath"]}, "sep": {"type": ["lib.nodefault", "str", "null"], "default": "<no_default>"}, "delimiter": {"type": ["lib.nodefault", "str", "null"], "default": null}, "header": {"type": ["\"int", "literal['infer']\"", "sequence[int]", "null"], "default": "infer"}, "names": {"type": ["lib.nodefault", "sequence[hashable]", "null"], "default": "<no_default>"}, "index_col": {"type": ["indexlabel", "literal[false]", "null"], "default": null}, "usecols": {"type": "usecolsargtype", "default": null}, "dtype": {"type": ["dtypearg", "null"], "default": null}, "engine": {"type": ["csvengine", "null"], "default": null}, "converters": {"type": ["mapping[hashable, callable]", "null"], "default": null}, "true_values": {"type": ["list", "null"], "default": null}, "false_values": {"type": ["list", "null"], "default": null}, "skipinitialspace": {"type": "bool", "default": false}, "skiprows": {"type": ["null", "list[int]", "callable[[hashable], bool]", "integer"], "default": null}, "skipfooter": {"type": "integer", "default": 0}, "nrows": {"type": ["integer", "null"], "default": null}, "na_values": {"type": ["hashable", "iterable[hashable]", "mapping[hashable"]}, "Iterable[Hashable]] | None'": {"type": "NoneType", "default": null}, "keep_default_na": {"type": "bool", "default": true}, "na_filter": {"type": "bool", "default": true}, "verbose": {"type": ["bool", "lib.nodefault"], "default": "<no_default>"}, "skip_blank_lines": {"type": "bool", "default": true}, "parse_dates": {"type": ["bool", "sequence[hashable]", "null"], "default": null}, "infer_datetime_format": {"type": ["bool", "lib.nodefault"], "default": "<no_default>"}, "keep_date_col": {"type": ["bool", "lib.nodefault"], "default": "<no_default>"}, "date_parser": {"type": ["callable", "lib.nodefault"], "default": "<no_default>"}, "date_format": {"type": ["dict[hashable, str]", "str", "null"], "default": null}, "dayfirst": {"type": "bool", "default": false}, "cache_dates": {"type": "bool", "default": true}, "iterator": {"type": "bool", "default": false}, "chunksize": {"type": ["integer", "null"], "default": null}, "compression": {"type": "compressionoptions", "default": "infer"}, "thousands": {"type": ["str", "null"], "default": null}, "decimal": {"type": "str", "default": "."}, "lineterminator": {"type": ["str", "null"], "default": null}, "quotechar": {"type": "str", "default": ""}, "quoting": {"type": "integer", "default": 0}, "doublequote": {"type": "bool", "default": true}, "escapechar": {"type": ["str", "null"], "default": null}, "comment": {"type": ["str", "null"], "default": null}, "encoding": {"type": ["str", "null"], "default": null}, "encoding_errors": {"type": ["str", "null"], "default": "strict"}, "dialect": {"type": ["csv.dialect", "str", "null"], "default": null}, "on_bad_lines": {"type": "str", "default": "error"}, "delim_whitespace": {"type": ["bool", "lib.nodefault"], "default": "<no_default>"}, "low_memory": {"type": "bool", "default": true}, "memory_map": {"type": "bool", "default": false}, "float_precision": {"type": ["none\"", "\"literal['high', 'legacy']"], "default": null}, "storage_options": {"type": ["storageoptions", "null"], "default": null}, "dtype_backend": {"type": ["dtypebackend", "lib.nodefault"], "default": "<no_default>"}}}}}
{"task_id": "BigCodeBench/1022", "data": {"name": "pandas.to_datetime", "type": "callable", "signature": "(arg: 'DatetimeScalarOrArrayConvertible | DictConvertible', format: 'str | None' = None)", "description": "Convert argument to datetime.\n\nThis function converts a scalar, array-like, :class:`Series` or\n:class:`DataFrame`/dict-like to a pandas datetime object.\n\nParameters\n----------\narg : int, float, str, datetime, list, tuple, 1-d array, Series, DataFrame/dict-like\n    The object to convert to a datetime. If a :class:`DataFrame` is provided, the\n    method expects minimally the following columns: :const:`\"year\"`,\n    :const:`\"month\"`, :const:`\"day\"`. The column \"year\"\n    must be specified in 4-digit format.\nerrors : {'ignore', 'raise', 'coerce'}, default 'raise'\n    - If :const:`'raise'`, then invalid parsing will raise an exception.\n    - If :const:`'coerce'`, then invalid parsing will be set as :const:`NaT`.\n    - If :const:`'ignore'`, then invalid parsing will return the input.\ndayfirst : bool, default False\n    Specify a date parse order if `arg` is str or is list-like.\n    If :const:`True`, parses dates with the day first, e.g. :const:`\"10/11/12\"`\n    is parsed as :const:`2012-11-10`.\n\n    .. warning::\n\n        ``dayfirst=True`` is not strict, but will prefer to parse\n        with day first.\n\nyearfirst : bool, default False\n    Specify a date parse order if `arg` is str or is list-like.\n\n    - If :const:`True` parses dates with the year first, e.g.\n      :const:`\"10/11/12\"` is parsed as :const:`2010-11-12`.\n    - If both `dayfirst` and `yearfirst` are :const:`True`, `yearfirst` is\n      preceded (same as :mod:`dateutil`).\n\n    .. warning::\n\n        ``yearfirst=True`` is not strict, but will prefer to parse\n        with year first.\n\nutc : bool, default False\n    Control timezone-related parsing, localization and conversion.\n\n    - If :const:`True`, the function *always* returns a timezone-aware\n      UTC-localized :class:`Timestamp`, :class:`Series` or\n      :class:`DatetimeIndex`. To do this, timezone-naive inputs are\n      *localized* as UTC, while timezone-aware inputs are *converted* to UTC.\n\n    - If :const:`False` (default), inputs will not be coerced to UTC.\n      Timezone-naive inputs will remain naive, while timezone-aware ones\n      will keep their time offsets. Limitations exist for mixed\n      offsets (typically, daylight savings), see :ref:`Examples\n      <to_datetime_tz_examples>` section for details.\n\n    .. warning::\n\n        In a future version of pandas, parsing datetimes with mixed time\n        zones will raise an error unless `utc=True`.\n        Please specify `utc=True` to opt in to the new behaviour\n        and silence this warning. To create a `Series` with mixed offsets and\n        `object` dtype, please use `apply` and `datetime.datetime.strptime`.\n\n    See also: pandas general documentation about `timezone conversion and\n    localization\n    <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html\n    #time-zone-handling>`_.\n\nformat : str, default None\n    The strftime to parse time, e.g. :const:`\"%d/%m/%Y\"`. See\n    `strftime documentation\n    <https://docs.python.org/3/library/datetime.html\n    #strftime-and-strptime-behavior>`_ for more information on choices, though\n    note that :const:`\"%f\"` will parse all the way up to nanoseconds.\n    You can also pass:\n\n    - \"ISO8601\", to parse any `ISO8601 <https://en.wikipedia.org/wiki/ISO_8601>`_\n      time string (not necessarily in exactly the same format);\n    - \"mixed\", to infer the format for each element individually. This is risky,\n      and you should probably use it along with `dayfirst`.\n\n    .. note::\n\n        If a :class:`DataFrame` is passed, then `format` has no effect.\n\nexact : bool, default True\n    Control how `format` is used:\n\n    - If :const:`True`, require an exact `format` match.\n    - If :const:`False`, allow the `format` to match anywhere in the target\n      string.\n\n    Cannot be used alongside ``format='ISO8601'`` or ``format='mixed'``.\nunit : str, default 'ns'\n    The unit of the arg (D,s,ms,us,ns) denote the unit, which is an\n    integer or float number. This will be based off the origin.\n    Example, with ``unit='ms'`` and ``origin='unix'``, this would calculate\n    the number of milliseconds to the unix epoch start.\ninfer_datetime_format : bool, default False\n    If :const:`True` and no `format` is given, attempt to infer the format\n    of the datetime strings based on the first non-NaN element,\n    and if it can be inferred, switch to a faster method of parsing them.\n    In some cases this can increase the parsing speed by ~5-10x.\n\n    .. deprecated:: 2.0.0\n        A strict version of this argument is now the default, passing it has\n        no effect.\n\norigin : scalar, default 'unix'\n    Define the reference date. The numeric values would be parsed as number\n    of units (defined by `unit`) since this reference date.\n\n    - If :const:`'unix'` (or POSIX) time; origin is set to 1970-01-01.\n    - If :const:`'julian'`, unit must be :const:`'D'`, and origin is set to\n      beginning of Julian Calendar. Julian day number :const:`0` is assigned\n      to the day starting at noon on January 1, 4713 BC.\n    - If Timestamp convertible (Timestamp, dt.datetime, np.datetimt64 or date\n      string), origin is set to Timestamp identified by origin.\n    - If a float or integer, origin is the difference\n      (in units determined by the ``unit`` argument) relative to 1970-01-01.\ncache : bool, default True\n    If :const:`True`, use a cache of unique, converted dates to apply the\n    datetime conversion. May produce significant speed-up when parsing\n    duplicate date strings, especially ones with timezone offsets. The cache\n    is only used when there are at least 50 values. The presence of\n    out-of-bounds values will render the cache unusable and may slow down\n    parsing.\n\nReturns\n-------\ndatetime\n    If parsing succeeded.\n    Return type depends on input (types in parenthesis correspond to\n    fallback in case of unsuccessful timezone or out-of-range timestamp\n    parsing):\n\n    - scalar: :class:`Timestamp` (or :class:`datetime.datetime`)\n    - array-like: :class:`DatetimeIndex` (or :class:`Series` with\n      :class:`object` dtype containing :class:`datetime.datetime`)\n    - Series: :class:`Series` of :class:`datetime64` dtype (or\n      :class:`Series` of :class:`object` dtype containing\n      :class:`datetime.datetime`)\n    - DataFrame: :class:`Series` of :class:`datetime64` dtype (or\n      :class:`Series` of :class:`object` dtype containing\n      :class:`datetime.datetime`)\n\nRaises\n------\nParserError\n    When parsing a date from string fails.\nValueError\n    When another datetime conversion error happens. For example when one\n    of 'year', 'month', day' columns is missing in a :class:`DataFrame`, or\n    when a Timezone-aware :class:`datetime.datetime` is found in an array-like\n    of mixed time offsets, and ``utc=False``.\n\nSee Also\n--------\nDataFrame.astype : Cast argument to a specified dtype.\nto_timedelta : Convert argument to timedelta.\nconvert_dtypes : Convert dtypes.\n\nNotes\n-----\n\nMany input types are supported, and lead to different output types:\n\n- **scalars** can be int, float, str, datetime object (from stdlib :mod:`datetime`\n  module or :mod:`numpy`). They are converted to :class:`Timestamp` when\n  possible, otherwise they are converted to :class:`datetime.datetime`.\n  None/NaN/null scalars are converted to :const:`NaT`.\n\n- **array-like** can contain int, float, str, datetime objects. They are\n  converted to :class:`DatetimeIndex` when possible, otherwise they are\n  converted to :class:`Index` with :class:`object` dtype, containing\n  :class:`datetime.datetime`. None/NaN/null entries are converted to\n  :const:`NaT` in both cases.\n\n- **Series** are converted to :class:`Series` with :class:`datetime64`\n  dtype when possible, otherwise they are converted to :class:`Series` with\n  :class:`object` dtype, containing :class:`datetime.datetime`. None/NaN/null\n  entries are converted to :const:`NaT` in both cases.\n\n- **DataFrame/dict-like** are converted to :class:`Series` with\n  :class:`datetime64` dtype. For each row a datetime is created from assembling\n  the various dataframe columns. Column keys can be common abbreviations\n  like ['year', 'month', 'day', 'minute', 'second', 'ms', 'us', 'ns']) or\n  plurals of the same.\n\nThe following causes are responsible for :class:`datetime.datetime` objects\nbeing returned (possibly inside an :class:`Index` or a :class:`Series` with\n:class:`object` dtype) instead of a proper pandas designated type\n(:class:`Timestamp`, :class:`DatetimeIndex` or :class:`Series`\nwith :class:`datetime64` dtype):\n\n- when any input element is before :const:`Timestamp.min` or after\n  :const:`Timestamp.max`, see `timestamp limitations\n  <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html\n  #timeseries-timestamp-limits>`_.\n\n- when ``utc=False`` (default) and the input is an array-like or\n  :class:`Series` containing mixed naive/aware datetime, or aware with mixed\n  time offsets. Note that this happens in the (quite frequent) situation when\n  the timezone has a daylight savings policy. In that case you may wish to\n  use ``utc=True``.\n\nExamples\n--------\n\n**Handling various input formats**\n\nAssembling a datetime from multiple columns of a :class:`DataFrame`. The keys\ncan be common abbreviations like ['year', 'month', 'day', 'minute', 'second',\n'ms', 'us', 'ns']) or plurals of the same\n\n>>> df = pd.DataFrame({'year': [2015, 2016],\n...                    'month': [2, 3],\n...                    'day': [4, 5]})\n>>> pd.to_datetime(df)\n0   2015-02-04\n1   2016-03-05\ndtype: datetime64[ns]\n\nUsing a unix epoch time\n\n>>> pd.to_datetime(1490195805, unit='s')\nTimestamp('2017-03-22 15:16:45')\n>>> pd.to_datetime(1490195805433502912, unit='ns')\nTimestamp('2017-03-22 15:16:45.433502912')\n\n.. warning:: For float arg, precision rounding might happen. To prevent\n    unexpected behavior use a fixed-width exact type.\n\nUsing a non-unix epoch origin\n\n>>> pd.to_datetime([1, 2, 3], unit='D',\n...                origin=pd.Timestamp('1960-01-01'))\nDatetimeIndex(['1960-01-02', '1960-01-03', '1960-01-04'],\n              dtype='datetime64[ns]', freq=None)\n\n**Differences with strptime behavior**\n\n:const:`\"%f\"` will parse all the way up to nanoseconds.\n\n>>> pd.to_datetime('2018-10-26 12:00:00.0000000011',\n...                format='%Y-%m-%d %H:%M:%S.%f')\nTimestamp('2018-10-26 12:00:00.000000001')\n\n**Non-convertible date/times**\n\nPassing ``errors='coerce'`` will force an out-of-bounds date to :const:`NaT`,\nin addition to forcing non-dates (or non-parseable dates) to :const:`NaT`.\n\n>>> pd.to_datetime('13000101', format='%Y%m%d', errors='coerce')\nNaT\n\n.. _to_datetime_tz_examples:\n\n**Timezones and time offsets**\n\nThe default behaviour (``utc=False``) is as follows:\n\n- Timezone-naive inputs are converted to timezone-naive :class:`DatetimeIndex`:\n\n>>> pd.to_datetime(['2018-10-26 12:00:00', '2018-10-26 13:00:15'])\nDatetimeIndex(['2018-10-26 12:00:00', '2018-10-26 13:00:15'],\n              dtype='datetime64[ns]', freq=None)\n\n- Timezone-aware inputs *with constant time offset* are converted to\n  timezone-aware :class:`DatetimeIndex`:\n\n>>> pd.to_datetime(['2018-10-26 12:00 -0500', '2018-10-26 13:00 -0500'])\nDatetimeIndex(['2018-10-26 12:00:00-05:00', '2018-10-26 13:00:00-05:00'],\n              dtype='datetime64[ns, UTC-05:00]', freq=None)\n\n- However, timezone-aware inputs *with mixed time offsets* (for example\n  issued from a timezone with daylight savings, such as Europe/Paris)\n  are **not successfully converted** to a :class:`DatetimeIndex`.\n  Parsing datetimes with mixed time zones will show a warning unless\n  `utc=True`. If you specify `utc=False` the warning below will be shown\n  and a simple :class:`Index` containing :class:`datetime.datetime`\n  objects will be returned:\n\n>>> pd.to_datetime(['2020-10-25 02:00 +0200',\n...                 '2020-10-25 04:00 +0100'])  # doctest: +SKIP\nFutureWarning: In a future version of pandas, parsing datetimes with mixed\ntime zones will raise an error unless `utc=True`. Please specify `utc=True`\nto opt in to the new behaviour and silence this warning. To create a `Series`\nwith mixed offsets and `object` dtype, please use `apply` and\n`datetime.datetime.strptime`.\nIndex([2020-10-25 02:00:00+02:00, 2020-10-25 04:00:00+01:00],\n      dtype='object')\n\n- A mix of timezone-aware and timezone-naive inputs is also converted to\n  a simple :class:`Index` containing :class:`datetime.datetime` objects:\n\n>>> from datetime import datetime\n>>> pd.to_datetime([\"2020-01-01 01:00:00-01:00\",\n...                 datetime(2020, 1, 1, 3, 0)])  # doctest: +SKIP\nFutureWarning: In a future version of pandas, parsing datetimes with mixed\ntime zones will raise an error unless `utc=True`. Please specify `utc=True`\nto opt in to the new behaviour and silence this warning. To create a `Series`\nwith mixed offsets and `object` dtype, please use `apply` and\n`datetime.datetime.strptime`.\nIndex([2020-01-01 01:00:00-01:00, 2020-01-01 03:00:00], dtype='object')\n\n|\n\nSetting ``utc=True`` solves most of the above issues:\n\n- Timezone-naive inputs are *localized* as UTC\n\n>>> pd.to_datetime(['2018-10-26 12:00', '2018-10-26 13:00'], utc=True)\nDatetimeIndex(['2018-10-26 12:00:00+00:00', '2018-10-26 13:00:00+00:00'],\n              dtype='datetime64[ns, UTC]', freq=None)\n\n- Timezone-aware inputs are *converted* to UTC (the output represents the\n  exact same datetime, but viewed from the UTC time offset `+00:00`).\n\n>>> pd.to_datetime(['2018-10-26 12:00 -0530', '2018-10-26 12:00 -0500'],\n...                utc=True)\nDatetimeIndex(['2018-10-26 17:30:00+00:00', '2018-10-26 17:00:00+00:00'],\n              dtype='datetime64[ns, UTC]', freq=None)\n\n- Inputs can contain both string or datetime, the above\n  rules still apply\n\n>>> pd.to_datetime(['2018-10-26 12:00', datetime(2020, 1, 1, 18)], utc=True)\nDatetimeIndex(['2018-10-26 12:00:00+00:00', '2020-01-01 18:00:00+00:00'],\n              dtype='datetime64[ns, UTC]', freq=None)", "parameters": {"type": "object", "properties": {"arg": {"type": ["dictconvertible", "datetimescalarorarrayconvertible"]}, "format": {"type": ["str", "null"], "default": null}}}}}
{"task_id": "BigCodeBench/1039", "data": {"name": "ssl.SSLContext", "type": "class", "signature": "(protocol=None, *args, **kwargs)", "description": "An SSLContext holds various SSL-related configuration options and\ndata, such as certificates and possibly a private key.", "parameters": {"type": "object", "properties": {"protocol": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/1039", "data": {"name": "ssl.SSLContext.wrap_socket", "type": "callable", "signature": "(self, sock, server_side=False)", "description": "", "parameters": {"type": "object", "properties": {"sock": {}, "server_side": {"type": "bool", "default": false}}}}}
{"task_id": "BigCodeBench/1039", "data": {"name": "ssl.SSLContext.load_cert_chain", "type": "callable", "signature": "(self, certfile, keyfile=None)", "description": "", "parameters": {"type": "object", "properties": {"certfile": {}, "keyfile": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/1039", "data": {"name": "context.wrap_socket(client_socket, server_side=True).close()"}}
{"task_id": "BigCodeBench/1039", "data": {"name": "context.wrap_socket(client_socket, server_side=True).recv(buffer_size)"}}
{"task_id": "BigCodeBench/1039", "data": {"name": "context.wrap_socket(client_socket, server_side=True).send(response.encode('utf-8'))"}}
{"task_id": "BigCodeBench/1039", "data": {"name": "hashlib.sha256", "type": "callable", "signature": "()", "description": "Returns a sha256 hash object; optionally initialized with a string", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/1039", "data": {"name": "hashlib.sha256.hexdigest", "type": "callable", "signature": "()", "description": "Returns a sha256 hash object; optionally initialized with a string", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/1039", "data": {"name": "hashlib.sha256.update", "type": "callable", "signature": "(string=b'')", "description": "Returns a sha256 hash object; optionally initialized with a string", "parameters": {"type": "object", "properties": {"string": {"type": "str", "default": "b''"}}}}}
{"task_id": "BigCodeBench/1039", "data": {"name": "open.read(4096)"}}
{"task_id": "BigCodeBench/1039", "data": {"name": "os.path.exists", "type": "callable", "signature": "(path)", "description": "Test whether a path exists.  Returns False for broken symbolic links", "parameters": {"type": "object", "properties": {"path": {}}}}}
{"task_id": "BigCodeBench/1039", "data": {"name": "sha256_hash.hexdigest().encode('utf-8')"}}
{"task_id": "BigCodeBench/1039", "data": {"name": "ssl.PROTOCOL_TLS_SERVER", "type": "constant", "signature": null, "description": "An enumeration.", "value": "_SSLMethod.PROTOCOL_TLS_SERVER", "parameters": {"type": "constant"}}}
{"task_id": "BigCodeBench/1040", "data": {"name": "datetime.datetime", "type": "class", "signature": "(self, /, *args, **kwargs)", "description": "datetime(year, month, day[, hour[, minute[, second[, microsecond[,tzinfo]]]]])\n\nThe year, month and day arguments are required. tzinfo may be None, or an\ninstance of a tzinfo subclass. The remaining arguments may be ints.", "parameters": {"type": "object", "properties": {"/": {}}}}}
{"task_id": "BigCodeBench/1040", "data": {"name": "datetime.datetime.now", "type": "callable", "signature": "()", "description": "Returns new datetime object representing current time local to tz.\n\n  tz\n    Timezone object.\n\nIf no tz is specified, uses local timezone.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/1040", "data": {"name": "socket.socket", "type": "class", "signature": "(family=-1, type=-1, proto=-1, fileno=None)", "description": "A subclass of _socket.socket adding the makefile() method.", "parameters": {"type": "object", "properties": {"family": {"type": "integer", "default": -1}, "type": {"type": "integer", "default": -1}, "proto": {"type": "integer", "default": -1}, "fileno": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/1040", "data": {"name": "socket.socket.close", "type": "callable", "signature": "(self)", "description": "close()\n\nClose the socket.  It cannot be used after this call.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/1040", "data": {"name": "socket.socket.listen", "type": "callable", "signature": "(*args, **kwargs)", "description": "listen([backlog])\n\nEnable a server to accept connections.  If backlog is specified, it must be\nat least 0 (if it is lower, it is set to 0); it specifies the number of\nunaccepted connections that the system will allow before refusing new\nconnections. If not specified, a default reasonable value is chosen.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/1040", "data": {"name": "socket.socket.setblocking", "type": "callable", "signature": "(*args, **kwargs)", "description": "setblocking(flag)\n\nSet the socket to blocking (flag is true) or non-blocking (false).\nsetblocking(True) is equivalent to settimeout(None);\nsetblocking(False) is equivalent to settimeout(0.0).", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/1040", "data": {"name": "socket.socket.bind", "type": "callable", "signature": "(*args, **kwargs)", "description": "bind(address)\n\nBind the socket to a local address.  For IP sockets, the address is a\npair (host, port); the host must refer to the local host. For raw packet\nsockets the address is a tuple (ifname, proto [,pkttype [,hatype [,addr]]])", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/1040", "data": {"name": "inputs.remove(s)"}}
{"task_id": "BigCodeBench/1040", "data": {"name": "inputs.append(connection)"}}
{"task_id": "BigCodeBench/1040", "data": {"name": "message_queues[s].get_nowait.encode"}}
{"task_id": "BigCodeBench/1040", "data": {"name": "outputs.append(s)"}}
{"task_id": "BigCodeBench/1040", "data": {"name": "outputs.remove(s)"}}
{"task_id": "BigCodeBench/1040", "data": {"name": "s.close()"}}
{"task_id": "BigCodeBench/1040", "data": {"name": "s.accept()"}}
{"task_id": "BigCodeBench/1040", "data": {"name": "s.recv(buffer_size)"}}
{"task_id": "BigCodeBench/1040", "data": {"name": "s.accept[0].setblocking"}}
{"task_id": "BigCodeBench/1040", "data": {"name": "s.recv.decode()"}}
{"task_id": "BigCodeBench/1040", "data": {"name": "s.sendall(next_msg.encode('utf-8'))"}}
{"task_id": "BigCodeBench/1040", "data": {"name": "select.select", "type": "callable", "signature": "(rlist, wlist, xlist, timeout=None)", "description": "Wait until one or more file descriptors are ready for some kind of I/O.\n\nThe first three arguments are iterables of file descriptors to be waited for:\nrlist -- wait until ready for reading\nwlist -- wait until ready for writing\nxlist -- wait for an \"exceptional condition\"\nIf only one kind of condition is required, pass [] for the other lists.\n\nA file descriptor is either a socket or file object, or a small integer\ngotten from a fileno() method call on one of those.\n\nThe optional 4th argument specifies a timeout in seconds; it may be\na floating point number to specify fractions of seconds.  If it is absent\nor None, the call will never time out.\n\nThe return value is a tuple of three lists corresponding to the first three\narguments; each contains the subset of the corresponding file descriptors\nthat are ready.\n\n*** IMPORTANT NOTICE ***\nOn Windows, only sockets are supported; on Unix, all file\ndescriptors can be used.", "parameters": {"type": "object", "properties": {"rlist": {}, "wlist": {}, "xlist": {}, "timeout": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/1040", "data": {"name": "socket.AF_INET", "type": "constant", "signature": null, "description": "An enumeration.", "value": "AddressFamily.AF_INET", "parameters": {"type": "constant"}}}
{"task_id": "BigCodeBench/1040", "data": {"name": "socket.SOCK_STREAM", "type": "constant", "signature": null, "description": "An enumeration.", "value": "SocketKind.SOCK_STREAM", "parameters": {"type": "constant"}}}
{"task_id": "BigCodeBench/1042", "data": {"name": "smtplib.SMTP", "type": "class", "signature": "(host='', port=0, local_hostname=None, timeout=<object object at 0x7fa57439cb10>, source_address=None)", "description": "This class manages a connection to an SMTP or ESMTP server.\nSMTP Objects:\n    SMTP objects have the following attributes:\n        helo_resp\n            This is the message given by the server in response to the\n            most recent HELO command.\n\n        ehlo_resp\n            This is the message given by the server in response to the\n            most recent EHLO command. This is usually multiline.\n\n        does_esmtp\n            This is a True value _after you do an EHLO command_, if the\n            server supports ESMTP.\n\n        esmtp_features\n            This is a dictionary, which, if the server supports ESMTP,\n            will _after you do an EHLO command_, contain the names of the\n            SMTP service extensions this server supports, and their\n            parameters (if any).\n\n            Note, all extension names are mapped to lower case in the\n            dictionary.\n\n    See each method's docstrings for details.  In general, there is a\n    method of the same name to perform each SMTP command.  There is also a\n    method called 'sendmail' that will do an entire mail transaction.\n    ", "parameters": {"type": "object", "properties": {"host": {"type": "str", "default": ""}, "port": {"type": "integer", "default": 0}, "local_hostname": {"type": "NoneType", "default": null}, "timeout": {"type": "str", "default": "<object object at 0x7fa57439cb10>"}, "source_address": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/1042", "data": {"name": "smtplib.SMTP.starttls", "type": "callable", "signature": "(self)", "description": "Puts the connection to the SMTP server into TLS mode.\n\nIf there has been no previous EHLO or HELO command this session, this\nmethod tries ESMTP EHLO first.\n\nIf the server supports TLS, this will encrypt the rest of the SMTP\nsession. If you provide the keyfile and certfile parameters,\nthe identity of the SMTP server and client can be checked. This,\nhowever, depends on whether the socket module really checks the\ncertificates.\n\nThis method may raise the following exceptions:\n\n SMTPHeloError            The server didn't reply properly to\n                          the helo greeting.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/1042", "data": {"name": "smtplib.SMTP.send_message", "type": "callable", "signature": "(self, msg)", "description": "Converts message to a bytestring and passes it to sendmail.\n\nThe arguments are as for sendmail, except that msg is an\nemail.message.Message object.  If from_addr is None or to_addrs is\nNone, these arguments are taken from the headers of the Message as\ndescribed in RFC 2822 (a ValueError is raised if there is more than\none set of 'Resent-' headers).  Regardless of the values of from_addr and\nto_addr, any Bcc field (or Resent-Bcc field, when the Message is a\nresent) of the Message object won't be transmitted.  The Message\nobject is then serialized using email.generator.BytesGenerator and\nsendmail is called to transmit the message.  If the sender or any of\nthe recipient addresses contain non-ASCII and the server advertises the\nSMTPUTF8 capability, the policy is cloned with utf8 set to True for the\nserialization, and SMTPUTF8 and BODY=8BITMIME are asserted on the send.\nIf the server does not support SMTPUTF8, an SMTPNotSupported error is\nraised.  Otherwise the generator is called without modifying the\npolicy.", "parameters": {"type": "object", "properties": {"msg": {}}}}}
{"task_id": "BigCodeBench/1042", "data": {"name": "smtplib.SMTP.login", "type": "callable", "signature": "(self, user, password)", "description": "Log in on an SMTP server that requires authentication.\n\nThe arguments are:\n    - user:         The user name to authenticate with.\n    - password:     The password for the authentication.\n\nKeyword arguments:\n    - initial_response_ok: Allow sending the RFC 4954 initial-response\n      to the AUTH command, if the authentication methods supports it.\n\nIf there has been no previous EHLO or HELO command this session, this\nmethod tries ESMTP EHLO first.\n\nThis method will return normally if the authentication was successful.\n\nThis method may raise the following exceptions:\n\n SMTPHeloError            The server didn't reply properly to\n                          the helo greeting.\n SMTPAuthenticationError  The server didn't accept the username/\n                          password combination.\n SMTPNotSupportedError    The AUTH command is not supported by the\n                          server.\n SMTPException            No suitable authentication method was\n                          found.", "parameters": {"type": "object", "properties": {"user": {}, "password": {}}}}}
{"task_id": "BigCodeBench/1042", "data": {"name": "client_socket.close()"}}
{"task_id": "BigCodeBench/1042", "data": {"name": "client_socket.recv(BUFFER_SIZE)"}}
{"task_id": "BigCodeBench/1042", "data": {"name": "client_socket.send(response.encode('utf-8'))"}}
{"task_id": "BigCodeBench/1042", "data": {"name": "email.message.EmailMessage", "type": "class", "signature": "()", "description": "Basic message object.\n\nA message object is defined as something that has a bunch of RFC 2822\nheaders and a payload.  It may optionally have an envelope header\n(a.k.a. Unix-From or From_ header).  If the message is a container (i.e. a\nmultipart or a message/rfc822), then the payload is a list of Message\nobjects, otherwise it is a string.\n\nMessage objects implement part of the `mapping' interface, which assumes\nthere is exactly one occurrence of the header per message.  Some headers\ndo in fact appear multiple times (e.g. Received) and for those headers,\nyou must use the explicit API to set or get all the headers.  Not all of\nthe mapping methods are implemented.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/1042", "data": {"name": "email.message.EmailMessage.set_content", "type": "callable", "signature": "(self, *args)", "description": "", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/1042", "data": {"name": "getpass.getpass", "type": "callable", "signature": "(prompt='Password: ')", "description": "Prompt for a password, with echo turned off.\n\nArgs:\n  prompt: Written on stream to ask for the input.  Default: 'Password: '\n  stream: A writable file object to display the prompt.  Defaults to\n          the tty.  If no tty is available defaults to sys.stderr.\nReturns:\n  The seKr3t input.\nRaises:\n  EOFError: If our input tty or stdin was closed.\n  GetPassWarning: When we were unable to turn echo off on the input.\n\nAlways restores terminal settings before returning.", "parameters": {"type": "object", "properties": {"prompt": {"type": "str", "default": "Password: "}}}}}
{"task_id": "BigCodeBench/1042", "data": {"name": "response.encode('utf-8')"}}
{"task_id": "BigCodeBench/1053", "data": {"name": "pandas.DataFrame", "type": "class", "signature": "(data=None, index: 'Axes | None' = None, columns: 'Axes | None' = None, dtype: 'Dtype | None' = None, copy: 'bool | None' = None) -> 'None'", "description": "Two-dimensional, size-mutable, potentially heterogeneous tabular data.\n\nData structure also contains labeled axes (rows and columns).\nArithmetic operations align on both row and column labels. Can be\nthought of as a dict-like container for Series objects. The primary\npandas data structure.\n\nParameters\n----------\ndata : ndarray (structured or homogeneous), Iterable, dict, or DataFrame\n    Dict can contain Series, arrays, constants, dataclass or list-like objects. If\n    data is a dict, column order follows insertion-order. If a dict contains Series\n    which have an index defined, it is aligned by its index. This alignment also\n    occurs if data is a Series or a DataFrame itself. Alignment is done on\n    Series/DataFrame inputs.\n\n    If data is a list of dicts, column order follows insertion-order.\n\nindex : Index or array-like\n    Index to use for resulting frame. Will default to RangeIndex if\n    no indexing information part of input data and no index provided.\ncolumns : Index or array-like\n    Column labels to use for resulting frame when data does not have them,\n    defaulting to RangeIndex(0, 1, 2, ..., n). If data contains column labels,\n    will perform column selection instead.\ndtype : dtype, default None\n    Data type to force. Only a single dtype is allowed. If None, infer.\ncopy : bool or None, default None\n    Copy data from inputs.\n    For dict data, the default of None behaves like ``copy=True``.  For DataFrame\n    or 2d ndarray input, the default of None behaves like ``copy=False``.\n    If data is a dict containing one or more Series (possibly of different dtypes),\n    ``copy=False`` will ensure that these inputs are not copied.\n\n    .. versionchanged:: 1.3.0\n\nSee Also\n--------\nDataFrame.from_records : Constructor from tuples, also record arrays.\nDataFrame.from_dict : From dicts of Series, arrays, or dicts.\nread_csv : Read a comma-separated values (csv) file into DataFrame.\nread_table : Read general delimited file into DataFrame.\nread_clipboard : Read text from clipboard into DataFrame.\n\nNotes\n-----\nPlease reference the :ref:`User Guide <basics.dataframe>` for more information.\n\nExamples\n--------\nConstructing DataFrame from a dictionary.\n\n>>> d = {'col1': [1, 2], 'col2': [3, 4]}\n>>> df = pd.DataFrame(data=d)\n>>> df\n   col1  col2\n0     1     3\n1     2     4\n\nNotice that the inferred dtype is int64.\n\n>>> df.dtypes\ncol1    int64\ncol2    int64\ndtype: object\n\nTo enforce a single dtype:\n\n>>> df = pd.DataFrame(data=d, dtype=np.int8)\n>>> df.dtypes\ncol1    int8\ncol2    int8\ndtype: object\n\nConstructing DataFrame from a dictionary including Series:\n\n>>> d = {'col1': [0, 1, 2, 3], 'col2': pd.Series([2, 3], index=[2, 3])}\n>>> pd.DataFrame(data=d, index=[0, 1, 2, 3])\n   col1  col2\n0     0   NaN\n1     1   NaN\n2     2   2.0\n3     3   3.0\n\nConstructing DataFrame from numpy ndarray:\n\n>>> df2 = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),\n...                    columns=['a', 'b', 'c'])\n>>> df2\n   a  b  c\n0  1  2  3\n1  4  5  6\n2  7  8  9\n\nConstructing DataFrame from a numpy ndarray that has labeled columns:\n\n>>> data = np.array([(1, 2, 3), (4, 5, 6), (7, 8, 9)],\n...                 dtype=[(\"a\", \"i4\"), (\"b\", \"i4\"), (\"c\", \"i4\")])\n>>> df3 = pd.DataFrame(data, columns=['c', 'a'])\n...\n>>> df3\n   c  a\n0  3  1\n1  6  4\n2  9  7\n\nConstructing DataFrame from dataclass:\n\n>>> from dataclasses import make_dataclass\n>>> Point = make_dataclass(\"Point\", [(\"x\", int), (\"y\", int)])\n>>> pd.DataFrame([Point(0, 0), Point(0, 3), Point(2, 3)])\n   x  y\n0  0  0\n1  0  3\n2  2  3\n\nConstructing DataFrame from Series/DataFrame:\n\n>>> ser = pd.Series([1, 2, 3], index=[\"a\", \"b\", \"c\"])\n>>> df = pd.DataFrame(data=ser, index=[\"a\", \"c\"])\n>>> df\n   0\na  1\nc  3\n\n>>> df1 = pd.DataFrame([1, 2, 3], index=[\"a\", \"b\", \"c\"], columns=[\"x\"])\n>>> df2 = pd.DataFrame(data=df1, index=[\"a\", \"c\"])\n>>> df2\n   x\na  1\nc  3", "parameters": {"type": "object", "properties": {"data": {"type": "NoneType", "default": null}, "index": {"type": ["axes", "null"], "default": null}, "columns": {"type": ["axes", "null"], "default": null}, "dtype": {"type": ["dtype", "null"], "default": null}, "copy": {"type": ["bool", "null"], "default": null}}}}}
{"task_id": "BigCodeBench/1053", "data": {"name": "pandas.DataFrame.plot", "type": "class", "signature": "(data: 'Series | DataFrame') -> 'None'", "description": "Make plots of Series or DataFrame.\n\nUses the backend specified by the\noption ``plotting.backend``. By default, matplotlib is used.\n\nParameters\n----------\ndata : Series or DataFrame\n    The object for which the method is called.\nx : label or position, default None\n    Only used if data is a DataFrame.\ny : label, position or list of label, positions, default None\n    Allows plotting of one column versus another. Only used if data is a\n    DataFrame.\nkind : str\n    The kind of plot to produce:\n\n    - 'line' : line plot (default)\n    - 'bar' : vertical bar plot\n    - 'barh' : horizontal bar plot\n    - 'hist' : histogram\n    - 'box' : boxplot\n    - 'kde' : Kernel Density Estimation plot\n    - 'density' : same as 'kde'\n    - 'area' : area plot\n    - 'pie' : pie plot\n    - 'scatter' : scatter plot (DataFrame only)\n    - 'hexbin' : hexbin plot (DataFrame only)\nax : matplotlib axes object, default None\n    An axes of the current figure.\nsubplots : bool or sequence of iterables, default False\n    Whether to group columns into subplots:\n\n    - ``False`` : No subplots will be used\n    - ``True`` : Make separate subplots for each column.\n    - sequence of iterables of column labels: Create a subplot for each\n      group of columns. For example `[('a', 'c'), ('b', 'd')]` will\n      create 2 subplots: one with columns 'a' and 'c', and one\n      with columns 'b' and 'd'. Remaining columns that aren't specified\n      will be plotted in additional subplots (one per column).\n\n      .. versionadded:: 1.5.0\n\nsharex : bool, default True if ax is None else False\n    In case ``subplots=True``, share x axis and set some x axis labels\n    to invisible; defaults to True if ax is None otherwise False if\n    an ax is passed in; Be aware, that passing in both an ax and\n    ``sharex=True`` will alter all x axis labels for all axis in a figure.\nsharey : bool, default False\n    In case ``subplots=True``, share y axis and set some y axis labels to invisible.\nlayout : tuple, optional\n    (rows, columns) for the layout of subplots.\nfigsize : a tuple (width, height) in inches\n    Size of a figure object.\nuse_index : bool, default True\n    Use index as ticks for x axis.\ntitle : str or list\n    Title to use for the plot. If a string is passed, print the string\n    at the top of the figure. If a list is passed and `subplots` is\n    True, print each item in the list above the corresponding subplot.\ngrid : bool, default None (matlab style default)\n    Axis grid lines.\nlegend : bool or {'reverse'}\n    Place legend on axis subplots.\nstyle : list or dict\n    The matplotlib line style per column.\nlogx : bool or 'sym', default False\n    Use log scaling or symlog scaling on x axis.\n\nlogy : bool or 'sym' default False\n    Use log scaling or symlog scaling on y axis.\n\nloglog : bool or 'sym', default False\n    Use log scaling or symlog scaling on both x and y axes.\n\nxticks : sequence\n    Values to use for the xticks.\nyticks : sequence\n    Values to use for the yticks.\nxlim : 2-tuple/list\n    Set the x limits of the current axes.\nylim : 2-tuple/list\n    Set the y limits of the current axes.\nxlabel : label, optional\n    Name to use for the xlabel on x-axis. Default uses index name as xlabel, or the\n    x-column name for planar plots.\n\n    .. versionchanged:: 2.0.0\n\n        Now applicable to histograms.\n\nylabel : label, optional\n    Name to use for the ylabel on y-axis. Default will show no ylabel, or the\n    y-column name for planar plots.\n\n    .. versionchanged:: 2.0.0\n\n        Now applicable to histograms.\n\nrot : float, default None\n    Rotation for ticks (xticks for vertical, yticks for horizontal\n    plots).\nfontsize : float, default None\n    Font size for xticks and yticks.\ncolormap : str or matplotlib colormap object, default None\n    Colormap to select colors from. If string, load colormap with that\n    name from matplotlib.\ncolorbar : bool, optional\n    If True, plot colorbar (only relevant for 'scatter' and 'hexbin'\n    plots).\nposition : float\n    Specify relative alignments for bar plot layout.\n    From 0 (left/bottom-end) to 1 (right/top-end). Default is 0.5\n    (center).\ntable : bool, Series or DataFrame, default False\n    If True, draw a table using the data in the DataFrame and the data\n    will be transposed to meet matplotlib's default layout.\n    If a Series or DataFrame is passed, use passed data to draw a\n    table.\nyerr : DataFrame, Series, array-like, dict and str\n    See :ref:`Plotting with Error Bars <visualization.errorbars>` for\n    detail.\nxerr : DataFrame, Series, array-like, dict and str\n    Equivalent to yerr.\nstacked : bool, default False in line and bar plots, and True in area plot\n    If True, create stacked plot.\nsecondary_y : bool or sequence, default False\n    Whether to plot on the secondary y-axis if a list/tuple, which\n    columns to plot on secondary y-axis.\nmark_right : bool, default True\n    When using a secondary_y axis, automatically mark the column\n    labels with \"(right)\" in the legend.\ninclude_bool : bool, default is False\n    If True, boolean values can be plotted.\nbackend : str, default None\n    Backend to use instead of the backend specified in the option\n    ``plotting.backend``. For instance, 'matplotlib'. Alternatively, to\n    specify the ``plotting.backend`` for the whole session, set\n    ``pd.options.plotting.backend``.\n**kwargs\n    Options to pass to matplotlib plotting method.\n\nReturns\n-------\n:class:`matplotlib.axes.Axes` or numpy.ndarray of them\n    If the backend is not the default matplotlib one, the return value\n    will be the object returned by the backend.\n\nNotes\n-----\n- See matplotlib documentation online for more on this subject\n- If `kind` = 'bar' or 'barh', you can specify relative alignments\n  for bar plot layout by `position` keyword.\n  From 0 (left/bottom-end) to 1 (right/top-end). Default is 0.5\n  (center)\n\nExamples\n--------\nFor Series:\n\n.. plot::\n    :context: close-figs\n\n    >>> ser = pd.Series([1, 2, 3, 3])\n    >>> plot = ser.plot(kind='hist', title=\"My plot\")\n\nFor DataFrame:\n\n.. plot::\n    :context: close-figs\n\n    >>> df = pd.DataFrame({'length': [1.5, 0.5, 1.2, 0.9, 3],\n    ...                   'width': [0.7, 0.2, 0.15, 0.2, 1.1]},\n    ...                   index=['pig', 'rabbit', 'duck', 'chicken', 'horse'])\n    >>> plot = df.plot(title=\"DataFrame Plot\")\n\nFor SeriesGroupBy:\n\n.. plot::\n    :context: close-figs\n\n    >>> lst = [-1, -2, -3, 1, 2, 3]\n    >>> ser = pd.Series([1, 2, 2, 4, 6, 6], index=lst)\n    >>> plot = ser.groupby(lambda x: x > 0).plot(title=\"SeriesGroupBy Plot\")\n\nFor DataFrameGroupBy:\n\n.. plot::\n    :context: close-figs\n\n    >>> df = pd.DataFrame({\"col1\" : [1, 2, 3, 4],\n    ...                   \"col2\" : [\"A\", \"B\", \"A\", \"B\"]})\n    >>> plot = df.groupby(\"col2\").plot(kind=\"bar\", title=\"DataFrameGroupBy Plot\")"}}
{"task_id": "BigCodeBench/1053", "data": {"name": "pandas.DataFrame.plot.bar", "type": "callable", "signature": "(self, x: 'Hashable | None' = None, y: 'Hashable | None' = None, **kwargs) -> 'PlotAccessor)", "description": "Vertical bar plot.\n\nA bar plot is a plot that presents categorical data with\nrectangular bars with lengths proportional to the values that they\nrepresent. A bar plot shows comparisons among discrete categories. One\naxis of the plot shows the specific categories being compared, and the\nother axis represents a measured value.\n\nParameters\n----------\nx : label or position, optional\n    Allows plotting of one column versus another. If not specified,\n    the index of the DataFrame is used.\ny : label or position, optional\n    Allows plotting of one column versus another. If not specified,\n    all numerical columns are used.\ncolor : str, array-like, or dict, optional\n    The color for each of the DataFrame's columns. Possible values are:\n\n    - A single color string referred to by name, RGB or RGBA code,\n        for instance 'red' or '#a98d19'.\n\n    - A sequence of color strings referred to by name, RGB or RGBA\n        code, which will be used for each column recursively. For\n        instance ['green','yellow'] each column's bar will be filled in\n        green or yellow, alternatively. If there is only a single column to\n        be plotted, then only the first color from the color list will be\n        used.\n\n    - A dict of the form {column name : color}, so that each column will be\n        colored accordingly. For example, if your columns are called `a` and\n        `b`, then passing {'a': 'green', 'b': 'red'} will color bars for\n        column `a` in green and bars for column `b` in red.\n\n**kwargs\n    Additional keyword arguments are documented in\n    :meth:`DataFrame.plot`.\n\nReturns\n-------\nmatplotlib.axes.Axes or np.ndarray of them\n    An ndarray is returned with one :class:`matplotlib.axes.Axes`\n    per column when ``subplots=True``.\n\n        See Also\n        --------\n        DataFrame.plot.barh : Horizontal bar plot.\n        DataFrame.plot : Make plots of a DataFrame.\n        matplotlib.pyplot.bar : Make a bar plot with matplotlib.\n\n        Examples\n        --------\n        Basic plot.\n\n        .. plot::\n            :context: close-figs\n\n            >>> df = pd.DataFrame({'lab':['A', 'B', 'C'], 'val':[10, 30, 20]})\n            >>> ax = df.plot.bar(x='lab', y='val', rot=0)\n\n        Plot a whole dataframe to a bar plot. Each column is assigned a\n        distinct color, and each row is nested in a group along the\n        horizontal axis.\n\n        .. plot::\n            :context: close-figs\n\n            >>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\n            >>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\n            >>> index = ['snail', 'pig', 'elephant',\n            ...          'rabbit', 'giraffe', 'coyote', 'horse']\n            >>> df = pd.DataFrame({'speed': speed,\n            ...                    'lifespan': lifespan}, index=index)\n            >>> ax = df.plot.bar(rot=0)\n\n        Plot stacked bar charts for the DataFrame\n\n        .. plot::\n            :context: close-figs\n\n            >>> ax = df.plot.bar(stacked=True)\n\n        Instead of nesting, the figure can be split by column with\n        ``subplots=True``. In this case, a :class:`numpy.ndarray` of\n        :class:`matplotlib.axes.Axes` are returned.\n\n        .. plot::\n            :context: close-figs\n\n            >>> axes = df.plot.bar(rot=0, subplots=True)\n            >>> axes[1].legend(loc=2)  # doctest: +SKIP\n\n        If you don't like the default colours, you can specify how you'd\n        like each column to be colored.\n\n        .. plot::\n            :context: close-figs\n\n            >>> axes = df.plot.bar(\n            ...     rot=0, subplots=True, color={\"speed\": \"red\", \"lifespan\": \"green\"}\n            ... )\n            >>> axes[1].legend(loc=2)  # doctest: +SKIP\n\n        Plot a single column.\n\n        .. plot::\n            :context: close-figs\n\n            >>> ax = df.plot.bar(y='speed', rot=0)\n\n        Plot only selected categories for the DataFrame.\n\n        .. plot::\n            :context: close-figs\n\n            >>> ax = df.plot.bar(x='lifespan', rot=0)", "parameters": {"type": "object", "properties": {"x": {"type": ["hashable", "null"], "default": null}, "y": {"type": ["hashable", "null"], "default": null}}}}}
{"task_id": "BigCodeBench/1053", "data": {"name": "matplotlib.pyplot.close", "type": "callable", "signature": "()", "description": "Close a figure window.\n\nParameters\n----------\nfig : None or int or str or `.Figure`\n    The figure to close. There are a number of ways to specify this:\n\n    - *None*: the current figure\n    - `.Figure`: the given `.Figure` instance\n    - ``int``: a figure number\n    - ``str``: a figure name\n    - 'all': all figures", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/1053", "data": {"name": "matplotlib.pyplot.savefig", "type": "callable", "signature": "(*args, **kwargs) -> 'None)", "description": "Save the current figure.\n\nCall signature::\n\n  savefig(fname, *, transparent=None, dpi='figure', format=None,\n          metadata=None, bbox_inches=None, pad_inches=0.1,\n          facecolor='auto', edgecolor='auto', backend=None,\n          **kwargs\n         )\n\nThe available output formats depend on the backend being used.\n\nParameters\n----------\nfname : str or path-like or binary file-like\n    A path, or a Python file-like object, or\n    possibly some backend-dependent object such as\n    `matplotlib.backends.backend_pdf.PdfPages`.\n\n    If *format* is set, it determines the output format, and the file\n    is saved as *fname*.  Note that *fname* is used verbatim, and there\n    is no attempt to make the extension, if any, of *fname* match\n    *format*, and no extension is appended.\n\n    If *format* is not set, then the format is inferred from the\n    extension of *fname*, if there is one.  If *format* is not\n    set and *fname* has no extension, then the file is saved with\n    :rc:`savefig.format` and the appropriate extension is appended to\n    *fname*.\n\nOther Parameters\n----------------\ntransparent : bool, default: :rc:`savefig.transparent`\n    If *True*, the Axes patches will all be transparent; the\n    Figure patch will also be transparent unless *facecolor*\n    and/or *edgecolor* are specified via kwargs.\n\n    If *False* has no effect and the color of the Axes and\n    Figure patches are unchanged (unless the Figure patch\n    is specified via the *facecolor* and/or *edgecolor* keyword\n    arguments in which case those colors are used).\n\n    The transparency of these patches will be restored to their\n    original values upon exit of this function.\n\n    This is useful, for example, for displaying\n    a plot on top of a colored background on a web page.\n\ndpi : float or 'figure', default: :rc:`savefig.dpi`\n    The resolution in dots per inch.  If 'figure', use the figure's\n    dpi value.\n\nformat : str\n    The file format, e.g. 'png', 'pdf', 'svg', ... The behavior when\n    this is unset is documented under *fname*.\n\nmetadata : dict, optional\n    Key/value pairs to store in the image metadata. The supported keys\n    and defaults depend on the image format and backend:\n\n    - 'png' with Agg backend: See the parameter ``metadata`` of\n      `~.FigureCanvasAgg.print_png`.\n    - 'pdf' with pdf backend: See the parameter ``metadata`` of\n      `~.backend_pdf.PdfPages`.\n    - 'svg' with svg backend: See the parameter ``metadata`` of\n      `~.FigureCanvasSVG.print_svg`.\n    - 'eps' and 'ps' with PS backend: Only 'Creator' is supported.\n\n    Not supported for 'pgf', 'raw', and 'rgba' as those formats do not support\n    embedding metadata.\n    Does not currently support 'jpg', 'tiff', or 'webp', but may include\n    embedding EXIF metadata in the future.\n\nbbox_inches : str or `.Bbox`, default: :rc:`savefig.bbox`\n    Bounding box in inches: only the given portion of the figure is\n    saved.  If 'tight', try to figure out the tight bbox of the figure.\n\npad_inches : float or 'layout', default: :rc:`savefig.pad_inches`\n    Amount of padding in inches around the figure when bbox_inches is\n    'tight'. If 'layout' use the padding from the constrained or\n    compressed layout engine; ignored if one of those engines is not in\n    use.\n\nfacecolor : color or 'auto', default: :rc:`savefig.facecolor`\n    The facecolor of the figure.  If 'auto', use the current figure\n    facecolor.\n\nedgecolor : color or 'auto', default: :rc:`savefig.edgecolor`\n    The edgecolor of the figure.  If 'auto', use the current figure\n    edgecolor.\n\nbackend : str, optional\n    Use a non-default backend to render the file, e.g. to render a\n    png file with the \"cairo\" backend rather than the default \"agg\",\n    or a pdf file with the \"pgf\" backend rather than the default\n    \"pdf\".  Note that the default backend is normally sufficient.  See\n    :ref:`the-builtin-backends` for a list of valid backends for each\n    file format.  Custom backends can be referenced as \"module://...\".\n\norientation : {'landscape', 'portrait'}\n    Currently only supported by the postscript backend.\n\npapertype : str\n    One of 'letter', 'legal', 'executive', 'ledger', 'a0' through\n    'a10', 'b0' through 'b10'. Only supported for postscript\n    output.\n\nbbox_extra_artists : list of `~matplotlib.artist.Artist`, optional\n    A list of extra artists that will be considered when the\n    tight bbox is calculated.\n\npil_kwargs : dict, optional\n    Additional keyword arguments that are passed to\n    `PIL.Image.Image.save` when saving the figure.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/1053", "data": {"name": "pandas.read_csv", "type": "callable", "signature": "(filepath_or_buffer: 'FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str]', header: \"int | Sequence[int] | None | Literal['infer']\" = 'infer', names: 'Sequence[Hashable] | None | lib.NoDefault' = <no_default>, usecols: 'UsecolsArgType' = None)", "description": "Read a comma-separated values (csv) file into DataFrame.\n\nAlso supports optionally iterating or breaking of the file\ninto chunks.\n\nAdditional help can be found in the online docs for\n`IO Tools <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html>`_.\n\nParameters\n----------\nfilepath_or_buffer : str, path object or file-like object\n    Any valid string path is acceptable. The string could be a URL. Valid\n    URL schemes include http, ftp, s3, gs, and file. For file URLs, a host is\n    expected. A local file could be: file://localhost/path/to/table.csv.\n\n    If you want to pass in a path object, pandas accepts any ``os.PathLike``.\n\n    By file-like object, we refer to objects with a ``read()`` method, such as\n    a file handle (e.g. via builtin ``open`` function) or ``StringIO``.\nsep : str, default ','\n    Character or regex pattern to treat as the delimiter. If ``sep=None``, the\n    C engine cannot automatically detect\n    the separator, but the Python parsing engine can, meaning the latter will\n    be used and automatically detect the separator from only the first valid\n    row of the file by Python's builtin sniffer tool, ``csv.Sniffer``.\n    In addition, separators longer than 1 character and different from\n    ``'\\s+'`` will be interpreted as regular expressions and will also force\n    the use of the Python parsing engine. Note that regex delimiters are prone\n    to ignoring quoted data. Regex example: ``'\\r\\t'``.\ndelimiter : str, optional\n    Alias for ``sep``.\nheader : int, Sequence of int, 'infer' or None, default 'infer'\n    Row number(s) containing column labels and marking the start of the\n    data (zero-indexed). Default behavior is to infer the column names: if no ``names``\n    are passed the behavior is identical to ``header=0`` and column\n    names are inferred from the first line of the file, if column\n    names are passed explicitly to ``names`` then the behavior is identical to\n    ``header=None``. Explicitly pass ``header=0`` to be able to\n    replace existing names. The header can be a list of integers that\n    specify row locations for a :class:`~pandas.MultiIndex` on the columns\n    e.g. ``[0, 1, 3]``. Intervening rows that are not specified will be\n    skipped (e.g. 2 in this example is skipped). Note that this\n    parameter ignores commented lines and empty lines if\n    ``skip_blank_lines=True``, so ``header=0`` denotes the first line of\n    data rather than the first line of the file.\nnames : Sequence of Hashable, optional\n    Sequence of column labels to apply. If the file contains a header row,\n    then you should explicitly pass ``header=0`` to override the column names.\n    Duplicates in this list are not allowed.\nindex_col : Hashable, Sequence of Hashable or False, optional\n  Column(s) to use as row label(s), denoted either by column labels or column\n  indices.  If a sequence of labels or indices is given, :class:`~pandas.MultiIndex`\n  will be formed for the row labels.\n\n  Note: ``index_col=False`` can be used to force pandas to *not* use the first\n  column as the index, e.g., when you have a malformed file with delimiters at\n  the end of each line.\nusecols : Sequence of Hashable or Callable, optional\n    Subset of columns to select, denoted either by column labels or column indices.\n    If list-like, all elements must either\n    be positional (i.e. integer indices into the document columns) or strings\n    that correspond to column names provided either by the user in ``names`` or\n    inferred from the document header row(s). If ``names`` are given, the document\n    header row(s) are not taken into account. For example, a valid list-like\n    ``usecols`` parameter would be ``[0, 1, 2]`` or ``['foo', 'bar', 'baz']``.\n    Element order is ignored, so ``usecols=[0, 1]`` is the same as ``[1, 0]``.\n    To instantiate a :class:`~pandas.DataFrame` from ``data`` with element order\n    preserved use ``pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']]``\n    for columns in ``['foo', 'bar']`` order or\n    ``pd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']]``\n    for ``['bar', 'foo']`` order.\n\n    If callable, the callable function will be evaluated against the column\n    names, returning names where the callable function evaluates to ``True``. An\n    example of a valid callable argument would be ``lambda x: x.upper() in\n    ['AAA', 'BBB', 'DDD']``. Using this parameter results in much faster\n    parsing time and lower memory usage.\ndtype : dtype or dict of {Hashable : dtype}, optional\n    Data type(s) to apply to either the whole dataset or individual columns.\n    E.g., ``{'a': np.float64, 'b': np.int32, 'c': 'Int64'}``\n    Use ``str`` or ``object`` together with suitable ``na_values`` settings\n    to preserve and not interpret ``dtype``.\n    If ``converters`` are specified, they will be applied INSTEAD\n    of ``dtype`` conversion.\n\n    .. versionadded:: 1.5.0\n\n        Support for ``defaultdict`` was added. Specify a ``defaultdict`` as input where\n        the default determines the ``dtype`` of the columns which are not explicitly\n        listed.\nengine : {'c', 'python', 'pyarrow'}, optional\n    Parser engine to use. The C and pyarrow engines are faster, while the python engine\n    is currently more feature-complete. Multithreading is currently only supported by\n    the pyarrow engine.\n\n    .. versionadded:: 1.4.0\n\n        The 'pyarrow' engine was added as an *experimental* engine, and some features\n        are unsupported, or may not work correctly, with this engine.\nconverters : dict of {Hashable : Callable}, optional\n    Functions for converting values in specified columns. Keys can either\n    be column labels or column indices.\ntrue_values : list, optional\n    Values to consider as ``True`` in addition to case-insensitive variants of 'True'.\nfalse_values : list, optional\n    Values to consider as ``False`` in addition to case-insensitive variants of 'False'.\nskipinitialspace : bool, default False\n    Skip spaces after delimiter.\nskiprows : int, list of int or Callable, optional\n    Line numbers to skip (0-indexed) or number of lines to skip (``int``)\n    at the start of the file.\n\n    If callable, the callable function will be evaluated against the row\n    indices, returning ``True`` if the row should be skipped and ``False`` otherwise.\n    An example of a valid callable argument would be ``lambda x: x in [0, 2]``.\nskipfooter : int, default 0\n    Number of lines at bottom of file to skip (Unsupported with ``engine='c'``).\nnrows : int, optional\n    Number of rows of file to read. Useful for reading pieces of large files.\nna_values : Hashable, Iterable of Hashable or dict of {Hashable : Iterable}, optional\n    Additional strings to recognize as ``NA``/``NaN``. If ``dict`` passed, specific\n    per-column ``NA`` values.  By default the following values are interpreted as\n    ``NaN``: \" \", \"#N/A\", \"#N/A N/A\", \"#NA\", \"-1.#IND\", \"-1.#QNAN\", \"-NaN\", \"-nan\",\n    \"1.#IND\", \"1.#QNAN\", \"<NA>\", \"N/A\", \"NA\", \"NULL\", \"NaN\", \"None\",\n    \"n/a\", \"nan\", \"null \".\n\nkeep_default_na : bool, default True\n    Whether or not to include the default ``NaN`` values when parsing the data.\n    Depending on whether ``na_values`` is passed in, the behavior is as follows:\n\n    * If ``keep_default_na`` is ``True``, and ``na_values`` are specified, ``na_values``\n      is appended to the default ``NaN`` values used for parsing.\n    * If ``keep_default_na`` is ``True``, and ``na_values`` are not specified, only\n      the default ``NaN`` values are used for parsing.\n    * If ``keep_default_na`` is ``False``, and ``na_values`` are specified, only\n      the ``NaN`` values specified ``na_values`` are used for parsing.\n    * If ``keep_default_na`` is ``False``, and ``na_values`` are not specified, no\n      strings will be parsed as ``NaN``.\n\n    Note that if ``na_filter`` is passed in as ``False``, the ``keep_default_na`` and\n    ``na_values`` parameters will be ignored.\nna_filter : bool, default True\n    Detect missing value markers (empty strings and the value of ``na_values``). In\n    data without any ``NA`` values, passing ``na_filter=False`` can improve the\n    performance of reading a large file.\nverbose : bool, default False\n    Indicate number of ``NA`` values placed in non-numeric columns.\n\n    .. deprecated:: 2.2.0\nskip_blank_lines : bool, default True\n    If ``True``, skip over blank lines rather than interpreting as ``NaN`` values.\nparse_dates : bool, list of Hashable, list of lists or dict of {Hashable : list}, default False\n    The behavior is as follows:\n\n    * ``bool``. If ``True`` -> try parsing the index. Note: Automatically set to\n      ``True`` if ``date_format`` or ``date_parser`` arguments have been passed.\n    * ``list`` of ``int`` or names. e.g. If ``[1, 2, 3]`` -> try parsing columns 1, 2, 3\n      each as a separate date column.\n    * ``list`` of ``list``. e.g.  If ``[[1, 3]]`` -> combine columns 1 and 3 and parse\n      as a single date column. Values are joined with a space before parsing.\n    * ``dict``, e.g. ``{'foo' : [1, 3]}`` -> parse columns 1, 3 as date and call\n      result 'foo'. Values are joined with a space before parsing.\n\n    If a column or index cannot be represented as an array of ``datetime``,\n    say because of an unparsable value or a mixture of timezones, the column\n    or index will be returned unaltered as an ``object`` data type. For\n    non-standard ``datetime`` parsing, use :func:`~pandas.to_datetime` after\n    :func:`~pandas.read_csv`.\n\n    Note: A fast-path exists for iso8601-formatted dates.\ninfer_datetime_format : bool, default False\n    If ``True`` and ``parse_dates`` is enabled, pandas will attempt to infer the\n    format of the ``datetime`` strings in the columns, and if it can be inferred,\n    switch to a faster method of parsing them. In some cases this can increase\n    the parsing speed by 5-10x.\n\n    .. deprecated:: 2.0.0\n        A strict version of this argument is now the default, passing it has no effect.\n\nkeep_date_col : bool, default False\n    If ``True`` and ``parse_dates`` specifies combining multiple columns then\n    keep the original columns.\ndate_parser : Callable, optional\n    Function to use for converting a sequence of string columns to an array of\n    ``datetime`` instances. The default uses ``dateutil.parser.parser`` to do the\n    conversion. pandas will try to call ``date_parser`` in three different ways,\n    advancing to the next if an exception occurs: 1) Pass one or more arrays\n    (as defined by ``parse_dates``) as arguments; 2) concatenate (row-wise) the\n    string values from the columns defined by ``parse_dates`` into a single array\n    and pass that; and 3) call ``date_parser`` once for each row using one or\n    more strings (corresponding to the columns defined by ``parse_dates``) as\n    arguments.\n\n    .. deprecated:: 2.0.0\n       Use ``date_format`` instead, or read in as ``object`` and then apply\n       :func:`~pandas.to_datetime` as-needed.\ndate_format : str or dict of column -> format, optional\n    Format to use for parsing dates when used in conjunction with ``parse_dates``.\n    The strftime to parse time, e.g. :const:`\"%d/%m/%Y\"`. See\n    `strftime documentation\n    <https://docs.python.org/3/library/datetime.html\n    #strftime-and-strptime-behavior>`_ for more information on choices, though\n    note that :const:`\"%f\"` will parse all the way up to nanoseconds.\n    You can also pass:\n\n    - \"ISO8601\", to parse any `ISO8601 <https://en.wikipedia.org/wiki/ISO_8601>`_\n        time string (not necessarily in exactly the same format);\n    - \"mixed\", to infer the format for each element individually. This is risky,\n        and you should probably use it along with `dayfirst`.\n\n    .. versionadded:: 2.0.0\ndayfirst : bool, default False\n    DD/MM format dates, international and European format.\ncache_dates : bool, default True\n    If ``True``, use a cache of unique, converted dates to apply the ``datetime``\n    conversion. May produce significant speed-up when parsing duplicate\n    date strings, especially ones with timezone offsets.\n\niterator : bool, default False\n    Return ``TextFileReader`` object for iteration or getting chunks with\n    ``get_chunk()``.\nchunksize : int, optional\n    Number of lines to read from the file per chunk. Passing a value will cause the\n    function to return a ``TextFileReader`` object for iteration.\n    See the `IO Tools docs\n    <https://pandas.pydata.org/pandas-docs/stable/io.html#io-chunking>`_\n    for more information on ``iterator`` and ``chunksize``.\n\ncompression : str or dict, default 'infer'\n    For on-the-fly decompression of on-disk data. If 'infer' and 'filepath_or_buffer' is\n    path-like, then detect compression from the following extensions: '.gz',\n    '.bz2', '.zip', '.xz', '.zst', '.tar', '.tar.gz', '.tar.xz' or '.tar.bz2'\n    (otherwise no compression).\n    If using 'zip' or 'tar', the ZIP file must contain only one data file to be read in.\n    Set to ``None`` for no decompression.\n    Can also be a dict with key ``'method'`` set\n    to one of {``'zip'``, ``'gzip'``, ``'bz2'``, ``'zstd'``, ``'xz'``, ``'tar'``} and\n    other key-value pairs are forwarded to\n    ``zipfile.ZipFile``, ``gzip.GzipFile``,\n    ``bz2.BZ2File``, ``zstandard.ZstdDecompressor``, ``lzma.LZMAFile`` or\n    ``tarfile.TarFile``, respectively.\n    As an example, the following could be passed for Zstandard decompression using a\n    custom compression dictionary:\n    ``compression={'method': 'zstd', 'dict_data': my_compression_dict}``.\n\n    .. versionadded:: 1.5.0\n        Added support for `.tar` files.\n\n    .. versionchanged:: 1.4.0 Zstandard support.\n\nthousands : str (length 1), optional\n    Character acting as the thousands separator in numerical values.\ndecimal : str (length 1), default '.'\n    Character to recognize as decimal point (e.g., use ',' for European data).\nlineterminator : str (length 1), optional\n    Character used to denote a line break. Only valid with C parser.\nquotechar : str (length 1), optional\n    Character used to denote the start and end of a quoted item. Quoted\n    items can include the ``delimiter`` and it will be ignored.\nquoting : {0 or csv.QUOTE_MINIMAL, 1 or csv.QUOTE_ALL, 2 or csv.QUOTE_NONNUMERIC, 3 or csv.QUOTE_NONE}, default csv.QUOTE_MINIMAL\n    Control field quoting behavior per ``csv.QUOTE_*`` constants. Default is\n    ``csv.QUOTE_MINIMAL`` (i.e., 0) which implies that only fields containing special\n    characters are quoted (e.g., characters defined in ``quotechar``, ``delimiter``,\n    or ``lineterminator``.\ndoublequote : bool, default True\n   When ``quotechar`` is specified and ``quoting`` is not ``QUOTE_NONE``, indicate\n   whether or not to interpret two consecutive ``quotechar`` elements INSIDE a\n   field as a single ``quotechar`` element.\nescapechar : str (length 1), optional\n    Character used to escape other characters.\ncomment : str (length 1), optional\n    Character indicating that the remainder of line should not be parsed.\n    If found at the beginning\n    of a line, the line will be ignored altogether. This parameter must be a\n    single character. Like empty lines (as long as ``skip_blank_lines=True``),\n    fully commented lines are ignored by the parameter ``header`` but not by\n    ``skiprows``. For example, if ``comment='#'``, parsing\n    ``#empty\\na,b,c\\n1,2,3`` with ``header=0`` will result in ``'a,b,c'`` being\n    treated as the header.\nencoding : str, optional, default 'utf-8'\n    Encoding to use for UTF when reading/writing (ex. ``'utf-8'``). `List of Python\n    standard encodings\n    <https://docs.python.org/3/library/codecs.html#standard-encodings>`_ .\n\nencoding_errors : str, optional, default 'strict'\n    How encoding errors are treated. `List of possible values\n    <https://docs.python.org/3/library/codecs.html#error-handlers>`_ .\n\n    .. versionadded:: 1.3.0\n\ndialect : str or csv.Dialect, optional\n    If provided, this parameter will override values (default or not) for the\n    following parameters: ``delimiter``, ``doublequote``, ``escapechar``,\n    ``skipinitialspace``, ``quotechar``, and ``quoting``. If it is necessary to\n    override values, a ``ParserWarning`` will be issued. See ``csv.Dialect``\n    documentation for more details.\non_bad_lines : {'error', 'warn', 'skip'} or Callable, default 'error'\n    Specifies what to do upon encountering a bad line (a line with too many fields).\n    Allowed values are :\n\n    - ``'error'``, raise an Exception when a bad line is encountered.\n    - ``'warn'``, raise a warning when a bad line is encountered and skip that line.\n    - ``'skip'``, skip bad lines without raising or warning when they are encountered.\n\n    .. versionadded:: 1.3.0\n\n    .. versionadded:: 1.4.0\n\n        - Callable, function with signature\n          ``(bad_line: list[str]) -> list[str] | None`` that will process a single\n          bad line. ``bad_line`` is a list of strings split by the ``sep``.\n          If the function returns ``None``, the bad line will be ignored.\n          If the function returns a new ``list`` of strings with more elements than\n          expected, a ``ParserWarning`` will be emitted while dropping extra elements.\n          Only supported when ``engine='python'``\n\n    .. versionchanged:: 2.2.0\n\n        - Callable, function with signature\n          as described in `pyarrow documentation\n          <https://arrow.apache.org/docs/python/generated/pyarrow.csv.ParseOptions.html\n          #pyarrow.csv.ParseOptions.invalid_row_handler>`_ when ``engine='pyarrow'``\n\ndelim_whitespace : bool, default False\n    Specifies whether or not whitespace (e.g. ``' '`` or ``'\\t'``) will be\n    used as the ``sep`` delimiter. Equivalent to setting ``sep='\\s+'``. If this option\n    is set to ``True``, nothing should be passed in for the ``delimiter``\n    parameter.\n\n    .. deprecated:: 2.2.0\n        Use ``sep=\"\\s+\"`` instead.\nlow_memory : bool, default True\n    Internally process the file in chunks, resulting in lower memory use\n    while parsing, but possibly mixed type inference.  To ensure no mixed\n    types either set ``False``, or specify the type with the ``dtype`` parameter.\n    Note that the entire file is read into a single :class:`~pandas.DataFrame`\n    regardless, use the ``chunksize`` or ``iterator`` parameter to return the data in\n    chunks. (Only valid with C parser).\nmemory_map : bool, default False\n    If a filepath is provided for ``filepath_or_buffer``, map the file object\n    directly onto memory and access the data directly from there. Using this\n    option can improve performance because there is no longer any I/O overhead.\nfloat_precision : {'high', 'legacy', 'round_trip'}, optional\n    Specifies which converter the C engine should use for floating-point\n    values. The options are ``None`` or ``'high'`` for the ordinary converter,\n    ``'legacy'`` for the original lower precision pandas converter, and\n    ``'round_trip'`` for the round-trip converter.\n\nstorage_options : dict, optional\n    Extra options that make sense for a particular storage connection, e.g.\n    host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n    are forwarded to ``urllib.request.Request`` as header options. For other\n    URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n    forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n    details, and for more examples on storage options refer `here\n    <https://pandas.pydata.org/docs/user_guide/io.html?\n    highlight=storage_options#reading-writing-remote-files>`_.\n\ndtype_backend : {'numpy_nullable', 'pyarrow'}, default 'numpy_nullable'\n    Back-end data type applied to the resultant :class:`DataFrame`\n    (still experimental). Behaviour is as follows:\n\n    * ``\"numpy_nullable\"``: returns nullable-dtype-backed :class:`DataFrame`\n      (default).\n    * ``\"pyarrow\"``: returns pyarrow-backed nullable :class:`ArrowDtype`\n      DataFrame.\n\n    .. versionadded:: 2.0\n\nReturns\n-------\nDataFrame or TextFileReader\n    A comma-separated values (csv) file is returned as two-dimensional\n    data structure with labeled axes.\n\nSee Also\n--------\nDataFrame.to_csv : Write DataFrame to a comma-separated values (csv) file.\nread_table : Read general delimited file into DataFrame.\nread_fwf : Read a table of fixed-width formatted lines into DataFrame.\n\nExamples\n--------\n>>> pd.read_csv('data.csv')  # doctest: +SKIP", "parameters": {"type": "object", "properties": {"filepath_or_buffer": {"type": ["readcsvbuffer[bytes]", "readcsvbuffer[str]", "filepath"]}, "header": {"type": ["\"int", "literal['infer']\"", "sequence[int]", "null"], "default": "infer"}, "names": {"type": ["lib.nodefault", "sequence[hashable]", "null"], "default": "<no_default>"}, "usecols": {"type": "usecolsargtype", "default": null}}}}}
{"task_id": "BigCodeBench/1053", "data": {"name": "sklearn.feature_extraction.text.CountVectorizer", "type": "class", "signature": "(stop_words=None)", "description": "Convert a collection of text documents to a matrix of token counts.\n\nThis implementation produces a sparse representation of the counts using\nscipy.sparse.csr_matrix.\n\nIf you do not provide an a-priori dictionary and you do not use an analyzer\nthat does some kind of feature selection then the number of features will\nbe equal to the vocabulary size found by analyzing the data.\n\nFor an efficiency comparison of the different feature extractors, see\n:ref:`sphx_glr_auto_examples_text_plot_hashing_vs_dict_vectorizer.py`.\n\nRead more in the :ref:`User Guide <text_feature_extraction>`.\n\nParameters\n----------\ninput : {'filename', 'file', 'content'}, default='content'\n    - If `'filename'`, the sequence passed as an argument to fit is\n      expected to be a list of filenames that need reading to fetch\n      the raw content to analyze.\n\n    - If `'file'`, the sequence items must have a 'read' method (file-like\n      object) that is called to fetch the bytes in memory.\n\n    - If `'content'`, the input is expected to be a sequence of items that\n      can be of type string or byte.\n\nencoding : str, default='utf-8'\n    If bytes or files are given to analyze, this encoding is used to\n    decode.\n\ndecode_error : {'strict', 'ignore', 'replace'}, default='strict'\n    Instruction on what to do if a byte sequence is given to analyze that\n    contains characters not of the given `encoding`. By default, it is\n    'strict', meaning that a UnicodeDecodeError will be raised. Other\n    values are 'ignore' and 'replace'.\n\nstrip_accents : {'ascii', 'unicode'} or callable, default=None\n    Remove accents and perform other character normalization\n    during the preprocessing step.\n    'ascii' is a fast method that only works on characters that have\n    a direct ASCII mapping.\n    'unicode' is a slightly slower method that works on any characters.\n    None (default) means no character normalization is performed.\n\n    Both 'ascii' and 'unicode' use NFKD normalization from\n    :func:`unicodedata.normalize`.\n\nlowercase : bool, default=True\n    Convert all characters to lowercase before tokenizing.\n\npreprocessor : callable, default=None\n    Override the preprocessing (strip_accents and lowercase) stage while\n    preserving the tokenizing and n-grams generation steps.\n    Only applies if ``analyzer`` is not callable.\n\ntokenizer : callable, default=None\n    Override the string tokenization step while preserving the\n    preprocessing and n-grams generation steps.\n    Only applies if ``analyzer == 'word'``.\n\nstop_words : {'english'}, list, default=None\n    If 'english', a built-in stop word list for English is used.\n    There are several known issues with 'english' and you should\n    consider an alternative (see :ref:`stop_words`).\n\n    If a list, that list is assumed to contain stop words, all of which\n    will be removed from the resulting tokens.\n    Only applies if ``analyzer == 'word'``.\n\n    If None, no stop words will be used. In this case, setting `max_df`\n    to a higher value, such as in the range (0.7, 1.0), can automatically detect\n    and filter stop words based on intra corpus document frequency of terms.\n\ntoken_pattern : str or None, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n    Regular expression denoting what constitutes a \"token\", only used\n    if ``analyzer == 'word'``. The default regexp select tokens of 2\n    or more alphanumeric characters (punctuation is completely ignored\n    and always treated as a token separator).\n\n    If there is a capturing group in token_pattern then the\n    captured group content, not the entire match, becomes the token.\n    At most one capturing group is permitted.\n\nngram_range : tuple (min_n, max_n), default=(1, 1)\n    The lower and upper boundary of the range of n-values for different\n    word n-grams or char n-grams to be extracted. All values of n such\n    such that min_n <= n <= max_n will be used. For example an\n    ``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means\n    unigrams and bigrams, and ``(2, 2)`` means only bigrams.\n    Only applies if ``analyzer`` is not callable.\n\nanalyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n    Whether the feature should be made of word n-gram or character\n    n-grams.\n    Option 'char_wb' creates character n-grams only from text inside\n    word boundaries; n-grams at the edges of words are padded with space.\n\n    If a callable is passed it is used to extract the sequence of features\n    out of the raw, unprocessed input.\n\n    .. versionchanged:: 0.21\n\n    Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n    first read from the file and then passed to the given callable\n    analyzer.\n\nmax_df : float in range [0.0, 1.0] or int, default=1.0\n    When building the vocabulary ignore terms that have a document\n    frequency strictly higher than the given threshold (corpus-specific\n    stop words).\n    If float, the parameter represents a proportion of documents, integer\n    absolute counts.\n    This parameter is ignored if vocabulary is not None.\n\nmin_df : float in range [0.0, 1.0] or int, default=1\n    When building the vocabulary ignore terms that have a document\n    frequency strictly lower than the given threshold. This value is also\n    called cut-off in the literature.\n    If float, the parameter represents a proportion of documents, integer\n    absolute counts.\n    This parameter is ignored if vocabulary is not None.\n\nmax_features : int, default=None\n    If not None, build a vocabulary that only consider the top\n    `max_features` ordered by term frequency across the corpus.\n    Otherwise, all features are used.\n\n    This parameter is ignored if vocabulary is not None.\n\nvocabulary : Mapping or iterable, default=None\n    Either a Mapping (e.g., a dict) where keys are terms and values are\n    indices in the feature matrix, or an iterable over terms. If not\n    given, a vocabulary is determined from the input documents. Indices\n    in the mapping should not be repeated and should not have any gap\n    between 0 and the largest index.\n\nbinary : bool, default=False\n    If True, all non zero counts are set to 1. This is useful for discrete\n    probabilistic models that model binary events rather than integer\n    counts.\n\ndtype : dtype, default=np.int64\n    Type of the matrix returned by fit_transform() or transform().\n\nAttributes\n----------\nvocabulary_ : dict\n    A mapping of terms to feature indices.\n\nfixed_vocabulary_ : bool\n    True if a fixed vocabulary of term to indices mapping\n    is provided by the user.\n\nstop_words_ : set\n    Terms that were ignored because they either:\n\n      - occurred in too many documents (`max_df`)\n      - occurred in too few documents (`min_df`)\n      - were cut off by feature selection (`max_features`).\n\n    This is only available if no vocabulary was given.\n\nSee Also\n--------\nHashingVectorizer : Convert a collection of text documents to a\n    matrix of token counts.\n\nTfidfVectorizer : Convert a collection of raw documents to a matrix\n    of TF-IDF features.\n\nNotes\n-----\nThe ``stop_words_`` attribute can get large and increase the model size\nwhen pickling. This attribute is provided only for introspection and can\nbe safely removed using delattr or set to None before pickling.\n\nExamples\n--------\n>>> from sklearn.feature_extraction.text import CountVectorizer\n>>> corpus = [\n...     'This is the first document.',\n...     'This document is the second document.',\n...     'And this is the third one.',\n...     'Is this the first document?',\n... ]\n>>> vectorizer = CountVectorizer()\n>>> X = vectorizer.fit_transform(corpus)\n>>> vectorizer.get_feature_names_out()\narray(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n       'this'], ...)\n>>> print(X.toarray())\n[[0 1 1 1 0 0 1 0 1]\n [0 2 0 1 0 1 1 0 1]\n [1 0 0 1 1 0 1 1 1]\n [0 1 1 1 0 0 1 0 1]]\n>>> vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n>>> X2 = vectorizer2.fit_transform(corpus)\n>>> vectorizer2.get_feature_names_out()\narray(['and this', 'document is', 'first document', 'is the', 'is this',\n       'second document', 'the first', 'the second', 'the third', 'third one',\n       'this document', 'this is', 'this the'], ...)\n >>> print(X2.toarray())\n [[0 0 1 1 0 0 1 0 0 0 0 1 0]\n [0 1 0 1 0 1 0 1 0 0 1 0 0]\n [1 0 0 1 0 0 0 0 1 1 0 1 0]\n [0 0 1 0 1 0 1 0 0 0 0 0 1]]", "parameters": {"type": "object", "properties": {"stop_words": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/1053", "data": {"name": "sklearn.feature_extraction.text.CountVectorizer.vocabulary_", "type": "class", "signature": "(*, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.int64'>)", "description": "Convert a collection of text documents to a matrix of token counts.\n\nThis implementation produces a sparse representation of the counts using\nscipy.sparse.csr_matrix.\n\nIf you do not provide an a-priori dictionary and you do not use an analyzer\nthat does some kind of feature selection then the number of features will\nbe equal to the vocabulary size found by analyzing the data.\n\nFor an efficiency comparison of the different feature extractors, see\n:ref:`sphx_glr_auto_examples_text_plot_hashing_vs_dict_vectorizer.py`.\n\nRead more in the :ref:`User Guide <text_feature_extraction>`.\n\nParameters\n----------\ninput : {'filename', 'file', 'content'}, default='content'\n    - If `'filename'`, the sequence passed as an argument to fit is\n      expected to be a list of filenames that need reading to fetch\n      the raw content to analyze.\n\n    - If `'file'`, the sequence items must have a 'read' method (file-like\n      object) that is called to fetch the bytes in memory.\n\n    - If `'content'`, the input is expected to be a sequence of items that\n      can be of type string or byte.\n\nencoding : str, default='utf-8'\n    If bytes or files are given to analyze, this encoding is used to\n    decode.\n\ndecode_error : {'strict', 'ignore', 'replace'}, default='strict'\n    Instruction on what to do if a byte sequence is given to analyze that\n    contains characters not of the given `encoding`. By default, it is\n    'strict', meaning that a UnicodeDecodeError will be raised. Other\n    values are 'ignore' and 'replace'.\n\nstrip_accents : {'ascii', 'unicode'} or callable, default=None\n    Remove accents and perform other character normalization\n    during the preprocessing step.\n    'ascii' is a fast method that only works on characters that have\n    a direct ASCII mapping.\n    'unicode' is a slightly slower method that works on any characters.\n    None (default) means no character normalization is performed.\n\n    Both 'ascii' and 'unicode' use NFKD normalization from\n    :func:`unicodedata.normalize`.\n\nlowercase : bool, default=True\n    Convert all characters to lowercase before tokenizing.\n\npreprocessor : callable, default=None\n    Override the preprocessing (strip_accents and lowercase) stage while\n    preserving the tokenizing and n-grams generation steps.\n    Only applies if ``analyzer`` is not callable.\n\ntokenizer : callable, default=None\n    Override the string tokenization step while preserving the\n    preprocessing and n-grams generation steps.\n    Only applies if ``analyzer == 'word'``.\n\nstop_words : {'english'}, list, default=None\n    If 'english', a built-in stop word list for English is used.\n    There are several known issues with 'english' and you should\n    consider an alternative (see :ref:`stop_words`).\n\n    If a list, that list is assumed to contain stop words, all of which\n    will be removed from the resulting tokens.\n    Only applies if ``analyzer == 'word'``.\n\n    If None, no stop words will be used. In this case, setting `max_df`\n    to a higher value, such as in the range (0.7, 1.0), can automatically detect\n    and filter stop words based on intra corpus document frequency of terms.\n\ntoken_pattern : str or None, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n    Regular expression denoting what constitutes a \"token\", only used\n    if ``analyzer == 'word'``. The default regexp select tokens of 2\n    or more alphanumeric characters (punctuation is completely ignored\n    and always treated as a token separator).\n\n    If there is a capturing group in token_pattern then the\n    captured group content, not the entire match, becomes the token.\n    At most one capturing group is permitted.\n\nngram_range : tuple (min_n, max_n), default=(1, 1)\n    The lower and upper boundary of the range of n-values for different\n    word n-grams or char n-grams to be extracted. All values of n such\n    such that min_n <= n <= max_n will be used. For example an\n    ``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means\n    unigrams and bigrams, and ``(2, 2)`` means only bigrams.\n    Only applies if ``analyzer`` is not callable.\n\nanalyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n    Whether the feature should be made of word n-gram or character\n    n-grams.\n    Option 'char_wb' creates character n-grams only from text inside\n    word boundaries; n-grams at the edges of words are padded with space.\n\n    If a callable is passed it is used to extract the sequence of features\n    out of the raw, unprocessed input.\n\n    .. versionchanged:: 0.21\n\n    Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n    first read from the file and then passed to the given callable\n    analyzer.\n\nmax_df : float in range [0.0, 1.0] or int, default=1.0\n    When building the vocabulary ignore terms that have a document\n    frequency strictly higher than the given threshold (corpus-specific\n    stop words).\n    If float, the parameter represents a proportion of documents, integer\n    absolute counts.\n    This parameter is ignored if vocabulary is not None.\n\nmin_df : float in range [0.0, 1.0] or int, default=1\n    When building the vocabulary ignore terms that have a document\n    frequency strictly lower than the given threshold. This value is also\n    called cut-off in the literature.\n    If float, the parameter represents a proportion of documents, integer\n    absolute counts.\n    This parameter is ignored if vocabulary is not None.\n\nmax_features : int, default=None\n    If not None, build a vocabulary that only consider the top\n    `max_features` ordered by term frequency across the corpus.\n    Otherwise, all features are used.\n\n    This parameter is ignored if vocabulary is not None.\n\nvocabulary : Mapping or iterable, default=None\n    Either a Mapping (e.g., a dict) where keys are terms and values are\n    indices in the feature matrix, or an iterable over terms. If not\n    given, a vocabulary is determined from the input documents. Indices\n    in the mapping should not be repeated and should not have any gap\n    between 0 and the largest index.\n\nbinary : bool, default=False\n    If True, all non zero counts are set to 1. This is useful for discrete\n    probabilistic models that model binary events rather than integer\n    counts.\n\ndtype : dtype, default=np.int64\n    Type of the matrix returned by fit_transform() or transform().\n\nAttributes\n----------\nvocabulary_ : dict\n    A mapping of terms to feature indices.\n\nfixed_vocabulary_ : bool\n    True if a fixed vocabulary of term to indices mapping\n    is provided by the user.\n\nstop_words_ : set\n    Terms that were ignored because they either:\n\n      - occurred in too many documents (`max_df`)\n      - occurred in too few documents (`min_df`)\n      - were cut off by feature selection (`max_features`).\n\n    This is only available if no vocabulary was given.\n\nSee Also\n--------\nHashingVectorizer : Convert a collection of text documents to a\n    matrix of token counts.\n\nTfidfVectorizer : Convert a collection of raw documents to a matrix\n    of TF-IDF features.\n\nNotes\n-----\nThe ``stop_words_`` attribute can get large and increase the model size\nwhen pickling. This attribute is provided only for introspection and can\nbe safely removed using delattr or set to None before pickling.\n\nExamples\n--------\n>>> from sklearn.feature_extraction.text import CountVectorizer\n>>> corpus = [\n...     'This is the first document.',\n...     'This document is the second document.',\n...     'And this is the third one.',\n...     'Is this the first document?',\n... ]\n>>> vectorizer = CountVectorizer()\n>>> X = vectorizer.fit_transform(corpus)\n>>> vectorizer.get_feature_names_out()\narray(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n       'this'], ...)\n>>> print(X.toarray())\n[[0 1 1 1 0 0 1 0 1]\n [0 2 0 1 0 1 1 0 1]\n [1 0 0 1 1 0 1 1 1]\n [0 1 1 1 0 0 1 0 1]]\n>>> vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n>>> X2 = vectorizer2.fit_transform(corpus)\n>>> vectorizer2.get_feature_names_out()\narray(['and this', 'document is', 'first document', 'is the', 'is this',\n       'second document', 'the first', 'the second', 'the third', 'third one',\n       'this document', 'this is', 'this the'], ...)\n >>> print(X2.toarray())\n [[0 0 1 1 0 0 1 0 0 0 0 1 0]\n [0 1 0 1 0 1 0 1 0 0 1 0 0]\n [1 0 0 1 0 0 0 0 1 1 0 1 0]\n [0 0 1 0 1 0 1 0 0 0 0 0 1]]"}}
{"task_id": "BigCodeBench/1053", "data": {"name": "sklearn.feature_extraction.text.CountVectorizer.vocabulary_.items", "type": "class", "signature": "()", "description": "Convert a collection of text documents to a matrix of token counts.\n\nThis implementation produces a sparse representation of the counts using\nscipy.sparse.csr_matrix.\n\nIf you do not provide an a-priori dictionary and you do not use an analyzer\nthat does some kind of feature selection then the number of features will\nbe equal to the vocabulary size found by analyzing the data.\n\nFor an efficiency comparison of the different feature extractors, see\n:ref:`sphx_glr_auto_examples_text_plot_hashing_vs_dict_vectorizer.py`.\n\nRead more in the :ref:`User Guide <text_feature_extraction>`.\n\nParameters\n----------\ninput : {'filename', 'file', 'content'}, default='content'\n    - If `'filename'`, the sequence passed as an argument to fit is\n      expected to be a list of filenames that need reading to fetch\n      the raw content to analyze.\n\n    - If `'file'`, the sequence items must have a 'read' method (file-like\n      object) that is called to fetch the bytes in memory.\n\n    - If `'content'`, the input is expected to be a sequence of items that\n      can be of type string or byte.\n\nencoding : str, default='utf-8'\n    If bytes or files are given to analyze, this encoding is used to\n    decode.\n\ndecode_error : {'strict', 'ignore', 'replace'}, default='strict'\n    Instruction on what to do if a byte sequence is given to analyze that\n    contains characters not of the given `encoding`. By default, it is\n    'strict', meaning that a UnicodeDecodeError will be raised. Other\n    values are 'ignore' and 'replace'.\n\nstrip_accents : {'ascii', 'unicode'} or callable, default=None\n    Remove accents and perform other character normalization\n    during the preprocessing step.\n    'ascii' is a fast method that only works on characters that have\n    a direct ASCII mapping.\n    'unicode' is a slightly slower method that works on any characters.\n    None (default) means no character normalization is performed.\n\n    Both 'ascii' and 'unicode' use NFKD normalization from\n    :func:`unicodedata.normalize`.\n\nlowercase : bool, default=True\n    Convert all characters to lowercase before tokenizing.\n\npreprocessor : callable, default=None\n    Override the preprocessing (strip_accents and lowercase) stage while\n    preserving the tokenizing and n-grams generation steps.\n    Only applies if ``analyzer`` is not callable.\n\ntokenizer : callable, default=None\n    Override the string tokenization step while preserving the\n    preprocessing and n-grams generation steps.\n    Only applies if ``analyzer == 'word'``.\n\nstop_words : {'english'}, list, default=None\n    If 'english', a built-in stop word list for English is used.\n    There are several known issues with 'english' and you should\n    consider an alternative (see :ref:`stop_words`).\n\n    If a list, that list is assumed to contain stop words, all of which\n    will be removed from the resulting tokens.\n    Only applies if ``analyzer == 'word'``.\n\n    If None, no stop words will be used. In this case, setting `max_df`\n    to a higher value, such as in the range (0.7, 1.0), can automatically detect\n    and filter stop words based on intra corpus document frequency of terms.\n\ntoken_pattern : str or None, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n    Regular expression denoting what constitutes a \"token\", only used\n    if ``analyzer == 'word'``. The default regexp select tokens of 2\n    or more alphanumeric characters (punctuation is completely ignored\n    and always treated as a token separator).\n\n    If there is a capturing group in token_pattern then the\n    captured group content, not the entire match, becomes the token.\n    At most one capturing group is permitted.\n\nngram_range : tuple (min_n, max_n), default=(1, 1)\n    The lower and upper boundary of the range of n-values for different\n    word n-grams or char n-grams to be extracted. All values of n such\n    such that min_n <= n <= max_n will be used. For example an\n    ``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means\n    unigrams and bigrams, and ``(2, 2)`` means only bigrams.\n    Only applies if ``analyzer`` is not callable.\n\nanalyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n    Whether the feature should be made of word n-gram or character\n    n-grams.\n    Option 'char_wb' creates character n-grams only from text inside\n    word boundaries; n-grams at the edges of words are padded with space.\n\n    If a callable is passed it is used to extract the sequence of features\n    out of the raw, unprocessed input.\n\n    .. versionchanged:: 0.21\n\n    Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n    first read from the file and then passed to the given callable\n    analyzer.\n\nmax_df : float in range [0.0, 1.0] or int, default=1.0\n    When building the vocabulary ignore terms that have a document\n    frequency strictly higher than the given threshold (corpus-specific\n    stop words).\n    If float, the parameter represents a proportion of documents, integer\n    absolute counts.\n    This parameter is ignored if vocabulary is not None.\n\nmin_df : float in range [0.0, 1.0] or int, default=1\n    When building the vocabulary ignore terms that have a document\n    frequency strictly lower than the given threshold. This value is also\n    called cut-off in the literature.\n    If float, the parameter represents a proportion of documents, integer\n    absolute counts.\n    This parameter is ignored if vocabulary is not None.\n\nmax_features : int, default=None\n    If not None, build a vocabulary that only consider the top\n    `max_features` ordered by term frequency across the corpus.\n    Otherwise, all features are used.\n\n    This parameter is ignored if vocabulary is not None.\n\nvocabulary : Mapping or iterable, default=None\n    Either a Mapping (e.g., a dict) where keys are terms and values are\n    indices in the feature matrix, or an iterable over terms. If not\n    given, a vocabulary is determined from the input documents. Indices\n    in the mapping should not be repeated and should not have any gap\n    between 0 and the largest index.\n\nbinary : bool, default=False\n    If True, all non zero counts are set to 1. This is useful for discrete\n    probabilistic models that model binary events rather than integer\n    counts.\n\ndtype : dtype, default=np.int64\n    Type of the matrix returned by fit_transform() or transform().\n\nAttributes\n----------\nvocabulary_ : dict\n    A mapping of terms to feature indices.\n\nfixed_vocabulary_ : bool\n    True if a fixed vocabulary of term to indices mapping\n    is provided by the user.\n\nstop_words_ : set\n    Terms that were ignored because they either:\n\n      - occurred in too many documents (`max_df`)\n      - occurred in too few documents (`min_df`)\n      - were cut off by feature selection (`max_features`).\n\n    This is only available if no vocabulary was given.\n\nSee Also\n--------\nHashingVectorizer : Convert a collection of text documents to a\n    matrix of token counts.\n\nTfidfVectorizer : Convert a collection of raw documents to a matrix\n    of TF-IDF features.\n\nNotes\n-----\nThe ``stop_words_`` attribute can get large and increase the model size\nwhen pickling. This attribute is provided only for introspection and can\nbe safely removed using delattr or set to None before pickling.\n\nExamples\n--------\n>>> from sklearn.feature_extraction.text import CountVectorizer\n>>> corpus = [\n...     'This is the first document.',\n...     'This document is the second document.',\n...     'And this is the third one.',\n...     'Is this the first document?',\n... ]\n>>> vectorizer = CountVectorizer()\n>>> X = vectorizer.fit_transform(corpus)\n>>> vectorizer.get_feature_names_out()\narray(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n       'this'], ...)\n>>> print(X.toarray())\n[[0 1 1 1 0 0 1 0 1]\n [0 2 0 1 0 1 1 0 1]\n [1 0 0 1 1 0 1 1 1]\n [0 1 1 1 0 0 1 0 1]]\n>>> vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n>>> X2 = vectorizer2.fit_transform(corpus)\n>>> vectorizer2.get_feature_names_out()\narray(['and this', 'document is', 'first document', 'is the', 'is this',\n       'second document', 'the first', 'the second', 'the third', 'third one',\n       'this document', 'this is', 'this the'], ...)\n >>> print(X2.toarray())\n [[0 0 1 1 0 0 1 0 0 0 0 1 0]\n [0 1 0 1 0 1 0 1 0 0 1 0 0]\n [1 0 0 1 0 0 0 0 1 1 0 1 0]\n [0 0 1 0 1 0 1 0 0 0 0 0 1]]", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/1053", "data": {"name": "sklearn.feature_extraction.text.CountVectorizer.fit_transform", "type": "callable", "signature": "(self, raw_documents)", "description": "Learn the vocabulary dictionary and return document-term matrix.\n\nThis is equivalent to fit followed by transform, but more efficiently\nimplemented.\n\nParameters\n----------\nraw_documents : iterable\n    An iterable which generates either str, unicode or file objects.\n\ny : None\n    This parameter is ignored.\n\nReturns\n-------\nX : array of shape (n_samples, n_features)\n    Document-term matrix.", "parameters": {"type": "object", "properties": {"raw_documents": {}}}}}
{"task_id": "BigCodeBench/1053", "data": {"name": "vectorizer.fit_transform(df['Text'].dropna()).sum(axis=0)"}}
{"task_id": "BigCodeBench/1057", "data": {"name": "numpy.array", "type": "callable", "signature": "(*args, **kwargs)", "description": "array(object, dtype=None, *, copy=True, order='K', subok=False, ndmin=0,\n      like=None)\n\nCreate an array.\n\nParameters\n----------\nobject : array_like\n    An array, any object exposing the array interface, an object whose\n    __array__ method returns an array, or any (nested) sequence.\n    If object is a scalar, a 0-dimensional array containing object is\n    returned.\ndtype : data-type, optional\n    The desired data-type for the array.  If not given, then the type will\n    be determined as the minimum type required to hold the objects in the\n    sequence.\ncopy : bool, optional\n    If true (default), then the object is copied.  Otherwise, a copy will\n    only be made if __array__ returns a copy, if obj is a nested sequence,\n    or if a copy is needed to satisfy any of the other requirements\n    (`dtype`, `order`, etc.).\norder : {'K', 'A', 'C', 'F'}, optional\n    Specify the memory layout of the array. If object is not an array, the\n    newly created array will be in C order (row major) unless 'F' is\n    specified, in which case it will be in Fortran order (column major).\n    If object is an array the following holds.\n\n    ===== ========= ===================================================\n    order  no copy                     copy=True\n    ===== ========= ===================================================\n    'K'   unchanged F & C order preserved, otherwise most similar order\n    'A'   unchanged F order if input is F and not C, otherwise C order\n    'C'   C order   C order\n    'F'   F order   F order\n    ===== ========= ===================================================\n\n    When ``copy=False`` and a copy is made for other reasons, the result is\n    the same as if ``copy=True``, with some exceptions for 'A', see the\n    Notes section. The default order is 'K'.\nsubok : bool, optional\n    If True, then sub-classes will be passed-through, otherwise\n    the returned array will be forced to be a base-class array (default).\nndmin : int, optional\n    Specifies the minimum number of dimensions that the resulting\n    array should have.  Ones will be prepended to the shape as\n    needed to meet this requirement.\nlike : array_like, optional\n    Reference object to allow the creation of arrays which are not\n    NumPy arrays. If an array-like passed in as ``like`` supports\n    the ``__array_function__`` protocol, the result will be defined\n    by it. In this case, it ensures the creation of an array object\n    compatible with that passed in via this argument.\n\n    .. versionadded:: 1.20.0\n\nReturns\n-------\nout : ndarray\n    An array object satisfying the specified requirements.\n\nSee Also\n--------\nempty_like : Return an empty array with shape and type of input.\nones_like : Return an array of ones with shape and type of input.\nzeros_like : Return an array of zeros with shape and type of input.\nfull_like : Return a new array with shape of input filled with value.\nempty : Return a new uninitialized array.\nones : Return a new array setting values to one.\nzeros : Return a new array setting values to zero.\nfull : Return a new array of given shape filled with value.\n\n\nNotes\n-----\nWhen order is 'A' and `object` is an array in neither 'C' nor 'F' order,\nand a copy is forced by a change in dtype, then the order of the result is\nnot necessarily 'C' as expected. This is likely a bug.\n\nExamples\n--------\n>>> np.array([1, 2, 3])\narray([1, 2, 3])\n\nUpcasting:\n\n>>> np.array([1, 2, 3.0])\narray([ 1.,  2.,  3.])\n\nMore than one dimension:\n\n>>> np.array([[1, 2], [3, 4]])\narray([[1, 2],\n       [3, 4]])\n\nMinimum dimensions 2:\n\n>>> np.array([1, 2, 3], ndmin=2)\narray([[1, 2, 3]])\n\nType provided:\n\n>>> np.array([1, 2, 3], dtype=complex)\narray([ 1.+0.j,  2.+0.j,  3.+0.j])\n\nData-type consisting of more than one element:\n\n>>> x = np.array([(1,2),(3,4)],dtype=[('a','<i4'),('b','<i4')])\n>>> x['a']\narray([1, 3])\n\nCreating an array from sub-classes:\n\n>>> np.array(np.mat('1 2; 3 4'))\narray([[1, 2],\n       [3, 4]])\n\n>>> np.array(np.mat('1 2; 3 4'), subok=True)\nmatrix([[1, 2],\n        [3, 4]])", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/1077", "data": {"name": "datetime.datetime", "type": "class", "signature": "(self, /, *args, **kwargs)", "description": "datetime(year, month, day[, hour[, minute[, second[, microsecond[,tzinfo]]]]])\n\nThe year, month and day arguments are required. tzinfo may be None, or an\ninstance of a tzinfo subclass. The remaining arguments may be ints.", "parameters": {"type": "object", "properties": {"/": {}}}}}
{"task_id": "BigCodeBench/1077", "data": {"name": "datetime.datetime.strptime", "type": "callable", "signature": "(*args, **kwargs)", "description": "string, format -> new datetime parsed from a string (like time.strptime()).", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/1077", "data": {"name": "numpy.mean", "type": "callable", "signature": "(a)", "description": "Compute the arithmetic mean along the specified axis.\n\nReturns the average of the array elements.  The average is taken over\nthe flattened array by default, otherwise over the specified axis.\n`float64` intermediate and return values are used for integer inputs.\n\nParameters\n----------\na : array_like\n    Array containing numbers whose mean is desired. If `a` is not an\n    array, a conversion is attempted.\naxis : None or int or tuple of ints, optional\n    Axis or axes along which the means are computed. The default is to\n    compute the mean of the flattened array.\n\n    .. versionadded:: 1.7.0\n\n    If this is a tuple of ints, a mean is performed over multiple axes,\n    instead of a single axis or all the axes as before.\ndtype : data-type, optional\n    Type to use in computing the mean.  For integer inputs, the default\n    is `float64`; for floating point inputs, it is the same as the\n    input dtype.\nout : ndarray, optional\n    Alternate output array in which to place the result.  The default\n    is ``None``; if provided, it must have the same shape as the\n    expected output, but the type will be cast if necessary.\n    See :ref:`ufuncs-output-type` for more details.\n\nkeepdims : bool, optional\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\n    If the default value is passed, then `keepdims` will not be\n    passed through to the `mean` method of sub-classes of\n    `ndarray`, however any non-default value will be.  If the\n    sub-class' method does not implement `keepdims` any\n    exceptions will be raised.\n\nwhere : array_like of bool, optional\n    Elements to include in the mean. See `~numpy.ufunc.reduce` for details.\n\n    .. versionadded:: 1.20.0\n\nReturns\n-------\nm : ndarray, see dtype parameter above\n    If `out=None`, returns a new array containing the mean values,\n    otherwise a reference to the output array is returned.\n\nSee Also\n--------\naverage : Weighted average\nstd, var, nanmean, nanstd, nanvar\n\nNotes\n-----\nThe arithmetic mean is the sum of the elements along the axis divided\nby the number of elements.\n\nNote that for floating-point input, the mean is computed using the\nsame precision the input has.  Depending on the input data, this can\ncause the results to be inaccurate, especially for `float32` (see\nexample below).  Specifying a higher-precision accumulator using the\n`dtype` keyword can alleviate this issue.\n\nBy default, `float16` results are computed using `float32` intermediates\nfor extra precision.\n\nExamples\n--------\n>>> a = np.array([[1, 2], [3, 4]])\n>>> np.mean(a)\n2.5\n>>> np.mean(a, axis=0)\narray([2., 3.])\n>>> np.mean(a, axis=1)\narray([1.5, 3.5])\n\nIn single precision, `mean` can be inaccurate:\n\n>>> a = np.zeros((2, 512*512), dtype=np.float32)\n>>> a[0, :] = 1.0\n>>> a[1, :] = 0.1\n>>> np.mean(a)\n0.54999924\n\nComputing the mean in float64 is more accurate:\n\n>>> np.mean(a, dtype=np.float64)\n0.55000000074505806 # may vary\n\nSpecifying a where argument:\n\n>>> a = np.array([[5, 9, 13], [14, 10, 12], [11, 15, 19]])\n>>> np.mean(a)\n12.0\n>>> np.mean(a, where=[[True], [False], [False]])\n9.0", "parameters": {"type": "object", "properties": {"a": {}}}}}
{"task_id": "BigCodeBench/1077", "data": {"name": "pytz.UTC", "type": "constant", "signature": null, "description": "UTC\n\nOptimized UTC implementation. It unpickles using the single module global\ninstance defined beneath this class declaration.", "value": "UTC", "parameters": {"type": "constant"}}}
{"task_id": "BigCodeBench/1077", "data": {"name": "pytz.timezone", "type": "callable", "signature": "(zone)", "description": "Return a datetime.tzinfo implementation for the given timezone\n\n>>> from datetime import datetime, timedelta\n>>> utc = timezone('UTC')\n>>> eastern = timezone('US/Eastern')\n>>> eastern.zone\n'US/Eastern'\n>>> timezone(unicode('US/Eastern')) is eastern\nTrue\n>>> utc_dt = datetime(2002, 10, 27, 6, 0, 0, tzinfo=utc)\n>>> loc_dt = utc_dt.astimezone(eastern)\n>>> fmt = '%Y-%m-%d %H:%M:%S %Z (%z)'\n>>> loc_dt.strftime(fmt)\n'2002-10-27 01:00:00 EST (-0500)'\n>>> (loc_dt - timedelta(minutes=10)).strftime(fmt)\n'2002-10-27 00:50:00 EST (-0500)'\n>>> eastern.normalize(loc_dt - timedelta(minutes=10)).strftime(fmt)\n'2002-10-27 01:50:00 EDT (-0400)'\n>>> (loc_dt + timedelta(minutes=10)).strftime(fmt)\n'2002-10-27 01:10:00 EST (-0500)'\n\nRaises UnknownTimeZoneError if passed an unknown zone.\n\n>>> try:\n...     timezone('Asia/Shangri-La')\n... except UnknownTimeZoneError:\n...     print('Unknown')\nUnknown\n\n>>> try:\n...     timezone(unicode('\\N{TRADE MARK SIGN}'))\n... except UnknownTimeZoneError:\n...     print('Unknown')\nUnknown", "parameters": {"type": "object", "properties": {"zone": {}}}}}
{"task_id": "BigCodeBench/1085", "data": {"name": "collections.Counter", "type": "class", "signature": "(iterable=None, /, **kwds)", "description": "Dict subclass for counting hashable items.  Sometimes called a bag\nor multiset.  Elements are stored as dictionary keys and their counts\nare stored as dictionary values.\n\n>>> c = Counter('abcdeabcdabcaba')  # count elements from a string\n\n>>> c.most_common(3)                # three most common elements\n[('a', 5), ('b', 4), ('c', 3)]\n>>> sorted(c)                       # list all unique elements\n['a', 'b', 'c', 'd', 'e']\n>>> ''.join(sorted(c.elements()))   # list elements with repetitions\n'aaaaabbbbcccdde'\n>>> sum(c.values())                 # total of all counts\n15\n\n>>> c['a']                          # count of letter 'a'\n5\n>>> for elem in 'shazam':           # update counts from an iterable\n...     c[elem] += 1                # by adding 1 to each element's count\n>>> c['a']                          # now there are seven 'a'\n7\n>>> del c['b']                      # remove all 'b'\n>>> c['b']                          # now there are zero 'b'\n0\n\n>>> d = Counter('simsalabim')       # make another counter\n>>> c.update(d)                     # add in the second counter\n>>> c['a']                          # now there are nine 'a'\n9\n\n>>> c.clear()                       # empty the counter\n>>> c\nCounter()\n\nNote:  If a count is set to zero or reduced to zero, it will remain\nin the counter until the entry is deleted or the counter is cleared:\n\n>>> c = Counter('aaabbc')\n>>> c['b'] -= 2                     # reduce the count of 'b' by two\n>>> c.most_common()                 # 'b' is still in, but its count is zero\n[('a', 3), ('c', 1), ('b', 0)]", "parameters": {"type": "object", "properties": {"iterable": {"type": "NoneType", "default": null}, "/": {}}}}}
{"task_id": "BigCodeBench/1085", "data": {"name": "collections.Counter.most_common", "type": "callable", "signature": "(self, n=None)", "description": "List the n most common elements and their counts from the most\ncommon to the least.  If n is None, then list all element counts.\n\n>>> Counter('abracadabra').most_common(3)\n[('a', 5), ('b', 2), ('r', 2)]", "parameters": {"type": "object", "properties": {"n": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/1085", "data": {"name": "matplotlib.pyplot.subplots", "type": "class", "signature": "()", "description": "Create a figure and a set of subplots.\n\nThis utility wrapper makes it convenient to create common layouts of\nsubplots, including the enclosing figure object, in a single call.\n\nParameters\n----------\nnrows, ncols : int, default: 1\n    Number of rows/columns of the subplot grid.\n\nsharex, sharey : bool or {'none', 'all', 'row', 'col'}, default: False\n    Controls sharing of properties among x (*sharex*) or y (*sharey*)\n    axes:\n\n    - True or 'all': x- or y-axis will be shared among all subplots.\n    - False or 'none': each subplot x- or y-axis will be independent.\n    - 'row': each subplot row will share an x- or y-axis.\n    - 'col': each subplot column will share an x- or y-axis.\n\n    When subplots have a shared x-axis along a column, only the x tick\n    labels of the bottom subplot are created. Similarly, when subplots\n    have a shared y-axis along a row, only the y tick labels of the first\n    column subplot are created. To later turn other subplots' ticklabels\n    on, use `~matplotlib.axes.Axes.tick_params`.\n\n    When subplots have a shared axis that has units, calling\n    `~matplotlib.axis.Axis.set_units` will update each axis with the\n    new units.\n\nsqueeze : bool, default: True\n    - If True, extra dimensions are squeezed out from the returned\n      array of `~matplotlib.axes.Axes`:\n\n      - if only one subplot is constructed (nrows=ncols=1), the\n        resulting single Axes object is returned as a scalar.\n      - for Nx1 or 1xM subplots, the returned object is a 1D numpy\n        object array of Axes objects.\n      - for NxM, subplots with N>1 and M>1 are returned as a 2D array.\n\n    - If False, no squeezing at all is done: the returned Axes object is\n      always a 2D array containing Axes instances, even if it ends up\n      being 1x1.\n\nwidth_ratios : array-like of length *ncols*, optional\n    Defines the relative widths of the columns. Each column gets a\n    relative width of ``width_ratios[i] / sum(width_ratios)``.\n    If not given, all columns will have the same width.  Equivalent\n    to ``gridspec_kw={'width_ratios': [...]}``.\n\nheight_ratios : array-like of length *nrows*, optional\n    Defines the relative heights of the rows. Each row gets a\n    relative height of ``height_ratios[i] / sum(height_ratios)``.\n    If not given, all rows will have the same height. Convenience\n    for ``gridspec_kw={'height_ratios': [...]}``.\n\nsubplot_kw : dict, optional\n    Dict with keywords passed to the\n    `~matplotlib.figure.Figure.add_subplot` call used to create each\n    subplot.\n\ngridspec_kw : dict, optional\n    Dict with keywords passed to the `~matplotlib.gridspec.GridSpec`\n    constructor used to create the grid the subplots are placed on.\n\n**fig_kw\n    All additional keyword arguments are passed to the\n    `.pyplot.figure` call.\n\nReturns\n-------\nfig : `.Figure`\n\nax : `~matplotlib.axes.Axes` or array of Axes\n    *ax* can be either a single `~.axes.Axes` object, or an array of Axes\n    objects if more than one subplot was created.  The dimensions of the\n    resulting array can be controlled with the squeeze keyword, see above.\n\n    Typical idioms for handling the return value are::\n\n        # using the variable ax for single a Axes\n        fig, ax = plt.subplots()\n\n        # using the variable axs for multiple Axes\n        fig, axs = plt.subplots(2, 2)\n\n        # using tuple unpacking for multiple Axes\n        fig, (ax1, ax2) = plt.subplots(1, 2)\n        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n\n    The names ``ax`` and pluralized ``axs`` are preferred over ``axes``\n    because for the latter it's not clear if it refers to a single\n    `~.axes.Axes` instance or a collection of these.\n\nSee Also\n--------\n.pyplot.figure\n.pyplot.subplot\n.pyplot.axes\n.Figure.subplots\n.Figure.add_subplot\n\nExamples\n--------\n::\n\n    # First create some toy data:\n    x = np.linspace(0, 2*np.pi, 400)\n    y = np.sin(x**2)\n\n    # Create just a figure and only one subplot\n    fig, ax = plt.subplots()\n    ax.plot(x, y)\n    ax.set_title('Simple plot')\n\n    # Create two subplots and unpack the output array immediately\n    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n    ax1.plot(x, y)\n    ax1.set_title('Sharing Y axis')\n    ax2.scatter(x, y)\n\n    # Create four polar axes and access them through the returned array\n    fig, axs = plt.subplots(2, 2, subplot_kw=dict(projection=\"polar\"))\n    axs[0, 0].plot(x, y)\n    axs[1, 1].scatter(x, y)\n\n    # Share a X axis with each column of subplots\n    plt.subplots(2, 2, sharex='col')\n\n    # Share a Y axis with each row of subplots\n    plt.subplots(2, 2, sharey='row')\n\n    # Share both X and Y axes with all subplots\n    plt.subplots(2, 2, sharex='all', sharey='all')\n\n    # Note that this is the same as\n    plt.subplots(2, 2, sharex=True, sharey=True)\n\n    # Create figure number 10 with a single subplot\n    # and clears it if it already exists.\n    fig, ax = plt.subplots(num=10, clear=True)", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/1085", "data": {"name": "matplotlib.pyplot.subplots[1].bar", "type": "method", "signature": "(x, height, width=0.8, bottom=None, *, align='center', data=None, **kwargs)", "description": "Make a bar plot.\n\nThe bars are positioned at *x* with the given *align*\\ment. Their\ndimensions are given by *height* and *width*. The vertical baseline\nis *bottom* (default 0).\n\nMany parameters can take either a single value applying to all bars\nor a sequence of values, one for each bar.\n\nParameters\n----------\nx : float or array-like\n    The x coordinates of the bars. See also *align* for the\n    alignment of the bars to the coordinates.\n\nheight : float or array-like\n    The height(s) of the bars.\n\n    Note that if *bottom* has units (e.g. datetime), *height* should be in\n    units that are a difference from the value of *bottom* (e.g. timedelta).\n\nwidth : float or array-like, default: 0.8\n    The width(s) of the bars.\n\n    Note that if *x* has units (e.g. datetime), then *width* should be in\n    units that are a difference (e.g. timedelta) around the *x* values.\n\nbottom : float or array-like, default: 0\n    The y coordinate(s) of the bottom side(s) of the bars.\n\n    Note that if *bottom* has units, then the y-axis will get a Locator and\n    Formatter appropriate for the units (e.g. dates, or categorical).\n\nalign : {'center', 'edge'}, default: 'center'\n    Alignment of the bars to the *x* coordinates:\n\n    - 'center': Center the base on the *x* positions.\n    - 'edge': Align the left edges of the bars with the *x* positions.\n\n    To align the bars on the right edge pass a negative *width* and\n    ``align='edge'``.\n\nReturns\n-------\n`.BarContainer`\n    Container with all the bars and optionally errorbars.\n\nOther Parameters\n----------------\ncolor : color or list of color, optional\n    The colors of the bar faces.\n\nedgecolor : color or list of color, optional\n    The colors of the bar edges.\n\nlinewidth : float or array-like, optional\n    Width of the bar edge(s). If 0, don't draw edges.\n\ntick_label : str or list of str, optional\n    The tick labels of the bars.\n    Default: None (Use default numeric labels.)\n\nlabel : str or list of str, optional\n    A single label is attached to the resulting `.BarContainer` as a\n    label for the whole dataset.\n    If a list is provided, it must be the same length as *x* and\n    labels the individual bars. Repeated labels are not de-duplicated\n    and will cause repeated label entries, so this is best used when\n    bars also differ in style (e.g., by passing a list to *color*.)\n\nxerr, yerr : float or array-like of shape(N,) or shape(2, N), optional\n    If not *None*, add horizontal / vertical errorbars to the bar tips.\n    The values are +/- sizes relative to the data:\n\n    - scalar: symmetric +/- values for all bars\n    - shape(N,): symmetric +/- values for each bar\n    - shape(2, N): Separate - and + values for each bar. First row\n      contains the lower errors, the second row contains the upper\n      errors.\n    - *None*: No errorbar. (Default)\n\n    See :doc:`/gallery/statistics/errorbar_features` for an example on\n    the usage of *xerr* and *yerr*.\n\necolor : color or list of color, default: 'black'\n    The line color of the errorbars.\n\ncapsize : float, default: :rc:`errorbar.capsize`\n   The length of the error bar caps in points.\n\nerror_kw : dict, optional\n    Dictionary of keyword arguments to be passed to the\n    `~.Axes.errorbar` method. Values of *ecolor* or *capsize* defined\n    here take precedence over the independent keyword arguments.\n\nlog : bool, default: False\n    If *True*, set the y-axis to be log scale.\n\ndata : indexable object, optional\n    If given, all parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` (unless this raises an exception).\n\n**kwargs : `.Rectangle` properties\n\nProperties:\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: scalar or None\n    angle: unknown\n    animated: bool\n    antialiased or aa: bool or None\n    bounds: (left, bottom, width, height)\n    capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    clip_box: `~matplotlib.transforms.BboxBase` or None\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    color: color\n    edgecolor or ec: color or None\n    facecolor or fc: color or None\n    figure: `~matplotlib.figure.Figure`\n    fill: bool\n    gid: str\n    hatch: {'/', '\\\\', '|', '-', '+', 'x', 'o', 'O', '.', '*'}\n    height: unknown\n    in_layout: bool\n    joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    label: object\n    linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n    linewidth or lw: float or None\n    mouseover: bool\n    path_effects: list of `.AbstractPathEffect`\n    picker: None or bool or float or callable\n    rasterized: bool\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    transform: `~matplotlib.transforms.Transform`\n    url: str\n    visible: bool\n    width: unknown\n    x: unknown\n    xy: (float, float)\n    y: unknown\n    zorder: float\n\nSee Also\n--------\nbarh : Plot a horizontal bar plot.\n\nNotes\n-----\nStacked bars can be achieved by passing individual *bottom* values per\nbar. See :doc:`/gallery/lines_bars_and_markers/bar_stacked`.", "parameters": {"type": "object", "properties": {"x": {}, "height": {}, "width": {"type": "float", "default": 0.8}, "bottom": {"type": "NoneType", "default": null}, "align": {"type": "str", "default": "center"}, "data": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/1085", "data": {"name": "re.sub", "type": "callable", "signature": "(pattern, repl, string)", "description": "Return the string obtained by replacing the leftmost\nnon-overlapping occurrences of the pattern in string by the\nreplacement repl.  repl can be either a string or a callable;\nif a string, backslash escapes in it are processed.  If it is\na callable, it's passed the Match object and must return\na replacement string to be used.", "parameters": {"type": "object", "properties": {"pattern": {}, "repl": {}, "string": {}}}}}
{"task_id": "BigCodeBench/1085", "data": {"name": "re.sub.lower.split", "type": "callable", "signature": "()", "description": "Return the string obtained by replacing the leftmost\nnon-overlapping occurrences of the pattern in string by the\nreplacement repl.  repl can be either a string or a callable;\nif a string, backslash escapes in it are processed.  If it is\na callable, it's passed the Match object and must return\na replacement string to be used.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/1124", "data": {"name": "bs4.BeautifulSoup", "type": "class", "signature": "(markup='', features=None, builder=None, parse_only=None, from_encoding=None, exclude_encodings=None, element_classes=None, **kwargs)", "description": "A data structure representing a parsed HTML or XML document.\n\nMost of the methods you'll call on a BeautifulSoup object are inherited from\nPageElement or Tag.\n\nInternally, this class defines the basic interface called by the\ntree builders when converting an HTML/XML document into a data\nstructure. The interface abstracts away the differences between\nparsers. To write a new tree builder, you'll need to understand\nthese methods as a whole.\n\nThese methods will be called by the BeautifulSoup constructor:\n  * reset()\n  * feed(markup)\n\nThe tree builder may call these methods from its feed() implementation:\n  * handle_starttag(name, attrs) # See note about return value\n  * handle_endtag(name)\n  * handle_data(data) # Appends to the current data node\n  * endData(containerClass) # Ends the current data node\n\nNo matter how complicated the underlying parser is, you should be\nable to build a tree using 'start tag' events, 'end tag' events,\n'data' events, and \"done with data\" events.\n\nIf you encounter an empty-element tag (aka a self-closing tag,\nlike HTML's <br> tag), call handle_starttag and then\nhandle_endtag.", "parameters": {"type": "object", "properties": {"markup": {"type": "str", "default": ""}, "features": {"type": "NoneType", "default": null}, "builder": {"type": "NoneType", "default": null}, "parse_only": {"type": "NoneType", "default": null}, "from_encoding": {"type": "NoneType", "default": null}, "exclude_encodings": {"type": "NoneType", "default": null}, "element_classes": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/1124", "data": {"name": "bs4.BeautifulSoup.title", "type": "class", "signature": "(markup='', features=None, builder=None, parse_only=None, from_encoding=None, exclude_encodings=None, element_classes=None, **kwargs)", "description": "A data structure representing a parsed HTML or XML document.\n\nMost of the methods you'll call on a BeautifulSoup object are inherited from\nPageElement or Tag.\n\nInternally, this class defines the basic interface called by the\ntree builders when converting an HTML/XML document into a data\nstructure. The interface abstracts away the differences between\nparsers. To write a new tree builder, you'll need to understand\nthese methods as a whole.\n\nThese methods will be called by the BeautifulSoup constructor:\n  * reset()\n  * feed(markup)\n\nThe tree builder may call these methods from its feed() implementation:\n  * handle_starttag(name, attrs) # See note about return value\n  * handle_endtag(name)\n  * handle_data(data) # Appends to the current data node\n  * endData(containerClass) # Ends the current data node\n\nNo matter how complicated the underlying parser is, you should be\nable to build a tree using 'start tag' events, 'end tag' events,\n'data' events, and \"done with data\" events.\n\nIf you encounter an empty-element tag (aka a self-closing tag,\nlike HTML's <br> tag), call handle_starttag and then\nhandle_endtag.", "parameters": {"type": "object", "properties": {"markup": {"type": "str", "default": ""}, "features": {"type": "NoneType", "default": null}, "builder": {"type": "NoneType", "default": null}, "parse_only": {"type": "NoneType", "default": null}, "from_encoding": {"type": "NoneType", "default": null}, "exclude_encodings": {"type": "NoneType", "default": null}, "element_classes": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/1124", "data": {"name": "re.search", "type": "callable", "signature": "(pattern, string)", "description": "Scan through string looking for a match to the pattern, returning\na Match object, or None if no match was found.", "parameters": {"type": "object", "properties": {"pattern": {}, "string": {}}}}}
{"task_id": "BigCodeBench/1124", "data": {"name": "re.search.group", "type": "callable", "signature": "()", "description": "Scan through string looking for a match to the pattern, returning\na Match object, or None if no match was found.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/1124", "data": {"name": "requests.get", "type": "callable", "signature": "(url, **kwargs)", "description": "Sends a GET request.\n\n:param url: URL for the new :class:`Request` object.\n:param params: (optional) Dictionary, list of tuples or bytes to send\n    in the query string for the :class:`Request`.\n:param \\*\\*kwargs: Optional arguments that ``request`` takes.\n:return: :class:`Response <Response>` object\n:rtype: requests.Response", "parameters": {"type": "object", "properties": {"url": {}}}}}
{"task_id": "BigCodeBench/1124", "data": {"name": "requests.get.text", "type": "callable", "signature": "(url, params=None, **kwargs)", "description": "Sends a GET request.\n\n:param url: URL for the new :class:`Request` object.\n:param params: (optional) Dictionary, list of tuples or bytes to send\n    in the query string for the :class:`Request`.\n:param \\*\\*kwargs: Optional arguments that ``request`` takes.\n:return: :class:`Response <Response>` object\n:rtype: requests.Response", "parameters": {"type": "object", "properties": {"url": {}, "params": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/1124", "data": {"name": "requests.get.raise_for_status", "type": "callable", "signature": "()", "description": "Sends a GET request.\n\n:param url: URL for the new :class:`Request` object.\n:param params: (optional) Dictionary, list of tuples or bytes to send\n    in the query string for the :class:`Request`.\n:param \\*\\*kwargs: Optional arguments that ``request`` takes.\n:return: :class:`Response <Response>` object\n:rtype: requests.Response", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/1124", "data": {"name": "title.string"}}
{"task_id": "BigCodeBench/1124", "data": {"name": "urllib.parse.urlparse", "type": "callable", "signature": "(url)", "description": "Parse a URL into 6 components:\n<scheme>://<netloc>/<path>;<params>?<query>#<fragment>\n\nThe result is a named 6-tuple with fields corresponding to the\nabove. It is either a ParseResult or ParseResultBytes object,\ndepending on the type of the url parameter.\n\nThe username, password, hostname, and port sub-components of netloc\ncan also be accessed as attributes of the returned object.\n\nThe scheme argument provides the default value of the scheme\ncomponent when no scheme is found in url.\n\nIf allow_fragments is False, no attempt is made to separate the\nfragment component from the previous component, which can be either\npath or query.\n\nNote that % escapes are not expanded.", "parameters": {"type": "object", "properties": {"url": {}}}}}
{"task_id": "BigCodeBench/1129", "data": {"name": "datetime.datetime", "type": "class", "signature": "(self, /, *args, **kwargs)", "description": "datetime(year, month, day[, hour[, minute[, second[, microsecond[,tzinfo]]]]])\n\nThe year, month and day arguments are required. tzinfo may be None, or an\ninstance of a tzinfo subclass. The remaining arguments may be ints.", "parameters": {"type": "object", "properties": {"/": {}}}}}
{"task_id": "BigCodeBench/1129", "data": {"name": "datetime.datetime.now", "type": "callable", "signature": "()", "description": "Returns new datetime object representing current time local to tz.\n\n  tz\n    Timezone object.\n\nIf no tz is specified, uses local timezone.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/1129", "data": {"name": "json.loads", "type": "callable", "signature": "(s)", "description": "Deserialize ``s`` (a ``str``, ``bytes`` or ``bytearray`` instance\ncontaining a JSON document) to a Python object.\n\n``object_hook`` is an optional function that will be called with the\nresult of any object literal decode (a ``dict``). The return value of\n``object_hook`` will be used instead of the ``dict``. This feature\ncan be used to implement custom decoders (e.g. JSON-RPC class hinting).\n\n``object_pairs_hook`` is an optional function that will be called with the\nresult of any object literal decoded with an ordered list of pairs.  The\nreturn value of ``object_pairs_hook`` will be used instead of the ``dict``.\nThis feature can be used to implement custom decoders.  If ``object_hook``\nis also defined, the ``object_pairs_hook`` takes priority.\n\n``parse_float``, if specified, will be called with the string\nof every JSON float to be decoded. By default this is equivalent to\nfloat(num_str). This can be used to use another datatype or parser\nfor JSON floats (e.g. decimal.Decimal).\n\n``parse_int``, if specified, will be called with the string\nof every JSON int to be decoded. By default this is equivalent to\nint(num_str). This can be used to use another datatype or parser\nfor JSON integers (e.g. float).\n\n``parse_constant``, if specified, will be called with one of the\nfollowing strings: -Infinity, Infinity, NaN.\nThis can be used to raise an exception if invalid JSON numbers\nare encountered.\n\nTo use a custom ``JSONDecoder`` subclass, specify it with the ``cls``\nkwarg; otherwise ``JSONDecoder`` is used.", "parameters": {"type": "object", "properties": {"s": {}}}}}
{"task_id": "BigCodeBench/1129", "data": {"name": "open.write(response.content)"}}
{"task_id": "BigCodeBench/1129", "data": {"name": "os.getcwd", "type": "callable", "signature": "()", "description": "Return a unicode string representing the current working directory.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/1129", "data": {"name": "os.path.join", "type": "callable", "signature": "(a, *p)", "description": "Join two or more pathname components, inserting '/' as needed.\nIf any component is an absolute path, all previous path components\nwill be discarded.  An empty last part will result in a path that\nends with a separator.", "parameters": {"type": "object", "properties": {"a": {}}}}}
{"task_id": "BigCodeBench/1129", "data": {"name": "requests.get", "type": "callable", "signature": "(url)", "description": "Sends a GET request.\n\n:param url: URL for the new :class:`Request` object.\n:param params: (optional) Dictionary, list of tuples or bytes to send\n    in the query string for the :class:`Request`.\n:param \\*\\*kwargs: Optional arguments that ``request`` takes.\n:return: :class:`Response <Response>` object\n:rtype: requests.Response", "parameters": {"type": "object", "properties": {"url": {}}}}}
{"task_id": "BigCodeBench/1129", "data": {"name": "requests.get.content", "type": "callable", "signature": "(url, params=None, **kwargs)", "description": "Sends a GET request.\n\n:param url: URL for the new :class:`Request` object.\n:param params: (optional) Dictionary, list of tuples or bytes to send\n    in the query string for the :class:`Request`.\n:param \\*\\*kwargs: Optional arguments that ``request`` takes.\n:return: :class:`Response <Response>` object\n:rtype: requests.Response", "parameters": {"type": "object", "properties": {"url": {}, "params": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/1137", "data": {"name": "bs4.BeautifulSoup", "type": "class", "signature": "(markup='', features=None, builder=None, parse_only=None, from_encoding=None, exclude_encodings=None, element_classes=None, **kwargs)", "description": "A data structure representing a parsed HTML or XML document.\n\nMost of the methods you'll call on a BeautifulSoup object are inherited from\nPageElement or Tag.\n\nInternally, this class defines the basic interface called by the\ntree builders when converting an HTML/XML document into a data\nstructure. The interface abstracts away the differences between\nparsers. To write a new tree builder, you'll need to understand\nthese methods as a whole.\n\nThese methods will be called by the BeautifulSoup constructor:\n  * reset()\n  * feed(markup)\n\nThe tree builder may call these methods from its feed() implementation:\n  * handle_starttag(name, attrs) # See note about return value\n  * handle_endtag(name)\n  * handle_data(data) # Appends to the current data node\n  * endData(containerClass) # Ends the current data node\n\nNo matter how complicated the underlying parser is, you should be\nable to build a tree using 'start tag' events, 'end tag' events,\n'data' events, and \"done with data\" events.\n\nIf you encounter an empty-element tag (aka a self-closing tag,\nlike HTML's <br> tag), call handle_starttag and then\nhandle_endtag.", "parameters": {"type": "object", "properties": {"markup": {"type": "str", "default": ""}, "features": {"type": "NoneType", "default": null}, "builder": {"type": "NoneType", "default": null}, "parse_only": {"type": "NoneType", "default": null}, "from_encoding": {"type": "NoneType", "default": null}, "exclude_encodings": {"type": "NoneType", "default": null}, "element_classes": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/1137", "data": {"name": "bs4.BeautifulSoup.get_text", "type": "callable", "signature": "(self)", "description": "Get all child strings, concatenated using the given separator.\n\n:param separator: Strings will be concatenated using this separator.\n\n:param strip: If True, strings will be stripped before being\n    concatenated.\n\n:types: A tuple of NavigableString subclasses. Any strings of\n    a subclass not found in this list will be ignored. By\n    default, this means only NavigableString and CData objects\n    will be considered. So no comments, processing instructions,\n    etc.\n\n:return: A string.", "parameters": {"type": "object", "properties": {}}}}
{"task_id": "BigCodeBench/1137", "data": {"name": "json.dump", "type": "callable", "signature": "(obj, fp)", "description": "Serialize ``obj`` as a JSON formatted stream to ``fp`` (a\n``.write()``-supporting file-like object).\n\nIf ``skipkeys`` is true then ``dict`` keys that are not basic types\n(``str``, ``int``, ``float``, ``bool``, ``None``) will be skipped\ninstead of raising a ``TypeError``.\n\nIf ``ensure_ascii`` is false, then the strings written to ``fp`` can\ncontain non-ASCII characters if they appear in strings contained in\n``obj``. Otherwise, all such characters are escaped in JSON strings.\n\nIf ``check_circular`` is false, then the circular reference check\nfor container types will be skipped and a circular reference will\nresult in an ``OverflowError`` (or worse).\n\nIf ``allow_nan`` is false, then it will be a ``ValueError`` to\nserialize out of range ``float`` values (``nan``, ``inf``, ``-inf``)\nin strict compliance of the JSON specification, instead of using the\nJavaScript equivalents (``NaN``, ``Infinity``, ``-Infinity``).\n\nIf ``indent`` is a non-negative integer, then JSON array elements and\nobject members will be pretty-printed with that indent level. An indent\nlevel of 0 will only insert newlines. ``None`` is the most compact\nrepresentation.\n\nIf specified, ``separators`` should be an ``(item_separator, key_separator)``\ntuple.  The default is ``(', ', ': ')`` if *indent* is ``None`` and\n``(',', ': ')`` otherwise.  To get the most compact JSON representation,\nyou should specify ``(',', ':')`` to eliminate whitespace.\n\n``default(obj)`` is a function that should return a serializable version\nof obj or raise TypeError. The default simply raises TypeError.\n\nIf *sort_keys* is true (default: ``False``), then the output of\ndictionaries will be sorted by key.\n\nTo use a custom ``JSONEncoder`` subclass (e.g. one that overrides the\n``.default()`` method to serialize additional types), specify it with\nthe ``cls`` kwarg; otherwise ``JSONEncoder`` is used.", "parameters": {"type": "object", "properties": {"obj": {}, "fp": {}}}}}
{"task_id": "BigCodeBench/1137", "data": {"name": "open.read()"}}
{"task_id": "BigCodeBench/1137", "data": {"name": "re.findall", "type": "callable", "signature": "(pattern, string)", "description": "Return a list of all non-overlapping matches in the string.\n\nIf one or more capturing groups are present in the pattern, return\na list of groups; this will be a list of tuples if the pattern\nhas more than one group.\n\nEmpty matches are included in the result.", "parameters": {"type": "object", "properties": {"pattern": {}, "string": {}}}}}
{"task_id": "BigCodeBench/1137", "data": {"name": "requests.get", "type": "callable", "signature": "(url, **kwargs)", "description": "Sends a GET request.\n\n:param url: URL for the new :class:`Request` object.\n:param params: (optional) Dictionary, list of tuples or bytes to send\n    in the query string for the :class:`Request`.\n:param \\*\\*kwargs: Optional arguments that ``request`` takes.\n:return: :class:`Response <Response>` object\n:rtype: requests.Response", "parameters": {"type": "object", "properties": {"url": {}}}}}
{"task_id": "BigCodeBench/1137", "data": {"name": "requests.get.text", "type": "callable", "signature": "(url, params=None, **kwargs)", "description": "Sends a GET request.\n\n:param url: URL for the new :class:`Request` object.\n:param params: (optional) Dictionary, list of tuples or bytes to send\n    in the query string for the :class:`Request`.\n:param \\*\\*kwargs: Optional arguments that ``request`` takes.\n:return: :class:`Response <Response>` object\n:rtype: requests.Response", "parameters": {"type": "object", "properties": {"url": {}, "params": {"type": "NoneType", "default": null}}}}}
{"task_id": "BigCodeBench/1137", "data": {"name": "url.startswith('file://')"}}
