' '.join((word for word in text.split() if word not in STOPWORDS)).split()
ALPHANUMERIC.sub(' ', text2).lower().split()
ALPHANUMERIC.sub(' ', text1).lower().split()
Crypto.Random.get_random_bytes(16)
Crypto.Cipher.AES.new(password, AES.MODE_EAX)
Crypto.Cipher.AES.new.encrypt_and_digest(priv_key.save_pkcs1())
Crypto.Cipher.AES.MODE_EAX
Crypto.Cipher.AES.new(password, AES.MODE_EAX).nonce
Levenshtein.ratio(text1, text2)
LinearRegression().fit(X_train, y_train).score(X_test, y_test)
LoginForm().validate_on_submit()
LoginForm().password.data
LoginForm().password
LoginForm().username
LoginForm().username.data
NMF(n_components=num_topics, random_state=1).fit(tfidf).components_
PIL.Image.open.info
PIL.Image.open.info.get('comment', '')
PIL.Image.open(filename)
RandomForestClassifier(random_state=42).fit(X, y).feature_importances_
T.shape
a.text
a.get('href')
activity.strftime('%A')
assignment_data.append([task_name, employee, due_date])
axes.append(ax)
base64.b64encode(decoded_str.encode())
base64.b64encode(priv_key_encrypted)
base64.b64encode(encrypted_aes_key)
binascii.hexlify(decoded_str.encode())
book.add_sheet(sheet_name).write(0, col_index, col)
book.add_sheet(sheet_name).write(row_index + 1, col_index, row[col])
bs4.BeautifulSoup(response.text, 'html.parser')
bs4.BeautifulSoup(html, 'html.parser')
bs4.BeautifulSoup(response.text, 'html.parser').title
bs4.BeautifulSoup.find('table', attrs={'class': 'data-table'})
bs4.BeautifulSoup.find('table', {'id': table_id})
bs4.BeautifulSoup.get_text()
bs4.BeautifulSoup.find('table')
bs4.BeautifulSoup.find_all('a', href=True)
bytes.fromhex.decode('utf-8').encode('ascii')
bytes.fromhex.decode('utf-8').encode('utf-32')
bytes.fromhex.decode('utf-8').encode()
bytes.fromhex.decode('utf-8').encode('utf-8')
bytes.fromhex.decode('utf-8').encode('utf-16')
bytes.fromhex(hex_string)
cell.value
cell.text_content()
cgi.parse_header(self.headers.get('content-type'))
chardet.detect(content)
child.text
child.tag
cipher.encryptor().finalize()
cipher.encryptor().update(padded_data)
client_socket.send(response.encode('utf-8'))
client_socket.close()
client_socket.recv(BUFFER_SIZE)
codecs.decode(comment, from_encoding)
codecs.encode(name, 'utf-8')
codecs.encode(decoded_str, 'rot_13')
collections.Counter.get(key, 0)
collections.Counter(text2.split())
collections.Counter.values()
collections.Counter(duplicates_df['age'])
collections.defaultdict.items()
collections.Counter.most_common(10)
collections.Counter(text1.split())
collections.Counter(text.split())
collections.defaultdict(int)
collections.Counter.items()
collections.Counter.update(words)
collections.defaultdict(list)
collections.Counter(values)
collections.Counter()
collections.Counter(words)
collections.Counter.keys()
collections.Counter(word_combinations)
collections.Counter(duplicates['value'])
collections.Counter((tuple(row) for row in rows if rows.count(row) > 1))
collections.Counter({'goals': 0, 'penalties': 0})
column_data.reshape(-1, 1)
content.decode(detected_encoding).decode(detected_encoding)
content.decode(from_encoding).decode(from_encoding)
content.encode(to_encoding).decode(to_encoding).encode(to_encoding)
context.wrap_socket(client_socket, server_side=True).send(response.encode('utf-8'))
context.wrap_socket(client_socket, server_side=True).close()
context.wrap_socket(client_socket, server_side=True).recv(buffer_size)
copied_files.append(str(target_file))
counts.plot(kind='bar').set_title('Non-Zero Value Counts')
cryptography.hazmat.primitives.ciphers.modes.CBC(iv)
cryptography.hazmat.primitives.ciphers.algorithms.AES(aes_key)
cryptography.hazmat.primitives.padding.PKCS7.padder().finalize()
cryptography.hazmat.backends.default_backend()
cryptography.hazmat.primitives.padding.PKCS7(128)
cryptography.hazmat.primitives.padding.PKCS7.padder().update(data)
cryptography.hazmat.primitives.ciphers.Cipher(algorithms.AES(aes_key), modes.CBC(iv), backend=default_backend())
cryptography.hazmat.primitives.ciphers.Cipher.encryptor()
csv.DictReader(file)
csv.writer.writerow(FIELDS)
csv.writer(csvfile)
csv.reader(infile)
csv.DictWriter.writerow({'key': key, 'mean': values['mean'], 'median': values['median']})
csv.writer.writerow([link])
csv.DictWriter(f, fieldnames=['key', 'mean', 'median'])
csv.DictWriter.writeheader()
csv.writer.writerows(content)
csv.writer(file)
csv.reader(file)
csv.writer.writerow(COLUMNS)
csv.writer.writerow(averages)
csv.writer(outfile)
csv.writer.writerow([timestamp, temperature, humidity])
csv.writer.writerows(reader)
csv.writer.writerows(data)
csv.DictReader(f).fieldnames
csv.reader(file, delimiter=delimiter, quotechar=quotechar)
csv.reader(f)
csv.writer(f)
csv.DictReader(f)
cv2.imread(image_path)
cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
cv2.imread.reshape(-1, 3)
cv2.imread.copy()
cv2.imwrite(f'cluster_{i + 1}.jpg', cluster_img)
cv2.imread(image_path).shape
cv2.COLOR_BGR2RGB
d.split('-')
d.keys()
d.rsplit('-', 1)
d.items()
d.get(key, np.nan)
data.strip()
data.get('Countries').items()
data.append(cols)
data.append([i, name, dob, email])
data.values()
data.columns
data.iloc
data.append(data_item)
data.drop(columns=[target_column])
data.append([date, activity, duration])
data.split(',')
data.append([i + 1, status, content])
data.append(row)
data.split('-').split('-')
data.dtypes
data.empty
datetime.datetime.strptime(ts, '%d/%m/%y %H:%M:%S.%f')
datetime.datetime.datetime(birth_year, np.random.randint(1, 13), np.random.randint(1, 29))
datetime.datetime.today()
datetime.datetime(2020, 1, 1)
datetime.datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S')
datetime.timedelta(days=i)
datetime.datetime(2020, 12, 31)
datetime.timezone.utc
datetime.datetime
datetime.datetime.fromtimestamp(ts / 1000)
datetime.datetime.now()
datetime.datetime.fromtimestamp(file_info.st_ctime, timezone.utc)
datetime.datetime.fromtimestamp(file_info.st_mtime, timezone.utc)
datetime.datetime.fromtimestamp(epoch_milliseconds / 1000.0)
datetime.datetime.strptime(d[0], '%Y-%m')
datetime.timedelta(days=randint(0, num_days))
datetime.timedelta(seconds=run_duration)
df.plot.line(x='Date', y=column).set_ylabel(column)
df.columns
df.astype(bool).sum(axis=0).plot(kind='bar')
df.plot(x='Time', y='Value').set_ylabel('Value')
df.asfreq(freq, method='pad').plot(y='value')
df.shape
df.empty
df.set_index('date').set_index('date')
df.drop(target_column, axis=1)
df.plot(kind='bar', legend=False, title='Duplicate Entries').set_ylabel('Count')
df.drop(target_column, axis=1).columns
df.asfreq(freq, method='pad').asfreq(freq, method='pad')
df.cumsum()
df.isnull()
df.select_dtypes(include=np.number)
df.set_index('Month').index
df.set_index('Month').set_index('Month')
df.sort_values(by=column_name).sort_values(by=column_name)
df.index
difflib.ndiff(csv_content1, csv_content2)
docx.Document(filepath).paragraphs
docx.Document(filepath)
downloaded_files.append(filename)
ele.text.strip()
ele.text
email.message.EmailMessage.set_content(request)
email.message.EmailMessage()
email.mime.text.MIMEText.as_string()
email.mime.text.MIMEText(email_data['message'])
entry.path
entry.name
entry.is_file()
exit_codes.append(process.poll())
extracted_dirs.append(extract_path)
file.name
file_details.append((entry.name, file_size, creation_time, modification_time))
file_name.endswith('.txt')
file_path.strip().strip()
file_path.strip().lower()
file_pattern.match(filename).group(1)
filename.split('.')
filename.endswith('.json')
filepath.parent.mkdir(parents=True, exist_ok=True)
filepath.parent
files_moved.append(dest_file_path)
flask.Flask(app_name).config
flask.redirect(url_for('login'))
flask.url_for('login')
flask.Flask.route('/protected')
flask.render_template('login.html', form=form)
flask.Flask(__name__, template_folder=template_folder)
flask.url_for('protected')
flask.redirect(url_for('protected'))
flask.Flask.route('/login', methods=['GET', 'POST'])
flask.Flask(__name__, template_folder=template_folder).config
flask.Flask.route('/logout')
flask.Flask(app_name)
flask_login.LoginManager().user_loader
flask_login.current_user.id
flask_login.login_user(user)
flask_login.logout_user()
flask_login.LoginManager.init_app(app)
flask_login.LoginManager()
flask_login.UserMixin.password_hash
flask_login.UserMixin.id
flask_mail.Mail(app)
ftplib.FTP.login(ftp_user, ftp_password)
ftplib.FTP(ftp_server)
ftplib.FTP.quit()
ftplib.FTP.cwd(ftp_dir)
ftplib.FTP.nlst()
functools.reduce(lambda a, b: a + b, [math.factorial(n) for n in permutation])
gensim.models.Word2Vec(vector_size=100)
gensim.models.Word2Vec(sentences=tokenized_texts, vector_size=100, window=5, min_count=1, workers=4)
geopandas.GeoDataFrame(data, geometry='Coordinates')
getpass.getpass('Recipient: ')
getpass.getpass('Email: ')
getpass.getpass('Password: ')
glob.glob(f'{source_directory}/**/*{extension}', recursive=True)
glob.glob(os.path.join(file_dir, '*' + file_ext))
glob.glob(directory_path + '/*.xlsx')
glob.glob(os.path.join(SOURCE_DIR, '*' + ext))
glob.glob(os.path.join(directory_path, '*.bat'))
glob.glob(pattern)
glob.glob(os.path.join(src_dir, '*.' + ext))
glob.glob(os.path.join(directory, '*'))
goals.items()
hashlib.sha256.hexdigest()
hashlib.sha256()
hashlib.sha256.update(byte_block)
hashlib.md5()
hashlib.md5.hexdigest()
hashlib.md5.update(byte_block)
http.server.BaseHTTPRequestHandler.headers.get('content-length')
http.server.BaseHTTPRequestHandler.send_response(400)
http.server.BaseHTTPRequestHandler.send_error(400, 'Content-Type header is not application/json')
http.server.BaseHTTPRequestHandler.send_header('content-type', 'application/json')
http.server.BaseHTTPRequestHandler.send_response(200)
http.server.BaseHTTPRequestHandler.headers.get('content-type')
http.server.BaseHTTPRequestHandler.wfile.write(response)
http.server.BaseHTTPRequestHandler
http.server.BaseHTTPRequestHandler.headers
http.server.BaseHTTPRequestHandler.wfile
http.server.BaseHTTPRequestHandler.end_headers()
http.server
http.server.BaseHTTPRequestHandler.send_response(535)
http.server.BaseHTTPRequestHandler.rfile.read(length)
http.server.BaseHTTPRequestHandler.send_error(400, 'No data key in request')
http.server.BaseHTTPRequestHandler.rfile
http.server.BaseHTTPRequestHandler.send_error(400, 'Invalid JSON')
indices.size
inputs.append(connection)
inputs.remove(s)
interesting_articles.empty
io.StringIO(str(table))
ipaddress.IPv4Network(ip_range)
itertools.product(animals, foods)
itertools.zip_longest(*data_list, fillvalue=np.nan)
itertools.permutations(ALPHABETS, 2)
itertools.permutations(numbers)
json.JSONDecodeError
json.loads.values()
json.loads(json_str)
json.loads(content)
json.loads(json_data)
json.load(f)
json.loads(self.rfile.read(length))
json.loads.get('Countries')
json.loads.items()
json.dumps(SUCCESS_RESPONSE)
json.dump(phone_numbers, f)
json.dump(dict(word_counts), file)
json.load(file)
keras.models.Sequential.compile(loss='binary_crossentropy', optimizer=SGD(learning_rate=0.1))
keras.optimizers.SGD(learning_rate=0.1)
keras.models.Sequential.fit(X_train, Y_train, epochs=200, batch_size=1, verbose=0, validation_data=(X_test, Y_test))
keras.models.Sequential([Dense(input_dim=2, units=1, activation='sigmoid')])
keras.layers.Dense(input_dim=2, units=1, activation='sigmoid')
kwargs.items()
librosa.stft(matrix)
librosa.display
librosa.display.specshow(D, sr=samplerate, x_axis='time', y_axis='log')
librosa.amplitude_to_db(np.abs(librosa.stft(matrix)), ref=np.max)
line.strip().strip()
list(reader).count(row)
lxml.etree.XMLSyntaxError
lxml.html.fromstring.xpath('//tr')
lxml.etree.XML(xml_data)
lxml.html.fromstring(content)
lxml.etree.XML.findall('.//item')
match.groups()[2].strip()
match_results.append([team, team_goals, penalty_cost])
math.floor(population)
math.factorial(n)
math.pi
matplotlib.pyplot.subplots[1].set_title('Top 10 Most Common Words')
matplotlib.pyplot.legend()
matplotlib.pyplot.subplots[1].set_xlabel('Day of the Week')
matplotlib.pyplot.subplots[1].set_title('Histogram of Dice Rolls')
matplotlib.pyplot.subplots[1].plot([0, 1], [0, 1], 'k--')
matplotlib.pyplot.subplots[1].bar(days, counts)
matplotlib.pyplot.subplots[1].plot(x, y_cos, label='cos')
matplotlib.pyplot.subplots()
matplotlib.pyplot.figure(figsize=(10, 8))
matplotlib.pyplot.subplots[1].scatter(data.iloc[:, 0], data.iloc[:, 1], c=labels, cmap='viridis', alpha=0.6, label='Data points')
matplotlib.pyplot.subplots[1].plot(np.abs(fft))
matplotlib.pyplot.xticks(rotation='vertical')
matplotlib.pyplot.subplots[1].set_xlabel('Epoch')
matplotlib.pyplot.subplots[1].pie(column_data, labels=df['Age'], autopct='%1.1f%%')
matplotlib.pyplot.subplots[1].hist(outcomes, bins=np.arange(1, 7 + 1.5) - 0.5, edgecolor='black')
matplotlib.pyplot.subplots[1].set_title('Random RGB Image')
matplotlib.pyplot.rc('font', family='Arial')
matplotlib.pyplot.rc('font', **font)
matplotlib.pyplot.gca()
matplotlib.pyplot.colorbar(format='%+2.0f dB')
matplotlib.pyplot.subplots[1].scatter(df['date'], df['closing_price'], color='black')
matplotlib.pyplot.subplots[1].plot(df['date'], df['closing_price'], label='Historical Closing Prices')
matplotlib.pyplot.subplots[1].set_xlabel('Word Length')
matplotlib.pyplot.subplots(figsize=(10, 5))
matplotlib.pyplot.title('Data without Outliers')
matplotlib.pyplot.subplots[1].set_title('Original vs. Normalized Data')
matplotlib.pyplot.subplots(nrows=2, ncols=1)
matplotlib.pyplot.subplots[1].plot(history.history['loss'], label='Train Loss')
matplotlib.pyplot.ylabel('Vehicle Count')
matplotlib.pyplot.subplots[1].plot(norm_arr, label='Normalized')
matplotlib.pyplot.xlim()
matplotlib.pyplot.subplots[1].plot(x, p, 'k', linewidth=2, label='PDF')
matplotlib.pyplot.subplots[1].bar(column, mean, yerr=std)
matplotlib.pyplot.subplots[1].plot(df['closing_price'], color='blue', label='Normal')
matplotlib.pyplot.subplots[1].set_title('Weekly Activity')
matplotlib.pyplot.subplots[1].set_title('Histogram of Random Numbers')
matplotlib.pyplot.subplots[1].set_title('Histogram with PDF')
matplotlib.pyplot.subplots[1].plot(future_dates, pred_prices, color='blue', linewidth=3)
matplotlib.pyplot.subplots[1].scatter(transformed_data[:, 0], transformed_data[:, 1])
matplotlib.pyplot.title('Correlation Heatmap')
matplotlib.pyplot.gca.set_title('Histogram of Values')
matplotlib.pyplot.subplots[1].set_ylabel('True positive rate')
matplotlib.pyplot.subplot(1, 2, 1)
matplotlib.pyplot.scatter(data_copy[:, 0], data_copy[:, 1])
matplotlib.pyplot.subplots[1].scatter(df[df['category'] == category]['x'], df[df['category'] == category]['y'], label=category)
matplotlib.pyplot.subplots[1].set_title('Distribution')
matplotlib.pyplot.subplots[1].set_xlabel('Frequency [Hz]')
matplotlib.pyplot.subplots[1].set_xlabel('Value')
matplotlib.pyplot.xlabel('Age')
matplotlib.pyplot.subplots[1].set_ylabel('Frequency Spectrum Magnitude')
matplotlib.pyplot.subplots[1].legend(loc='best')
matplotlib.pyplot.axis('off')
matplotlib.pyplot.savefig(save_path)
matplotlib.pyplot.subplots[1].set_xlabel('Columns')
matplotlib.pyplot.title('Distribution of Ages for Duplicate Names')
matplotlib.pyplot.subplots[1].bar(*zip(*most_common_words))
matplotlib.pyplot.ylabel('Value')
matplotlib.pyplot.gca.hist(df['Values'], bins=np.arange(df['Values'].min(), df['Values'].max() + 2) - 0.5, edgecolor='black')
matplotlib.pyplot.subplots[1].scatter(centroids[:, 0], centroids[:, 1], marker='x', s=200, c='red', label='Centroids')
matplotlib.pyplot.subplots[1].set_xlabel('Dice Value')
matplotlib.pyplot.subplots[1].set_title('Course-wise Average and Passing Grade Counts')
matplotlib.pyplot.subplots[1].set_ylabel('Feature 2')
matplotlib.pyplot.subplots[1].set_title('Outliers in Closing Prices')
matplotlib.pyplot.subplots[1].set_title('ROC curve')
matplotlib.pyplot.subplots[1].set_xlabel('x')
matplotlib.pyplot.title('Time Series Decomposition')
matplotlib.pyplot.subplots[1].plot(arr, label='Original')
matplotlib.pyplot.subplots[1].set_xlabel('Feature 1')
matplotlib.pyplot.subplots[1].set_xlabel('False positive rate')
matplotlib.pyplot.subplots[1].plot(x, func(x, *popt), 'r-', label='fit: a=%5.3f, b=%5.3f' % tuple(popt))
matplotlib.pyplot.subplots[1].hist(arr, density=True, alpha=0.6, bins='auto', label='Histogram')
matplotlib.pyplot.imshow(wordcloud)
matplotlib.pyplot.title(f'Line Chart of {column}')
matplotlib.pyplot.subplots[1].plot(fpr, tpr, label='AUC = {:.3f}'.format(auc_score))
matplotlib.pyplot.subplots[1].set_ylabel('Frequency')
matplotlib.pyplot.title('Data with Outliers')
matplotlib.pyplot.subplots[1].set_ylabel('Values')
matplotlib.pyplot.plot(x_data, y_data, 'bo', label='Data')
matplotlib.pyplot.ylabel('Count')
matplotlib.pyplot.tight_layout()
matplotlib.pyplot.subplots[1].set_title('Mean and Standard Deviation')
matplotlib.pyplot.gca.set_xticks(sorted(list(set(data))))
matplotlib.pyplot.subplots[1].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
matplotlib.pyplot.subplots[1].set_xlabel('Date')
matplotlib.pyplot.subplots[1].set_xticklabels(words)
matplotlib.pyplot.subplots[1].bar(indices, frequencies)
matplotlib.pyplot.subplots[1].set_title(f'Monthly Data for {list(unique_years)[0]}')
matplotlib.pyplot.subplots[1].set_xlabel('Month')
matplotlib.pyplot.subplots[1].legend(['Train', 'Test'], loc='upper left')
matplotlib.pyplot.close('all')
matplotlib.pyplot.subplots[1].legend()
matplotlib.pyplot.subplots[1].plot(x, y, 'b-', label='data')
matplotlib.pyplot.subplots[1].set_title('Random Time Series Data')
matplotlib.pyplot.Axes
matplotlib.pyplot.figure(figsize=(10, 5))
matplotlib.pyplot.title('Random Walk')
matplotlib.pyplot.subplots[1].set_xticks(indices)
matplotlib.pyplot.show()
matplotlib.pyplot.subplots[1].set_ylabel('Loss')
matplotlib.pyplot.subplots(1, 2, figsize=(12, 6))
matplotlib.pyplot.gca.set_xlabel('Value')
matplotlib.pyplot.subplots[1].set_title('K-Means Clustering')
matplotlib.pyplot.subplots[1].set_ylabel('y')
matplotlib.pyplot.subplots[1].plot(outliers['closing_price'], linestyle='none', marker='X', color='red', markersize=12, label='Outlier')
matplotlib.pyplot.subplots[1].set_ylabel('Value')
matplotlib.pyplot.subplots[1].set_title('Model loss')
matplotlib.pyplot.plot(walk)
matplotlib.pyplot.subplots[1].set_title(f'Pie Chart of {column}')
matplotlib.pyplot.gca.set_ylabel('Frequency')
matplotlib.pyplot.subplots(figsize=(10, 6))
matplotlib.pyplot.subplots[1].grid(True)
matplotlib.pyplot.subplots[1].hist(df['value'], bins=bins, density=True, alpha=0.6, color='g')
matplotlib.pyplot.subplots[1].bar(df.index, df['Value'])
matplotlib.pyplot.figure()
matplotlib.pyplot.subplots[1].set_xlabel('Number')
matplotlib.pyplot.figure(figsize=(10, 6))
matplotlib.pyplot.subplots[1].set_title('Category-wise Sales Trends')
matplotlib.pyplot.subplots[1].plot(history.history['val_loss'], label='Validation Loss')
matplotlib.pyplot.subplots(figsize=(12, 8))
matplotlib.pyplot.gcf()
matplotlib.pyplot.subplots[1].plot(x, y_sin, label='sin')
matplotlib.pyplot.subplots[1].hist(word_lengths, bins=bins, rwidth=rwidth)
matplotlib.pyplot.close(fig)
matplotlib.pyplot.subplots[1].hist(random_list, bins=20)
matplotlib.pyplot.subplots[1].plot(dates, values, label='Value over Time')
matplotlib.pyplot.subplots[1].set_title('KMeans Clustering Visualization')
matplotlib.pyplot.subplots(figsize=(12, 6))
matplotlib.pyplot.subplots[1].set_ylabel('Number of Activities')
matplotlib.pyplot.subplots[1].set_ylabel('Closing Price')
matplotlib.pyplot.xlabel('Time')
matplotlib.pyplot.subplots[1].set_title('Distribution of Word Lengths')
matplotlib.pyplot.subplots[1].scatter(flattened_result[:, 0], flattened_result[:, 1], c=cluster_result)
matplotlib.pyplot.subplots[1].plot(forecast_dates, forecast, label='Forecasted Closing Prices')
matplotlib.pyplot.subplots[1].get_xlim()
matplotlib.pyplot.subplots[1].set_xlabel('Index')
matplotlib.pyplot.plot(x_fit, func(x_fit, *popt), 'r-', label='Fit')
matplotlib.pyplot.title('Spectrogram')
matplotlib.pyplot.subplots[1].set_title('FFT of the Signal')
matplotlib.pyplot.subplots[1].hist(data, bins=number_of_bins, density=True, alpha=0.6, color='g')
matplotlib.pyplot.subplots[1].set_xlabel('Words')
matplotlib.pyplot.gca.set_title('Non-Zero Value Counts')
matplotlib.pyplot.subplots[1].bar([word[0] for word in top_words], [word[1] for word in top_words])
matplotlib.pyplot.subplots[1].bar([], [])
matplotlib.pyplot.close()
matplotlib.pyplot.subplot(1, 2, 2)
matplotlib.pyplot.scatter(data_without_outliers[:, 0], data_without_outliers[:, 1])
matplotlib.pyplot.subplots[1].bar(labels, values)
matplotlib.pyplot.subplots[1].plot(x, p, 'k', linewidth=2)
mean_values.append(np.nan)
mean_values.append(np.nanmean(numeric_values))
message_queues[s].get_nowait().encode('utf-8')
model.predict(future_dates).tolist()
model.fit(X_train, Y_train, epochs=200, batch_size=1, verbose=0, validation_data=(X_test, Y_test)).history
model.fit().forecast(steps=7)
model_fit.forecast(steps=7).tolist()
my_list.append(12)
name.lower()
new_files.append(new_filename)
nltk.corpus.stopwords
nltk.corpus.stopwords.words('english')
nltk.corpus
nltk.download('stopwords')
nltk.word_tokenize(content)
numeric_cols.size
numpy.random.seed(random_seed)
numpy.array(weights)
numpy.number
numpy.nanmean(numeric_values)
numpy.abs(df['Z_score'])
numpy.random.choice(CATEGORIES, N - len(CATEGORIES))
numpy.bincount(outcomes, minlength=7)
numpy.nan
numpy.random.randint(10, size=(rows, len(COLUMNS)))
numpy.random.shuffle(all_categories)
numpy.random.choice(latin_names)
numpy.mean(df[column])
numpy.linspace(0, 2 * math.pi, sample_size)
numpy.floor
numpy.random.randint(1, 29)
numpy.sum(column_data)
numpy.array([df['date'].max() + i * 24 * 60 * 60 for i in range(1, 8)])
numpy.sin(frequency * x)
numpy.random.randint(40, 101, size=(num_students, len(COURSES)))
numpy.mean(data ** 2)
numpy.log10(np.sqrt(np.mean(data ** 2)))
numpy.median(df[column_name])
numpy.delete(data_copy, outliers, axis=0)
numpy.cos(frequency * x)
numpy.std(arr)
numpy.stack([mask] * 3, axis=-1)
numpy.median(v)
numpy.array(segmented_image)
numpy.random.rand(N)
numpy.arange(max(word_lengths) + 2)
numpy.random.rand(array_length)
numpy.array(img)
numpy.zeros(image_size, dtype=np.uint8)
numpy.sin(b * x)
numpy.abs(fft)
numpy.uint8
numpy.linspace(min(x_data), max(x_data), 500)
numpy.copy(data)
numpy.cumsum(steps)
numpy.max(column_data)
numpy.random.randn(size)
numpy.exp(-b * x)
numpy.arange(1, 7 + 1.5)
numpy.where(np.stack([mask] * 3, axis=-1), segmented_image, np.array([255, 255, 255], dtype=np.uint8))
numpy.random
numpy.random.choice(other_names)
numpy.mean(differences)
numpy.sin(x)
numpy.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])
numpy.arange(df['Values'].min(), df['Values'].max() + 2)
numpy.random.randint(low=100, high=500)
numpy.random.choice([-1, 1], size=elements)
numpy.array([b for (a, b) in original]).size
numpy.random.rand(days_in_past, len(stock_names))
numpy.linspace(xmin, xmax, 100)
numpy.random.randint(1, 13)
numpy.max
numpy.histogram_bin_edges(data, bins='auto')
numpy.sqrt(sum2)
numpy.std(df[column])
numpy.random.seed(42)
numpy.mean(arr)
numpy.random.randint(low=100, high=500, size=periods)
numpy.sin(np.outer(time, signal) * np.pi)
numpy.linspace(0, 4 * np.pi, array_length)
numpy.mean(v)
numpy.array([b for (a, b) in original])
numpy.concatenate([guaranteed_categories, remaining_categories])
numpy.outer(time, signal)
numpy.arange(len(sales_df), 2 * len(sales_df))
numpy.tensordot.swapaxes(0, 1).reshape(-1, tensor_shape[2])
numpy.random.seed(seed)
numpy.array(L)
numpy.min(column_data)
numpy.sqrt(np.mean(data ** 2))
numpy.random.randn(len(a), len(b))
numpy.random.choice([True, False])
numpy.issubdtype(data[col1].dtype, np.number)
numpy.std(df[column_name])
numpy.abs(librosa.stft(matrix))
numpy.nanmean(v)
numpy.tensordot(P, T, axes=[1, 1])
numpy.issubdtype(dtype, np.number)
numpy.sqrt(sum1)
numpy.where(array[:, 0] == target_value)
numpy.abs(stats.zscore(standardized_data))
numpy.random.seed(0)
numpy.arange(len(word_counts))
numpy.max(matrix)
numpy.random.uniform(lat_min, lat_max)
numpy.random.normal(size=len(timestamps))
numpy.arange(start_time, end_time, step)
numpy.pi
numpy.nanmedian(v)
numpy.random.seed(rng_seed)
numpy.ndarray
numpy.where(z_scores > outlier_z_score)
numpy.array([255, 255, 255], dtype=np.uint8)
numpy.array(pairs)
numpy.max(arr)
numpy.array(CATEGORIES)
numpy.mean(column_data)
numpy.random.normal(0, 1e-10, points.shape)
numpy.issubdtype(data[col2].dtype, np.number)
numpy.arange(len(indices))
numpy.linspace(xmin, xmax, size)
numpy.arange(min_age, max_age + 1)
numpy.arange(len(sales_df))
numpy.mean(df[column_name])
numpy.min(arr)
numpy.array(list(data.values()))
numpy.random.uniform(lon_min, lon_max)
numpy.random.choice(CATEGORIES, N, replace=False)
numpy.linspace(0, 2, 2 * sample_rate, False)
numpy.random.randint(start_year, end_year + 1)
open.write(priv_key_encrypted)
open.readlines()
open.read()
open.write(response.content)
open.write(encrypted_data)
open.write(b64encode(encrypted_aes_key))
open.read(4096)
open.write(data)
open.write(f'\nError executing command, exited with code {ret_code}')
openpyxl.load_workbook.save(xlsx_file)
openpyxl.load_workbook(filename=xlsx_file)
openpyxl.load_workbook(filename=xlsx_file).sheetnames
os.path.join(os.getcwd(), filename)
os.makedirs(os.path.join(directory, subdirectory))
os.listdir(path)
os.path.exists(archive_file)
os.path.isdir(directory_path)
os.urandom(16)
os.makedirs(extract_path, exist_ok=True)
os.makedirs(extract_path)
os.path.join(directory, subdirectory)
os.path.join(path, 'processed')
os.path.join(directory, subdirectory, new_filename)
os.remove(CSV_FILE_PATH)
os.path.splitext(file)
os.getenv('MAIL_PASSWORD', None)
os.path.exists(image_path)
os.stat(entry.path)
os.path.join(directory, file_name)
os.remove(csv_file_path)
os.path.dirname(FILE_NAME)
os.path.exists(file_location)
os.makedirs(target_dir)
os.makedirs(ARCHIVE_DIR)
os.path.exists(dest_dir)
os.path.exists(request)
os.makedirs(os.path.dirname(FILE_NAME), exist_ok=True)
os.listdir(src_dir)
os.getcwd()
os.path.join(dir_path, item)
os.stat(entry.path).st_ctime
os.path.join(target_directory, Path(file).stem + '.csv')
os.path.exists(dest_file_path)
os.path.join(file_dir, '*' + file_ext)
os.path.exists(output_dir_path)
os.path.join(ARCHIVE_DIR, 'archive')
os.path.exists(output_dir)
os.path.isdir(dir_path)
os.path.isfile(script_path)
os.path.exists(dir_path)
os.makedirs(processed_path)
os.path.exists(source_directory)
os.listdir(directory)
os.path.join(directory, '*')
os.path.isfile(f)
os.path
os.path.join(root, file)
os.getenv('MAIL_PORT', 25)
os.path.join(directory, base_name)
os.path.basename(file)
os.path.exists(download_dir)
os.path.exists(extract_path)
os.path.join(target_dir, archive_name)
os.path.join(save_dir, filename)
os.path.exists(excel_file)
os.path.exists(source_dir)
os.path.join(directory, 'files.zip')
os.path.exists(target_directory)
os.makedirs(BACKUP_PATH)
os.getenv('MAIL_USERNAME', None)
os.makedirs(target_directory, exist_ok=True)
os.path.exists(BACKUP_PATH)
os.makedirs(download_dir)
os.makedirs(output_dir_path)
os.path.isfile(file)
os.walk(source_directory)
os.path.join(output_dir, 'weather_data.csv')
os.path.exists(target_dir)
os.path.exists(script_path)
os.path.basename(url)
os.makedirs(target_dir, exist_ok=True)
os.remove(TARGET_TAR_FILE)
os.path.exists(ARCHIVE_DIR)
os.makedirs(output_dir)
os.path.join(path, filename)
os.getenv('MAIL_SERVER', 'localhost')
os.path.exists(src_dir)
os.path.isfile(csv_file_path)
os.makedirs(source_dir, exist_ok=True)
os.path.join(destination_directory, filename)
os.path.exists(processed_path)
os.path.join(SOURCE_DIR, '*' + ext)
os.urandom(32)
os.path.join(output_dir, 'backup/')
os.remove(file)
os.listdir(destination_directory)
os.getenv('MAIL_USE_TLS', False)
os.path.exists(directory)
os.listdir(dir_path)
os.urandom(8)
os.path.basename(file_path)
os.path.basename(src_file)
os.path.exists(csv_file_path)
os.path.join(source_dir, file)
os.path.join(dest_dir, filename)
os.path.join(output_dir, 'traffic_data.csv')
os.scandir(directory_path)
os.path.join(directory, filename)
os.path.join(target_directory, f'{zip_name.strip()}.zip')
os.stat(entry.path).st_mtime
os.path.join(directory_path, '*.bat')
os.path.abspath(zip_path)
os.path.join(source_dir, filename)
os.path.splitext(file_name)
os.path.join(dest_dir, file_name)
os.listdir(file_dir)
os.path.join(download_path, os.path.basename(url))
os.listdir(source_dir)
os.path.join(target_dir, filename)
os.path.join(excel_file_path, file_name)
os.path.isfile(audio_file)
os.makedirs(download_path)
os.path.join(output_dir, 'sensor_data.csv')
os.path.exists(CSV_FILE_PATH)
os.path.join(src_dir, file_name)
os.path.isfile(FILE_NAME)
os.path.join(output_dir, file_name)
os.path.exists(download_path)
os.stat(entry.path).st_size
os.path.join(src_dir, '*.' + ext)
os.path.exists(os.path.join(directory, subdirectory))
os.makedirs(output_dir, exist_ok=True)
os.path.exists(commands_file_path)
os.path.abspath(filename)
output_files.append(output_file)
outputs.append(s)
outputs.remove(s)
p.text
pandas.read_csv(FILE_PATH)
pandas.read_excel(filepath, engine='openpyxl')
pandas.read_json.iterrows()
pandas.api.types.is_numeric_dtype(df[col])
pandas.DataFrame.corr()
pandas.DataFrame(duplicates.values(), duplicates.keys())
pandas.DataFrame(data, columns=headers)
pandas.DataFrame(scaler.fit_transform(df_cumsum), columns=df.columns)
pandas.DataFrame
pandas.DataFrame(columns=COLUMNS)
pandas.DataFrame(data=data, columns=columns)
pandas.DataFrame({'Item': items, 'Normalized Count': counts_normalized, 'Normalized Weight': weights_normalized})
pandas.DataFrame(transformed_data, columns=[f'PC{i + 1}' for i in range(n_components)])
pandas.DataFrame(report_data, columns=['City', 'Local Time', 'Weather Condition'])
pandas.concat([df['Date'], df['Value'].apply(pd.Series)], axis=1)
pandas.DataFrame(data, columns=COLUMNS).empty
pandas.read_csv(file_path, usecols=[0], names=['Text'], header=None)
pandas.concat(data_frames, ignore_index=True)
pandas.to_numeric(df[col], errors='coerce')
pandas.read_excel(file_location, sheet_name=sheet_name).columns
pandas.read_csv.plot(x='Time', y=VEHICLE_TYPES, kind='line', title='Traffic Data Over Time')
pandas.DataFrame(result)
pandas.to_datetime(df[column_name], format=date_format)
pandas.DataFrame.plot(kind='bar', legend=False, title='Duplicate Entries')
pandas.DataFrame(data, columns=['Date', 'Activity', 'Duration'])
pandas.crosstab(data[col1], data[col2])
pandas.read_html(StringIO(str(table)))
pandas.DataFrame(data, columns=['Month', 'Value'])
pandas.date_range(end=datetime.now(), periods=30)
pandas.date_range(start=df['date'].iloc[-1] + pd.Timedelta(days=1), periods=7)
pandas.DataFrame(mean_values, columns=['Mean Value'], index=['Position {}'.format(i) for i in range(len(mean_values))])
pandas.DataFrame(columns=['Time', 'Value'])
pandas.DataFrame(parsed_data, columns=['Type', 'Timestamp', 'Message'])
pandas.read_csv(csv_file_path)
pandas.to_datetime(df['Date'])
pandas.read_csv(file)
pandas.read_csv(data_url, sep='\\s+', skiprows=22, header=None).values
pandas.DataFrame({'Text': data})
pandas.DataFrame(country_data, columns=['Country', 'Population'])
pandas.Timestamp
pandas.date_range(start=start_date, periods=periods, freq=freq)
pandas.read_json(json_str)
pandas.DataFrame(grades, index=students_sample, columns=COURSES)
pandas.DataFrame(sales_data, columns=['Product', 'Date', 'Sales'])
pandas.read_csv(data_url, sep='\\s+', skiprows=22, header=None)
pandas.Series(walk)
pandas.DataFrame.to_csv(target_filepath, index=False)
pandas.DataFrame(report_data, index=STUDENTS)
pandas.read_json(json_str).empty
pandas.Timestamp.timestamp
pandas.DataFrame(match_results, columns=['Team', 'Goals', 'Penalty Cost'])
pandas.DataFrame(report_data, columns=['Date', 'Category', 'Sales'])
pandas.Series(model.feature_importances_, index=X.columns)
pandas.DataFrame({'x': np.random.rand(N), 'y': np.random.rand(N), 'category': all_categories})
pandas.DataFrame.plot.bar(x='Word', y='Count', rot=0, legend=False)
pandas.DataFrame(assignment_data, columns=['Task Name', 'Assigned To', 'Due Date'])
pandas.DataFrame(vectorized_data.toarray(), columns=vectorizer.get_feature_names_out())
pandas.DataFrame(my_dict).columns
pandas.DataFrame(top_words, columns=['Word', 'Count'])
pandas.DataFrame(report_data, index=STUDENTS).loc
pandas.DataFrame(prices, columns=stock_names, index=dates)
pandas.read_csv(filepath)
pandas.DataFrame(columns=['Time', 'Value']).loc
pandas.DataFrame(top_words, columns=['Word', 'Count']).plot
pandas.Series([start_date + timedelta(days=randint(0, num_days)) for _ in range(num_days)])
pandas.DataFrame.plot(kind='bar')
pandas.date_range(start=start_date, freq=freq, periods=periods)
pandas.DataFrame.mean()
pandas.Series
pandas.concat([df, temp_df])
pandas.DataFrame(data, columns=foods)
pandas.DataFrame(anchors, columns=['text', 'href'])
pandas.api.types
pandas.DataFrame.apply(mean, axis=1)
pandas.Series(model.feature_importances_, index=X.columns).sort_values(ascending=False).index
pandas.read_excel(excel_file)
pandas.DataFrame(data, columns=COLUMNS)
pandas.DataFrame.plot.line(x='Date', y=column)
pandas.DataFrame(data, columns=['ID', 'Name', 'Date of Birth', 'Email'])
pandas.to_numeric.isnull()
pandas.DataFrame()
pandas.DataFrame.to_csv(output_csv_path, index=False)
pandas.to_numeric(df['value'], errors='coerce')
pandas.DataFrame.apply(mean)
pandas.DataFrame(data, columns=headers if headers else None)
pandas.read_json(json_str).columns
pandas.api
pandas.DataFrame(data=iris.data, columns=iris.feature_names)
pandas.read_excel(excel_file).columns
pandas.DataFrame([my_dict])
pandas.DataFrame.pivot(index='Date', columns='Category', values='Sales')
pandas.DataFrame(my_dict)
pandas.DataFrame.to_csv(file_path, index=False)
pandas.DataFrame(data).empty
pandas.date_range(end=datetime.now().date(), periods=days_in_past)
pandas.DataFrame.plot(x='Time', y='Value')
pandas.DataFrame(data, columns=['Values'])
pandas.DataFrame.astype(bool)
pandas.read_csv(FILE_PATH).empty
pandas.read_excel(file_location, sheet_name=sheet_name)
pandas.to_datetime(df['date'])
pandas.read_csv(csv_file_path).columns
pandas.DataFrame({'Date': date_range, 'Sales': sales_data})
pandas.date_range(start_date, end_date, freq='D')
pandas.Timedelta(days=1)
pandas.DataFrame(data, columns=COLUMNS).plot
pandas.concat([df['Date'], df['Value'].apply(pd.Series)], axis=1).iloc
pandas.DataFrame(np.random.randn(len(a), len(b)), index=a, columns=selected_columns)
pandas.DataFrame.to_sql('my_table', conn, if_exists='replace', index=False)
pandas.DataFrame(data)
parsed_data.append([log_type, timestamp, message.strip()])
pathlib.Path(directory)
pathlib.Path(os.path.join(dir_path, item))
pathlib.Path(target_dir)
pathlib.Path(directory_path)
pathlib.Path('unzipped_files')
pathlib.Path('downloads')
pathlib.Path(file)
penalties.items()
points.shape
predicate_functions.items()
proc.name()
proc.terminate()
psutil.process_iter()
psutil.NoSuchProcess
psutil.Process(pid)
psutil.ZombieProcess
psutil.Process.is_running()
psutil.Process.memory_info()
psutil.Process.cpu_percent(interval=0.05)
pyquery.PyQuery(html)
pytesseract.image_to_string(image)
pytesseract.image_to_string.encode(from_encoding)
pytz.UTC
pytz.timezone(timezone)
pytz.timezone(timezones[city])
queue.Queue()
queue.Empty
random.randint(0, goals)
random.randint(20, 50)
random.randint(0, 120)
random.seed(seed)
random.randint(0, 100)
random.randint(0, 50)
random.uniform(50, 60)
random.randint(10, 50)
random.randint(1, 100)
random.randint(0, len(weather_conditions) - 1)
random.randint(0, penalties)
random.choice(NUMBERS)
random.randint(0, len(WEATHER_CONDITIONS) - 1)
random.randint(range_low, range_high)
random.uniform(20, 30)
random.randint(0, num_days)
random.choice(files)
random.seed(random_seed)
random.choice(employees)
random.randint(50, 100)
random.sample(STUDENTS, num_students)
random.choice(task_list)
random.randint(150, 200)
re.match(pattern, filename)
re.sub('\\s+', '.', name.lower())
re.compile(pattern)
re.findall('\\b\\w+\\b', text)
re.compile.sub(' ', text)
re.sub(f'[{punctuation}]', '', text)
re.match(file_pattern, filename)
re.search('(.*?)\\[.*?\\]', content)
re.sub.split()
re.compile('^-?\\d+(?:\\.\\d+)?$')
re.split('\\W+', text)
re.search('(https?://\\S+)', myString)
re.sub.lower().split()
re.compile('(like|what)', re.IGNORECASE)
re.IGNORECASE
re.search('\\W', x.stem)
re.findall(PHONE_REGEX, text)
re.sub('\\W+', ' ', text)
re.search('\\d', x.name)
re.match(log_pattern, line)
re.compile.sub(' ', text2)
re.sub(word, word.replace(' ', '_'), text, flags=re.IGNORECASE)
re.compile.match(filename)
re.sub('http[s]?://\\S+', '', text)
re.compile.sub(' ', text1)
re.compile.match(value)
re.compile('[\\W_]+')
re.sub.strip()
re.compile.search(x)
re.sub('\\d+', '', text)
re.match.group(1)
re.search.group(1)
re.match.groups()
re.search('_processed$', os.path.splitext(file)[0])
re.search.group()
regex.sub('(?<=(^|[^\\\\])(\\\\\\\\)*)"', '\\"', cell.value)
report_data.append([city, city_time.strftime('%Y-%m-%d %H:%M:%S %Z'), weather])
report_data.append([date, category, sales])
requests.exceptions
requests.get.iter_content()
requests.get(url, headers=HEADERS)
requests.get(webpage_url, timeout=5)
requests.get(webpage_url, timeout=5).content
requests.RequestException
requests.exceptions.HTTPError
requests.get(url, headers=HEADERS).text
requests.HTTPError
requests.get(url).content
requests.get(url, stream=True, timeout=5).status_code
requests.exceptions.RequestException
requests.get(url, headers=headers).content
requests.get(full_url).text
requests.get.raise_for_status()
requests.ConnectionError
requests.get.headers.get('Content-Type', '')
requests.get(url, timeout=5).text
requests.get(url, timeout=5).content
requests.get(url, headers=headers)
requests.get(url, timeout=5)
requests.HTTPError(f'HTTP error occurred: {e}')
requests.get(url).text
requests.get(url, stream=True, timeout=5)
requests.get(url, timeout=5).headers
requests.get(url)
requests.RequestException(f'Error accessing URL {webpage_url}: {e}')
requests.get(full_url)
response.encode('utf-8')
result.items()
results.append((os.path.basename(file_path), exit_code))
results.append((os.path.basename(file_path), None))
row.get('goals', 0)
row.find_all('td')
row.get('penalties', 0)
row.xpath('.//td')
rsa.newkeys[1].save_pkcs1()
rsa.encrypt(aes_key, pub_key)
rsa.newkeys(512)
s.recv.decode()
s.recv(buffer_size)
s.accept[0].setblocking(0)
s.close()
s.accept()
s.sendall(next_msg.encode('utf-8'))
sales_data.append([product, date, sales])
scipy.stats.zscore(counts)
scipy.stats.zscore(df['closing_price'])
scipy.stats.zscore(standardized_data)
scipy.stats.norm.fit(data)
scipy.stats.chi2_contingency(contingency_table)
scipy.stats.norm.pdf(x, computed_stats['mean'], computed_stats['std'])
scipy.stats.norm.fit(df['value'])
scipy.optimize.curve_fit(func, x, y, p0=[1, 1])
scipy.stats.norm
scipy.spatial.Voronoi(jittered_points)
scipy.spatial.voronoi_plot_2d(vor, ax=ax)
scipy.stats.norm.pdf(x, mu, std)
scipy.fftpack.fft(signal)
scipy.optimize.curve_fit(func, x_data, y_data, p0=initial_guess, maxfev=10000)
seaborn.heatmap(corr, annot=True)
seaborn.histplot(duplicates_df['age'], bins=bins)
seaborn.stripplot(x=df[col], ax=axes[1], jitter=True)
seaborn.pairplot.fig.suptitle('Iris Dataset Pair Plot', fontsize=16)
seaborn.barplot.set_xlabel('Feature Importance Score')
seaborn.barplot(x='Team', y='Goals', data=results_df, palette='viridis')
seaborn.heatmap(corr_df, annot=True, cmap='coolwarm')
seaborn.pairplot(iris_df, hue='species', vars=iris.feature_names)
seaborn.set_theme(style='white')
seaborn.barplot.set_title('Visualizing Important Features')
seaborn.boxplot.set_title('Box Plot of Closing Prices')
seaborn.pairplot(iris_df, hue='species', vars=iris.feature_names).fig
seaborn.boxplot(x=df[col], ax=axes[1])
seaborn.countplot(x=df[col], ax=axes[0])
seaborn.boxplot(x=df['closing_price'], ax=axes[0])
seaborn.lineplot(data=df, x='Date', y='Duration', hue='Activity')
seaborn.barplot.set_ylabel('Features')
seaborn.barplot(x=feature_imp, y=feature_imp.index)
seaborn.histplot.set_title('Histogram of Closing Prices')
seaborn.histplot(df['closing_price'], kde=True, ax=axes[1])
seaborn.barplot(x='Team', y='Penalty Cost', data=results_df, palette='viridis')
segmented_image.reshape(img.shape).astype('uint8').reshape(img.shape)
select.select(inputs, outputs, inputs, 1)
sha256_hash.hexdigest().encode('utf-8')
shapely.geometry.Point(np.random.uniform(lon_min, lon_max), np.random.uniform(lat_min, lat_max))
shutil.move(src_file, dest_file)
shutil.copyfile(file, target_file)
shutil.copy(FILE_PATH, BACKUP_PATH)
shutil.move(os.path.join(directory, filename), os.path.join(directory, subdirectory, new_filename))
shutil.move(src_file, DEST_DIR)
shutil.move(file_path, processed_path)
shutil.move(file, dest_dir)
shutil.move(os.path.join(source_dir, file), target_dir)
shutil.move(os.path.join(source_dir, filename), os.path.join(target_dir, filename))
sklearn.feature_extraction.text.CountVectorizer.get_feature_names_out()
sklearn.model_selection.train_test_split(X, y, test_size=test_size, random_state=random_state)
sklearn.feature_extraction.text.CountVectorizer(stop_words=STOP_WORDS)
sklearn.feature_extraction.text.TfidfVectorizer.fit_transform(tokenized_texts)
sklearn.feature_extraction.text.TfidfVectorizer(max_df=1.0, min_df=1, stop_words='english')
sklearn.feature_extraction.text.CountVectorizer()
sklearn.linear_model.LinearRegression()
sklearn.cluster.KMeans(n_clusters=n_clusters).cluster_centers_
sklearn.metrics.roc_curve(Y_test, Y_pred)
sklearn.preprocessing.StandardScaler()
sklearn.ensemble.RandomForestClassifier(random_state=42)
sklearn.cluster.KMeans(n_clusters=n_clusters)
sklearn.preprocessing.MinMaxScaler.fit_transform(df_cumsum)
sklearn.preprocessing.StandardScaler.fit_transform(column_data.reshape(-1, 1))
sklearn.preprocessing.MinMaxScaler.fit_transform(np.array(weights).reshape(-1, 1))
sklearn.linear_model.LinearRegression.fit(X, y)
sklearn.metrics.auc(fpr, tpr)
sklearn.cluster.KMeans(n_clusters=n_clusters, random_state=random_state, n_init=n_init)
sklearn.cluster.KMeans.labels_.reshape(img.shape[:2])
sklearn.feature_extraction.text.CountVectorizer.fit_transform(df['Text'].dropna())
sklearn.cluster.KMeans(n_clusters=n_clusters, random_state=random_seed).labels_
sklearn.decomposition.NMF(n_components=num_topics, random_state=1)
sklearn.datasets.load_iris().feature_names
sklearn.model_selection.train_test_split(X, Y, test_size=0.3)
sklearn.decomposition.PCA.fit_transform(data)
sklearn.preprocessing.MinMaxScaler()
sklearn.datasets.load_iris().data
sklearn.feature_extraction.text.TfidfVectorizer.get_feature_names()
sklearn.feature_extraction.text.CountVectorizer(stop_words=STOP_WORDS).vocabulary_
sklearn.cluster.KMeans.fit_predict(flattened_result)
sklearn.feature_extraction.text.CountVectorizer.vocabulary_.items()
sklearn.decomposition.PCA(n_components=n_components)
sklearn.cluster.KMeans.fit_predict(data)
sklearn.cluster.KMeans(n_clusters=n_clusters, random_state=random_seed).cluster_centers_
sklearn.preprocessing.normalize([arr])
sklearn.model_selection.train_test_split(X, Y, test_size=0.25)
sklearn.feature_extraction.text.TfidfVectorizer.get_feature_names_out()
sklearn.datasets.load_iris()
sklearn.feature_extraction.text.CountVectorizer.fit_transform(dataframe[text_column])
sklearn.linear_model.LinearRegression.predict(future_dates)
sklearn.cluster.KMeans.fit(pixels)
sklearn.cluster.KMeans(n_clusters=n_clusters, random_state=random_seed)
sklearn.datasets.load_iris().target
smtplib.SMTP(SMTP_SERVER, SMTP_PORT)
smtplib.SMTP.starttls()
smtplib.SMTP.login(smtp_username, smtp_password)
smtplib.SMTP.login(email['From'], getpass.getpass('Password: '))
smtplib.SMTP(smtp_server, smtp_port)
smtplib.SMTP.send_message(email)
smtplib.SMTP.sendmail(smtp_username, [email_data['to']], msg.as_string())
smtplib.SMTPAuthenticationError
socket.socket.setblocking(0)
socket.SOCK_STREAM
socket.socket(socket.AF_INET, socket.SOCK_STREAM)
socket.socket.listen(5)
socket.socket.close()
socket.AF_INET
socket.error
socket.socket.connect((str(ip), port))
socket.socket.settimeout(1)
socket.socket.bind((server_address, server_port))
soundfile.read(audio_file)
soup.find('table', {'id': table_id}).find_all('tr')
soup.find('table').find_all('tr')
soup.find('table', attrs={'class': 'data-table'}).find_all('tr')
soup.find('table').find_all('th')
sqlite3.DatabaseError
sqlite3.connect(database_name)
sqlite3.DatabaseError(f'Database error with {database_name}: {e}')
sqlite3.connect.close()
ssl.SSLContext.wrap_socket(client_socket, server_side=True)
ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER)
ssl.SSLContext.load_cert_chain(certfile=cert_file, keyfile=key_file)
ssl.PROTOCOL_TLS_SERVER
statistics.mean([row[1] for row in data])
statistics.mean([row[3] for row in data])
statistics.mean([row[2] for row in data])
statistics.mean
statsmodels.tsa.arima.model.ARIMA.fit()
statsmodels.tsa.arima.model.ARIMA(df['closing_price'], order=(5, 1, 0))
statsmodels.tsa.seasonal.seasonal_decompose(df['value'], model=decomposition_model)
string.ascii_lowercase
subprocess.Popen.communicate()
subprocess.PIPE
subprocess.Popen([sys.executable, script_path, *args], stderr=subprocess.PIPE, stdout=subprocess.PIPE).returncode
subprocess.run(['tar', '-czf', archive_file] + file_list)
subprocess.CalledProcessError(process.returncode, process.args)
subprocess.Popen([sys.executable, script_path, *args], stderr=subprocess.PIPE, stdout=subprocess.PIPE)
subprocess.Popen(['bash', script_path])
subprocess.Popen(file_path, shell=True)
subprocess.Popen.poll()
subprocess.Popen(process_name)
subprocess.Popen.wait()
subprocess.STDOUT
subprocess.call(command, shell=True, stdout=f, stderr=subprocess.STDOUT)
subprocess.Popen(file_path)
subprocess.Popen.terminate()
subprocess.Popen(['bash', script_path]).pid
subprocess.Popen([sys.executable, script_path, *args], stderr=subprocess.PIPE, stdout=subprocess.PIPE).args
subprocess.call(command, shell=True)
sys.executable
sys.stderr
tarfile.open.extractall()
tarfile.open(TARGET_TAR_FILE, 'r:gz')
tensorflow.keras.layers
tensorflow.keras.optimizers
tensorflow.keras.layers.Dense(input_dim=2, units=1, activation='sigmoid')
tensorflow.keras.Sequential.fit(X_train, Y_train, epochs=200, batch_size=1, verbose=0)
tensorflow.keras.Sequential.predict(X_test, verbose=0)
tensorflow.keras.optimizers.SGD(learning_rate=0.1)
tensorflow.keras.Sequential([keras.layers.Dense(input_dim=2, units=1, activation='sigmoid')])
tensorflow.keras.Sequential.compile(loss='binary_crossentropy', optimizer=keras.optimizers.SGD(learning_rate=0.1))
text.lower().lower()
text.split()
th.text
th.text.strip()
thread.start()
thread.join()
threading.Thread.start()
threading.Thread.join()
threading.Thread(target=check_port, args=(ip,))
threading.Thread(target=execute_file, args=(file,))
threads.append(thread)
time.time()
time.sleep(0.05)
time.sleep(1)
time.sleep(5)
title.string
topic.argsort()
topics.append(topic_keywords)
tr.find_all('td')
tr.text
transferred_files.append(os.path.basename(src_file))
typing.List
typing.Tuple
unicodedata.normalize('NFKD', word)
url.startswith('file://')
urllib.error.URLError(f'Error fetching URL {url}: {e}')
urllib.error
urllib.parse.urljoin(base_url, a['href'])
urllib.parse.urlparse(url)
urllib.request.urlopen(url)
urllib.parse.quote(decoded_str)
urllib.request.urlopen.read()
urllib.parse
urllib.request.urlretrieve(url, TARGET_TAR_FILE)
urllib.request.urlretrieve(url, csv_file_path)
urllib.parse.urljoin(base_url, url)
urllib.request
urllib.error.URLError
utc_datetime.astimezone(city_tz)
utc_datetime.astimezone.strftime('%Y-%m-%d %H:%M:%S %Z')
vectorizer.fit_transform(df['Text'].dropna()).sum(axis=0)
vectorizer.fit_transform(dataframe[text_column]).toarray()
warnings.simplefilter('always')
warnings.warn(f'Unable to move file {src_file}: {str(e)}')
webpage_url.startswith('file://')
werkzeug.security.check_password_hash(self.password_hash, password)
werkzeug.security.generate_password_hash(password)
word.replace(' ', '_')
wordcloud.WordCloud()
words.append(normalized_word)
wtforms.validators.Length(min=4, max=25)
wtforms.SubmitField('Log In')
wtforms.validators.DataRequired()
wtforms.validators.Length(min=8, max=80)
wtforms.StringField('Username', validators=[DataRequired(), Length(min=4, max=25)])
wtforms.PasswordField('Password', validators=[DataRequired(), Length(min=8, max=80)])
x.name
x.is_file()
x.stem
x.is_dir()
xlwt.Workbook()
xlwt.Workbook.save(filename)
xlwt.Workbook.add_sheet(sheet_name)
zip_dir.iterdir()
zip_dir.mkdir(parents=True, exist_ok=True)
zip_name.strip()
zipfile.ZipFile.write(file, arcname=Path(file).name)
zipfile.ZipFile(archive_path, 'w')
zipfile.ZipFile(zip_file_path, 'w')
zipfile.ZipFile.extractall(zip_dir)
zipfile.ZipFile(zip_path, 'w')
zipfile.ZipFile.write(file, os.path.basename(file))
zipfile.ZipFile(file_name, 'r')
zipfile.ZipFile(filepath, 'r')
zipfile.ZipFile.extractall(destination_directory)
zipfile.ZipFile(file_path, 'r')
zipfile.BadZipFile
zipfile.ZipFile.extractall(extract_path)
zipfile.ZipFile.write(os.path.join(source_dir, file), arcname=file)
zipfile.ZipFile(zip_path, 'r')
