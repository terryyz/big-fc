Crypto.Cipher.AES.MODE_EAX
Crypto.Cipher.AES.new(password, AES.MODE_EAX)
Crypto.Random.get_random_bytes(16)
Levenshtein.ratio(text1, text2)
PIL.Image.open(filename)
base64.b64encode(encrypted_aes_key)
base64.b64encode(decoded_str.encode())
base64.b64encode(priv_key_encrypted)
binascii.hexlify(decoded_str.encode())
bs4.BeautifulSoup(html, 'html.parser')
bs4.BeautifulSoup(response.text, 'html.parser')
cgi.parse_header(self.headers.get('content-type'))
chardet.detect(content)
codecs.encode(decoded_str, 'rot_13')
codecs.decode(comment, from_encoding)
codecs.encode(name, 'utf-8')
collections.Counter(text2.split())
collections.Counter()
collections.Counter(words)
collections.Counter(word_combinations)
collections.Counter(duplicates_df['age'])
collections.Counter(text.split())
collections.Counter((tuple(row) for row in rows if rows.count(row) > 1))
collections.Counter({'goals': 0, 'penalties': 0})
collections.defaultdict(int)
collections.Counter(duplicates['value'])
collections.Counter(values)
collections.Counter(text1.split())
collections.defaultdict(list)
cryptography.hazmat.primitives.ciphers.algorithms.AES(aes_key)
cryptography.hazmat.primitives.ciphers.modes.CBC(iv)
cryptography.hazmat.primitives.ciphers.Cipher(algorithms.AES(aes_key), modes.CBC(iv), backend=default_backend())
cryptography.hazmat.backends.default_backend()
cryptography.hazmat.primitives.padding.PKCS7(128)
csv.writer(file)
csv.DictReader(f)
csv.reader(file)
csv.reader(file, delimiter=delimiter, quotechar=quotechar)
csv.DictWriter(f, fieldnames=['key', 'mean', 'median'])
csv.reader(infile)
csv.writer(outfile)
csv.writer(f)
csv.DictReader(file)
csv.writer(csvfile)
csv.reader(f)
cv2.imwrite(f'cluster_{i + 1}.jpg', cluster_img)
cv2.COLOR_BGR2RGB
cv2.imread(image_path)
cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
datetime.datetime(2020, 1, 1)
datetime.timedelta(days=randint(0, num_days))
datetime.datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S')
datetime.datetime.fromtimestamp(file_info.st_ctime, timezone.utc)
datetime.datetime
datetime.datetime.strptime(ts, '%d/%m/%y %H:%M:%S.%f')
datetime.datetime.today()
datetime.timedelta(seconds=run_duration)
datetime.timedelta(days=i)
datetime.datetime(2020, 12, 31)
datetime.datetime.fromtimestamp(file_info.st_mtime, timezone.utc)
datetime.datetime.now()
datetime.datetime.strptime(d[0], '%Y-%m')
datetime.datetime.fromtimestamp(ts / 1000)
datetime.datetime.fromtimestamp(epoch_milliseconds / 1000.0)
datetime.timezone.utc
datetime.datetime.datetime(birth_year, np.random.randint(1, 13), np.random.randint(1, 29))
difflib.ndiff(csv_content1, csv_content2)
docx.Document(filepath)
email.mime.text.MIMEText(email_data['message'])
email.message.EmailMessage()
flask.Flask(__name__, template_folder=template_folder)
flask.Flask(app_name)
flask.redirect(url_for('login'))
flask.url_for('login')
flask.redirect(url_for('protected'))
flask.render_template('login.html', form=form)
flask.url_for('protected')
flask_login.logout_user()
flask_login.UserMixin
flask_login.LoginManager()
flask_login.login_user(user)
flask_login.current_user.id
flask_login.login_required
flask_mail.Mail(app)
flask_wtf.FlaskForm
ftplib.FTP(ftp_server)
functools.reduce(lambda a, b: a + b, [math.factorial(n) for n in permutation])
gensim.models.Word2Vec(sentences=tokenized_texts, vector_size=100, window=5, min_count=1, workers=4)
gensim.models.Word2Vec(vector_size=100)
geopandas.GeoDataFrame(data, geometry='Coordinates')
getpass.getpass('Recipient: ')
getpass.getpass('Password: ')
getpass.getpass('Email: ')
glob.glob(os.path.join(directory, '*'))
glob.glob(os.path.join(file_dir, '*' + file_ext))
glob.glob(os.path.join(SOURCE_DIR, '*' + ext))
glob.glob(os.path.join(directory_path, '*.bat'))
glob.glob(f'{source_directory}/**/*{extension}', recursive=True)
glob.glob(pattern)
glob.glob(os.path.join(src_dir, '*.' + ext))
glob.glob(directory_path + '/*.xlsx')
hashlib.md5()
hashlib.sha256()
io.StringIO(str(table))
ipaddress.IPv4Network(ip_range)
itertools.product(animals, foods)
itertools.zip_longest(*data_list, fillvalue=np.nan)
itertools.permutations(ALPHABETS, 2)
itertools.permutations(numbers)
json.loads(content)
json.dumps(SUCCESS_RESPONSE)
json.loads(json_str)
json.JSONDecodeError
json.load(f)
json.dump(phone_numbers, f)
json.dump(dict(word_counts), file)
json.loads(self.rfile.read(length))
json.load(file)
json.loads(json_data)
keras.models.Sequential([Dense(input_dim=2, units=1, activation='sigmoid')])
keras.layers.Dense(input_dim=2, units=1, activation='sigmoid')
keras.optimizers.SGD(learning_rate=0.1)
librosa.display.specshow(D, sr=samplerate, x_axis='time', y_axis='log')
librosa.stft(matrix)
librosa.amplitude_to_db(np.abs(librosa.stft(matrix)), ref=np.max)
lxml.etree.XML(xml_data)
lxml.etree.XMLSyntaxError
lxml.html.fromstring(content)
math.pi
math.floor(population)
math.factorial(n)
matplotlib.pyplot.subplots(nrows=2, ncols=1)
matplotlib.pyplot.rc('font', family='Arial')
matplotlib.pyplot.tight_layout()
matplotlib.axes.Axes
matplotlib.pyplot.subplots(figsize=(10, 6))
matplotlib.pyplot.subplot(1, 2, 1)
matplotlib.pyplot.title(f'Line Chart of {column}')
matplotlib.pyplot.show()
matplotlib.pyplot.axis('off')
matplotlib.pyplot.title('Spectrogram')
matplotlib.pyplot.title('Random Walk')
matplotlib.pyplot.title('Correlation Heatmap')
matplotlib.pyplot.title('Time Series Decomposition')
matplotlib.pyplot.figure(figsize=(10, 8))
matplotlib.pyplot.xlim()
matplotlib.pyplot.plot(x_data, y_data, 'bo', label='Data')
matplotlib.pyplot.Axes
matplotlib.pyplot.subplots(figsize=(12, 8))
matplotlib.pyplot.close('all')
matplotlib.pyplot.scatter(data_copy[:, 0], data_copy[:, 1])
matplotlib.pyplot.subplots(figsize=(12, 6))
matplotlib.pyplot.subplots(1, 2, figsize=(12, 6))
matplotlib.pyplot.xlabel('Time')
matplotlib.pyplot.figure(figsize=(10, 6))
matplotlib.pyplot.xlabel('Age')
matplotlib.pyplot.gcf()
matplotlib.pyplot.title('Data without Outliers')
matplotlib.pyplot.rc('font', None=font)
matplotlib.pyplot.savefig(save_path)
matplotlib.pyplot.xticks(rotation='vertical')
matplotlib.pyplot.imshow(wordcloud)
matplotlib.pyplot.gca()
matplotlib.pyplot.subplots(figsize=(10, 5))
matplotlib.pyplot.close()
matplotlib.pyplot.subplot(1, 2, 2)
matplotlib.pyplot.colorbar(format='%+2.0f dB')
matplotlib.pyplot.scatter(data_without_outliers[:, 0], data_without_outliers[:, 1])
matplotlib.pyplot.ylabel('Vehicle Count')
matplotlib.pyplot.ylabel('Value')
matplotlib.pyplot.close(fig)
matplotlib.pyplot.title('Distribution of Ages for Duplicate Names')
matplotlib.pyplot.plot(walk)
matplotlib.pyplot.legend()
matplotlib.pyplot.ylabel('Count')
matplotlib.pyplot.subplots()
matplotlib.pyplot.title('Data with Outliers')
matplotlib.pyplot.figure(figsize=(10, 5))
matplotlib.pyplot.figure()
matplotlib.pyplot.plot(x_fit, func(x_fit, *popt), 'r-', label='Fit')
nltk.word_tokenize(content)
nltk.corpus.stopwords.words('english')
nltk.download('stopwords')
numpy.sqrt(sum2)
numpy.arange(len(indices))
numpy.random.shuffle(all_categories)
numpy.copy(data)
numpy.number
numpy.abs(librosa.stft(matrix))
numpy.nanmean(v)
numpy.nanmedian(v)
numpy.random.randint(start_year, end_year + 1)
numpy.mean(df[column])
numpy.uint8
numpy.array(img)
numpy.issubdtype(data[col1].dtype, np.number)
numpy.issubdtype(data[col2].dtype, np.number)
numpy.random.choice([True, False])
numpy.random.randint(low=100, high=500)
numpy.array(weights)
numpy.where(array[:, 0] == target_value)
numpy.linspace(xmin, xmax, size)
numpy.nan
numpy.concatenate([guaranteed_categories, remaining_categories])
numpy.array([b for (a, b) in original])
numpy.median(v)
numpy.log10(np.sqrt(np.mean(data ** 2)))
numpy.stack([mask] * 3, axis=-1)
numpy.mean(v)
numpy.arange(len(sales_df))
numpy.random.seed(42)
numpy.linspace(0, 2, 2 * sample_rate, False)
numpy.histogram_bin_edges(data, bins='auto')
numpy.array(list(data.values()))
numpy.nanmean(numeric_values)
numpy.delete(data_copy, outliers, axis=0)
numpy.random.choice(latin_names)
numpy.sin(frequency * x)
numpy.array(L)
numpy.mean(df[column_name])
numpy.max(matrix)
numpy.random.choice(other_names)
numpy.cumsum(steps)
numpy.sqrt(sum1)
numpy.random.uniform(lat_min, lat_max)
numpy.linspace(0, 4 * np.pi, array_length)
numpy.random.seed(random_seed)
numpy.mean(differences)
numpy.max(arr)
numpy.max(column_data)
numpy.abs(fft)
numpy.random.randint(1, 13)
numpy.sin(b * x)
numpy.array([df['date'].max() + i * 24 * 60 * 60 for i in range(1, 8)])
numpy.arange(start_time, end_time, step)
numpy.random.rand(days_in_past, len(stock_names))
numpy.random.seed(seed)
numpy.random.seed(rng_seed)
numpy.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])
numpy.random.uniform(lon_min, lon_max)
numpy.random.rand(array_length)
numpy.std(df[column_name])
numpy.where(np.stack([mask] * 3, axis=-1), segmented_image, np.array([255, 255, 255], dtype=np.uint8))
numpy.std(df[column])
numpy.min(column_data)
numpy.array(pairs)
numpy.ndarray
numpy.random.randn(size)
numpy.zeros(image_size, dtype=np.uint8)
numpy.mean(column_data)
numpy.floor
numpy.random.randint(low=100, high=500, size=periods)
numpy.random.rand(N)
numpy.random.choice([-1, 1], size=elements)
numpy.arange(len(sales_df), 2 * len(sales_df))
numpy.exp(-b * x)
numpy.random.normal(size=len(timestamps))
numpy.cos(frequency * x)
numpy.random.seed(0)
numpy.arange(df['Values'].min(), df['Values'].max() + 2)
numpy.array(segmented_image)
numpy.sum(column_data)
numpy.arange(min_age, max_age + 1)
numpy.linspace(0, 2 * math.pi, sample_size)
numpy.linspace(min(x_data), max(x_data), 500)
numpy.arange(1, 7 + 1.5)
numpy.arange(len(word_counts))
numpy.sin(x)
numpy.mean(data ** 2)
numpy.outer(time, signal)
numpy.array([255, 255, 255], dtype=np.uint8)
numpy.mean(arr)
numpy.std(arr)
numpy.arange(max(word_lengths) + 2)
numpy.issubdtype(dtype, np.number)
numpy.random.normal(0, 1e-10, points.shape)
numpy.sin(np.outer(time, signal) * np.pi)
numpy.array(CATEGORIES)
numpy.random.randint(1, 29)
numpy.abs(df['Z_score'])
numpy.where(z_scores > outlier_z_score)
numpy.random.choice(CATEGORIES, N, replace=False)
numpy.min(arr)
numpy.pi
numpy.random.randint(40, 101, size=(num_students, len(COURSES)))
numpy.random.randn(len(a), len(b))
numpy.sqrt(np.mean(data ** 2))
numpy.linspace(xmin, xmax, 100)
numpy.abs(stats.zscore(standardized_data))
numpy.random.choice(CATEGORIES, N - len(CATEGORIES))
numpy.bincount(outcomes, minlength=7)
numpy.tensordot(P, T, axes=[1, 1])
numpy.random.randint(10, size=(rows, len(COLUMNS)))
numpy.median(df[column_name])
openpyxl.load_workbook(filename=xlsx_file)
os.path.exists(dir_path)
os.getenv('MAIL_SERVER', 'localhost')
os.makedirs(target_dir)
os.urandom(32)
os.path.isfile(f)
os.path.join(src_dir, '*.' + ext)
os.path.isfile(FILE_NAME)
os.makedirs(download_path)
os.path.exists(download_path)
os.path.join(src_dir, file_name)
os.stat(entry.path)
os.path.join(dest_dir, filename)
os.path.exists(file_location)
os.path.exists(commands_file_path)
os.path.isfile(csv_file_path)
os.path.join(download_path, os.path.basename(url))
os.path.abspath(zip_path)
os.path.join(target_directory, Path(file).stem + '.csv')
os.getenv('MAIL_PASSWORD', None)
os.remove(CSV_FILE_PATH)
os.path.exists(excel_file)
os.path.dirname(FILE_NAME)
os.path.join(directory, subdirectory)
os.path.exists(image_path)
os.path.splitext(file_name)
os.path.splitext(file)
os.path.exists(target_directory)
os.makedirs(extract_path)
os.getcwd()
os.path.join(directory, 'files.zip')
os.path.exists(request)
os.listdir(file_dir)
os.path.join(path, filename)
os.remove(file)
os.makedirs(target_directory, exist_ok=True)
os.path.join(dir_path, item)
os.path.exists(os.path.join(directory, subdirectory))
os.scandir(directory_path)
os.path.join(excel_file_path, file_name)
os.makedirs(BACKUP_PATH)
os.listdir(path)
os.path.abspath(filename)
os.path.join(root, file)
os.makedirs(processed_path)
os.path.join(output_dir, 'weather_data.csv')
os.makedirs(extract_path, exist_ok=True)
os.makedirs(source_dir, exist_ok=True)
os.remove(TARGET_TAR_FILE)
os.path.join(directory_path, '*.bat')
os.makedirs(output_dir, exist_ok=True)
os.path.join(os.getcwd(), filename)
os.path.exists(dest_file_path)
os.path.isfile(audio_file)
os.path.join(target_directory, f'{zip_name.strip()}.zip')
os.path.join(SOURCE_DIR, '*' + ext)
os.makedirs(os.path.dirname(FILE_NAME), exist_ok=True)
os.path.join(ARCHIVE_DIR, 'archive')
os.path.exists(archive_file)
os.path.join(directory, filename)
os.path.exists(source_directory)
os.path.join(directory, '*')
os.path.exists(directory)
os.path.exists(download_dir)
os.path.join(source_dir, filename)
os.path.basename(url)
os.path.join(target_dir, filename)
os.path.exists(ARCHIVE_DIR)
os.path.basename(file)
os.path.exists(processed_path)
os.path.join(output_dir, file_name)
os.listdir(src_dir)
os.path.exists(src_dir)
os.path.join(output_dir, 'traffic_data.csv')
os.getenv('MAIL_USERNAME', None)
os.makedirs(os.path.join(directory, subdirectory))
os.path.isfile(file)
os.makedirs(output_dir_path)
os.path.exists(output_dir)
os.path.exists(script_path)
os.urandom(16)
os.path.join(file_dir, '*' + file_ext)
os.path.isdir(dir_path)
os.path.basename(file_path)
os.listdir(dir_path)
os.path.join(directory, file_name)
os.path.isfile(script_path)
os.remove(csv_file_path)
os.path.exists(source_dir)
os.path.join(dest_dir, file_name)
os.path.exists(extract_path)
os.makedirs(target_dir, exist_ok=True)
os.path.exists(csv_file_path)
os.path.join(save_dir, filename)
os.path.isdir(directory_path)
os.walk(source_directory)
os.path.join(directory, subdirectory, new_filename)
os.path.join(output_dir, 'backup/')
os.path.exists(BACKUP_PATH)
os.path.join(path, 'processed')
os.path.join(directory, base_name)
os.path.join(source_dir, file)
os.listdir(source_dir)
os.path.join(output_dir, 'sensor_data.csv')
os.getenv('MAIL_USE_TLS', False)
os.path.exists(CSV_FILE_PATH)
os.path.join(target_dir, archive_name)
os.path.exists(dest_dir)
os.listdir(directory)
os.makedirs(output_dir)
os.makedirs(ARCHIVE_DIR)
os.makedirs(download_dir)
os.urandom(8)
os.path.basename(src_file)
os.path.exists(output_dir_path)
os.path.exists(target_dir)
os.listdir(destination_directory)
os.getenv('MAIL_PORT', 25)
os.path.join(destination_directory, filename)
pandas.date_range(end=datetime.now().date(), periods=days_in_past)
pandas.DataFrame(sales_data, columns=['Product', 'Date', 'Sales'])
pandas.DataFrame(prices, columns=stock_names, index=dates)
pandas.DataFrame(report_data, columns=['Date', 'Category', 'Sales'])
pandas.read_html(StringIO(str(table)))
pandas.DataFrame(data, columns=headers)
pandas.Series
pandas.DataFrame(data)
pandas.Series([start_date + timedelta(days=randint(0, num_days)) for _ in range(num_days)])
pandas.DataFrame(grades, index=students_sample, columns=COURSES)
pandas.DataFrame(top_words, columns=['Word', 'Count'])
pandas.concat([df['Date'], df['Value'].apply(pd.Series)], axis=1)
pandas.to_datetime(df['date'])
pandas.DataFrame(parsed_data, columns=['Type', 'Timestamp', 'Message'])
pandas.DataFrame(data, columns=['Date', 'Activity', 'Duration'])
pandas.date_range(end=datetime.now(), periods=30)
pandas.read_json(json_str)
pandas.read_csv(FILE_PATH)
pandas.to_datetime(df[column_name], format=date_format)
pandas.read_csv(file_path, usecols=[0], names=['Text'], header=None)
pandas.to_numeric(df[col], errors='coerce')
pandas.DataFrame(data=data, columns=columns)
pandas.date_range(start=start_date, freq=freq, periods=periods)
pandas.DataFrame({'x': np.random.rand(N), 'y': np.random.rand(N), 'category': all_categories})
pandas.concat(data_frames, ignore_index=True)
pandas.DataFrame(columns=COLUMNS)
pandas.crosstab(data[col1], data[col2])
pandas.DataFrame(result)
pandas.DataFrame([my_dict])
pandas.Timedelta(days=1)
pandas.DataFrame(data, columns=foods)
pandas.DataFrame(duplicates.values(), duplicates.keys())
pandas.read_csv(filepath)
pandas.api.types.is_numeric_dtype(df[col])
pandas.DataFrame
pandas.read_excel(file_location, sheet_name=sheet_name)
pandas.DataFrame()
pandas.concat([df, temp_df])
pandas.to_datetime(df['Date'])
pandas.DataFrame(data, columns=headers if headers else None)
pandas.DataFrame(data=iris.data, columns=iris.feature_names)
pandas.DataFrame(data, columns=['Values'])
pandas.read_csv(file)
pandas.Timestamp.timestamp
pandas.date_range(start_date, end_date, freq='D')
pandas.read_excel(excel_file)
pandas.DataFrame(data, columns=['ID', 'Name', 'Date of Birth', 'Email'])
pandas.DataFrame({'Item': items, 'Normalized Count': counts_normalized, 'Normalized Weight': weights_normalized})
pandas.DataFrame(scaler.fit_transform(df_cumsum), columns=df.columns)
pandas.DataFrame(report_data, index=STUDENTS)
pandas.DataFrame(data, columns=COLUMNS)
pandas.DataFrame(anchors, columns=['text', 'href'])
pandas.read_csv(data_url, sep='\\s+', skiprows=22, header=None)
pandas.date_range(start=start_date, periods=periods, freq=freq)
pandas.Series(walk)
pandas.DataFrame(country_data, columns=['Country', 'Population'])
pandas.DataFrame({'Text': data})
pandas.DataFrame(transformed_data, columns=[f'PC{i + 1}' for i in range(n_components)])
pandas.to_numeric(df['value'], errors='coerce')
pandas.date_range(start=df['date'].iloc[-1] + pd.Timedelta(days=1), periods=7)
pandas.DataFrame(vectorized_data.toarray(), columns=vectorizer.get_feature_names_out())
pandas.DataFrame(np.random.randn(len(a), len(b)), index=a, columns=selected_columns)
pandas.DataFrame({'Date': date_range, 'Sales': sales_data})
pandas.DataFrame(my_dict)
pandas.DataFrame(data, columns=['Month', 'Value'])
pandas.DataFrame(assignment_data, columns=['Task Name', 'Assigned To', 'Due Date'])
pandas.read_excel(filepath, engine='openpyxl')
pandas.read_csv(csv_file_path)
pandas.errors.EmptyDataError
pandas.DataFrame(match_results, columns=['Team', 'Goals', 'Penalty Cost'])
pandas.DataFrame(mean_values, columns=['Mean Value'], index=['Position {}'.format(i) for i in range(len(mean_values))])
pandas.DataFrame(columns=['Time', 'Value'])
pandas.DataFrame(report_data, columns=['City', 'Local Time', 'Weather Condition'])
pathlib.Path('unzipped_files')
pathlib.Path('downloads')
pathlib.Path(directory_path)
pathlib.Path(directory)
pathlib.Path(target_dir)
pathlib.Path(file)
pathlib.Path(os.path.join(dir_path, item))
psutil.process_iter()
psutil.Process(pid)
psutil.ZombieProcess
psutil.NoSuchProcess
pyquery.PyQuery(html)
pytesseract.image_to_string(image)
pytz.UTC
pytz.timezone(timezones[city])
pytz.timezone(timezone)
queue.Queue()
queue.Empty
random.choice(task_list)
random.randint(0, 50)
random.randint(0, num_days)
random.randint(50, 100)
random.randint(range_low, range_high)
random.randint(0, goals)
random.choice(employees)
random.seed(random_seed)
random.choice(NUMBERS)
random.uniform(50, 60)
random.choice(files)
random.seed(seed)
random.randint(0, penalties)
random.randint(1, 100)
random.randint(0, len(WEATHER_CONDITIONS) - 1)
random.randint(0, len(weather_conditions) - 1)
random.randint(10, 50)
random.uniform(20, 30)
random.randint(150, 200)
random.randint(20, 50)
random.randint(0, 100)
random.sample(STUDENTS, num_students)
random.randint(0, 120)
re.search('(.*?)\\[.*?\\]', content)
re.compile('[\\W_]+')
re.search('(https?://\\S+)', myString)
re.search('\\d', x.name)
re.sub(word, word.replace(' ', '_'), text, flags=re.IGNORECASE)
re.findall('\\b\\w+\\b', text)
re.compile(pattern)
re.sub('\\s+', '.', name.lower())
re.findall(PHONE_REGEX, text)
re.sub('\\d+', '', text)
re.match(file_pattern, filename)
re.compile('^-?\\d+(?:\\.\\d+)?$')
re.sub('http[s]?://\\S+', '', text)
re.sub('\\W+', ' ', text)
re.compile('(like|what)', re.IGNORECASE)
re.match(log_pattern, line)
re.split('\\W+', text)
re.search('_processed$', os.path.splitext(file)[0])
re.match(pattern, filename)
re.sub(f'[{punctuation}]', '', text)
re.search('\\W', x.stem)
re.IGNORECASE
regex.sub('(?<=(^|[^\\\\])(\\\\\\\\)*)"', '\\"', cell.value)
requests.RequestException(f'Error accessing URL {webpage_url}: {e}')
requests.get(url)
requests.exceptions.HTTPError
requests.get(webpage_url, timeout=5)
requests.get(url, headers=HEADERS)
requests.get(url, headers=headers)
requests.RequestException
requests.exceptions.RequestException
requests.ConnectionError
requests.get(url, timeout=5)
requests.get(url, stream=True, timeout=5)
requests.HTTPError(f'HTTP error occurred: {e}')
requests.get(full_url)
rsa.newkeys(512)
rsa.encrypt(aes_key, pub_key)
scipy.spatial.Voronoi(jittered_points)
scipy.optimize.curve_fit(func, x, y, p0=[1, 1])
scipy.stats.norm.pdf(x, mu, std)
scipy.stats.norm.pdf(x, computed_stats['mean'], computed_stats['std'])
scipy.stats.zscore(standardized_data)
scipy.stats.norm.fit(df['value'])
scipy.stats.chi2_contingency(contingency_table)
scipy.fftpack.fft(signal)
scipy.optimize.curve_fit(func, x_data, y_data, p0=initial_guess, maxfev=10000)
scipy.stats.norm.fit(data)
scipy.spatial.voronoi_plot_2d(vor, ax=ax)
scipy.stats.zscore(counts)
scipy.stats.zscore(df['closing_price'])
seaborn.pairplot(iris_df, hue='species', vars=iris.feature_names)
seaborn.barplot(x=feature_imp, y=feature_imp.index)
seaborn.lineplot(data=df, x='Date', y='Duration', hue='Activity')
seaborn.barplot(x='Team', y='Goals', data=results_df, palette='viridis')
seaborn.boxplot(x=df[col], ax=axes[1])
seaborn.stripplot(x=df[col], ax=axes[1], jitter=True)
seaborn.heatmap(corr_df, annot=True, cmap='coolwarm')
seaborn.heatmap(corr, annot=True)
seaborn.boxplot(x=df['closing_price'], ax=axes[0])
seaborn.histplot(duplicates_df['age'], bins=bins)
seaborn.barplot(x='Team', y='Penalty Cost', data=results_df, palette='viridis')
seaborn.histplot(df['closing_price'], kde=True, ax=axes[1])
seaborn.set_theme(style='white')
seaborn.countplot(x=df[col], ax=axes[0])
select.select(inputs, outputs, inputs, 1)
shapely.geometry.Point(np.random.uniform(lon_min, lon_max), np.random.uniform(lat_min, lat_max))
shutil.move(os.path.join(source_dir, filename), os.path.join(target_dir, filename))
shutil.copy(FILE_PATH, BACKUP_PATH)
shutil.copyfile(file, target_file)
shutil.move(os.path.join(source_dir, file), target_dir)
shutil.move(src_file, dest_file)
shutil.move(os.path.join(directory, filename), os.path.join(directory, subdirectory, new_filename))
shutil.move(src_file, DEST_DIR)
shutil.move(file, dest_dir)
shutil.move(file_path, processed_path)
sklearn.model_selection.train_test_split(X, Y, test_size=0.3)
sklearn.datasets.load_iris()
sklearn.metrics.roc_curve(Y_test, Y_pred)
sklearn.ensemble.RandomForestClassifier(random_state=42)
sklearn.feature_extraction.text.CountVectorizer(stop_words=STOP_WORDS)
sklearn.model_selection.train_test_split(X, Y, test_size=0.25)
sklearn.feature_extraction.text.CountVectorizer()
sklearn.decomposition.PCA(n_components=n_components)
sklearn.preprocessing.StandardScaler()
sklearn.model_selection.train_test_split(X, y, test_size=test_size, random_state=random_state)
sklearn.preprocessing.MinMaxScaler()
sklearn.decomposition.NMF(n_components=num_topics, random_state=1)
sklearn.metrics.auc(fpr, tpr)
sklearn.cluster.KMeans(n_clusters=n_clusters)
sklearn.feature_extraction.text.TfidfVectorizer(max_df=1.0, min_df=1, stop_words='english')
sklearn.cluster.KMeans(n_clusters=n_clusters, random_state=random_state, n_init=n_init)
sklearn.cluster.KMeans(n_clusters=n_clusters, random_state=random_seed)
sklearn.preprocessing.normalize([arr])
sklearn.linear_model.LinearRegression()
smtplib.SMTPAuthenticationError
smtplib.SMTP(smtp_server, smtp_port)
smtplib.SMTP(SMTP_SERVER, SMTP_PORT)
socket.SOCK_STREAM
socket.AF_INET
socket.socket(socket.AF_INET, socket.SOCK_STREAM)
socket.error
soundfile.read(audio_file)
sqlite3.DatabaseError(f'Database error with {database_name}: {e}')
sqlite3.connect(database_name)
ssl.PROTOCOL_TLS_SERVER
ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER)
statistics.mean([row[1] for row in data])
statistics.mean([row[2] for row in data])
statistics.mean([row[3] for row in data])
statistics.mean
statsmodels.tsa.seasonal.seasonal_decompose(df['value'], model=decomposition_model)
statsmodels.tsa.arima.model.ARIMA(df['closing_price'], order=(5, 1, 0))
string.punctuation
string.ascii_lowercase
subprocess.call(command, shell=True, stdout=f, stderr=subprocess.STDOUT)
subprocess.Popen(file_path, shell=True)
subprocess.Popen(['bash', script_path])
subprocess.Popen(file_path)
subprocess.STDOUT
subprocess.Popen([sys.executable, script_path, *args], stderr=subprocess.PIPE, stdout=subprocess.PIPE)
subprocess.run(['tar', '-czf', archive_file] + file_list)
subprocess.Popen(process_name)
subprocess.call(command, shell=True)
subprocess.CalledProcessError(process.returncode, process.args)
subprocess.PIPE
sys.stderr
sys.executable
tarfile.open(TARGET_TAR_FILE, 'r:gz')
tensorflow.keras.optimizers.SGD(learning_rate=0.1)
tensorflow.keras.Sequential([keras.layers.Dense(input_dim=2, units=1, activation='sigmoid')])
tensorflow.keras.layers.Dense(input_dim=2, units=1, activation='sigmoid')
threading.Thread(target=check_port, args=(ip,))
threading.Thread(target=execute_file, args=(file,))
time.sleep(1)
time.time()
time.sleep(0.05)
time.sleep(5)
typing.List
typing.Tuple
unicodedata.normalize('NFKD', word)
urllib.parse.urljoin(base_url, url)
urllib.parse.urljoin(base_url, a['href'])
urllib.parse.urlparse(url)
warnings.warn(f'Unable to move file {src_file}: {str(e)}')
warnings.simplefilter('always')
werkzeug.security.generate_password_hash(password)
werkzeug.security.check_password_hash(self.password_hash, password)
wordcloud.WordCloud()
wtforms.validators.Length(min=8, max=80)
wtforms.validators.Length(min=4, max=25)
wtforms.SubmitField('Log In')
wtforms.StringField('Username', validators=[DataRequired(), Length(min=4, max=25)])
wtforms.PasswordField('Password', validators=[DataRequired(), Length(min=8, max=80)])
wtforms.validators.DataRequired()
xlwt.Workbook()
zipfile.ZipFile(file_path, 'r')
zipfile.ZipFile(archive_path, 'w')
zipfile.ZipFile(filepath, 'r')
zipfile.ZipFile(zip_file_path, 'w')
zipfile.ZipFile(zip_path, 'r')
zipfile.BadZipFile
zipfile.ZipFile(zip_path, 'w')
zipfile.ZipFile(file_name, 'r')
